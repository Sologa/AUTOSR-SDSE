[
  {
    "arxiv_id": "1706.07793",
    "anchor": "dblp_title",
    "search_term": "Personalized acoustic modeling by weakly supervised multi-task deep learning using acoustic tokens discovered from unlabeled data.",
    "search_record": {
      "id": "http://arxiv.org/abs/1706.07793v1",
      "title": "Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data",
      "summary": "It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario \"weakly supervised\". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.",
      "published": "2017-06-23T12:54:43Z"
    },
    "metadata": {
      "arxiv_id": "1706.07793",
      "title": "Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data",
      "summary": "It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario \"weakly supervised\". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.",
      "authors": [
        "Cheng-Kuan Wei",
        "Cheng-Tao Chung",
        "Hung-Yi Lee",
        "Lin-Shan Lee"
      ],
      "published": "2017-06-23T12:54:43Z",
      "updated": "2017-06-23T12:54:43Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/1706.07793v1",
      "landing_url": "https://arxiv.org/abs/1706.07793v1",
      "doi": "https://doi.org/10.1109/ICASSP.2017.7953141"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Personalized acoustic modeling by weakly supervised multi-task deep learning using acoustic tokens discovered from unlabeled data."
      }
    ]
  },
  {
    "arxiv_id": "2206.07086",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07086v1",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "published": "2022-06-14T18:21:01Z"
    },
    "metadata": {
      "arxiv_id": "2206.07086",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "authors": [
        "Ian Briggs",
        "Pavel Panchekha"
      ],
      "published": "2022-06-14T18:21:01Z",
      "updated": "2022-06-14T18:21:01Z",
      "categories": [
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07086v1",
      "landing_url": "https://arxiv.org/abs/2206.07086v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07086"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2206.09680",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.09680v1",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "published": "2022-06-20T09:42:50Z"
    },
    "metadata": {
      "arxiv_id": "2206.09680",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "authors": [
        "Pakawat Nakwijit",
        "Matthew Purver"
      ],
      "published": "2022-06-20T09:42:50Z",
      "updated": "2022-06-20T09:42:50Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09680v1",
      "landing_url": "https://arxiv.org/abs/2206.09680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.09680"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2206.12117",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.12117v1",
      "title": "Self Supervised Learning for Few Shot Hyperspectral Image Classification",
      "summary": "Deep learning has proven to be a very effective approach for Hyperspectral Image (HSI) classification. However, deep neural networks require large annotated datasets to generalize well. This limits the applicability of deep learning for HSI classification, where manually labelling thousands of pixels for every scene is impractical. In this paper, we propose to leverage Self Supervised Learning (SSL) for HSI classification. We show that by pre-training an encoder on unlabeled pixels using Barlow-Twins, a state-of-the-art SSL algorithm, we can obtain accurate models with a handful of labels. Experimental results demonstrate that this approach significantly outperforms vanilla supervised learning.",
      "published": "2022-06-24T07:21:53Z"
    },
    "metadata": {
      "arxiv_id": "2206.12117",
      "title": "Self Supervised Learning for Few Shot Hyperspectral Image Classification",
      "summary": "Deep learning has proven to be a very effective approach for Hyperspectral Image (HSI) classification. However, deep neural networks require large annotated datasets to generalize well. This limits the applicability of deep learning for HSI classification, where manually labelling thousands of pixels for every scene is impractical. In this paper, we propose to leverage Self Supervised Learning (SSL) for HSI classification. We show that by pre-training an encoder on unlabeled pixels using Barlow-Twins, a state-of-the-art SSL algorithm, we can obtain accurate models with a handful of labels. Experimental results demonstrate that this approach significantly outperforms vanilla supervised learning.",
      "authors": [
        "Nassim Ait Ali Braham",
        "Lichao Mou",
        "Jocelyn Chanussot",
        "Julien Mairal",
        "Xiao Xiang Zhu"
      ],
      "published": "2022-06-24T07:21:53Z",
      "updated": "2022-06-24T07:21:53Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12117v1",
      "landing_url": "https://arxiv.org/abs/2206.12117v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2206.13680",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.13680v1",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "published": "2022-06-28T01:14:09Z"
    },
    "metadata": {
      "arxiv_id": "2206.13680",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "authors": [
        "Amber Afshan",
        "Abeer Alwan"
      ],
      "published": "2022-06-28T01:14:09Z",
      "updated": "2022-06-28T01:14:09Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13680v1",
      "landing_url": "https://arxiv.org/abs/2206.13680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.13680"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2206.14962",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.14962v1",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "published": "2022-06-30T01:16:40Z"
    },
    "metadata": {
      "arxiv_id": "2206.14962",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "authors": [
        "Xinmeng Xu",
        "Yang Wang",
        "Jie Jia",
        "Binbin Chen",
        "Jianjun Hao"
      ],
      "published": "2022-06-30T01:16:40Z",
      "updated": "2022-06-30T01:16:40Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.14962v1",
      "landing_url": "https://arxiv.org/abs/2206.14962v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.14962"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2206.15147",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.15147v2",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "published": "2022-06-30T09:29:18Z"
    },
    "metadata": {
      "arxiv_id": "2206.15147",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "authors": [
        "Asier Gutiérrez-Fandiño",
        "David Pérez-Fernández",
        "Jordi Armengol-Estapé",
        "David Griol",
        "Zoraida Callejas"
      ],
      "published": "2022-06-30T09:29:18Z",
      "updated": "2022-07-01T08:22:32Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.15147v2",
      "landing_url": "https://arxiv.org/abs/2206.15147v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.15147"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2207.05920",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.05920v1",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "published": "2022-07-13T01:56:31Z"
    },
    "metadata": {
      "arxiv_id": "2207.05920",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "authors": [
        "Weiqing Wang",
        "Qingjian Lin",
        "Ming Li"
      ],
      "published": "2022-07-13T01:56:31Z",
      "updated": "2022-07-13T01:56:31Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05920v1",
      "landing_url": "https://arxiv.org/abs/2207.05920v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2207.07036",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.07036v2",
      "title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality",
      "summary": "While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",
      "published": "2022-07-14T16:21:33Z"
    },
    "metadata": {
      "arxiv_id": "2207.07036",
      "title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality",
      "summary": "While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",
      "authors": [
        "Wei-Ning Hsu",
        "Bowen Shi"
      ],
      "published": "2022-07-14T16:21:33Z",
      "updated": "2022-11-28T03:12:14Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07036v2",
      "landing_url": "https://arxiv.org/abs/2207.07036v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.07036"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2207.08187",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.08187v1",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "published": "2022-07-17T14:15:45Z"
    },
    "metadata": {
      "arxiv_id": "2207.08187",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "authors": [
        "Sannara Ek",
        "Romain Rombourg",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-07-17T14:15:45Z",
      "updated": "2022-07-17T14:15:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08187v1",
      "landing_url": "https://arxiv.org/abs/2207.08187v1",
      "doi": "https://doi.org/10.1109/PerComWorkshops53856.2022.9767369"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2207.13286",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.13286v1",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "published": "2022-07-27T04:22:29Z"
    },
    "metadata": {
      "arxiv_id": "2207.13286",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "authors": [
        "Yu-Jie Chen",
        "Shin-I Cheng",
        "Wei-Chen Chiu",
        "Hung-Yu Tseng",
        "Hsin-Ying Lee"
      ],
      "published": "2022-07-27T04:22:29Z",
      "updated": "2022-07-27T04:22:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13286v1",
      "landing_url": "https://arxiv.org/abs/2207.13286v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13286"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2208.02960",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.02960v1",
      "title": "Memory-Guided Collaborative Attention for Nighttime Thermal Infrared Image Colorization",
      "summary": "Nighttime thermal infrared (NTIR) image colorization, also known as translation of NTIR images into daytime color images (NTIR2DC), is a promising research direction to facilitate nighttime scene perception for humans and intelligent systems under unfavorable conditions (e.g., complete darkness). However, previously developed methods have poor colorization performance for small sample classes. Moreover, reducing the high confidence noise in pseudo-labels and addressing the problem of image gradient disappearance during translation are still under-explored, and keeping edges from being distorted during translation is also challenging. To address the aforementioned issues, we propose a novel learning framework called Memory-guided cOllaboRative atteNtion Generative Adversarial Network (MornGAN), which is inspired by the analogical reasoning mechanisms of humans. Specifically, a memory-guided sample selection strategy and adaptive collaborative attention loss are devised to enhance the semantic preservation of small sample categories. In addition, we propose an online semantic distillation module to mine and refine the pseudo-labels of NTIR images. Further, conditional gradient repair loss is introduced for reducing edge distortion during translation. Extensive experiments on the NTIR2DC task show that the proposed MornGAN significantly outperforms other image-to-image translation methods in terms of semantic preservation and edge consistency, which helps improve the object detection accuracy remarkably.",
      "published": "2022-08-05T03:04:04Z"
    },
    "metadata": {
      "arxiv_id": "2208.02960",
      "title": "Memory-Guided Collaborative Attention for Nighttime Thermal Infrared Image Colorization",
      "summary": "Nighttime thermal infrared (NTIR) image colorization, also known as translation of NTIR images into daytime color images (NTIR2DC), is a promising research direction to facilitate nighttime scene perception for humans and intelligent systems under unfavorable conditions (e.g., complete darkness). However, previously developed methods have poor colorization performance for small sample classes. Moreover, reducing the high confidence noise in pseudo-labels and addressing the problem of image gradient disappearance during translation are still under-explored, and keeping edges from being distorted during translation is also challenging. To address the aforementioned issues, we propose a novel learning framework called Memory-guided cOllaboRative atteNtion Generative Adversarial Network (MornGAN), which is inspired by the analogical reasoning mechanisms of humans. Specifically, a memory-guided sample selection strategy and adaptive collaborative attention loss are devised to enhance the semantic preservation of small sample categories. In addition, we propose an online semantic distillation module to mine and refine the pseudo-labels of NTIR images. Further, conditional gradient repair loss is introduced for reducing edge distortion during translation. Extensive experiments on the NTIR2DC task show that the proposed MornGAN significantly outperforms other image-to-image translation methods in terms of semantic preservation and edge consistency, which helps improve the object detection accuracy remarkably.",
      "authors": [
        "Fu-Ya Luo",
        "Yi-Jun Cao",
        "Kai-Fu Yang",
        "Yong-Jie Li"
      ],
      "published": "2022-08-05T03:04:04Z",
      "updated": "2022-08-05T03:04:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.02960v1",
      "landing_url": "https://arxiv.org/abs/2208.02960v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.02960"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2208.05445",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.05445v1",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "published": "2022-08-10T16:56:39Z"
    },
    "metadata": {
      "arxiv_id": "2208.05445",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "authors": [
        "Jaejin Cho",
        "Jes'us Villalba",
        "Laureano Moro-Velazquez",
        "Najim Dehak"
      ],
      "published": "2022-08-10T16:56:39Z",
      "updated": "2022-08-10T16:56:39Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05445v1",
      "landing_url": "https://arxiv.org/abs/2208.05445v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3197315"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2208.09030",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.09030v3",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "published": "2022-08-18T19:02:30Z"
    },
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2208.11079",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.11079v2",
      "title": "Robot Active Neural Sensing and Planning in Unknown Cluttered Environments",
      "summary": "Active sensing and planning in unknown, cluttered environments is an open challenge for robots intending to provide home service, search and rescue, narrow-passage inspection, and medical assistance. Although many active sensing methods exist, they often consider open spaces, assume known settings, or mostly do not generalize to real-world scenarios. We present the active neural sensing approach that generates the kinematically feasible viewpoint sequences for the robot manipulator with an in-hand camera to gather the minimum number of observations needed to reconstruct the underlying environment. Our framework actively collects the visual RGBD observations, aggregates them into scene representation, and performs object shape inference to avoid unnecessary robot interactions with the environment. We train our approach on synthetic data with domain randomization and demonstrate its successful execution via sim-to-real transfer in reconstructing narrow, covered, real-world cabinet environments cluttered with unknown objects. The natural cabinet scenarios impose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting conditions. However, despite unfavorable settings, our method exhibits high performance compared to its baselines in terms of various environment reconstruction metrics, including planning speed, the number of viewpoints, and overall scene coverage.",
      "published": "2022-08-23T16:56:54Z"
    },
    "metadata": {
      "arxiv_id": "2208.11079",
      "title": "Robot Active Neural Sensing and Planning in Unknown Cluttered Environments",
      "summary": "Active sensing and planning in unknown, cluttered environments is an open challenge for robots intending to provide home service, search and rescue, narrow-passage inspection, and medical assistance. Although many active sensing methods exist, they often consider open spaces, assume known settings, or mostly do not generalize to real-world scenarios. We present the active neural sensing approach that generates the kinematically feasible viewpoint sequences for the robot manipulator with an in-hand camera to gather the minimum number of observations needed to reconstruct the underlying environment. Our framework actively collects the visual RGBD observations, aggregates them into scene representation, and performs object shape inference to avoid unnecessary robot interactions with the environment. We train our approach on synthetic data with domain randomization and demonstrate its successful execution via sim-to-real transfer in reconstructing narrow, covered, real-world cabinet environments cluttered with unknown objects. The natural cabinet scenarios impose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting conditions. However, despite unfavorable settings, our method exhibits high performance compared to its baselines in terms of various environment reconstruction metrics, including planning speed, the number of viewpoints, and overall scene coverage.",
      "authors": [
        "Hanwen Ren",
        "Ahmed H. Qureshi"
      ],
      "published": "2022-08-23T16:56:54Z",
      "updated": "2022-08-24T00:52:09Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11079v2",
      "landing_url": "https://arxiv.org/abs/2208.11079v2",
      "doi": "https://doi.org/10.48550/arXiv.2208.11079"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2209.04213",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.04213v2",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "published": "2022-09-09T09:59:56Z"
    },
    "metadata": {
      "arxiv_id": "2209.04213",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "authors": [
        "Jonas Köhne",
        "Lars Henning",
        "Clemens Gühmann"
      ],
      "published": "2022-09-09T09:59:56Z",
      "updated": "2022-09-23T11:07:27Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04213v2",
      "landing_url": "https://arxiv.org/abs/2209.04213v2",
      "doi": "https://doi.org/10.1109/ACCESS.2023.3247564"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2209.06683",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.06683v1",
      "title": "Critical Gaussian Multiplicative Chaos revisited",
      "summary": "We present new, short and self-contained proofs of the convergence (with an adequate renormalization) of four different sequences to the critical Gaussian Multiplicative Chaos:(a) the derivative martingale (b) the critical martingale (c) the exponential of the mollified field (d) the subcritical Gaussian Multiplicative Chaos.",
      "published": "2022-09-14T14:43:14Z"
    },
    "metadata": {
      "arxiv_id": "2209.06683",
      "title": "Critical Gaussian Multiplicative Chaos revisited",
      "summary": "We present new, short and self-contained proofs of the convergence (with an adequate renormalization) of four different sequences to the critical Gaussian Multiplicative Chaos:(a) the derivative martingale (b) the critical martingale (c) the exponential of the mollified field (d) the subcritical Gaussian Multiplicative Chaos.",
      "authors": [
        "Hubert Lacoin"
      ],
      "published": "2022-09-14T14:43:14Z",
      "updated": "2022-09-14T14:43:14Z",
      "categories": [
        "math.PR",
        "math-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.06683v1",
      "landing_url": "https://arxiv.org/abs/2209.06683v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.06683"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2209.07999",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.07999v1",
      "title": "Self-Supervised Learning with an Information Maximization Criterion",
      "summary": "Self-supervised learning allows AI systems to learn effective representations from large amounts of data using tasks that do not require costly labeling. Mode collapse, i.e., the model producing identical representations for all inputs, is a central problem to many self-supervised learning approaches, making self-supervised tasks, such as matching distorted variants of the inputs, ineffective. In this article, we argue that a straightforward application of information maximization among alternative latent representations of the same input naturally solves the collapse problem and achieves competitive empirical results. We propose a self-supervised learning method, CorInfoMax, that uses a second-order statistics-based mutual information measure that reflects the level of correlation among its arguments. Maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. An approximation of the proposed information maximization objective simplifies to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. The regularization term acts as a natural barrier against feature space degeneracy. Consequently, beyond avoiding complete output collapse to a single point, the proposed approach also prevents dimensional collapse by encouraging the spread of information across the whole feature space. Numerical experiments demonstrate that CorInfoMax achieves better or competitive performance results relative to the state-of-the-art SSL approaches.",
      "published": "2022-09-16T15:26:19Z"
    },
    "metadata": {
      "arxiv_id": "2209.07999",
      "title": "Self-Supervised Learning with an Information Maximization Criterion",
      "summary": "Self-supervised learning allows AI systems to learn effective representations from large amounts of data using tasks that do not require costly labeling. Mode collapse, i.e., the model producing identical representations for all inputs, is a central problem to many self-supervised learning approaches, making self-supervised tasks, such as matching distorted variants of the inputs, ineffective. In this article, we argue that a straightforward application of information maximization among alternative latent representations of the same input naturally solves the collapse problem and achieves competitive empirical results. We propose a self-supervised learning method, CorInfoMax, that uses a second-order statistics-based mutual information measure that reflects the level of correlation among its arguments. Maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. An approximation of the proposed information maximization objective simplifies to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. The regularization term acts as a natural barrier against feature space degeneracy. Consequently, beyond avoiding complete output collapse to a single point, the proposed approach also prevents dimensional collapse by encouraging the spread of information across the whole feature space. Numerical experiments demonstrate that CorInfoMax achieves better or competitive performance results relative to the state-of-the-art SSL approaches.",
      "authors": [
        "Serdar Ozsoy",
        "Shadi Hamdan",
        "Sercan Ö. Arik",
        "Deniz Yuret",
        "Alper T. Erdogan"
      ],
      "published": "2022-09-16T15:26:19Z",
      "updated": "2022-09-16T15:26:19Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07999v1",
      "landing_url": "https://arxiv.org/abs/2209.07999v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07999"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2209.08288",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.08288v2",
      "title": "Self-supervised learning of hologram reconstruction using physics consistency",
      "summary": "The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet successfully generalized to experimental holograms of various unseen biological samples, reconstructing the phase and amplitude images of different types of objects using experimentally acquired test holograms. Without access to experimental data or knowledge of real samples of interest or their spatial features, GedankenNet's self-supervised learning achieved complex-valued image reconstructions that are consistent with the Maxwell's equations, and its output inference and object solutions accurately represent the wave propagation in free-space. GedankenNet framework also exhibits resilience to random, unknown perturbations in the physical forward model, including changes in the hologram distances, pixel size and illumination wavelength. This self-supervised learning of image reconstruction tasks creates new opportunities for various inverse problems in holography, microscopy and computational imaging fields.",
      "published": "2022-09-17T09:02:10Z"
    },
    "metadata": {
      "arxiv_id": "2209.08288",
      "title": "Self-supervised learning of hologram reconstruction using physics consistency",
      "summary": "The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet successfully generalized to experimental holograms of various unseen biological samples, reconstructing the phase and amplitude images of different types of objects using experimentally acquired test holograms. Without access to experimental data or knowledge of real samples of interest or their spatial features, GedankenNet's self-supervised learning achieved complex-valued image reconstructions that are consistent with the Maxwell's equations, and its output inference and object solutions accurately represent the wave propagation in free-space. GedankenNet framework also exhibits resilience to random, unknown perturbations in the physical forward model, including changes in the hologram distances, pixel size and illumination wavelength. This self-supervised learning of image reconstruction tasks creates new opportunities for various inverse problems in holography, microscopy and computational imaging fields.",
      "authors": [
        "Luzhe Huang",
        "Hanlong Chen",
        "Tairan Liu",
        "Aydogan Ozcan"
      ],
      "published": "2022-09-17T09:02:10Z",
      "updated": "2023-06-17T08:53:11Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.08288v2",
      "landing_url": "https://arxiv.org/abs/2209.08288v2",
      "doi": "https://doi.org/10.1038/s42256-023-00704-7"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2209.12139",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.12139v1",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "published": "2022-09-25T04:14:26Z"
    },
    "metadata": {
      "arxiv_id": "2209.12139",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "authors": [
        "Yifan Wang",
        "Zhanxuan Mei",
        "Ioannis Katsavounidis",
        "C. -C. Jay Kuo"
      ],
      "published": "2022-09-25T04:14:26Z",
      "updated": "2022-09-25T04:14:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.12139v1",
      "landing_url": "https://arxiv.org/abs/2209.12139v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.12139"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2209.14884",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.14884v1",
      "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
      "summary": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
      "published": "2022-09-29T15:53:19Z"
    },
    "metadata": {
      "arxiv_id": "2209.14884",
      "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
      "summary": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
      "authors": [
        "Bobak T. Kiani",
        "Randall Balestriero",
        "Yubei Chen",
        "Seth Lloyd",
        "Yann LeCun"
      ],
      "published": "2022-09-29T15:53:19Z",
      "updated": "2022-09-29T15:53:19Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14884v1",
      "landing_url": "https://arxiv.org/abs/2209.14884v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.14884"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2209.15472",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.15472v1",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "published": "2022-09-30T13:56:25Z"
    },
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2210.11024",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.11024v1",
      "title": "A survey on Self Supervised learning approaches for improving Multimodal representation learning",
      "summary": "Recently self supervised learning has seen explosive growth and use in variety of machine learning tasks because of its ability to avoid the cost of annotating large-scale datasets.\n  This paper gives an overview for best self supervised learning approaches for multimodal learning. The presented approaches have been aggregated by extensive study of the literature and tackle the application of self supervised learning in different ways. The approaches discussed are cross modal generation, cross modal pretraining, cyclic translation, and generating unimodal labels in self supervised fashion.",
      "published": "2022-10-20T05:19:49Z"
    },
    "metadata": {
      "arxiv_id": "2210.11024",
      "title": "A survey on Self Supervised learning approaches for improving Multimodal representation learning",
      "summary": "Recently self supervised learning has seen explosive growth and use in variety of machine learning tasks because of its ability to avoid the cost of annotating large-scale datasets.\n  This paper gives an overview for best self supervised learning approaches for multimodal learning. The presented approaches have been aggregated by extensive study of the literature and tackle the application of self supervised learning in different ways. The approaches discussed are cross modal generation, cross modal pretraining, cyclic translation, and generating unimodal labels in self supervised fashion.",
      "authors": [
        "Naman Goyal"
      ],
      "published": "2022-10-20T05:19:49Z",
      "updated": "2022-10-20T05:19:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.11024v1",
      "landing_url": "https://arxiv.org/abs/2210.11024v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.11024"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2210.12826",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.12826v1",
      "title": "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization",
      "summary": "We introduce an approach to generating videos based on a series of given language descriptions. Frames of the video are generated sequentially and optimized by guidance from the CLIP image-text encoder; iterating through language descriptions, weighting the current description higher than others. As opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the CLIP loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. The approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. Please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .",
      "published": "2022-10-23T19:14:50Z"
    },
    "metadata": {
      "arxiv_id": "2210.12826",
      "title": "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization",
      "summary": "We introduce an approach to generating videos based on a series of given language descriptions. Frames of the video are generated sequentially and optimized by guidance from the CLIP image-text encoder; iterating through language descriptions, weighting the current description higher than others. As opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the CLIP loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. The approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. Please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .",
      "authors": [
        "Peter Schaldenbrand",
        "Zhixuan Liu",
        "Jean Oh"
      ],
      "published": "2022-10-23T19:14:50Z",
      "updated": "2022-10-23T19:14:50Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12826v1",
      "landing_url": "https://arxiv.org/abs/2210.12826v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12826"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.12995",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.12995v1",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "published": "2022-10-24T07:30:42Z"
    },
    "metadata": {
      "arxiv_id": "2210.12995",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "authors": [
        "Dacheng Yin",
        "Zhiyuan Zhao",
        "Chuanxin Tang",
        "Zhiwei Xiong",
        "Chong Luo"
      ],
      "published": "2022-10-24T07:30:42Z",
      "updated": "2022-10-24T07:30:42Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12995v1",
      "landing_url": "https://arxiv.org/abs/2210.12995v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12995"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2210.15818",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.15818v1",
      "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
      "summary": "Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. One main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. Extensive experiments under multiple settings show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.",
      "published": "2022-10-28T01:06:10Z"
    },
    "metadata": {
      "arxiv_id": "2210.15818",
      "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
      "summary": "Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. One main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. Extensive experiments under multiple settings show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.",
      "authors": [
        "Salman Mohamadi",
        "Gianfranco Doretto",
        "Donald A. Adjeroh"
      ],
      "published": "2022-10-28T01:06:10Z",
      "updated": "2022-10-28T01:06:10Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15818v1",
      "landing_url": "https://arxiv.org/abs/2210.15818v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15818"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2210.16365",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16365v1",
      "title": "Elastic Weight Consolidation Improves the Robustness of Self-Supervised Learning Methods under Transfer",
      "summary": "Self-supervised representation learning (SSL) methods provide an effective label-free initial condition for fine-tuning downstream tasks. However, in numerous realistic scenarios, the downstream task might be biased with respect to the target label distribution. This in turn moves the learned fine-tuned model posterior away from the initial (label) bias-free self-supervised model posterior. In this work, we re-interpret SSL fine-tuning under the lens of Bayesian continual learning and consider regularization through the Elastic Weight Consolidation (EWC) framework. We demonstrate that self-regularization against an initial SSL backbone improves worst sub-group performance in Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture. Furthermore, to help simplify the use of EWC with SSL, we pre-compute and publicly release the Fisher Information Matrix (FIM), evaluated with 10,000 ImageNet-1K variates evaluated on large modern SSL architectures including ViT-B/16 and ResNet50 trained with DINO.",
      "published": "2022-10-28T19:00:25Z"
    },
    "metadata": {
      "arxiv_id": "2210.16365",
      "title": "Elastic Weight Consolidation Improves the Robustness of Self-Supervised Learning Methods under Transfer",
      "summary": "Self-supervised representation learning (SSL) methods provide an effective label-free initial condition for fine-tuning downstream tasks. However, in numerous realistic scenarios, the downstream task might be biased with respect to the target label distribution. This in turn moves the learned fine-tuned model posterior away from the initial (label) bias-free self-supervised model posterior. In this work, we re-interpret SSL fine-tuning under the lens of Bayesian continual learning and consider regularization through the Elastic Weight Consolidation (EWC) framework. We demonstrate that self-regularization against an initial SSL backbone improves worst sub-group performance in Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture. Furthermore, to help simplify the use of EWC with SSL, we pre-compute and publicly release the Fisher Information Matrix (FIM), evaluated with 10,000 ImageNet-1K variates evaluated on large modern SSL architectures including ViT-B/16 and ResNet50 trained with DINO.",
      "authors": [
        "Andrius Ovsianas",
        "Jason Ramapuram",
        "Dan Busbridge",
        "Eeshan Gunesh Dhekane",
        "Russ Webb"
      ],
      "published": "2022-10-28T19:00:25Z",
      "updated": "2022-10-28T19:00:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16365v1",
      "landing_url": "https://arxiv.org/abs/2210.16365v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16365"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2210.16611",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16611v2",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "published": "2022-10-29T14:22:43Z"
    },
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2210.16755",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16755v1",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "published": "2022-10-30T06:38:19Z"
    },
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2210.17052",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.17052v1",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "published": "2022-10-31T04:04:48Z"
    },
    "metadata": {
      "arxiv_id": "2210.17052",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "authors": [
        "Won-Seok Choi",
        "Dong-Sig Han",
        "Hyundo Lee",
        "Junseok Park",
        "Byoung-Tak Zhang"
      ],
      "published": "2022-10-31T04:04:48Z",
      "updated": "2022-10-31T04:04:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17052v1",
      "landing_url": "https://arxiv.org/abs/2210.17052v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.17052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.05239",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.05239v4",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "published": "2022-11-09T22:21:19Z"
    },
    "metadata": {
      "arxiv_id": "2211.05239",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "authors": [
        "Mark Zhao",
        "Dhruv Choudhary",
        "Devashish Tyagi",
        "Ajay Somani",
        "Max Kaplan",
        "Sung-Han Lin",
        "Sarunya Pumma",
        "Jongsoo Park",
        "Aarti Basant",
        "Niket Agarwal",
        "Carole-Jean Wu",
        "Christos Kozyrakis"
      ],
      "published": "2022-11-09T22:21:19Z",
      "updated": "2023-05-01T19:37:39Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.IR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05239v4",
      "landing_url": "https://arxiv.org/abs/2211.05239v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.05239"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2211.05304",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.05304v1",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "published": "2022-11-10T02:45:36Z"
    },
    "metadata": {
      "arxiv_id": "2211.05304",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "authors": [
        "Nico Lingg",
        "Miguel Sarabia",
        "Luca Zappella",
        "Barry-John Theobald"
      ],
      "published": "2022-11-10T02:45:36Z",
      "updated": "2022-11-10T02:45:36Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05304v1",
      "landing_url": "https://arxiv.org/abs/2211.05304v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05304"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.07057",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.07057v1",
      "title": "Is the Statistical Interpretation of Quantum Mechanics $ψ$-Ontic or $ψ$-Epistemic?",
      "summary": "The ontological models framework distinguishes $ψ$-ontic from $ψ$-epistemic wavefunctions. It is, in general, quite straightforward to categorize the wave-function of a certain quantum theory. Nevertheless, there has been a debate about the ontological status of the wave-function in the statistical interpretation of quantum mechanics: is it $ψ$-epistemic and incomplete or $ψ$-ontic and complete? I will argue that the wavefunction in this interpretation is best regarded as $ψ$-ontic and incomplete.",
      "published": "2022-11-14T00:30:56Z"
    },
    "metadata": {
      "arxiv_id": "2211.07057",
      "title": "Is the Statistical Interpretation of Quantum Mechanics $ψ$-Ontic or $ψ$-Epistemic?",
      "summary": "The ontological models framework distinguishes $ψ$-ontic from $ψ$-epistemic wavefunctions. It is, in general, quite straightforward to categorize the wave-function of a certain quantum theory. Nevertheless, there has been a debate about the ontological status of the wave-function in the statistical interpretation of quantum mechanics: is it $ψ$-epistemic and incomplete or $ψ$-ontic and complete? I will argue that the wavefunction in this interpretation is best regarded as $ψ$-ontic and incomplete.",
      "authors": [
        "Mario Hubert"
      ],
      "published": "2022-11-14T00:30:56Z",
      "updated": "2022-11-14T00:30:56Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07057v1",
      "landing_url": "https://arxiv.org/abs/2211.07057v1",
      "doi": "https://doi.org/10.1007/s10701-022-00651-0"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2211.08282",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.08282v1",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "published": "2022-11-15T16:32:36Z"
    },
    "metadata": {
      "arxiv_id": "2211.08282",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "authors": [
        "T. Anderson Keller",
        "Xavier Suau",
        "Luca Zappella"
      ],
      "published": "2022-11-15T16:32:36Z",
      "updated": "2022-11-15T16:32:36Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08282v1",
      "landing_url": "https://arxiv.org/abs/2211.08282v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08282"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.09117",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09117v2",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "published": "2022-11-16T18:59:02Z"
    },
    "metadata": {
      "arxiv_id": "2211.09117",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "authors": [
        "Tianhong Li",
        "Huiwen Chang",
        "Shlok Kumar Mishra",
        "Han Zhang",
        "Dina Katabi",
        "Dilip Krishnan"
      ],
      "published": "2022-11-16T18:59:02Z",
      "updated": "2023-06-29T15:30:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09117v2",
      "landing_url": "https://arxiv.org/abs/2211.09117v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.09117"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2211.09944",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09944v3",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "published": "2022-11-17T23:38:29Z"
    },
    "metadata": {
      "arxiv_id": "2211.09944",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "authors": [
        "Tzu-Quan Lin",
        "Hung-yi Lee",
        "Hao Tang"
      ],
      "published": "2022-11-17T23:38:29Z",
      "updated": "2024-08-29T19:25:59Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09944v3",
      "landing_url": "https://arxiv.org/abs/2211.09944v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09944"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2211.09981",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09981v3",
      "title": "Weighted Ensemble Self-Supervised Learning",
      "summary": "Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.",
      "published": "2022-11-18T02:00:17Z"
    },
    "metadata": {
      "arxiv_id": "2211.09981",
      "title": "Weighted Ensemble Self-Supervised Learning",
      "summary": "Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.",
      "authors": [
        "Yangjun Ruan",
        "Saurabh Singh",
        "Warren Morningstar",
        "Alexander A. Alemi",
        "Sergey Ioffe",
        "Ian Fischer",
        "Joshua V. Dillon"
      ],
      "published": "2022-11-18T02:00:17Z",
      "updated": "2023-04-09T19:15:39Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09981v3",
      "landing_url": "https://arxiv.org/abs/2211.09981v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09981"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.12174",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.12174v1",
      "title": "The Monocular Depth Estimation Challenge",
      "summary": "This paper summarizes the results of the first Monocular Depth Estimation Challenge (MDEC) organized at WACV2023. This challenge evaluated the progress of self-supervised monocular depth estimation on the challenging SYNS-Patches dataset. The challenge was organized on CodaLab and received submissions from 4 valid teams. Participants were provided a devkit containing updated reference implementations for 16 State-of-the-Art algorithms and 4 novel techniques. The threshold for acceptance for novel techniques was to outperform every one of the 16 SotA baselines. All participants outperformed the baseline in traditional metrics such as MAE or AbsRel. However, pointcloud reconstruction metrics were challenging to improve upon. We found predictions were characterized by interpolation artefacts at object boundaries and errors in relative object positioning. We hope this challenge is a valuable contribution to the community and encourage authors to participate in future editions.",
      "published": "2022-11-22T11:04:15Z"
    },
    "metadata": {
      "arxiv_id": "2211.12174",
      "title": "The Monocular Depth Estimation Challenge",
      "summary": "This paper summarizes the results of the first Monocular Depth Estimation Challenge (MDEC) organized at WACV2023. This challenge evaluated the progress of self-supervised monocular depth estimation on the challenging SYNS-Patches dataset. The challenge was organized on CodaLab and received submissions from 4 valid teams. Participants were provided a devkit containing updated reference implementations for 16 State-of-the-Art algorithms and 4 novel techniques. The threshold for acceptance for novel techniques was to outperform every one of the 16 SotA baselines. All participants outperformed the baseline in traditional metrics such as MAE or AbsRel. However, pointcloud reconstruction metrics were challenging to improve upon. We found predictions were characterized by interpolation artefacts at object boundaries and errors in relative object positioning. We hope this challenge is a valuable contribution to the community and encourage authors to participate in future editions.",
      "authors": [
        "Jaime Spencer",
        "C. Stella Qian",
        "Chris Russell",
        "Simon Hadfield",
        "Erich Graf",
        "Wendy Adams",
        "Andrew J. Schofield",
        "James Elder",
        "Richard Bowden",
        "Heng Cong",
        "Stefano Mattoccia",
        "Matteo Poggi",
        "Zeeshan Khan Suri",
        "Yang Tang",
        "Fabio Tosi",
        "Hao Wang",
        "Youmin Zhang",
        "Yusheng Zhang",
        "Chaoqiang Zhao"
      ],
      "published": "2022-11-22T11:04:15Z",
      "updated": "2022-11-22T11:04:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12174v1",
      "landing_url": "https://arxiv.org/abs/2211.12174v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.12174"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2211.12271",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.12271v3",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "published": "2022-11-22T13:42:53Z"
    },
    "metadata": {
      "arxiv_id": "2211.12271",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "authors": [
        "Georgios Vardakas",
        "Aristidis Likas"
      ],
      "published": "2022-11-22T13:42:53Z",
      "updated": "2023-07-14T11:39:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12271v3",
      "landing_url": "https://arxiv.org/abs/2211.12271v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.12271"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2211.14036",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.14036v1",
      "title": "Privileged Prior Information Distillation for Image Matting",
      "summary": "Performance of trimap-free image matting methods is limited when trying to decouple the deterministic and undetermined regions, especially in the scenes where foregrounds are semantically ambiguous, chromaless, or high transmittance. In this paper, we propose a novel framework named Privileged Prior Information Distillation for Image Matting (PPID-IM) that can effectively transfer privileged prior environment-aware information to improve the performance of students in solving hard foregrounds. The prior information of trimap regulates only the teacher model during the training stage, while not being fed into the student network during actual inference. In order to achieve effective privileged cross-modality (i.e. trimap and RGB) information distillation, we introduce a Cross-Level Semantic Distillation (CLSD) module that reinforces the trimap-free students with more knowledgeable semantic representations and environment-aware information. We also propose an Attention-Guided Local Distillation module that efficiently transfers privileged local attributes from the trimap-based teacher to trimap-free students for the guidance of local-region optimization. Extensive experiments demonstrate the effectiveness and superiority of our PPID framework on the task of image matting. In addition, our trimap-free IndexNet-PPID surpasses the other competing state-of-the-art methods by a large margin, especially in scenarios with chromaless, weak texture, or irregular objects.",
      "published": "2022-11-25T11:24:04Z"
    },
    "metadata": {
      "arxiv_id": "2211.14036",
      "title": "Privileged Prior Information Distillation for Image Matting",
      "summary": "Performance of trimap-free image matting methods is limited when trying to decouple the deterministic and undetermined regions, especially in the scenes where foregrounds are semantically ambiguous, chromaless, or high transmittance. In this paper, we propose a novel framework named Privileged Prior Information Distillation for Image Matting (PPID-IM) that can effectively transfer privileged prior environment-aware information to improve the performance of students in solving hard foregrounds. The prior information of trimap regulates only the teacher model during the training stage, while not being fed into the student network during actual inference. In order to achieve effective privileged cross-modality (i.e. trimap and RGB) information distillation, we introduce a Cross-Level Semantic Distillation (CLSD) module that reinforces the trimap-free students with more knowledgeable semantic representations and environment-aware information. We also propose an Attention-Guided Local Distillation module that efficiently transfers privileged local attributes from the trimap-based teacher to trimap-free students for the guidance of local-region optimization. Extensive experiments demonstrate the effectiveness and superiority of our PPID framework on the task of image matting. In addition, our trimap-free IndexNet-PPID surpasses the other competing state-of-the-art methods by a large margin, especially in scenarios with chromaless, weak texture, or irregular objects.",
      "authors": [
        "Cheng Lyu",
        "Jiake Xie",
        "Bo Xu",
        "Cheng Lu",
        "Han Huang",
        "Xin Huang",
        "Ming Wu",
        "Chuang Zhang",
        "Yong Tang"
      ],
      "published": "2022-11-25T11:24:04Z",
      "updated": "2022-11-25T11:24:04Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14036v1",
      "landing_url": "https://arxiv.org/abs/2211.14036v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14036"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2211.14363",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.14363v1",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "published": "2022-11-25T20:09:22Z"
    },
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2212.03589",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.03589v1",
      "title": "On the Global Solution of Soft k-Means",
      "summary": "This paper presents an algorithm to solve the Soft k-Means problem globally. Unlike Fuzzy c-Means, Soft k-Means (SkM) has a matrix factorization-type objective and has been shown to have a close relation with the popular probability decomposition-type clustering methods, e.g., Left Stochastic Clustering (LSC). Though some work has been done for solving the Soft k-Means problem, they usually use an alternating minimization scheme or the projected gradient descent method, which cannot guarantee global optimality since the non-convexity of SkM. In this paper, we present a sufficient condition for a feasible solution of Soft k-Means problem to be globally optimal and show the output of the proposed algorithm satisfies it. Moreover, for the Soft k-Means problem, we provide interesting discussions on stability, solutions non-uniqueness, and connection with LSC. Then, a new model, named Minimal Volume Soft k-Means (MVSkM), is proposed to address the solutions non-uniqueness issue. Finally, experimental results support our theoretical results.",
      "published": "2022-12-07T12:06:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.03589",
      "title": "On the Global Solution of Soft k-Means",
      "summary": "This paper presents an algorithm to solve the Soft k-Means problem globally. Unlike Fuzzy c-Means, Soft k-Means (SkM) has a matrix factorization-type objective and has been shown to have a close relation with the popular probability decomposition-type clustering methods, e.g., Left Stochastic Clustering (LSC). Though some work has been done for solving the Soft k-Means problem, they usually use an alternating minimization scheme or the projected gradient descent method, which cannot guarantee global optimality since the non-convexity of SkM. In this paper, we present a sufficient condition for a feasible solution of Soft k-Means problem to be globally optimal and show the output of the proposed algorithm satisfies it. Moreover, for the Soft k-Means problem, we provide interesting discussions on stability, solutions non-uniqueness, and connection with LSC. Then, a new model, named Minimal Volume Soft k-Means (MVSkM), is proposed to address the solutions non-uniqueness issue. Finally, experimental results support our theoretical results.",
      "authors": [
        "Feiping Nie",
        "Hong Chen",
        "Rong Wang",
        "Xuelong Li"
      ],
      "published": "2022-12-07T12:06:55Z",
      "updated": "2022-12-07T12:06:55Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03589v1",
      "landing_url": "https://arxiv.org/abs/2212.03589v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03589"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2212.09058",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.09058v1",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "published": "2022-12-18T10:41:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2212.09096",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.09096v1",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "published": "2022-12-18T14:40:52Z"
    },
    "metadata": {
      "arxiv_id": "2212.09096",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "authors": [
        "Hechuan Guo",
        "Minghui Xu",
        "Jiahao Zhang",
        "Chunchi Liu",
        "Dongxiao Yu",
        "Schahram Dustdar",
        "Xiuzhen Cheng"
      ],
      "published": "2022-12-18T14:40:52Z",
      "updated": "2022-12-18T14:40:52Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09096v1",
      "landing_url": "https://arxiv.org/abs/2212.09096v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09096"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2212.11187",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.11187v1",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "published": "2022-12-21T16:56:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.11187",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "authors": [
        "Julien Denize",
        "Jaonary Rabarisoa",
        "Astrid Orcesi",
        "Romain Hérault"
      ],
      "published": "2022-12-21T16:56:55Z",
      "updated": "2022-12-21T16:56:55Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11187v1",
      "landing_url": "https://arxiv.org/abs/2212.11187v1",
      "doi": "https://doi.org/10.1007/s00138-023-01444-9"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2212.11444",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.11444v1",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "published": "2022-12-22T01:26:38Z"
    },
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2212.13425",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.13425v4",
      "title": "GEDI: GEnerative and DIscriminative Training for Self-Supervised Learning",
      "summary": "Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also demonstrate that GEDI can be integrated into a neural-symbolic framework to address tasks in the small data regime, where it can use logical constraints to further improve clustering and classification performance.",
      "published": "2022-12-27T09:33:50Z"
    },
    "metadata": {
      "arxiv_id": "2212.13425",
      "title": "GEDI: GEnerative and DIscriminative Training for Self-Supervised Learning",
      "summary": "Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also demonstrate that GEDI can be integrated into a neural-symbolic framework to address tasks in the small data regime, where it can use logical constraints to further improve clustering and classification performance.",
      "authors": [
        "Emanuele Sansone",
        "Robin Manhaeve"
      ],
      "published": "2022-12-27T09:33:50Z",
      "updated": "2023-02-07T04:19:54Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.13425v4",
      "landing_url": "https://arxiv.org/abs/2212.13425v4",
      "doi": "https://doi.org/10.48550/arXiv.2212.13425"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2301.02111",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.02111v1",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "published": "2023-01-05T15:37:15Z"
    },
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2301.03432",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.03432v2",
      "title": "Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark",
      "summary": "Cloud removal is a significant and challenging problem in remote sensing, and in recent years, there have been notable advancements in this area. However, two major issues remain hindering the development of cloud removal: the unavailability of high-resolution imagery for existing datasets and the absence of evaluation regarding the semantic meaningfulness of the generated structures. In this paper, we introduce M3R-CR, a benchmark dataset for high-resolution Cloud Removal with Multi-Modal and Multi-Resolution data fusion. With this dataset, we consider the problem of cloud removal in high-resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. In this context, we have to take into account the alignment errors caused by the multi-resolution nature, along with the more pronounced misalignment issues in high-resolution images due to inherent imaging mechanism differences and other factors. Existing multi-modal data fusion based methods, which assume the image pairs are aligned accurately at pixel-level, are thus not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution SAR image guided high-resolution optical image cloud removal. It gradually warps and fuses the features of the multi-modal and multi-resolution data during the reconstruction process, effectively mitigating concerns associated with misalignment. In the experiments, we evaluate the performance of cloud removal by analyzing the quality of visually pleasing textures using image reconstruction metrics and further analyze the generation of semantically meaningful structures using a well-established semantic segmentation task. The proposed Align-CR method is superior to other baseline methods in both areas.",
      "published": "2023-01-09T15:31:28Z"
    },
    "metadata": {
      "arxiv_id": "2301.03432",
      "title": "Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark",
      "summary": "Cloud removal is a significant and challenging problem in remote sensing, and in recent years, there have been notable advancements in this area. However, two major issues remain hindering the development of cloud removal: the unavailability of high-resolution imagery for existing datasets and the absence of evaluation regarding the semantic meaningfulness of the generated structures. In this paper, we introduce M3R-CR, a benchmark dataset for high-resolution Cloud Removal with Multi-Modal and Multi-Resolution data fusion. With this dataset, we consider the problem of cloud removal in high-resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. In this context, we have to take into account the alignment errors caused by the multi-resolution nature, along with the more pronounced misalignment issues in high-resolution images due to inherent imaging mechanism differences and other factors. Existing multi-modal data fusion based methods, which assume the image pairs are aligned accurately at pixel-level, are thus not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution SAR image guided high-resolution optical image cloud removal. It gradually warps and fuses the features of the multi-modal and multi-resolution data during the reconstruction process, effectively mitigating concerns associated with misalignment. In the experiments, we evaluate the performance of cloud removal by analyzing the quality of visually pleasing textures using image reconstruction metrics and further analyze the generation of semantically meaningful structures using a well-established semantic segmentation task. The proposed Align-CR method is superior to other baseline methods in both areas.",
      "authors": [
        "Fang Xu",
        "Yilei Shi",
        "Patrick Ebel",
        "Wen Yang",
        "Xiao Xiang Zhu"
      ],
      "published": "2023-01-09T15:31:28Z",
      "updated": "2024-10-11T05:43:05Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.03432v2",
      "landing_url": "https://arxiv.org/abs/2301.03432v2",
      "doi": "https://doi.org/10.1109/TGRS.2023.3337845"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2301.05489",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.05489v3",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "published": "2023-01-13T11:27:26Z"
    },
    "metadata": {
      "arxiv_id": "2301.05489",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "authors": [
        "Noor Fathima Ghouse",
        "Jens Petersen",
        "Auke Wiggers",
        "Tianlin Xu",
        "Guillaume Sautière"
      ],
      "published": "2023-01-13T11:27:26Z",
      "updated": "2023-03-29T16:13:22Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05489v3",
      "landing_url": "https://arxiv.org/abs/2301.05489v3",
      "doi": "https://doi.org/10.48550/arXiv.2301.05489"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2301.06626",
    "anchor": "acoustic tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.06626v2",
      "title": "Masked Vector Quantization",
      "summary": "Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\\%$ at 2 tokens per instance and $57\\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token sampling during inference. As an additional benefit, we find that smaller latent spaces lead to MVQ identifying transferable visual representations where multiple can be smoothly combined.",
      "published": "2023-01-16T22:30:53Z"
    },
    "metadata": {
      "arxiv_id": "2301.06626",
      "title": "Masked Vector Quantization",
      "summary": "Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\\%$ at 2 tokens per instance and $57\\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token sampling during inference. As an additional benefit, we find that smaller latent spaces lead to MVQ identifying transferable visual representations where multiple can be smoothly combined.",
      "authors": [
        "David D. Nguyen",
        "David Leibowitz",
        "Surya Nepal",
        "Salil S. Kanhere"
      ],
      "published": "2023-01-16T22:30:53Z",
      "updated": "2024-03-25T00:45:30Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06626v2",
      "landing_url": "https://arxiv.org/abs/2301.06626v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.06626"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2301.09027",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.09027v1",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "published": "2023-01-22T00:18:10Z"
    },
    "metadata": {
      "arxiv_id": "2301.09027",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "authors": [
        "Amanda Shu",
        "Hamza Khalid",
        "Haohui Liu",
        "Shikhar Agnihotri",
        "Joseph Konan",
        "Ojas Bhargave"
      ],
      "published": "2023-01-22T00:18:10Z",
      "updated": "2023-01-22T00:18:10Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09027v1",
      "landing_url": "https://arxiv.org/abs/2301.09027v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.09027"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2301.13662",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.13662v2",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "published": "2023-01-31T14:26:52Z"
    },
    "metadata": {
      "arxiv_id": "2301.13662",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Chao Weng",
        "Helen Meng"
      ],
      "published": "2023-01-31T14:26:52Z",
      "updated": "2023-06-25T11:42:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13662v2",
      "landing_url": "https://arxiv.org/abs/2301.13662v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.13662"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2302.00903",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.00903v3",
      "title": "No One Left Behind: Real-World Federated Class-Incremental Learning",
      "summary": "Federated learning (FL) is a hot collaborative training framework via aggregating model parameters of decentralized local clients. However, most FL methods unreasonably assume data categories of FL framework are known and fixed in advance. Moreover, some new local clients that collect novel categories unseen by other clients may be introduced to FL training irregularly. These issues render global model to undergo catastrophic forgetting on old categories, when local clients receive new categories consecutively under limited memory of storing old categories. To tackle the above issues, we propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local clients are left behind as they learn new classes continually, by addressing local and global catastrophic forgetting. Specifically, considering tackling class imbalance of local client to surmount local forgetting, we develop a category-balanced gradient-adaptive compensation loss and a category gradient-induced semantic distillation loss. They can balance heterogeneous forgetting speeds of hard-to-forget and easy-to-forget old categories, while ensure consistent class-relations within different tasks. Moreover, a proxy server is designed to tackle global forgetting caused by Non-IID class imbalance between different clients. It augments perturbed prototype images of new categories collected from local clients via self-supervised prototype augmentation, thus improving robustness to choose the best old global model for local-side semantic distillation loss. Experiments on representative datasets verify superior performance of our model against comparison methods. The code is available at https://github.com/JiahuaDong/LGA.",
      "published": "2023-02-02T06:41:02Z"
    },
    "metadata": {
      "arxiv_id": "2302.00903",
      "title": "No One Left Behind: Real-World Federated Class-Incremental Learning",
      "summary": "Federated learning (FL) is a hot collaborative training framework via aggregating model parameters of decentralized local clients. However, most FL methods unreasonably assume data categories of FL framework are known and fixed in advance. Moreover, some new local clients that collect novel categories unseen by other clients may be introduced to FL training irregularly. These issues render global model to undergo catastrophic forgetting on old categories, when local clients receive new categories consecutively under limited memory of storing old categories. To tackle the above issues, we propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local clients are left behind as they learn new classes continually, by addressing local and global catastrophic forgetting. Specifically, considering tackling class imbalance of local client to surmount local forgetting, we develop a category-balanced gradient-adaptive compensation loss and a category gradient-induced semantic distillation loss. They can balance heterogeneous forgetting speeds of hard-to-forget and easy-to-forget old categories, while ensure consistent class-relations within different tasks. Moreover, a proxy server is designed to tackle global forgetting caused by Non-IID class imbalance between different clients. It augments perturbed prototype images of new categories collected from local clients via self-supervised prototype augmentation, thus improving robustness to choose the best old global model for local-side semantic distillation loss. Experiments on representative datasets verify superior performance of our model against comparison methods. The code is available at https://github.com/JiahuaDong/LGA.",
      "authors": [
        "Jiahua Dong",
        "Hongliu Li",
        "Yang Cong",
        "Gan Sun",
        "Yulun Zhang",
        "Luc Van Gool"
      ],
      "published": "2023-02-02T06:41:02Z",
      "updated": "2023-11-16T03:53:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.00903v3",
      "landing_url": "https://arxiv.org/abs/2302.00903v3",
      "doi": "https://doi.org/10.48550/arXiv.2302.00903"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2302.01647",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.01647v2",
      "title": "Blockwise Self-Supervised Learning at Scale",
      "summary": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
      "published": "2023-02-03T10:48:24Z"
    },
    "metadata": {
      "arxiv_id": "2302.01647",
      "title": "Blockwise Self-Supervised Learning at Scale",
      "summary": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
      "authors": [
        "Shoaib Ahmed Siddiqui",
        "David Krueger",
        "Yann LeCun",
        "Stéphane Deny"
      ],
      "published": "2023-02-03T10:48:24Z",
      "updated": "2024-08-11T15:59:30Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01647v2",
      "landing_url": "https://arxiv.org/abs/2302.01647v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.01647"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2302.03540",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.03540v1",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "published": "2023-02-07T15:48:31Z"
    },
    "metadata": {
      "arxiv_id": "2302.03540",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2023-02-07T15:48:31Z",
      "updated": "2023-02-07T15:48:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.03540v1",
      "landing_url": "https://arxiv.org/abs/2302.03540v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.03540"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2302.07045",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.07045v1",
      "title": "Multi-Prototypes Convex Merging Based K-Means Clustering Algorithm",
      "summary": "K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. A theoretical proof is given to guarantee that the multi-prototypes selected by MPS can achieve a constant factor approximation to the optimal cost of the K-Means problem. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.",
      "published": "2023-02-14T13:57:33Z"
    },
    "metadata": {
      "arxiv_id": "2302.07045",
      "title": "Multi-Prototypes Convex Merging Based K-Means Clustering Algorithm",
      "summary": "K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. A theoretical proof is given to guarantee that the multi-prototypes selected by MPS can achieve a constant factor approximation to the optimal cost of the K-Means problem. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.",
      "authors": [
        "Dong Li",
        "Shuisheng Zhou",
        "Tieyong Zeng",
        "Raymond H. Chan"
      ],
      "published": "2023-02-14T13:57:33Z",
      "updated": "2023-02-14T13:57:33Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.07045v1",
      "landing_url": "https://arxiv.org/abs/2302.07045v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.07045"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2302.08342",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.08342v1",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "published": "2023-02-16T14:53:41Z"
    },
    "metadata": {
      "arxiv_id": "2302.08342",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "authors": [
        "Xiao-Ying Zhao",
        "Qiu-Shi Zhu",
        "Jie Zhang"
      ],
      "published": "2023-02-16T14:53:41Z",
      "updated": "2023-02-16T14:53:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08342v1",
      "landing_url": "https://arxiv.org/abs/2302.08342v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08342"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2302.10606",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.10606v1",
      "title": "Giant Magneto-Optical Schäfer-Hubert Effect in Two-Dimensional van der Waals Antiferromagnets \\textit{M}PS$_3$ (\\textit{M}=Mn, Fe, Ni)",
      "summary": "The recent discovery of long-range magnetic order in atomically thin films has triggered particular interest in two-dimensional (2D) van der Waals (vdW) magnetic materials. In this paper, we perform a systematic theoretical study of the magneto-optical Schäfer-Hubert effect (MOSHE) in 2D vdW antiferromagnetic \\textit{M}PS$_3$ (\\textit{M} = Mn, Fe, Ni) with multifold intralayer and interlayer magnetic orders. The formula for evaluating the MOSHE in 2D magnets is derived by considering the influence of a non-magnetic substrate. The MOSHE of monolayer and bilayer \\textit{M}PS$_3$ are considerably large ($>2^{\\circ}$), originating from the strong anisotropy of in-plane optical conductivity. The Schäfer-Hubert rotation angles are surprisingly insensitive to the orientations of the Néel vector, while the Schäfer-Hubert ellipticities are identified to be a good criterion to distinguish different interlayer magnetic orders. Our work establishes a theoretical framework for exploring novel 2D vdW magnets and facilitates the promising applications of the 2D \\textit{M}PS$_3$ family in antiferromagnetic nanophotonic devices.",
      "published": "2023-02-21T11:28:41Z"
    },
    "metadata": {
      "arxiv_id": "2302.10606",
      "title": "Giant Magneto-Optical Schäfer-Hubert Effect in Two-Dimensional van der Waals Antiferromagnets \\textit{M}PS$_3$ (\\textit{M}=Mn, Fe, Ni)",
      "summary": "The recent discovery of long-range magnetic order in atomically thin films has triggered particular interest in two-dimensional (2D) van der Waals (vdW) magnetic materials. In this paper, we perform a systematic theoretical study of the magneto-optical Schäfer-Hubert effect (MOSHE) in 2D vdW antiferromagnetic \\textit{M}PS$_3$ (\\textit{M} = Mn, Fe, Ni) with multifold intralayer and interlayer magnetic orders. The formula for evaluating the MOSHE in 2D magnets is derived by considering the influence of a non-magnetic substrate. The MOSHE of monolayer and bilayer \\textit{M}PS$_3$ are considerably large ($>2^{\\circ}$), originating from the strong anisotropy of in-plane optical conductivity. The Schäfer-Hubert rotation angles are surprisingly insensitive to the orientations of the Néel vector, while the Schäfer-Hubert ellipticities are identified to be a good criterion to distinguish different interlayer magnetic orders. Our work establishes a theoretical framework for exploring novel 2D vdW magnets and facilitates the promising applications of the 2D \\textit{M}PS$_3$ family in antiferromagnetic nanophotonic devices.",
      "authors": [
        "Ping Yang",
        "Wanxiang Feng",
        "Gui-Bin Liu",
        "Guang-Yu Guo",
        "Yugui Yao"
      ],
      "published": "2023-02-21T11:28:41Z",
      "updated": "2023-02-21T11:28:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10606v1",
      "landing_url": "https://arxiv.org/abs/2302.10606v1",
      "doi": "https://doi.org/10.1103/PhysRevB.107.214437"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.00111",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.00111v2",
      "title": "PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification",
      "summary": "Deep learning (DL) models are capable of successfully exploiting latent representations in MR data and have become state-of-the-art for accelerated MRI reconstruction. However, undersampling the measurements in k-space as well as the over- or under-parameterized and non-transparent nature of DL make these models exposed to uncertainty. Consequently, uncertainty estimation has become a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo (MC) inference techniques have become a common practice where multiple reconstructions are utilized to compute the variance in reconstruction as a measurement of uncertainty. However, these methods demand high computational costs as they require multiple inferences through the DL model. To this end, we introduce a method to estimate uncertainty during MRI reconstruction using a pixel classification framework. The proposed method, PixCUE (stands for Pixel Classification Uncertainty Estimation) produces the reconstructed image along with an uncertainty map during a single forward pass through the DL model. We demonstrate that this approach generates uncertainty maps that highly correlate with the reconstruction errors with respect to various MR imaging sequences and under numerous adversarial conditions. We also show that the estimated uncertainties are correlated to that of the conventional MC method. We further provide an empirical relationship between the uncertainty estimations using PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty in MRI reconstruction with a minimum additional computational cost.",
      "published": "2023-02-28T22:26:18Z"
    },
    "metadata": {
      "arxiv_id": "2303.00111",
      "title": "PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification",
      "summary": "Deep learning (DL) models are capable of successfully exploiting latent representations in MR data and have become state-of-the-art for accelerated MRI reconstruction. However, undersampling the measurements in k-space as well as the over- or under-parameterized and non-transparent nature of DL make these models exposed to uncertainty. Consequently, uncertainty estimation has become a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo (MC) inference techniques have become a common practice where multiple reconstructions are utilized to compute the variance in reconstruction as a measurement of uncertainty. However, these methods demand high computational costs as they require multiple inferences through the DL model. To this end, we introduce a method to estimate uncertainty during MRI reconstruction using a pixel classification framework. The proposed method, PixCUE (stands for Pixel Classification Uncertainty Estimation) produces the reconstructed image along with an uncertainty map during a single forward pass through the DL model. We demonstrate that this approach generates uncertainty maps that highly correlate with the reconstruction errors with respect to various MR imaging sequences and under numerous adversarial conditions. We also show that the estimated uncertainties are correlated to that of the conventional MC method. We further provide an empirical relationship between the uncertainty estimations using PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty in MRI reconstruction with a minimum additional computational cost.",
      "authors": [
        "Mevan Ekanayake",
        "Kamlesh Pawar",
        "Gary Egan",
        "Zhaolin Chen"
      ],
      "published": "2023-02-28T22:26:18Z",
      "updated": "2023-03-08T07:55:37Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.00111v2",
      "landing_url": "https://arxiv.org/abs/2303.00111v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.00111"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2303.01584",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.01584v2",
      "title": "Evolutionary Augmentation Policy Optimization for Self-supervised Learning",
      "summary": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",
      "published": "2023-03-02T21:16:53Z"
    },
    "metadata": {
      "arxiv_id": "2303.01584",
      "title": "Evolutionary Augmentation Policy Optimization for Self-supervised Learning",
      "summary": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",
      "authors": [
        "Noah Barrett",
        "Zahra Sadeghi",
        "Stan Matwin"
      ],
      "published": "2023-03-02T21:16:53Z",
      "updated": "2023-08-02T15:38:37Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01584v2",
      "landing_url": "https://arxiv.org/abs/2303.01584v2",
      "doi": "https://doi.org/10.54364/AAIML.2023.1167"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2303.02939",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.02939v3",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "published": "2023-03-06T07:17:15Z"
    },
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2303.03926",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.03926v1",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "published": "2023-03-07T14:31:55Z"
    },
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2303.06424",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.06424v2",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "published": "2023-03-11T15:20:54Z"
    },
    "metadata": {
      "arxiv_id": "2303.06424",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "authors": [
        "Jiahui Zhang",
        "Fangneng Zhan",
        "Christian Theobalt",
        "Shijian Lu"
      ],
      "published": "2023-03-11T15:20:54Z",
      "updated": "2023-10-14T06:17:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06424v2",
      "landing_url": "https://arxiv.org/abs/2303.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.06424"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2303.08685",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.08685v2",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "published": "2023-03-15T15:12:36Z"
    },
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2303.11131",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.11131v1",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "published": "2023-03-20T14:07:13Z"
    },
    "metadata": {
      "arxiv_id": "2303.11131",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "authors": [
        "Maryam Fazel-Zarandi",
        "Wei-Ning Hsu"
      ],
      "published": "2023-03-20T14:07:13Z",
      "updated": "2023-03-20T14:07:13Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.11131v1",
      "landing_url": "https://arxiv.org/abs/2303.11131v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.11131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.12187",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.12187v1",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "published": "2023-02-28T02:10:13Z"
    },
    "metadata": {
      "arxiv_id": "2303.12187",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "authors": [
        "Xiaoming Ren",
        "Chao Li",
        "Shenjian Wang",
        "Biao Li"
      ],
      "published": "2023-02-28T02:10:13Z",
      "updated": "2023-02-28T02:10:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12187v1",
      "landing_url": "https://arxiv.org/abs/2303.12187v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12187"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.14593",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.14593v1",
      "title": "Time-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder",
      "summary": "Time-domain speech enhancement (SE) has recently been intensively investigated. Among recent works, DEMUCS introduces multi-resolution STFT loss to enhance performance. However, some resolutions used for STFT contain non-stationary signals, and it is challenging to learn multi-resolution frequency losses simultaneously with only one output. For better use of multi-resolution frequency information, we supplement multiple spectrograms in different frame lengths into the time-domain encoders. They extract stationary frequency information in both narrowband and wideband. We also adopt multiple decoder outputs, each of which computes its corresponding resolution frequency loss. Experimental results show that (1) it is more effective to fuse stationary frequency features than non-stationary features in the encoder, and (2) the multiple outputs consistent with the frequency loss improve performance. Experiments on the Voice-Bank dataset show that the proposed method obtained a 0.14 PESQ improvement.",
      "published": "2023-03-26T00:30:06Z"
    },
    "metadata": {
      "arxiv_id": "2303.14593",
      "title": "Time-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder",
      "summary": "Time-domain speech enhancement (SE) has recently been intensively investigated. Among recent works, DEMUCS introduces multi-resolution STFT loss to enhance performance. However, some resolutions used for STFT contain non-stationary signals, and it is challenging to learn multi-resolution frequency losses simultaneously with only one output. For better use of multi-resolution frequency information, we supplement multiple spectrograms in different frame lengths into the time-domain encoders. They extract stationary frequency information in both narrowband and wideband. We also adopt multiple decoder outputs, each of which computes its corresponding resolution frequency loss. Experimental results show that (1) it is more effective to fuse stationary frequency features than non-stationary features in the encoder, and (2) the multiple outputs consistent with the frequency loss improve performance. Experiments on the Voice-Bank dataset show that the proposed method obtained a 0.14 PESQ improvement.",
      "authors": [
        "Hao Shi",
        "Masato Mimura",
        "Longbiao Wang",
        "Jianwu Dang",
        "Tatsuya Kawahara"
      ],
      "published": "2023-03-26T00:30:06Z",
      "updated": "2023-03-26T00:30:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14593v1",
      "landing_url": "https://arxiv.org/abs/2303.14593v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.14593"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2303.15438",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.15438v2",
      "title": "On the Stepwise Nature of Self-Supervised Learning",
      "summary": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
      "published": "2023-03-27T17:59:20Z"
    },
    "metadata": {
      "arxiv_id": "2303.15438",
      "title": "On the Stepwise Nature of Self-Supervised Learning",
      "summary": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
      "authors": [
        "James B. Simon",
        "Maksis Knutins",
        "Liu Ziyin",
        "Daniel Geisz",
        "Abraham J. Fetterman",
        "Joshua Albrecht"
      ],
      "published": "2023-03-27T17:59:20Z",
      "updated": "2023-05-30T17:25:42Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.15438v2",
      "landing_url": "https://arxiv.org/abs/2303.15438v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.15438"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2304.01448",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.01448v1",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "published": "2023-04-04T01:44:24Z"
    },
    "metadata": {
      "arxiv_id": "2304.01448",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "authors": [
        "Anurag Kumar",
        "Ke Tan",
        "Zhaoheng Ni",
        "Pranay Manocha",
        "Xiaohui Zhang",
        "Ethan Henderson",
        "Buye Xu"
      ],
      "published": "2023-04-04T01:44:24Z",
      "updated": "2023-04-04T01:44:24Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01448v1",
      "landing_url": "https://arxiv.org/abs/2304.01448v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.01448"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2304.01480",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.01480v2",
      "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
      "summary": "Recent works on 3D reconstruction from posed images have demonstrated that direct inference of scene-level 3D geometry without test-time optimization is feasible using deep neural networks, showing remarkable promise and high efficiency. However, the reconstructed geometry, typically represented as a 3D truncated signed distance function (TSDF), is often coarse without fine geometric details. To address this problem, we propose three effective solutions for improving the fidelity of inference-based 3D reconstructions. We first present a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work. We then introduce a depth guidance strategy using multi-view depth estimates to enhance the scene representation and recover more accurate surfaces. Finally, we develop a novel architecture for the final layers of the network, conditioning the output TSDF prediction on high-resolution image features in addition to coarse voxel features, enabling sharper reconstruction of fine details. Our method, FineRecon, produces smooth and highly accurate reconstructions, showing significant improvements across multiple depth and 3D reconstruction metrics.",
      "published": "2023-04-04T02:50:29Z"
    },
    "metadata": {
      "arxiv_id": "2304.01480",
      "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
      "summary": "Recent works on 3D reconstruction from posed images have demonstrated that direct inference of scene-level 3D geometry without test-time optimization is feasible using deep neural networks, showing remarkable promise and high efficiency. However, the reconstructed geometry, typically represented as a 3D truncated signed distance function (TSDF), is often coarse without fine geometric details. To address this problem, we propose three effective solutions for improving the fidelity of inference-based 3D reconstructions. We first present a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work. We then introduce a depth guidance strategy using multi-view depth estimates to enhance the scene representation and recover more accurate surfaces. Finally, we develop a novel architecture for the final layers of the network, conditioning the output TSDF prediction on high-resolution image features in addition to coarse voxel features, enabling sharper reconstruction of fine details. Our method, FineRecon, produces smooth and highly accurate reconstructions, showing significant improvements across multiple depth and 3D reconstruction metrics.",
      "authors": [
        "Noah Stier",
        "Anurag Ranjan",
        "Alex Colburn",
        "Yajie Yan",
        "Liang Yang",
        "Fangchang Ma",
        "Baptiste Angles"
      ],
      "published": "2023-04-04T02:50:29Z",
      "updated": "2023-08-18T22:35:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01480v2",
      "landing_url": "https://arxiv.org/abs/2304.01480v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.01480"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2304.02160",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.02160v1",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "published": "2023-04-04T23:19:53Z"
    },
    "metadata": {
      "arxiv_id": "2304.02160",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "authors": [
        "Ke Chen",
        "Gordon Wichern",
        "François G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2023-04-04T23:19:53Z",
      "updated": "2023-04-04T23:19:53Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02160v1",
      "landing_url": "https://arxiv.org/abs/2304.02160v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02160"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2304.03940",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.03940v1",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "published": "2023-04-08T07:03:01Z"
    },
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2304.07240",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.07240v1",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "published": "2023-04-14T16:43:31Z"
    },
    "metadata": {
      "arxiv_id": "2304.07240",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "authors": [
        "Aaron Hurst",
        "Daniel E. Lucani",
        "Qi Zhang"
      ],
      "published": "2023-04-14T16:43:31Z",
      "updated": "2023-04-14T16:43:31Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.07240v1",
      "landing_url": "https://arxiv.org/abs/2304.07240v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.07240"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2304.08269",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.08269v1",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "published": "2023-04-17T13:26:31Z"
    },
    "metadata": {
      "arxiv_id": "2304.08269",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "authors": [
        "Qian Zhang",
        "Tongda Xu",
        "Yanghao Li",
        "Yan Wang"
      ],
      "published": "2023-04-17T13:26:31Z",
      "updated": "2023-04-17T13:26:31Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08269v1",
      "landing_url": "https://arxiv.org/abs/2304.08269v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08269"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2304.09226",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.09226v1",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "published": "2023-04-18T18:26:56Z"
    },
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2304.09355",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.09355v5",
      "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
      "summary": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \\textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",
      "published": "2023-04-19T00:33:59Z"
    },
    "metadata": {
      "arxiv_id": "2304.09355",
      "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
      "summary": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \\textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",
      "authors": [
        "Ravid Shwartz-Ziv",
        "Yann LeCun"
      ],
      "published": "2023-04-19T00:33:59Z",
      "updated": "2023-11-21T13:12:21Z",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09355v5",
      "landing_url": "https://arxiv.org/abs/2304.09355v5",
      "doi": "https://doi.org/10.48550/arXiv.2304.09355"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2304.12404",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.12404v1",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "published": "2023-04-24T19:33:41Z"
    },
    "metadata": {
      "arxiv_id": "2304.12404",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "authors": [
        "Sandeep Mehta",
        "Darpan Shah",
        "Ravindra Kulkarni",
        "Cornelia Caragea"
      ],
      "published": "2023-04-24T19:33:41Z",
      "updated": "2023-04-24T19:33:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12404v1",
      "landing_url": "https://arxiv.org/abs/2304.12404v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12404"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2305.02528",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.02528v1",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "published": "2023-05-04T03:33:40Z"
    },
    "metadata": {
      "arxiv_id": "2305.02528",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "authors": [
        "Yaqi Shen",
        "Le Hui",
        "Jin Xie",
        "Jian Yang"
      ],
      "published": "2023-05-04T03:33:40Z",
      "updated": "2023-05-04T03:33:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02528v1",
      "landing_url": "https://arxiv.org/abs/2305.02528v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02528"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2305.03568",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.03568v3",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "published": "2023-05-05T14:19:46Z"
    },
    "metadata": {
      "arxiv_id": "2305.03568",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:19:46Z",
      "updated": "2025-05-09T08:19:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03568v3",
      "landing_url": "https://arxiv.org/abs/2305.03568v3",
      "doi": "https://doi.org/10.1016/j.cviu.2025.104362"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2305.06788",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.06788v4",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "published": "2023-05-11T13:23:42Z"
    },
    "metadata": {
      "arxiv_id": "2305.06788",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "authors": [
        "Chih Wei Ling",
        "Cheuk Ting Li"
      ],
      "published": "2023-05-11T13:23:42Z",
      "updated": "2024-01-24T13:44:44Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06788v4",
      "landing_url": "https://arxiv.org/abs/2305.06788v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.06788"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2305.08286",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.08286v1",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "published": "2023-05-15T00:22:02Z"
    },
    "metadata": {
      "arxiv_id": "2305.08286",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "authors": [
        "Chia-Yi Su",
        "Aakash Bansal",
        "Vijayanta Jain",
        "Sepideh Ghanavati",
        "Collin McMillan"
      ],
      "published": "2023-05-15T00:22:02Z",
      "updated": "2023-05-15T00:22:02Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.08286v1",
      "landing_url": "https://arxiv.org/abs/2305.08286v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.08286"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2305.09636",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.09636v1",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "published": "2023-05-16T17:41:25Z"
    },
    "metadata": {
      "arxiv_id": "2305.09636",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "authors": [
        "Zalán Borsos",
        "Matt Sharifi",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-05-16T17:41:25Z",
      "updated": "2023-05-16T17:41:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.09636v1",
      "landing_url": "https://arxiv.org/abs/2305.09636v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.09636"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2305.13651",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13651v2",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "published": "2023-05-23T03:49:41Z"
    },
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2305.13686",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13686v1",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "published": "2023-05-23T04:48:51Z"
    },
    "metadata": {
      "arxiv_id": "2305.13686",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2023-05-23T04:48:51Z",
      "updated": "2023-05-23T04:48:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13686v1",
      "landing_url": "https://arxiv.org/abs/2305.13686v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1441"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2305.15719",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15719v1",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "published": "2023-05-25T05:02:35Z"
    },
    "metadata": {
      "arxiv_id": "2305.15719",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Qiao Tian",
        "Tang Li",
        "Zongyu Yin",
        "Siyuan Feng",
        "Ming Tu",
        "Yuliang Ji",
        "Rui Xia",
        "Mingbo Ma",
        "Xuchen Song",
        "Jitong Chen",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2023-05-25T05:02:35Z",
      "updated": "2023-05-25T05:02:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15719v1",
      "landing_url": "https://arxiv.org/abs/2305.15719v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15719"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2305.16753",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.16753v1",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "published": "2023-05-26T09:06:04Z"
    },
    "metadata": {
      "arxiv_id": "2305.16753",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "authors": [
        "Enoch Hsin-Ho Huang",
        "Rong Chao",
        "Yu Tsao",
        "Chao-Min Wu"
      ],
      "published": "2023-05-26T09:06:04Z",
      "updated": "2023-05-26T09:06:04Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16753v1",
      "landing_url": "https://arxiv.org/abs/2305.16753v1",
      "doi": "https://doi.org/10.1109/TCDS.2023.3275587"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2305.17310",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17310v1",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "published": "2023-05-27T00:05:39Z"
    },
    "metadata": {
      "arxiv_id": "2305.17310",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "authors": [
        "Igor Nunes",
        "Mike Heddes",
        "Pere Vergés",
        "Danny Abraham",
        "Alexander Veidenbaum",
        "Alexandru Nicolau",
        "Tony Givargis"
      ],
      "published": "2023-05-27T00:05:39Z",
      "updated": "2023-05-27T00:05:39Z",
      "categories": [
        "cs.SI",
        "cs.DS",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17310v1",
      "landing_url": "https://arxiv.org/abs/2305.17310v1",
      "doi": "https://doi.org/10.1145/3580305.3599314"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2305.18238",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18238v1",
      "title": "Multi-behavior Self-supervised Learning for Recommendation",
      "summary": "Modern recommender systems often deal with a variety of user interactions, e.g., click, forward, purchase, etc., which requires the underlying recommender engines to fully understand and leverage multi-behavior data from users. Despite recent efforts towards making use of heterogeneous data, multi-behavior recommendation still faces great challenges. Firstly, sparse target signals and noisy auxiliary interactions remain an issue. Secondly, existing methods utilizing self-supervised learning (SSL) to tackle the data sparsity neglect the serious optimization imbalance between the SSL task and the target task. Hence, we propose a Multi-Behavior Self-Supervised Learning (MBSSL) framework together with an adaptive optimization method. Specifically, we devise a behavior-aware graph neural network incorporating the self-attention mechanism to capture behavior multiplicity and dependencies. To increase the robustness to data sparsity under the target behavior and noisy interactions from auxiliary behaviors, we propose a novel self-supervised learning paradigm to conduct node self-discrimination at both inter-behavior and intra-behavior levels. In addition, we develop a customized optimization strategy through hybrid manipulation on gradients to adaptively balance the self-supervised learning task and the main supervised recommendation task. Extensive experiments on five real-world datasets demonstrate the consistent improvements obtained by MBSSL over ten state-of-the art (SOTA) baselines. We release our model implementation at: https://github.com/Scofield666/MBSSL.git.",
      "published": "2023-05-22T15:57:32Z"
    },
    "metadata": {
      "arxiv_id": "2305.18238",
      "title": "Multi-behavior Self-supervised Learning for Recommendation",
      "summary": "Modern recommender systems often deal with a variety of user interactions, e.g., click, forward, purchase, etc., which requires the underlying recommender engines to fully understand and leverage multi-behavior data from users. Despite recent efforts towards making use of heterogeneous data, multi-behavior recommendation still faces great challenges. Firstly, sparse target signals and noisy auxiliary interactions remain an issue. Secondly, existing methods utilizing self-supervised learning (SSL) to tackle the data sparsity neglect the serious optimization imbalance between the SSL task and the target task. Hence, we propose a Multi-Behavior Self-Supervised Learning (MBSSL) framework together with an adaptive optimization method. Specifically, we devise a behavior-aware graph neural network incorporating the self-attention mechanism to capture behavior multiplicity and dependencies. To increase the robustness to data sparsity under the target behavior and noisy interactions from auxiliary behaviors, we propose a novel self-supervised learning paradigm to conduct node self-discrimination at both inter-behavior and intra-behavior levels. In addition, we develop a customized optimization strategy through hybrid manipulation on gradients to adaptively balance the self-supervised learning task and the main supervised recommendation task. Extensive experiments on five real-world datasets demonstrate the consistent improvements obtained by MBSSL over ten state-of-the art (SOTA) baselines. We release our model implementation at: https://github.com/Scofield666/MBSSL.git.",
      "authors": [
        "Jingcao Xu",
        "Chaokun Wang",
        "Cheng Wu",
        "Yang Song",
        "Kai Zheng",
        "Xiaowei Wang",
        "Changping Wang",
        "Guorui Zhou",
        "Kun Gai"
      ],
      "published": "2023-05-22T15:57:32Z",
      "updated": "2023-05-22T15:57:32Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18238v1",
      "landing_url": "https://arxiv.org/abs/2305.18238v1",
      "doi": "https://doi.org/10.1145/3539618.3591734"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2305.19269",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.19269v1",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "published": "2023-05-30T17:59:26Z"
    },
    "metadata": {
      "arxiv_id": "2305.19269",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Luping Liu",
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "published": "2023-05-30T17:59:26Z",
      "updated": "2023-05-30T17:59:26Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19269v1",
      "landing_url": "https://arxiv.org/abs/2305.19269v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19269"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2306.00331",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.00331v1",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "published": "2023-06-01T04:19:57Z"
    },
    "metadata": {
      "arxiv_id": "2306.00331",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "authors": [
        "Pin-Jui Ku",
        "Chao-Han Huck Yang",
        "Sabato Marco Siniscalchi",
        "Chin-Hui Lee"
      ],
      "published": "2023-06-01T04:19:57Z",
      "updated": "2023-06-01T04:19:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00331v1",
      "landing_url": "https://arxiv.org/abs/2306.00331v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1084"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2306.01084",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01084v2",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "published": "2023-06-01T18:51:34Z"
    },
    "metadata": {
      "arxiv_id": "2306.01084",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "authors": [
        "Jiatong Shi",
        "Yun Tang",
        "Hirofumi Inaguma",
        "Hongyu GOng",
        "Juan Pino",
        "Shinji Watanabe"
      ],
      "published": "2023-06-01T18:51:34Z",
      "updated": "2023-06-22T18:34:22Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01084v2",
      "landing_url": "https://arxiv.org/abs/2306.01084v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.01084"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2306.06082",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06082v4",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "published": "2023-05-31T12:24:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.06082",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "authors": [
        "Marcin Przewięźlikowski",
        "Mateusz Pyla",
        "Bartosz Zieliński",
        "Bartłomiej Twardowski",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "published": "2023-05-31T12:24:06Z",
      "updated": "2024-10-19T08:00:13Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06082v4",
      "landing_url": "https://arxiv.org/abs/2306.06082v4",
      "doi": "https://doi.org/10.1016/j.knosys.2024.112572"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2306.06246",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06246v1",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "published": "2023-06-09T20:42:11Z"
    },
    "metadata": {
      "arxiv_id": "2306.06246",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "authors": [
        "Tianyu Huang",
        "Chung Hoon Hong",
        "Carl Wivagg",
        "Kanna Shimizu"
      ],
      "published": "2023-06-09T20:42:11Z",
      "updated": "2023-06-09T20:42:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06246v1",
      "landing_url": "https://arxiv.org/abs/2306.06246v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06246"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2306.06672",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06672v1",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "published": "2023-06-11T12:53:46Z"
    },
    "metadata": {
      "arxiv_id": "2306.06672",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "authors": [
        "William Chen",
        "Xuankai Chang",
        "Yifan Peng",
        "Zhaoheng Ni",
        "Soumi Maiti",
        "Shinji Watanabe"
      ],
      "published": "2023-06-11T12:53:46Z",
      "updated": "2023-06-11T12:53:46Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06672v1",
      "landing_url": "https://arxiv.org/abs/2306.06672v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06672"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2306.07547",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.07547v6",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "published": "2023-06-13T05:38:34Z"
    },
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2306.08920",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.08920v1",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "published": "2023-06-15T07:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2306.10125",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.10125v4",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "published": "2023-06-16T18:23:10Z"
    },
    "metadata": {
      "arxiv_id": "2306.10125",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "authors": [
        "Kexin Zhang",
        "Qingsong Wen",
        "Chaoli Zhang",
        "Rongyao Cai",
        "Ming Jin",
        "Yong Liu",
        "James Zhang",
        "Yuxuan Liang",
        "Guansong Pang",
        "Dongjin Song",
        "Shirui Pan"
      ],
      "published": "2023-06-16T18:23:10Z",
      "updated": "2024-04-08T15:38:59Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10125v4",
      "landing_url": "https://arxiv.org/abs/2306.10125v4",
      "doi": "https://doi.org/10.48550/arXiv.2306.10125"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2306.10521",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.10521v2",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "published": "2023-06-18T10:59:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2306.12785",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.12785v1",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "published": "2023-06-22T10:29:24Z"
    },
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2306.17012",
    "anchor": "speech tokenization",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.17012v2",
      "title": "Evaluation of Virtual Acoustic Environments with Different Acoustic Level of Detail",
      "summary": "Virtual acoustic environments enable the creation and simulation of realistic and ecologically valid daily-life situations with applications in hearing research and audiology. Hereby, reverberant indoor environments play an important role. For real-time applications, simplifications in the room acoustics simulation are required, however, it remains unclear what acoustic level of detail (ALOD) is necessary to capture all perceptually relevant effects. This study investigates the effect of varying ALOD in the simulation of three different real environments, a living room with a coupled kitchen, a pub, and an underground station. ALOD was varied by generating different numbers of image sources for early reflections, or by excluding geometrical room details specific for each environment. The simulations were perceptually evaluated using headphones in comparison to binaural room impulse responses measured with a dummy head in the corresponding real environments, and partly using loudspeakers. The study assessed the perceived overall difference for a pulse, and a speech token. Furthermore, plausibility and externalization were evaluated. The results show that a strong reduction in ALOD is possible while obtaining similar plausibility and externalization as with the dummy head recordings. The number and accuracy of early reflections appear less relevant, provided diffuse late reverberation is appropriately accounted for.",
      "published": "2023-06-29T15:07:13Z"
    },
    "metadata": {
      "arxiv_id": "2306.17012",
      "title": "Evaluation of Virtual Acoustic Environments with Different Acoustic Level of Detail",
      "summary": "Virtual acoustic environments enable the creation and simulation of realistic and ecologically valid daily-life situations with applications in hearing research and audiology. Hereby, reverberant indoor environments play an important role. For real-time applications, simplifications in the room acoustics simulation are required, however, it remains unclear what acoustic level of detail (ALOD) is necessary to capture all perceptually relevant effects. This study investigates the effect of varying ALOD in the simulation of three different real environments, a living room with a coupled kitchen, a pub, and an underground station. ALOD was varied by generating different numbers of image sources for early reflections, or by excluding geometrical room details specific for each environment. The simulations were perceptually evaluated using headphones in comparison to binaural room impulse responses measured with a dummy head in the corresponding real environments, and partly using loudspeakers. The study assessed the perceived overall difference for a pulse, and a speech token. Furthermore, plausibility and externalization were evaluated. The results show that a strong reduction in ALOD is possible while obtaining similar plausibility and externalization as with the dummy head recordings. The number and accuracy of early reflections appear less relevant, provided diffuse late reverberation is appropriately accounted for.",
      "authors": [
        "Stefan Fichna",
        "Steven van de Par",
        "Stephan D. Ewert"
      ],
      "published": "2023-06-29T15:07:13Z",
      "updated": "2023-08-10T14:39:50Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.17012v2",
      "landing_url": "https://arxiv.org/abs/2306.17012v2",
      "doi": "https://doi.org/10.1109/I3DA57090.2023.10289496"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2307.02273",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.02273v4",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "published": "2023-07-05T13:17:14Z"
    },
    "metadata": {
      "arxiv_id": "2307.02273",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-05T13:17:14Z",
      "updated": "2024-01-22T17:37:03Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02273v4",
      "landing_url": "https://arxiv.org/abs/2307.02273v4",
      "doi": "https://doi.org/10.48550/arXiv.2307.02273"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2307.04179",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.04179v1",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "published": "2023-07-09T14:04:58Z"
    },
    "metadata": {
      "arxiv_id": "2307.04179",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "authors": [
        "Wen-Yuan Ting",
        "Syu-Siang Wang",
        "Yu Tsao",
        "Borching Su"
      ],
      "published": "2023-07-09T14:04:58Z",
      "updated": "2023-07-09T14:04:58Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04179v1",
      "landing_url": "https://arxiv.org/abs/2307.04179v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.04179"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2307.04686",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.04686v2",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "published": "2023-07-10T16:42:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2307.05586",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.05586v2",
      "title": "Circularity in Finite Fields and Solutions of the Equations $\\boldsymbol{x^{m}+y^{m}-z^{m}=1}$",
      "summary": "An explicit formula for the number of solutions of the equation in the title is given when a certain condition, depending only on the exponent and the characteristic of the field, holds. This formula improves the one given by the authors in an earlier paper.",
      "published": "2023-07-10T15:46:40Z"
    },
    "metadata": {
      "arxiv_id": "2307.05586",
      "title": "Circularity in Finite Fields and Solutions of the Equations $\\boldsymbol{x^{m}+y^{m}-z^{m}=1}$",
      "summary": "An explicit formula for the number of solutions of the equation in the title is given when a certain condition, depending only on the exponent and the characteristic of the field, holds. This formula improves the one given by the authors in an earlier paper.",
      "authors": [
        "Wen-Fong Ke",
        "Hubert Kiechle"
      ],
      "published": "2023-07-10T15:46:40Z",
      "updated": "2023-07-13T10:40:18Z",
      "categories": [
        "math.NT",
        "math.CO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.05586v2",
      "landing_url": "https://arxiv.org/abs/2307.05586v2",
      "doi": "https://doi.org/10.1016/j.ffa.2024.102467"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2307.06091",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.06091v1",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "published": "2023-07-12T11:32:02Z"
    },
    "metadata": {
      "arxiv_id": "2307.06091",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-12T11:32:02Z",
      "updated": "2023-07-12T11:32:02Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06091v1",
      "landing_url": "https://arxiv.org/abs/2307.06091v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06091"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2307.07303",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.07303v2",
      "title": "Overlaps in Field Generated Circular Planar Nearrings",
      "summary": "We investigate circular planar nearrings constructed from finite fields as well the complex number field using a multiplicative subgroup of order $k$, and characterize the overlaps of the basic graphs which arise in the associated $2$-designs.",
      "published": "2023-07-14T12:28:55Z"
    },
    "metadata": {
      "arxiv_id": "2307.07303",
      "title": "Overlaps in Field Generated Circular Planar Nearrings",
      "summary": "We investigate circular planar nearrings constructed from finite fields as well the complex number field using a multiplicative subgroup of order $k$, and characterize the overlaps of the basic graphs which arise in the associated $2$-designs.",
      "authors": [
        "Wen-Fong Ke",
        "Hubert Kiechle"
      ],
      "published": "2023-07-14T12:28:55Z",
      "updated": "2023-08-05T02:33:41Z",
      "categories": [
        "math.CO",
        "math.NT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07303v2",
      "landing_url": "https://arxiv.org/abs/2307.07303v2",
      "doi": "https://doi.org/10.12958/adm2130"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2307.07940",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.07940v2",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "published": "2023-07-16T04:20:26Z"
    },
    "metadata": {
      "arxiv_id": "2307.07940",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "authors": [
        "Atsushi Shirafuji",
        "Yutaka Watanobe"
      ],
      "published": "2023-07-16T04:20:26Z",
      "updated": "2023-09-11T09:42:37Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07940v2",
      "landing_url": "https://arxiv.org/abs/2307.07940v2",
      "doi": "https://doi.org/10.1145/3634814.3634828"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2307.09899",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.09899v1",
      "title": "The History of Moral Certainty as the Pre-History of Typicality",
      "summary": "This paper investigates the historical origin and ancestors of typicality, which is now a central concept in Boltzmannian Statistical Mechanics and Bohmian Mechanics. Although Ludwig Boltzmann did not use the word typicality, its main idea, namely, that something happens almost always or is valid for almost all cases, plays a crucial role for his explanation of how thermodynamic systems approach equilibrium. At the beginning of the 20th century, the focus on almost always or almost everywhere was fruitful for developing measure theory and probability theory. It was apparently Hugh Everett III who first mentioned typicality in physics in 1957 while searching for a justification of the Born rule in his interpretation of quantum mechanics. The historically closest concept before these developments is moral certainty, which was invented by the medieval French theologian Jean Gerson, and it became a standard concept at least until the Age of Enlightenment, when Jakob Bernoulli proved the Law of Large numbers.",
      "published": "2023-07-19T10:58:01Z"
    },
    "metadata": {
      "arxiv_id": "2307.09899",
      "title": "The History of Moral Certainty as the Pre-History of Typicality",
      "summary": "This paper investigates the historical origin and ancestors of typicality, which is now a central concept in Boltzmannian Statistical Mechanics and Bohmian Mechanics. Although Ludwig Boltzmann did not use the word typicality, its main idea, namely, that something happens almost always or is valid for almost all cases, plays a crucial role for his explanation of how thermodynamic systems approach equilibrium. At the beginning of the 20th century, the focus on almost always or almost everywhere was fruitful for developing measure theory and probability theory. It was apparently Hugh Everett III who first mentioned typicality in physics in 1957 while searching for a justification of the Born rule in his interpretation of quantum mechanics. The historically closest concept before these developments is moral certainty, which was invented by the medieval French theologian Jean Gerson, and it became a standard concept at least until the Age of Enlightenment, when Jakob Bernoulli proved the Law of Large numbers.",
      "authors": [
        "Mario Hubert"
      ],
      "published": "2023-07-19T10:58:01Z",
      "updated": "2023-07-19T10:58:01Z",
      "categories": [
        "physics.hist-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09899v1",
      "landing_url": "https://arxiv.org/abs/2307.09899v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09899"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2307.11794",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.11794v1",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "published": "2023-07-21T02:49:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.11794",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "authors": [
        "Yangpeng Huang",
        "Naixing Feng",
        "Yijun Cai"
      ],
      "published": "2023-07-21T02:49:03Z",
      "updated": "2023-07-21T02:49:03Z",
      "categories": [
        "physics.optics",
        "cs.LG",
        "physics.app-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11794v1",
      "landing_url": "https://arxiv.org/abs/2307.11794v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.11794"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2307.12052",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.12052v1",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "published": "2023-07-22T11:27:05Z"
    },
    "metadata": {
      "arxiv_id": "2307.12052",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "authors": [
        "Mallikarjun Reddy Dorsala",
        "V. N. Sastry",
        "Sudhakar Chapram"
      ],
      "published": "2023-07-22T11:27:05Z",
      "updated": "2023-07-22T11:27:05Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12052v1",
      "landing_url": "https://arxiv.org/abs/2307.12052v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2307.12343",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.12343v1",
      "title": "Self-Supervised Learning for Audio-Based Emotion Recognition",
      "summary": "Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.",
      "published": "2023-07-23T14:40:50Z"
    },
    "metadata": {
      "arxiv_id": "2307.12343",
      "title": "Self-Supervised Learning for Audio-Based Emotion Recognition",
      "summary": "Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.",
      "authors": [
        "Peranut Nimitsurachat",
        "Peter Washington"
      ],
      "published": "2023-07-23T14:40:50Z",
      "updated": "2023-07-23T14:40:50Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12343v1",
      "landing_url": "https://arxiv.org/abs/2307.12343v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12343"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2308.00721",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00721v4",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "published": "2023-07-31T03:56:46Z"
    },
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2308.00725",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00725v1",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "published": "2023-08-01T15:12:36Z"
    },
    "metadata": {
      "arxiv_id": "2308.00725",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2023-08-01T15:12:36Z",
      "updated": "2023-08-01T15:12:36Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00725v1",
      "landing_url": "https://arxiv.org/abs/2308.00725v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00725"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2308.01271",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.01271v1",
      "title": "A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC",
      "summary": "In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.",
      "published": "2023-08-02T16:52:56Z"
    },
    "metadata": {
      "arxiv_id": "2308.01271",
      "title": "A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC",
      "summary": "In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.",
      "authors": [
        "Masoumeh Javanbakhat",
        "Christoph Lippert"
      ],
      "published": "2023-08-02T16:52:56Z",
      "updated": "2023-08-02T16:52:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.01271v1",
      "landing_url": "https://arxiv.org/abs/2308.01271v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.01271"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2308.03332",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.03332v1",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "published": "2023-08-07T06:26:53Z"
    },
    "metadata": {
      "arxiv_id": "2308.03332",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "authors": [
        "Rawad Melhem",
        "Assef Jafar",
        "Riad Hamadeh"
      ],
      "published": "2023-08-07T06:26:53Z",
      "updated": "2023-08-07T06:26:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.03332v1",
      "landing_url": "https://arxiv.org/abs/2308.03332v1",
      "doi": "https://doi.org/10.11916/j.issn.1005-9113.2019044"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2308.06873",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.06873v2",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "published": "2023-08-14T01:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2308.10415",
    "anchor": "dblp_title",
    "search_term": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition.",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.10415v1",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "published": "2023-08-21T01:52:01Z"
    },
    "metadata": {
      "arxiv_id": "2308.10415",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "authors": [
        "Hakan Erdogan",
        "Scott Wisdom",
        "Xuankai Chang",
        "Zalán Borsos",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "John R. Hershey"
      ],
      "published": "2023-08-21T01:52:01Z",
      "updated": "2023-08-21T01:52:01Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.10415v1",
      "landing_url": "https://arxiv.org/abs/2308.10415v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.10415"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition."
      }
    ]
  },
  {
    "arxiv_id": "2308.14267",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.14267v1",
      "title": "Unleash Model Potential: Bootstrapped Meta Self-supervised Learning",
      "summary": "The long-term goal of machine learning is to learn general visual representations from a small amount of data without supervision, mimicking three advantages of human cognition: i) no need for labels, ii) robustness to data scarcity, and iii) learning from experience. Self-supervised learning and meta-learning are two promising techniques to achieve this goal, but they both only partially capture the advantages and fail to address all the problems. Self-supervised learning struggles to overcome the drawbacks of data scarcity, while ignoring prior knowledge that can facilitate learning and generalization. Meta-learning relies on supervised information and suffers from a bottleneck of insufficient learning. To address these issues, we propose a novel Bootstrapped Meta Self-Supervised Learning (BMSSL) framework that aims to simulate the human learning process. We first analyze the close relationship between meta-learning and self-supervised learning. Based on this insight, we reconstruct tasks to leverage the strengths of both paradigms, achieving advantages i and ii. Moreover, we employ a bi-level optimization framework that alternates between solving specific tasks with a learned ability (first level) and improving this ability (second level), attaining advantage iii. To fully harness its power, we introduce a bootstrapped target based on meta-gradient to make the model its own teacher. We validate the effectiveness of our approach with comprehensive theoretical and empirical study.",
      "published": "2023-08-28T02:49:07Z"
    },
    "metadata": {
      "arxiv_id": "2308.14267",
      "title": "Unleash Model Potential: Bootstrapped Meta Self-supervised Learning",
      "summary": "The long-term goal of machine learning is to learn general visual representations from a small amount of data without supervision, mimicking three advantages of human cognition: i) no need for labels, ii) robustness to data scarcity, and iii) learning from experience. Self-supervised learning and meta-learning are two promising techniques to achieve this goal, but they both only partially capture the advantages and fail to address all the problems. Self-supervised learning struggles to overcome the drawbacks of data scarcity, while ignoring prior knowledge that can facilitate learning and generalization. Meta-learning relies on supervised information and suffers from a bottleneck of insufficient learning. To address these issues, we propose a novel Bootstrapped Meta Self-Supervised Learning (BMSSL) framework that aims to simulate the human learning process. We first analyze the close relationship between meta-learning and self-supervised learning. Based on this insight, we reconstruct tasks to leverage the strengths of both paradigms, achieving advantages i and ii. Moreover, we employ a bi-level optimization framework that alternates between solving specific tasks with a learned ability (first level) and improving this ability (second level), attaining advantage iii. To fully harness its power, we introduce a bootstrapped target based on meta-gradient to make the model its own teacher. We validate the effectiveness of our approach with comprehensive theoretical and empirical study.",
      "authors": [
        "Jingyao Wang",
        "Zeen Song",
        "Wenwen Qiang",
        "Changwen Zheng"
      ],
      "published": "2023-08-28T02:49:07Z",
      "updated": "2023-08-28T02:49:07Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.14267v1",
      "landing_url": "https://arxiv.org/abs/2308.14267v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.14267"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2308.16692",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.16692v2",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "published": "2023-08-31T12:53:09Z"
    },
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2309.00169",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00169v3",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "published": "2023-08-31T23:26:10Z"
    },
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "dblp_title",
        "search_term": "RepCodec: A Speech Representation Codec for Speech Tokenization."
      }
    ]
  },
  {
    "arxiv_id": "2309.02432",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.02432v1",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "published": "2023-09-05T17:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2309.06649",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.06649v1",
      "title": "Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis",
      "summary": "Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.",
      "published": "2023-09-13T00:21:04Z"
    },
    "metadata": {
      "arxiv_id": "2309.06649",
      "title": "Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis",
      "summary": "Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.",
      "authors": [
        "Jordie Shier",
        "Franco Caspe",
        "Andrew Robertson",
        "Mark Sandler",
        "Charalampos Saitis",
        "Andrew McPherson"
      ],
      "published": "2023-09-13T00:21:04Z",
      "updated": "2023-09-13T00:21:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06649v1",
      "landing_url": "https://arxiv.org/abs/2309.06649v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06649"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2309.07416",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07416v4",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "published": "2023-09-14T04:04:50Z"
    },
    "metadata": {
      "arxiv_id": "2309.07416",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "authors": [
        "Anton Ratnarajah",
        "Shi-Xiong Zhang",
        "Dong Yu"
      ],
      "published": "2023-09-14T04:04:50Z",
      "updated": "2024-11-25T03:50:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07416v4",
      "landing_url": "https://arxiv.org/abs/2309.07416v4",
      "doi": "https://doi.org/10.48550/arXiv.2309.07416"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.07937",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07937v3",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "published": "2023-09-14T03:13:18Z"
    },
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.09630",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09630v1",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "published": "2023-09-18T10:05:41Z"
    },
    "metadata": {
      "arxiv_id": "2309.09630",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "authors": [
        "Julitta Bartolewska",
        "Stanisław Kacprzak",
        "Konrad Kowalczyk"
      ],
      "published": "2023-09-18T10:05:41Z",
      "updated": "2023-09-18T10:05:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09630v1",
      "landing_url": "https://arxiv.org/abs/2309.09630v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10632"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2309.09920",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09920v1",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "published": "2023-09-18T16:34:40Z"
    },
    "metadata": {
      "arxiv_id": "2309.09920",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "authors": [
        "Danilo de Oliveira",
        "Timo Gerkmann"
      ],
      "published": "2023-09-18T16:34:40Z",
      "updated": "2023-09-18T16:34:40Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09920v1",
      "landing_url": "https://arxiv.org/abs/2309.09920v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2309.10379",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10379v1",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "published": "2023-09-19T07:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2309.10379",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "authors": [
        "Jiahui Pan",
        "Shulin He",
        "Tianci Wu",
        "Hui Zhang",
        "Xueliang Zhang"
      ],
      "published": "2023-09-19T07:27:38Z",
      "updated": "2023-09-19T07:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10379v1",
      "landing_url": "https://arxiv.org/abs/2309.10379v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10379"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2309.10818",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10818v3",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "published": "2023-09-19T17:59:54Z"
    },
    "metadata": {
      "arxiv_id": "2309.10818",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "authors": [
        "Zhiqiang Shen",
        "Tianhua Tao",
        "Liqun Ma",
        "Willie Neiswanger",
        "Zhengzhong Liu",
        "Hongyi Wang",
        "Bowen Tan",
        "Joel Hestness",
        "Natalia Vassilieva",
        "Daria Soboleva",
        "Eric Xing"
      ],
      "published": "2023-09-19T17:59:54Z",
      "updated": "2024-05-09T13:56:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10818v3",
      "landing_url": "https://arxiv.org/abs/2309.10818v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.10818"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2309.11977",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11977v3",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "published": "2023-09-21T11:22:22Z"
    },
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.13860",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.13860v2",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "published": "2023-09-25T04:07:34Z"
    },
    "metadata": {
      "arxiv_id": "2309.13860",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Yakun Song",
        "Zhikang Niu",
        "Xie Chen"
      ],
      "published": "2023-09-25T04:07:34Z",
      "updated": "2023-09-29T06:48:11Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13860v2",
      "landing_url": "https://arxiv.org/abs/2309.13860v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.13860"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2309.14324",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14324v2",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "published": "2023-09-25T17:52:09Z"
    },
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.14392",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14392v1",
      "title": "Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction",
      "summary": "Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equity in medical AI applications.",
      "published": "2023-09-25T11:07:25Z"
    },
    "metadata": {
      "arxiv_id": "2309.14392",
      "title": "Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction",
      "summary": "Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equity in medical AI applications.",
      "authors": [
        "Yuning Du",
        "Yuyang Xue",
        "Rohan Dharmakumar",
        "Sotirios A. Tsaftaris"
      ],
      "published": "2023-09-25T11:07:25Z",
      "updated": "2023-09-25T11:07:25Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14392v1",
      "landing_url": "https://arxiv.org/abs/2309.14392v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.14392"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2310.00559",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.00559v1",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "published": "2023-10-01T03:29:21Z"
    },
    "metadata": {
      "arxiv_id": "2310.00559",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "authors": [
        "Chen-Hsiu Huang",
        "Ja-Ling Wu"
      ],
      "published": "2023-10-01T03:29:21Z",
      "updated": "2023-10-01T03:29:21Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00559v1",
      "landing_url": "https://arxiv.org/abs/2310.00559v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00559"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2310.02720",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.02720v2",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "published": "2023-10-04T10:52:13Z"
    },
    "metadata": {
      "arxiv_id": "2310.02720",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "authors": [
        "Jiatong Shi",
        "Hirofumi Inaguma",
        "Xutai Ma",
        "Ilia Kulikov",
        "Anna Sun"
      ],
      "published": "2023-10-04T10:52:13Z",
      "updated": "2024-01-30T08:52:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02720v2",
      "landing_url": "https://arxiv.org/abs/2310.02720v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.02720"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.03975",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.03975v1",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "published": "2023-10-06T02:19:09Z"
    },
    "metadata": {
      "arxiv_id": "2310.03975",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "authors": [
        "Takashi Maekaku",
        "Jiatong Shi",
        "Xuankai Chang",
        "Yuya Fujita",
        "Shinji Watanabe"
      ],
      "published": "2023-10-06T02:19:09Z",
      "updated": "2023-10-06T02:19:09Z",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03975v1",
      "landing_url": "https://arxiv.org/abs/2310.03975v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.03975"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.07161",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.07161v3",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "published": "2023-10-11T03:19:22Z"
    },
    "metadata": {
      "arxiv_id": "2310.07161",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "authors": [
        "Joseph Konan",
        "Shikhar Agnihotri",
        "Ojas Bhargave",
        "Shuo Han",
        "Yunyang Zeng",
        "Ankit Shah",
        "Bhiksha Raj"
      ],
      "published": "2023-10-11T03:19:22Z",
      "updated": "2024-08-01T11:37:16Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07161v3",
      "landing_url": "https://arxiv.org/abs/2310.07161v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.07161"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.07246",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.07246v2",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "published": "2023-10-11T07:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2310.07246",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "authors": [
        "Xinfa Zhu",
        "Yuanjun Lv",
        "Yi Lei",
        "Tao Li",
        "Wendi He",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-10-11T07:23:27Z",
      "updated": "2023-10-12T05:49:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07246v2",
      "landing_url": "https://arxiv.org/abs/2310.07246v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.07246"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation."
      }
    ]
  },
  {
    "arxiv_id": "2310.08696",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08696v1",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "published": "2023-10-12T20:02:07Z"
    },
    "metadata": {
      "arxiv_id": "2310.08696",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "authors": [
        "Weiqing Wang",
        "Ming Li"
      ],
      "published": "2023-10-12T20:02:07Z",
      "updated": "2023-10-12T20:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08696v1",
      "landing_url": "https://arxiv.org/abs/2310.08696v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08696"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2310.08981",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08981v3",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "published": "2023-10-13T09:57:09Z"
    },
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2310.09382",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09382v1",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "published": "2023-10-13T20:03:18Z"
    },
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2310.10803",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10803v3",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "published": "2023-10-16T20:05:36Z"
    },
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.10922",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10922v1",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "published": "2023-10-17T01:31:59Z"
    },
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.14580",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14580v4",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "published": "2023-10-23T05:38:41Z"
    },
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Acoustic BPE for Speech Generation with Discrete Tokens."
      }
    ]
  },
  {
    "arxiv_id": "2310.14858",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14858v2",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "published": "2023-10-23T12:28:21Z"
    },
    "metadata": {
      "arxiv_id": "2310.14858",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "authors": [
        "Patrick Holzer",
        "Tania Jacob",
        "Shubham Kavane"
      ],
      "published": "2023-10-23T12:28:21Z",
      "updated": "2023-11-17T10:35:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14858v2",
      "landing_url": "https://arxiv.org/abs/2310.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14858"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2310.15399",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.15399v3",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "published": "2023-10-23T23:01:33Z"
    },
    "metadata": {
      "arxiv_id": "2310.15399",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "authors": [
        "Ayako Yamamoto",
        "Toshio Irino",
        "Fuki Miyazaki",
        "Honoka Tamaru"
      ],
      "published": "2023-10-23T23:01:33Z",
      "updated": "2024-03-14T02:14:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15399v3",
      "landing_url": "https://arxiv.org/abs/2310.15399v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.15399"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.16550",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.16550v1",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "published": "2023-10-25T11:04:32Z"
    },
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.16858",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.16858v2",
      "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
      "summary": "This paper targets interactive object-level editing (e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame. We propose an extension to the original dynamic NeRF by incorporating a hybrid semantic feature distillation to maintain spatial-temporal consistency after editing. In addition, we design Recursive Selection Refinement that significantly boosts object segmentation accuracy within a dynamic NeRF to aid the editing process. Moreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. Project page: https://patrickddj.github.io/4D-Editor",
      "published": "2023-10-25T02:20:03Z"
    },
    "metadata": {
      "arxiv_id": "2310.16858",
      "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
      "summary": "This paper targets interactive object-level editing (e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame. We propose an extension to the original dynamic NeRF by incorporating a hybrid semantic feature distillation to maintain spatial-temporal consistency after editing. In addition, we design Recursive Selection Refinement that significantly boosts object segmentation accuracy within a dynamic NeRF to aid the editing process. Moreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. Project page: https://patrickddj.github.io/4D-Editor",
      "authors": [
        "Dadong Jiang",
        "Zhihui Ke",
        "Xiaobo Zhou",
        "Xidong Shi"
      ],
      "published": "2023-10-25T02:20:03Z",
      "updated": "2023-11-06T03:38:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16858v2",
      "landing_url": "https://arxiv.org/abs/2310.16858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.16858"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2310.17142",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.17142v1",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "published": "2023-10-26T04:29:27Z"
    },
    "metadata": {
      "arxiv_id": "2310.17142",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "authors": [
        "Sania Gul",
        "Muhammad Salman Khan",
        "Muhammad Fazeel"
      ],
      "published": "2023-10-26T04:29:27Z",
      "updated": "2023-10-26T04:29:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.17142v1",
      "landing_url": "https://arxiv.org/abs/2310.17142v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.17142"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2311.01635",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.01635v1",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "published": "2023-11-02T23:12:42Z"
    },
    "metadata": {
      "arxiv_id": "2311.01635",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "authors": [
        "Cheng Luo",
        "Tianle Zhong",
        "Geoffrey Fox"
      ],
      "published": "2023-11-02T23:12:42Z",
      "updated": "2023-11-02T23:12:42Z",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01635v1",
      "landing_url": "https://arxiv.org/abs/2311.01635v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01635"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2311.02733",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.02733v2",
      "title": "AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos",
      "summary": "Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multimodal models that can exploit both pieces of information simultaneously. However, previous methods mainly adopt unimodal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multimodal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multimodal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.",
      "published": "2023-11-05T18:35:03Z"
    },
    "metadata": {
      "arxiv_id": "2311.02733",
      "title": "AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos",
      "summary": "Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multimodal models that can exploit both pieces of information simultaneously. However, previous methods mainly adopt unimodal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multimodal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multimodal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.",
      "authors": [
        "Sahibzada Adil Shahzad",
        "Ammarah Hashmi",
        "Yan-Tsung Peng",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "published": "2023-11-05T18:35:03Z",
      "updated": "2025-11-21T05:23:01Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02733v2",
      "landing_url": "https://arxiv.org/abs/2311.02733v2",
      "doi": "https://doi.org/10.1109/THMS.2025.3618409"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2311.02898",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.02898v2",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "published": "2023-11-06T06:13:39Z"
    },
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2311.04526",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.04526v1",
      "title": "Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech",
      "summary": "Self-supervised pre-trained speech models were shown effective for various downstream speech processing tasks. Since they are mainly pre-trained to map input speech to pseudo-labels, the resulting representations are only effective for the type of pre-train data used, either clean or mixture speech. With the idea of selective auditory attention, we propose a novel pre-training solution called Selective-HuBERT, or SHuBERT, which learns the selective extraction of target speech representations from either clean or mixture speech. Specifically, SHuBERT is trained to predict pseudo labels of a target speaker, conditioned on an enrolled speech from the target speaker. By doing so, SHuBERT is expected to selectively attend to the target speaker in a complex acoustic environment, thus benefiting various downstream tasks. We further introduce a dual-path training strategy and use the cross-correlation constraint between the two branches to encourage the model to generate noise-invariant representation. Experiments on SUPERB benchmark and LibriMix dataset demonstrate the universality and noise-robustness of SHuBERT. Furthermore, we find that our high-quality representation can be easily integrated with conventional supervised learning methods to achieve significant performance, even under extremely low-resource labeled data.",
      "published": "2023-11-08T08:28:25Z"
    },
    "metadata": {
      "arxiv_id": "2311.04526",
      "title": "Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech",
      "summary": "Self-supervised pre-trained speech models were shown effective for various downstream speech processing tasks. Since they are mainly pre-trained to map input speech to pseudo-labels, the resulting representations are only effective for the type of pre-train data used, either clean or mixture speech. With the idea of selective auditory attention, we propose a novel pre-training solution called Selective-HuBERT, or SHuBERT, which learns the selective extraction of target speech representations from either clean or mixture speech. Specifically, SHuBERT is trained to predict pseudo labels of a target speaker, conditioned on an enrolled speech from the target speaker. By doing so, SHuBERT is expected to selectively attend to the target speaker in a complex acoustic environment, thus benefiting various downstream tasks. We further introduce a dual-path training strategy and use the cross-correlation constraint between the two branches to encourage the model to generate noise-invariant representation. Experiments on SUPERB benchmark and LibriMix dataset demonstrate the universality and noise-robustness of SHuBERT. Furthermore, we find that our high-quality representation can be easily integrated with conventional supervised learning methods to achieve significant performance, even under extremely low-resource labeled data.",
      "authors": [
        "Jingru Lin",
        "Meng Ge",
        "Wupeng Wang",
        "Haizhou Li",
        "Mengling Feng"
      ],
      "published": "2023-11-08T08:28:25Z",
      "updated": "2023-11-08T08:28:25Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04526v1",
      "landing_url": "https://arxiv.org/abs/2311.04526v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.04526"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2311.04534",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.04534v2",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "published": "2023-11-08T08:45:14Z"
    },
    "metadata": {
      "arxiv_id": "2311.04534",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "published": "2023-11-08T08:45:14Z",
      "updated": "2024-02-05T02:42:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04534v2",
      "landing_url": "https://arxiv.org/abs/2311.04534v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.04534"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2311.10319",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10319v6",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "published": "2023-11-17T04:04:29Z"
    },
    "metadata": {
      "arxiv_id": "2311.10319",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "authors": [
        "Pranav Singh",
        "Raviteja Chukkapalli",
        "Shravan Chaudhari",
        "Luoyao Chen",
        "Mei Chen",
        "Jinqian Pan",
        "Craig Smuda",
        "Jacopo Cirrone"
      ],
      "published": "2023-11-17T04:04:29Z",
      "updated": "2024-05-17T17:42:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10319v6",
      "landing_url": "https://arxiv.org/abs/2311.10319v6",
      "doi": "https://doi.org/10.1038/s41598-024-61822-9"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2311.11171",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.11171v2",
      "title": "LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation",
      "summary": "This work proposes a non-iterative, scalable, and statistically optimal way to triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that minimize the reprojection ($L_2$) error, LOSTU will still provide the maximum likelihood estimate when there are errors in camera pose or parameters. This generic framework is used to contextualize other triangulation methods like the direct linear transform (DLT) or the midpoint. Synthetic experiments show that LOSTU can be substantially faster than using uncertainty-aware Levenberg-Marquardt (or similar) optimization schemes, while providing results of comparable precision. Finally, LOSTU is implemented in sequential reconstruction in conjunction with uncertainty-aware pose estimation, where it yields better reconstruction metrics.",
      "published": "2023-11-18T21:27:04Z"
    },
    "metadata": {
      "arxiv_id": "2311.11171",
      "title": "LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation",
      "summary": "This work proposes a non-iterative, scalable, and statistically optimal way to triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that minimize the reprojection ($L_2$) error, LOSTU will still provide the maximum likelihood estimate when there are errors in camera pose or parameters. This generic framework is used to contextualize other triangulation methods like the direct linear transform (DLT) or the midpoint. Synthetic experiments show that LOSTU can be substantially faster than using uncertainty-aware Levenberg-Marquardt (or similar) optimization schemes, while providing results of comparable precision. Finally, LOSTU is implemented in sequential reconstruction in conjunction with uncertainty-aware pose estimation, where it yields better reconstruction metrics.",
      "authors": [
        "Sébastien Henry",
        "John A. Christian"
      ],
      "published": "2023-11-18T21:27:04Z",
      "updated": "2024-03-18T02:12:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11171v2",
      "landing_url": "https://arxiv.org/abs/2311.11171v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.11171"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2311.11335",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.11335v1",
      "title": "Self-Distilled Representation Learning for Time Series",
      "summary": "Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.",
      "published": "2023-11-19T14:34:01Z"
    },
    "metadata": {
      "arxiv_id": "2311.11335",
      "title": "Self-Distilled Representation Learning for Time Series",
      "summary": "Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.",
      "authors": [
        "Felix Pieper",
        "Konstantin Ditschuneit",
        "Martin Genzel",
        "Alexandra Lindt",
        "Johannes Otterbach"
      ],
      "published": "2023-11-19T14:34:01Z",
      "updated": "2023-11-19T14:34:01Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11335v1",
      "landing_url": "https://arxiv.org/abs/2311.11335v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.11335"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2311.11863",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.11863v2",
      "title": "GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding",
      "summary": "Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \\textit{i.e.}, the \"label rendering\" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\\%, 11.76\\%, and 8.47\\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",
      "published": "2023-11-20T15:59:41Z"
    },
    "metadata": {
      "arxiv_id": "2311.11863",
      "title": "GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding",
      "summary": "Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \\textit{i.e.}, the \"label rendering\" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\\%, 11.76\\%, and 8.47\\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",
      "authors": [
        "Hao Li",
        "Dingwen Zhang",
        "Yalun Dai",
        "Nian Liu",
        "Lechao Cheng",
        "Jingfeng Li",
        "Jingdong Wang",
        "Junwei Han"
      ],
      "published": "2023-11-20T15:59:41Z",
      "updated": "2024-04-07T07:37:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11863v2",
      "landing_url": "https://arxiv.org/abs/2311.11863v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.11863"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2311.13588",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.13588v1",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "published": "2023-11-22T18:49:00Z"
    },
    "metadata": {
      "arxiv_id": "2311.13588",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "authors": [
        "Wei Qiu",
        "Marcin Copik",
        "Yun Wang",
        "Alexandru Calotoiu",
        "Torsten Hoefler"
      ],
      "published": "2023-11-22T18:49:00Z",
      "updated": "2023-11-22T18:49:00Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.13588v1",
      "landing_url": "https://arxiv.org/abs/2311.13588v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.13588"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2311.16361",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.16361v2",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "published": "2023-11-27T22:52:45Z"
    },
    "metadata": {
      "arxiv_id": "2311.16361",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "authors": [
        "Weicheng Zhu",
        "Sheng Liu",
        "Carlos Fernandez-Granda",
        "Narges Razavian"
      ],
      "published": "2023-11-27T22:52:45Z",
      "updated": "2023-11-29T23:19:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16361v2",
      "landing_url": "https://arxiv.org/abs/2311.16361v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.16361"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2311.17264",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.17264v1",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "published": "2023-11-28T22:54:33Z"
    },
    "metadata": {
      "arxiv_id": "2311.17264",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "authors": [
        "Marina Zhang",
        "Owen Vallis",
        "Aysegul Bumin",
        "Tanay Vakharia",
        "Elie Bursztein"
      ],
      "published": "2023-11-28T22:54:33Z",
      "updated": "2023-11-28T22:54:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17264v1",
      "landing_url": "https://arxiv.org/abs/2311.17264v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17264"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2311.17790",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.17790v1",
      "title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition",
      "summary": "Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech. However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process. In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR. To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals. During training, the SE frontend is randomly selected from a pool of models. We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset. The experimental results demonstrate a significant relative reduction in word error rate (WER).",
      "published": "2023-11-29T16:35:13Z"
    },
    "metadata": {
      "arxiv_id": "2311.17790",
      "title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition",
      "summary": "Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech. However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process. In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR. To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals. During training, the SE frontend is randomly selected from a pool of models. We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset. The experimental results demonstrate a significant relative reduction in word error rate (WER).",
      "authors": [
        "Dongning Yang",
        "Wei Wang",
        "Yanmin Qian"
      ],
      "published": "2023-11-29T16:35:13Z",
      "updated": "2023-11-29T16:35:13Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17790v1",
      "landing_url": "https://arxiv.org/abs/2311.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17790"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2312.01187",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.01187v4",
      "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
      "summary": "Existing data augmentation in self-supervised learning, while diverse, fails to preserve the inherent structure of natural images. This results in distorted augmented samples with compromised semantic information, ultimately impacting downstream performance. To overcome this limitation, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel data augmentation technique based on Neural Style Transfer. SASSL decouples semantic and stylistic attributes in images and applies transformations exclusively to their style while preserving content, generating diverse samples that better retain semantic information. SASSL boosts top-1 image classification accuracy on ImageNet by up to 2 percentage points compared to established self-supervised methods like MoCo, SimCLR, and BYOL, while achieving superior transfer learning performance across various datasets. Because SASSL can be performed asynchronously as part of the data augmentation pipeline, these performance impacts can be obtained with no change in pretraining throughput.",
      "published": "2023-12-02T17:25:30Z"
    },
    "metadata": {
      "arxiv_id": "2312.01187",
      "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
      "summary": "Existing data augmentation in self-supervised learning, while diverse, fails to preserve the inherent structure of natural images. This results in distorted augmented samples with compromised semantic information, ultimately impacting downstream performance. To overcome this limitation, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel data augmentation technique based on Neural Style Transfer. SASSL decouples semantic and stylistic attributes in images and applies transformations exclusively to their style while preserving content, generating diverse samples that better retain semantic information. SASSL boosts top-1 image classification accuracy on ImageNet by up to 2 percentage points compared to established self-supervised methods like MoCo, SimCLR, and BYOL, while achieving superior transfer learning performance across various datasets. Because SASSL can be performed asynchronously as part of the data augmentation pipeline, these performance impacts can be obtained with no change in pretraining throughput.",
      "authors": [
        "Renan A. Rojas-Gomez",
        "Karan Singhal",
        "Ali Etemad",
        "Alex Bijamov",
        "Warren R. Morningstar",
        "Philip Andrew Mansfield"
      ],
      "published": "2023-12-02T17:25:30Z",
      "updated": "2024-11-02T17:08:45Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.01187v4",
      "landing_url": "https://arxiv.org/abs/2312.01187v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.01187"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2312.02147",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.02147v2",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "published": "2023-12-04T18:59:20Z"
    },
    "metadata": {
      "arxiv_id": "2312.02147",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "authors": [
        "Sucheng Ren",
        "Zeyu Wang",
        "Hongru Zhu",
        "Junfei Xiao",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "published": "2023-12-04T18:59:20Z",
      "updated": "2024-07-05T05:07:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02147v2",
      "landing_url": "https://arxiv.org/abs/2312.02147v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.02147"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2312.03406",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.03406v4",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "published": "2023-12-06T10:42:40Z"
    },
    "metadata": {
      "arxiv_id": "2312.03406",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "authors": [
        "Chao Chen",
        "Tian Zhou",
        "Yanjun Zhao",
        "Hui Liu",
        "Liang Sun",
        "Rong Jin"
      ],
      "published": "2023-12-06T10:42:40Z",
      "updated": "2025-05-18T09:11:15Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03406v4",
      "landing_url": "https://arxiv.org/abs/2312.03406v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.03406"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2312.03782",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.03782v2",
      "title": "Novel class discovery meets foundation models for 3D semantic segmentation",
      "summary": "The task of Novel Class Discovery (NCD) in semantic segmentation entails training a model able to accurately segment unlabelled (novel) classes, relying on the available supervision from annotated (base) classes. Although extensively investigated in 2D image data, the extension of the NCD task to the domain of 3D point clouds represents a pioneering effort, characterized by assumptions and challenges that are not present in the 2D case. This paper represents an advancement in the analysis of point cloud data in four directions. Firstly, it introduces the novel task of NCD for point cloud semantic segmentation. Secondly, it demonstrates that directly transposing the only existing NCD method for 2D image semantic segmentation to 3D data yields suboptimal results. Thirdly, a new NCD approach based on online clustering, uncertainty estimation, and semantic distillation is presented. Lastly, a novel evaluation protocol is proposed to rigorously assess the performance of NCD in point cloud semantic segmentation. Through comprehensive evaluations on the SemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates substantial superiority of the proposed method over the considered baselines.",
      "published": "2023-12-06T09:59:30Z"
    },
    "metadata": {
      "arxiv_id": "2312.03782",
      "title": "Novel class discovery meets foundation models for 3D semantic segmentation",
      "summary": "The task of Novel Class Discovery (NCD) in semantic segmentation entails training a model able to accurately segment unlabelled (novel) classes, relying on the available supervision from annotated (base) classes. Although extensively investigated in 2D image data, the extension of the NCD task to the domain of 3D point clouds represents a pioneering effort, characterized by assumptions and challenges that are not present in the 2D case. This paper represents an advancement in the analysis of point cloud data in four directions. Firstly, it introduces the novel task of NCD for point cloud semantic segmentation. Secondly, it demonstrates that directly transposing the only existing NCD method for 2D image semantic segmentation to 3D data yields suboptimal results. Thirdly, a new NCD approach based on online clustering, uncertainty estimation, and semantic distillation is presented. Lastly, a novel evaluation protocol is proposed to rigorously assess the performance of NCD in point cloud semantic segmentation. Through comprehensive evaluations on the SemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates substantial superiority of the proposed method over the considered baselines.",
      "authors": [
        "Luigi Riz",
        "Cristiano Saltori",
        "Yiming Wang",
        "Elisa Ricci",
        "Fabio Poiesi"
      ],
      "published": "2023-12-06T09:59:30Z",
      "updated": "2024-08-20T09:13:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03782v2",
      "landing_url": "https://arxiv.org/abs/2312.03782v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.03782"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2312.08309",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08309v1",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "published": "2023-12-13T17:27:17Z"
    },
    "metadata": {
      "arxiv_id": "2312.08309",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "authors": [
        "Sabbir Ahmed",
        "Md Nahiduzzaman",
        "Tariqul Islam",
        "Faisal Haque Bappy",
        "Tarannum Shaila Zaman",
        "Raiful Hasan"
      ],
      "published": "2023-12-13T17:27:17Z",
      "updated": "2023-12-13T17:27:17Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08309v1",
      "landing_url": "https://arxiv.org/abs/2312.08309v1",
      "doi": "https://doi.org/10.1109/CCNC51664.2024.10454894"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2312.08676",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08676v2",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "published": "2023-12-14T06:26:55Z"
    },
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2312.09128",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09128v2",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "published": "2023-12-14T17:01:02Z"
    },
    "metadata": {
      "arxiv_id": "2312.09128",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "authors": [
        "Ting Pan",
        "Lulu Tang",
        "Xinlong Wang",
        "Shiguang Shan"
      ],
      "published": "2023-12-14T17:01:02Z",
      "updated": "2024-07-17T04:34:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09128v2",
      "landing_url": "https://arxiv.org/abs/2312.09128v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09128"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2312.09469",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09469v1",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "published": "2023-09-29T18:35:52Z"
    },
    "metadata": {
      "arxiv_id": "2312.09469",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "authors": [
        "Isotta Landi",
        "Eugenia Alleva",
        "Alissa A. Valentine",
        "Lauren A. Lepow",
        "Alexander W. Charney"
      ],
      "published": "2023-09-29T18:35:52Z",
      "updated": "2023-09-29T18:35:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09469v1",
      "landing_url": "https://arxiv.org/abs/2312.09469v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09469"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2312.14685",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.14685v1",
      "title": "Kernel Heterogeneity Improves Sparseness of Natural Images Representations",
      "summary": "Both biological and artificial neural networks inherently balance their performance with their operational cost, which balances their computational abilities. Typically, an efficient neuromorphic neural network is one that learns representations that reduce the redundancies and dimensionality of its input. This is for instance achieved in sparse coding, and sparse representations derived from natural images yield representations that are heterogeneous, both in their sampling of input features and in the variance of those features. Here, we investigated the connection between natural images' structure, particularly oriented features, and their corresponding sparse codes. We showed that representations of input features scattered across multiple levels of variance substantially improve the sparseness and resilience of sparse codes, at the cost of reconstruction performance. This echoes the structure of the model's input, allowing to account for the heterogeneously aleatoric structures of natural images. We demonstrate that learning kernel from natural images produces heterogeneity by balancing between approximate and dense representations, which improves all reconstruction metrics. Using a parametrized control of the kernels' heterogeneity used by a convolutional sparse coding algorithm, we show that heterogeneity emphasizes sparseness, while homogeneity improves representation granularity. In a broader context, these encoding strategy can serve as inputs to deep convolutional neural networks. We prove that such variance-encoded sparse image datasets enhance computational efficiency, emphasizing the benefits of kernel heterogeneity to leverage naturalistic and variant input structures and possible applications to improve the throughput of neuromorphic hardware.",
      "published": "2023-12-22T13:36:27Z"
    },
    "metadata": {
      "arxiv_id": "2312.14685",
      "title": "Kernel Heterogeneity Improves Sparseness of Natural Images Representations",
      "summary": "Both biological and artificial neural networks inherently balance their performance with their operational cost, which balances their computational abilities. Typically, an efficient neuromorphic neural network is one that learns representations that reduce the redundancies and dimensionality of its input. This is for instance achieved in sparse coding, and sparse representations derived from natural images yield representations that are heterogeneous, both in their sampling of input features and in the variance of those features. Here, we investigated the connection between natural images' structure, particularly oriented features, and their corresponding sparse codes. We showed that representations of input features scattered across multiple levels of variance substantially improve the sparseness and resilience of sparse codes, at the cost of reconstruction performance. This echoes the structure of the model's input, allowing to account for the heterogeneously aleatoric structures of natural images. We demonstrate that learning kernel from natural images produces heterogeneity by balancing between approximate and dense representations, which improves all reconstruction metrics. Using a parametrized control of the kernels' heterogeneity used by a convolutional sparse coding algorithm, we show that heterogeneity emphasizes sparseness, while homogeneity improves representation granularity. In a broader context, these encoding strategy can serve as inputs to deep convolutional neural networks. We prove that such variance-encoded sparse image datasets enhance computational efficiency, emphasizing the benefits of kernel heterogeneity to leverage naturalistic and variant input structures and possible applications to improve the throughput of neuromorphic hardware.",
      "authors": [
        "Hugo J. Ladret",
        "Christian Casanova",
        "Laurent Udo Perrinet"
      ],
      "published": "2023-12-22T13:36:27Z",
      "updated": "2023-12-22T13:36:27Z",
      "categories": [
        "q-bio.NC",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14685v1",
      "landing_url": "https://arxiv.org/abs/2312.14685v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14685"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2312.17255",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.17255v1",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "published": "2023-12-20T00:25:55Z"
    },
    "metadata": {
      "arxiv_id": "2312.17255",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "authors": [
        "Oscar Chang",
        "Dung N. Tran",
        "Kazuhito Koishida"
      ],
      "published": "2023-12-20T00:25:55Z",
      "updated": "2023-12-20T00:25:55Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.17255v1",
      "landing_url": "https://arxiv.org/abs/2312.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.17255"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2401.01498",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01498v1",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "published": "2024-01-03T02:03:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2401.01690",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01690v1",
      "title": "Zero-shot Active Learning Using Self Supervised Learning",
      "summary": "Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.",
      "published": "2024-01-03T11:49:07Z"
    },
    "metadata": {
      "arxiv_id": "2401.01690",
      "title": "Zero-shot Active Learning Using Self Supervised Learning",
      "summary": "Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.",
      "authors": [
        "Abhishek Sinha",
        "Shreya Singh"
      ],
      "published": "2024-01-03T11:49:07Z",
      "updated": "2024-01-03T11:49:07Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01690v1",
      "landing_url": "https://arxiv.org/abs/2401.01690v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01690"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2401.04511",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04511v1",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "published": "2024-01-09T12:10:04Z"
    },
    "metadata": {
      "arxiv_id": "2401.04511",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "published": "2024-01-09T12:10:04Z",
      "updated": "2024-01-09T12:10:04Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04511v1",
      "landing_url": "https://arxiv.org/abs/2401.04511v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04511"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2401.05883",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.05883v3",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "published": "2024-01-11T12:43:26Z"
    },
    "metadata": {
      "arxiv_id": "2401.05883",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "authors": [
        "Xianming Li",
        "Jing Li"
      ],
      "published": "2024-01-11T12:43:26Z",
      "updated": "2024-10-03T03:34:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05883v3",
      "landing_url": "https://arxiv.org/abs/2401.05883v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.05883"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2401.07333",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.07333v1",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "published": "2024-01-14T17:43:55Z"
    },
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2401.11647",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.11647v5",
      "title": "Resource-efficient Layer-wise Federated Self-supervised Learning",
      "summary": "Many studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw data distributed across edge devices. However, edge devices often struggle with high computational and communication costs imposed by SSL and FL algorithms. With the deployment of more complex and large-scale models, these challenges are exacerbated. To tackle this, we propose Layer-Wise Federated Self-Supervised Learning (LW-FedSSL), which allows edge devices to incrementally train a small part of the model at a time. Specifically, in LW-FedSSL, training is decomposed into multiple stages, with each stage responsible for only a specific layer of the model. Since only a portion of the model is active for training at any given time, LW-FedSSL significantly reduces computational requirements. Additionally, only the active model portion needs to be exchanged between the FL server and clients, reducing communication overhead. This enables LW-FedSSL to jointly address both computational and communication challenges of FL client devices. It can achieve up to a $3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational operations (giga floating point operations, GFLOPs), and a $5.07 \\times$ lower communication cost while maintaining performance comparable to its end-to-end training counterpart. Furthermore, we explore a progressive training strategy called Progressive Federated Self-Supervised Learning (Prog-FedSSL), which offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in communication costs while maintaining the same memory requirements as end-to-end training. Although the resource efficiency of Prog-FedSSL is lower than that of LW-FedSSL, its performance improvements make it a viable candidate for FL environments with more lenient resource constraints.",
      "published": "2024-01-22T01:57:31Z"
    },
    "metadata": {
      "arxiv_id": "2401.11647",
      "title": "Resource-efficient Layer-wise Federated Self-supervised Learning",
      "summary": "Many studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw data distributed across edge devices. However, edge devices often struggle with high computational and communication costs imposed by SSL and FL algorithms. With the deployment of more complex and large-scale models, these challenges are exacerbated. To tackle this, we propose Layer-Wise Federated Self-Supervised Learning (LW-FedSSL), which allows edge devices to incrementally train a small part of the model at a time. Specifically, in LW-FedSSL, training is decomposed into multiple stages, with each stage responsible for only a specific layer of the model. Since only a portion of the model is active for training at any given time, LW-FedSSL significantly reduces computational requirements. Additionally, only the active model portion needs to be exchanged between the FL server and clients, reducing communication overhead. This enables LW-FedSSL to jointly address both computational and communication challenges of FL client devices. It can achieve up to a $3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational operations (giga floating point operations, GFLOPs), and a $5.07 \\times$ lower communication cost while maintaining performance comparable to its end-to-end training counterpart. Furthermore, we explore a progressive training strategy called Progressive Federated Self-Supervised Learning (Prog-FedSSL), which offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in communication costs while maintaining the same memory requirements as end-to-end training. Although the resource efficiency of Prog-FedSSL is lower than that of LW-FedSSL, its performance improvements make it a viable candidate for FL environments with more lenient resource constraints.",
      "authors": [
        "Ye Lin Tun",
        "Chu Myaet Thwal",
        "Huy Q. Le",
        "Minh N. H. Nguyen",
        "Eui-Nam Huh",
        "Choong Seon Hong"
      ],
      "published": "2024-01-22T01:57:31Z",
      "updated": "2025-12-01T06:49:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11647v5",
      "landing_url": "https://arxiv.org/abs/2401.11647v5",
      "doi": "https://doi.org/10.48550/arXiv.2401.11647"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2401.12725",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.12725v1",
      "title": "Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization",
      "summary": "To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.",
      "published": "2024-01-23T12:53:37Z"
    },
    "metadata": {
      "arxiv_id": "2401.12725",
      "title": "Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization",
      "summary": "To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.",
      "authors": [
        "Chang Liu",
        "Laura Klein",
        "Yixing Huang",
        "Edith Baader",
        "Michael Lell",
        "Marc Kachelrieß",
        "Andreas Maier"
      ],
      "published": "2024-01-23T12:53:37Z",
      "updated": "2024-01-23T12:53:37Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12725v1",
      "landing_url": "https://arxiv.org/abs/2401.12725v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.12725"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2402.01204",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01204v4",
      "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
      "summary": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
      "published": "2024-02-02T08:17:41Z"
    },
    "metadata": {
      "arxiv_id": "2402.01204",
      "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
      "summary": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
      "authors": [
        "Wei-Yao Wang",
        "Wei-Wei Du",
        "Derek Xu",
        "Wei Wang",
        "Wen-Chih Peng"
      ],
      "published": "2024-02-02T08:17:41Z",
      "updated": "2024-09-10T07:02:47Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01204v4",
      "landing_url": "https://arxiv.org/abs/2402.01204v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.01204"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2402.01399",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01399v3",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "published": "2024-02-02T13:31:17Z"
    },
    "metadata": {
      "arxiv_id": "2402.01399",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "authors": [
        "Alice Bizeul",
        "Bernhard Schölkopf",
        "Carl Allen"
      ],
      "published": "2024-02-02T13:31:17Z",
      "updated": "2024-10-15T13:16:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01399v3",
      "landing_url": "https://arxiv.org/abs/2402.01399v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.01399"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2402.02302",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.02302v1",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "published": "2024-02-03T23:54:03Z"
    },
    "metadata": {
      "arxiv_id": "2402.02302",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "authors": [
        "Nay San",
        "Georgios Paraskevopoulos",
        "Aryaman Arora",
        "Xiluo He",
        "Prabhjot Kaur",
        "Oliver Adams",
        "Dan Jurafsky"
      ],
      "published": "2024-02-03T23:54:03Z",
      "updated": "2024-02-03T23:54:03Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02302v1",
      "landing_url": "https://arxiv.org/abs/2402.02302v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.02302"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens."
      }
    ]
  },
  {
    "arxiv_id": "2402.03158",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03158v2",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "published": "2024-02-05T16:27:59Z"
    },
    "metadata": {
      "arxiv_id": "2402.03158",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "authors": [
        "Ran Ben-Basat",
        "Yaniv Ben-Itzhak",
        "Michael Mitzenmacher",
        "Shay Vargaftik"
      ],
      "published": "2024-02-05T16:27:59Z",
      "updated": "2025-07-31T13:53:50Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03158v2",
      "landing_url": "https://arxiv.org/abs/2402.03158v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03158"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2402.08093",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.08093v2",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "published": "2024-02-12T22:21:30Z"
    },
    "metadata": {
      "arxiv_id": "2402.08093",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "authors": [
        "Mateusz Łajszczak",
        "Guillermo Cámbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "Álvaro Martín-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "Alexis Moinet",
        "Sri Karlapati",
        "Ewa Muszyńska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad López Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
      ],
      "published": "2024-02-12T22:21:30Z",
      "updated": "2024-02-15T18:57:26Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08093v2",
      "landing_url": "https://arxiv.org/abs/2402.08093v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.08093"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2402.12208",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.12208v4",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "published": "2024-02-19T15:12:12Z"
    },
    "metadata": {
      "arxiv_id": "2402.12208",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "authors": [
        "Shengpeng Ji",
        "Minghui Fang",
        "Jialong Zuo",
        "Ziyue Jiang",
        "Dingdong Wang",
        "Hanting Wang",
        "Hai Huang",
        "Zhou Zhao"
      ],
      "published": "2024-02-19T15:12:12Z",
      "updated": "2025-06-04T05:50:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12208v4",
      "landing_url": "https://arxiv.org/abs/2402.12208v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.12208"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2402.12712",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.12712v3",
      "title": "MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
      "summary": "This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model. The project page is at https://mvdiffusion-plusplus.github.io.",
      "published": "2024-02-20T04:25:57Z"
    },
    "metadata": {
      "arxiv_id": "2402.12712",
      "title": "MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
      "summary": "This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model. The project page is at https://mvdiffusion-plusplus.github.io.",
      "authors": [
        "Shitao Tang",
        "Jiacheng Chen",
        "Dilin Wang",
        "Chengzhou Tang",
        "Fuyang Zhang",
        "Yuchen Fan",
        "Vikas Chandra",
        "Yasutaka Furukawa",
        "Rakesh Ranjan"
      ],
      "published": "2024-02-20T04:25:57Z",
      "updated": "2024-04-30T04:11:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12712v3",
      "landing_url": "https://arxiv.org/abs/2402.12712v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.12712"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2402.13827",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.13827v2",
      "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
      "summary": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
      "published": "2024-02-21T14:16:49Z"
    },
    "metadata": {
      "arxiv_id": "2402.13827",
      "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
      "summary": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
      "authors": [
        "Joongho Jo",
        "Hyeongwon Kim",
        "Jongsun Park"
      ],
      "published": "2024-02-21T14:16:49Z",
      "updated": "2024-09-25T01:29:37Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13827v2",
      "landing_url": "https://arxiv.org/abs/2402.13827v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.13827"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2402.14490",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.14490v3",
      "title": "Imbalanced Data Clustering using Equilibrium K-Means",
      "summary": "Centroid-based clustering algorithms, such as hard K-means (HKM) and fuzzy K-means (FKM), have suffered from learning bias towards large clusters. Their centroids tend to be crowded in large clusters, compromising performance when the true underlying data groups vary in size (i.e., imbalanced data). To address this, we propose a new clustering objective function based on the Boltzmann operator, which introduces a novel centroid repulsion mechanism, where data points surrounding the centroids repel other centroids. Larger clusters repel more, effectively mitigating the issue of large cluster learning bias. The proposed new algorithm, called equilibrium K-means (EKM), is simple, alternating between two steps; resource-saving, with the same time and space complexity as FKM; and scalable to large datasets via batch learning. We substantially evaluate the performance of EKM on synthetic and real-world datasets. The results show that EKM performs competitively on balanced data and significantly outperforms benchmark algorithms on imbalanced data. Deep clustering experiments demonstrate that EKM is a better alternative to HKM and FKM on imbalanced data as more discriminative representation can be obtained. Additionally, we reformulate HKM, FKM, and EKM in a general form of gradient descent and demonstrate how this general form facilitates a uniform study of K-means algorithms.",
      "published": "2024-02-22T12:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2402.14490",
      "title": "Imbalanced Data Clustering using Equilibrium K-Means",
      "summary": "Centroid-based clustering algorithms, such as hard K-means (HKM) and fuzzy K-means (FKM), have suffered from learning bias towards large clusters. Their centroids tend to be crowded in large clusters, compromising performance when the true underlying data groups vary in size (i.e., imbalanced data). To address this, we propose a new clustering objective function based on the Boltzmann operator, which introduces a novel centroid repulsion mechanism, where data points surrounding the centroids repel other centroids. Larger clusters repel more, effectively mitigating the issue of large cluster learning bias. The proposed new algorithm, called equilibrium K-means (EKM), is simple, alternating between two steps; resource-saving, with the same time and space complexity as FKM; and scalable to large datasets via batch learning. We substantially evaluate the performance of EKM on synthetic and real-world datasets. The results show that EKM performs competitively on balanced data and significantly outperforms benchmark algorithms on imbalanced data. Deep clustering experiments demonstrate that EKM is a better alternative to HKM and FKM on imbalanced data as more discriminative representation can be obtained. Additionally, we reformulate HKM, FKM, and EKM in a general form of gradient descent and demonstrate how this general form facilitates a uniform study of K-means algorithms.",
      "authors": [
        "Yudong He"
      ],
      "published": "2024-02-22T12:27:38Z",
      "updated": "2024-06-06T15:51:21Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14490v3",
      "landing_url": "https://arxiv.org/abs/2402.14490v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.14490"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2402.14789",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.14789v1",
      "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
      "summary": "Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",
      "published": "2024-02-22T18:46:22Z"
    },
    "metadata": {
      "arxiv_id": "2402.14789",
      "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
      "summary": "Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",
      "authors": [
        "Johnathan Xie",
        "Yoonho Lee",
        "Annie S. Chen",
        "Chelsea Finn"
      ],
      "published": "2024-02-22T18:46:22Z",
      "updated": "2024-02-22T18:46:22Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14789v1",
      "landing_url": "https://arxiv.org/abs/2402.14789v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14789"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2402.15725",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.15725v5",
      "title": "Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks",
      "summary": "Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset.",
      "published": "2024-02-24T05:30:23Z"
    },
    "metadata": {
      "arxiv_id": "2402.15725",
      "title": "Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks",
      "summary": "Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset.",
      "authors": [
        "Duo Ma",
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2024-02-24T05:30:23Z",
      "updated": "2024-08-03T12:58:45Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.15725v5",
      "landing_url": "https://arxiv.org/abs/2402.15725v5",
      "doi": "https://doi.org/10.48550/arXiv.2402.15725"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2402.15985",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.15985v1",
      "title": "Phonetic and Lexical Discovery of a Canine Language using HuBERT",
      "summary": "This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.",
      "published": "2024-02-25T04:35:45Z"
    },
    "metadata": {
      "arxiv_id": "2402.15985",
      "title": "Phonetic and Lexical Discovery of a Canine Language using HuBERT",
      "summary": "This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.",
      "authors": [
        "Xingyuan Li",
        "Sinong Wang",
        "Zeyu Xie",
        "Mengyue Wu",
        "Kenny Q. Zhu"
      ],
      "published": "2024-02-25T04:35:45Z",
      "updated": "2024-02-25T04:35:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.15985v1",
      "landing_url": "https://arxiv.org/abs/2402.15985v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.15985"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2402.17863",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.17863v1",
      "title": "Vision Transformers with Natural Language Semantics",
      "summary": "Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.",
      "published": "2024-02-27T19:54:42Z"
    },
    "metadata": {
      "arxiv_id": "2402.17863",
      "title": "Vision Transformers with Natural Language Semantics",
      "summary": "Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.",
      "authors": [
        "Young Kyung Kim",
        "J. Matías Di Martino",
        "Guillermo Sapiro"
      ],
      "published": "2024-02-27T19:54:42Z",
      "updated": "2024-02-27T19:54:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.17863v1",
      "landing_url": "https://arxiv.org/abs/2402.17863v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.17863"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2402.19104",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.19104v1",
      "title": "The Author of a Quotation Goethe Adduced Against Newton",
      "summary": "The hitherto unknown author of a citation by Goethe in his History of Colours is identified as J. E. Montucla and the context of Montucla's quotation is discussed.",
      "published": "2024-02-29T12:36:04Z"
    },
    "metadata": {
      "arxiv_id": "2402.19104",
      "title": "The Author of a Quotation Goethe Adduced Against Newton",
      "summary": "The hitherto unknown author of a citation by Goethe in his History of Colours is identified as J. E. Montucla and the context of Montucla's quotation is discussed.",
      "authors": [
        "Hubert Kalf"
      ],
      "published": "2024-02-29T12:36:04Z",
      "updated": "2024-02-29T12:36:04Z",
      "categories": [
        "math.HO",
        "physics.hist-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.19104v1",
      "landing_url": "https://arxiv.org/abs/2402.19104v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.19104"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2403.00642",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.00642v2",
      "title": "Rethinking The Uniformity Metric in Self-Supervised Learning",
      "summary": "Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and can not account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the Wasserstein distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving CIFAR-10 and CIFAR-100 datasets. Code is available at \\url{https://github.com/statsle/WassersteinSSL}.",
      "published": "2024-03-01T16:22:05Z"
    },
    "metadata": {
      "arxiv_id": "2403.00642",
      "title": "Rethinking The Uniformity Metric in Self-Supervised Learning",
      "summary": "Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and can not account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the Wasserstein distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving CIFAR-10 and CIFAR-100 datasets. Code is available at \\url{https://github.com/statsle/WassersteinSSL}.",
      "authors": [
        "Xianghong Fang",
        "Jian Li",
        "Qiang Sun",
        "Benyou Wang"
      ],
      "published": "2024-03-01T16:22:05Z",
      "updated": "2024-04-26T08:24:11Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.00642v2",
      "landing_url": "https://arxiv.org/abs/2403.00642v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.00642"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2403.05010",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.05010v3",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "published": "2024-03-08T03:16:47Z"
    },
    "metadata": {
      "arxiv_id": "2403.05010",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "published": "2024-03-08T03:16:47Z",
      "updated": "2024-10-07T02:08:05Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.05010v3",
      "landing_url": "https://arxiv.org/abs/2403.05010v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.05010"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2403.06128",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.06128v1",
      "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
      "summary": "While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder. Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at https://github.com/hao1635/LEDA.",
      "published": "2024-03-10T08:21:50Z"
    },
    "metadata": {
      "arxiv_id": "2403.06128",
      "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
      "summary": "While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder. Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at https://github.com/hao1635/LEDA.",
      "authors": [
        "Zhihao Chen",
        "Tao Chen",
        "Chenhui Wang",
        "Chuang Niu",
        "Ge Wang",
        "Hongming Shan"
      ],
      "published": "2024-03-10T08:21:50Z",
      "updated": "2024-03-10T08:21:50Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06128v1",
      "landing_url": "https://arxiv.org/abs/2403.06128v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06128"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2403.06361",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.06361v2",
      "title": "See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI",
      "summary": "Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.",
      "published": "2024-03-11T01:18:49Z"
    },
    "metadata": {
      "arxiv_id": "2403.06361",
      "title": "See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI",
      "summary": "Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.",
      "authors": [
        "Yulong Liu",
        "Yongqiang Ma",
        "Guibo Zhu",
        "Haodong Jing",
        "Nanning Zheng"
      ],
      "published": "2024-03-11T01:18:49Z",
      "updated": "2024-06-13T14:17:04Z",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06361v2",
      "landing_url": "https://arxiv.org/abs/2403.06361v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.06361"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2403.07355",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.07355v2",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "published": "2024-03-12T06:28:41Z"
    },
    "metadata": {
      "arxiv_id": "2403.07355",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "authors": [
        "Junyong Shin",
        "Yujin Kang",
        "Yo-Seb Jeon"
      ],
      "published": "2024-03-12T06:28:41Z",
      "updated": "2024-03-13T02:29:29Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07355v2",
      "landing_url": "https://arxiv.org/abs/2403.07355v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07355"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2403.08206",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.08206v2",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "published": "2024-03-13T03:03:15Z"
    },
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2403.09673",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.09673v2",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "published": "2024-02-04T12:18:51Z"
    },
    "metadata": {
      "arxiv_id": "2403.09673",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Jue Wang",
        "Yufei Huang",
        "Lirong Wu",
        "Stan Z. Li"
      ],
      "published": "2024-02-04T12:18:51Z",
      "updated": "2024-03-19T05:29:23Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09673v2",
      "landing_url": "https://arxiv.org/abs/2403.09673v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.09673"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2403.11207",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.11207v2",
      "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
      "summary": "Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",
      "published": "2024-03-17T13:15:22Z"
    },
    "metadata": {
      "arxiv_id": "2403.11207",
      "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
      "summary": "Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",
      "authors": [
        "Paul S. Scotti",
        "Mihir Tripathy",
        "Cesar Kadir Torrico Villanueva",
        "Reese Kneeland",
        "Tong Chen",
        "Ashutosh Narang",
        "Charan Santhirasegaran",
        "Jonathan Xu",
        "Thomas Naselaris",
        "Kenneth A. Norman",
        "Tanishq Mathew Abraham"
      ],
      "published": "2024-03-17T13:15:22Z",
      "updated": "2024-06-15T23:07:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11207v2",
      "landing_url": "https://arxiv.org/abs/2403.11207v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.11207"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2403.14562",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.14562v2",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "published": "2024-03-21T17:06:17Z"
    },
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2403.16078",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16078v1",
      "title": "Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy",
      "summary": "Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning. The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.",
      "published": "2024-03-24T09:42:05Z"
    },
    "metadata": {
      "arxiv_id": "2403.16078",
      "title": "Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy",
      "summary": "Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning. The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.",
      "authors": [
        "Wenxuan Wu",
        "Xueyuan Chen",
        "Xixin Wu",
        "Haizhou Li",
        "Helen Meng"
      ],
      "published": "2024-03-24T09:42:05Z",
      "updated": "2024-03-24T09:42:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16078v1",
      "landing_url": "https://arxiv.org/abs/2403.16078v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16078"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2403.16092",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16092v2",
      "title": "Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap",
      "summary": "Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page.",
      "published": "2024-03-24T11:09:41Z"
    },
    "metadata": {
      "arxiv_id": "2403.16092",
      "title": "Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap",
      "summary": "Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page.",
      "authors": [
        "Carl Lindström",
        "Georg Hess",
        "Adam Lilja",
        "Maryam Fatemi",
        "Lars Hammarstrand",
        "Christoffer Petersson",
        "Lennart Svensson"
      ],
      "published": "2024-03-24T11:09:41Z",
      "updated": "2024-04-15T10:06:41Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16092v2",
      "landing_url": "https://arxiv.org/abs/2403.16092v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.16092"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2403.16258",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16258v1",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "published": "2024-03-24T18:33:16Z"
    },
    "metadata": {
      "arxiv_id": "2403.16258",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "authors": [
        "Atefeh Khoshkhahtinat",
        "Ali Zafari",
        "Piyush M. Mehta",
        "Nasser M. Nasrabadi"
      ],
      "published": "2024-03-24T18:33:16Z",
      "updated": "2024-03-24T18:33:16Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16258v1",
      "landing_url": "https://arxiv.org/abs/2403.16258v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16258"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.16973",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16973v3",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "published": "2024-03-25T17:38:32Z"
    },
    "metadata": {
      "arxiv_id": "2403.16973",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "authors": [
        "Puyuan Peng",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2024-03-25T17:38:32Z",
      "updated": "2024-06-14T00:29:46Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16973v3",
      "landing_url": "https://arxiv.org/abs/2403.16973v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.16973"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.17879",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.17879v1",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "published": "2024-03-26T17:11:51Z"
    },
    "metadata": {
      "arxiv_id": "2403.17879",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "authors": [
        "Qiqi Hou",
        "Farzad Farhadzadeh",
        "Amir Said",
        "Guillaume Sautiere",
        "Hoang Le"
      ],
      "published": "2024-03-26T17:11:51Z",
      "updated": "2024-03-26T17:11:51Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17879v1",
      "landing_url": "https://arxiv.org/abs/2403.17879v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17879"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.00685",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.00685v2",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "published": "2024-03-31T13:30:12Z"
    },
    "metadata": {
      "arxiv_id": "2404.00685",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "authors": [
        "Santiago Cuervo",
        "Ricard Marxer"
      ],
      "published": "2024-03-31T13:30:12Z",
      "updated": "2024-04-16T06:46:18Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00685v2",
      "landing_url": "https://arxiv.org/abs/2404.00685v2",
      "doi": "https://doi.org/10.18653/v1/2024.emnlp-main.21"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2404.01284",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.01284v1",
      "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
      "summary": "Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.",
      "published": "2024-04-01T17:55:11Z"
    },
    "metadata": {
      "arxiv_id": "2404.01284",
      "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
      "summary": "Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.",
      "authors": [
        "Mingyuan Zhang",
        "Daisheng Jin",
        "Chenyang Gu",
        "Fangzhou Hong",
        "Zhongang Cai",
        "Jingfang Huang",
        "Chongzhi Zhang",
        "Xinying Guo",
        "Lei Yang",
        "Ying He",
        "Ziwei Liu"
      ],
      "published": "2024-04-01T17:55:11Z",
      "updated": "2024-04-01T17:55:11Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.01284v1",
      "landing_url": "https://arxiv.org/abs/2404.01284v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.01284"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2404.02781",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.02781v1",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "published": "2024-04-03T14:52:20Z"
    },
    "metadata": {
      "arxiv_id": "2404.02781",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "published": "2024-04-03T14:52:20Z",
      "updated": "2024-04-03T14:52:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02781v1",
      "landing_url": "https://arxiv.org/abs/2404.02781v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.02781"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.03204",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.03204v3",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "published": "2024-04-04T05:15:07Z"
    },
    "metadata": {
      "arxiv_id": "2404.03204",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "authors": [
        "Detai Xin",
        "Xu Tan",
        "Kai Shen",
        "Zeqian Ju",
        "Dongchao Yang",
        "Yuancheng Wang",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari",
        "Shujie Liu",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-04-04T05:15:07Z",
      "updated": "2024-05-19T21:34:28Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03204v3",
      "landing_url": "https://arxiv.org/abs/2404.03204v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.03204"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2404.04904",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04904v2",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "published": "2024-04-07T10:10:15Z"
    },
    "metadata": {
      "arxiv_id": "2404.04904",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "authors": [
        "Yuang Li",
        "Min Zhang",
        "Mengxin Ren",
        "Miaomiao Ma",
        "Daimeng Wei",
        "Hao Yang"
      ],
      "published": "2024-04-07T10:10:15Z",
      "updated": "2024-09-20T08:08:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04904v2",
      "landing_url": "https://arxiv.org/abs/2404.04904v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04904"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.04913",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04913v3",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "published": "2024-04-07T10:49:59Z"
    },
    "metadata": {
      "arxiv_id": "2404.04913",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "authors": [
        "Gyeongjin Kang",
        "Younggeun Lee",
        "Seungjun Oh",
        "Eunbyung Park"
      ],
      "published": "2024-04-07T10:49:59Z",
      "updated": "2024-09-25T07:16:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04913v3",
      "landing_url": "https://arxiv.org/abs/2404.04913v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.04913"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.04940",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04940v2",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "published": "2024-04-07T12:25:03Z"
    },
    "metadata": {
      "arxiv_id": "2404.04940",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "authors": [
        "Yichen Bao",
        "Han Lu",
        "Quanxue Gao"
      ],
      "published": "2024-04-07T12:25:03Z",
      "updated": "2024-11-07T08:59:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04940v2",
      "landing_url": "https://arxiv.org/abs/2404.04940v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2404.06079",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.06079v2",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "published": "2024-04-09T07:37:41Z"
    },
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2404.14774",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.14774v2",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "published": "2024-04-23T06:29:48Z"
    },
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2404.15370",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.15370v1",
      "title": "Self-Supervised Learning for User Localization",
      "summary": "Machine learning techniques have shown remarkable accuracy in localization tasks, but their dependency on vast amounts of labeled data, particularly Channel State Information (CSI) and corresponding coordinates, remains a bottleneck. Self-supervised learning techniques alleviate the need for labeled data, a potential that remains largely untapped and underexplored in existing research. Addressing this gap, we propose a pioneering approach that leverages self-supervised pretraining on unlabeled data to boost the performance of supervised learning for user localization based on CSI. We introduce two pretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) to glean representations from unlabeled data via self-supervised learning. Following this, we utilize the encoder portion of the AE models to extract relevant features from labeled data, and finetune an MLP-based Position Estimation Model to accurately deduce user locations. Our experimentation on the CTW-2020 dataset, which features a substantial volume of unlabeled data but limited labeled samples, demonstrates the viability of our approach. Notably, the dataset covers a vast area spanning over 646x943x41 meters, and our approach demonstrates promising results even for such expansive localization tasks.",
      "published": "2024-04-19T21:49:10Z"
    },
    "metadata": {
      "arxiv_id": "2404.15370",
      "title": "Self-Supervised Learning for User Localization",
      "summary": "Machine learning techniques have shown remarkable accuracy in localization tasks, but their dependency on vast amounts of labeled data, particularly Channel State Information (CSI) and corresponding coordinates, remains a bottleneck. Self-supervised learning techniques alleviate the need for labeled data, a potential that remains largely untapped and underexplored in existing research. Addressing this gap, we propose a pioneering approach that leverages self-supervised pretraining on unlabeled data to boost the performance of supervised learning for user localization based on CSI. We introduce two pretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) to glean representations from unlabeled data via self-supervised learning. Following this, we utilize the encoder portion of the AE models to extract relevant features from labeled data, and finetune an MLP-based Position Estimation Model to accurately deduce user locations. Our experimentation on the CTW-2020 dataset, which features a substantial volume of unlabeled data but limited labeled samples, demonstrates the viability of our approach. Notably, the dataset covers a vast area spanning over 646x943x41 meters, and our approach demonstrates promising results even for such expansive localization tasks.",
      "authors": [
        "Ankan Dash",
        "Jingyi Gu",
        "Guiling Wang",
        "Nirwan Ansari"
      ],
      "published": "2024-04-19T21:49:10Z",
      "updated": "2024-04-19T21:49:10Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.15370v1",
      "landing_url": "https://arxiv.org/abs/2404.15370v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.15370"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2404.16123",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16123v1",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "published": "2024-04-24T18:28:17Z"
    },
    "metadata": {
      "arxiv_id": "2404.16123",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "authors": [
        "Eric Slyman",
        "Stefan Lee",
        "Scott Cohen",
        "Kushal Kafle"
      ],
      "published": "2024-04-24T18:28:17Z",
      "updated": "2024-04-24T18:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16123v1",
      "landing_url": "https://arxiv.org/abs/2404.16123v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16123"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2404.16507",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16507v1",
      "title": "Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception",
      "summary": "Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.",
      "published": "2024-04-25T11:01:40Z"
    },
    "metadata": {
      "arxiv_id": "2404.16507",
      "title": "Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception",
      "summary": "Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.",
      "authors": [
        "Xiaotong Yu",
        "Chang-Wen Chen"
      ],
      "published": "2024-04-25T11:01:40Z",
      "updated": "2024-04-25T11:01:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16507v1",
      "landing_url": "https://arxiv.org/abs/2404.16507v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16507"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2404.19441",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.19441v3",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "published": "2024-04-30T10:44:33Z"
    },
    "metadata": {
      "arxiv_id": "2404.19441",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "authors": [
        "Yuzhe Gu",
        "Enmao Diao"
      ],
      "published": "2024-04-30T10:44:33Z",
      "updated": "2024-10-03T12:23:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19441v3",
      "landing_url": "https://arxiv.org/abs/2404.19441v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.19441"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2405.01242",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.01242v3",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "published": "2024-05-02T12:45:48Z"
    },
    "metadata": {
      "arxiv_id": "2405.01242",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "authors": [
        "Yueyuan Sui",
        "Minghui Zhao",
        "Junxi Xia",
        "Xiaofan Jiang",
        "Stephen Xia"
      ],
      "published": "2024-05-02T12:45:48Z",
      "updated": "2024-05-29T15:46:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.01242v3",
      "landing_url": "https://arxiv.org/abs/2405.01242v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.01242"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2405.02171",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02171v1",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "published": "2024-05-03T15:20:30Z"
    },
    "metadata": {
      "arxiv_id": "2405.02171",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "authors": [
        "Zhilu Zhang",
        "Ruohao Wang",
        "Hongzhi Zhang",
        "Wangmeng Zuo"
      ],
      "published": "2024-05-03T15:20:30Z",
      "updated": "2024-05-03T15:20:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02171v1",
      "landing_url": "https://arxiv.org/abs/2405.02171v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02171"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2405.02330",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02330v1",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "published": "2024-04-25T13:49:50Z"
    },
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2405.02682",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02682v1",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "published": "2024-05-04T14:48:19Z"
    },
    "metadata": {
      "arxiv_id": "2405.02682",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "authors": [
        "Md Washik Al Azad",
        "Spyridon Mastorakis"
      ],
      "published": "2024-05-04T14:48:19Z",
      "updated": "2024-05-04T14:48:19Z",
      "categories": [
        "cs.DC",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02682v1",
      "landing_url": "https://arxiv.org/abs/2405.02682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02682"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2405.03110",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03110v1",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "published": "2024-05-06T02:06:26Z"
    },
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2405.03376",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03376v2",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "published": "2024-05-06T11:30:55Z"
    },
    "metadata": {
      "arxiv_id": "2405.03376",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "authors": [
        "Tao Han",
        "Zhenghao Chen",
        "Song Guo",
        "Wanghan Xu",
        "Lei Bai"
      ],
      "published": "2024-05-06T11:30:55Z",
      "updated": "2024-05-08T03:27:04Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03376v2",
      "landing_url": "https://arxiv.org/abs/2405.03376v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.03376"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.06573",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.06573v2",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "published": "2024-05-10T16:18:49Z"
    },
    "metadata": {
      "arxiv_id": "2405.06573",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "authors": [
        "Rong Chao",
        "Wen-Huang Cheng",
        "Moreno La Quatra",
        "Sabato Marco Siniscalchi",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Yu Tsao"
      ],
      "published": "2024-05-10T16:18:49Z",
      "updated": "2025-10-07T07:07:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06573v2",
      "landing_url": "https://arxiv.org/abs/2405.06573v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.06573"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2405.07682",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.07682v1",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "published": "2024-05-13T12:14:54Z"
    },
    "metadata": {
      "arxiv_id": "2405.07682",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "authors": [
        "Jianyi Chen",
        "Wei Xue",
        "Xu Tan",
        "Zhen Ye",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "published": "2024-05-13T12:14:54Z",
      "updated": "2024-05-13T12:14:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07682v1",
      "landing_url": "https://arxiv.org/abs/2405.07682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.07682"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2405.08402",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.08402v1",
      "title": "Investigating the 'Autoencoder Behavior' in Speech Self-Supervised Models: a focus on HuBERT's Pretraining",
      "summary": "Self-supervised learning has shown great success in Speech Recognition. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.",
      "published": "2024-05-14T07:55:37Z"
    },
    "metadata": {
      "arxiv_id": "2405.08402",
      "title": "Investigating the 'Autoencoder Behavior' in Speech Self-Supervised Models: a focus on HuBERT's Pretraining",
      "summary": "Self-supervised learning has shown great success in Speech Recognition. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.",
      "authors": [
        "Valentin Vielzeuf"
      ],
      "published": "2024-05-14T07:55:37Z",
      "updated": "2024-05-14T07:55:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08402v1",
      "landing_url": "https://arxiv.org/abs/2405.08402v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.08402"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2405.09768",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09768v1",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "published": "2024-05-16T02:18:41Z"
    },
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2405.15399",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.15399v2",
      "title": "Stochastic super-resolution for Gaussian microtextures",
      "summary": "Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.",
      "published": "2024-05-24T09:53:24Z"
    },
    "metadata": {
      "arxiv_id": "2405.15399",
      "title": "Stochastic super-resolution for Gaussian microtextures",
      "summary": "Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.",
      "authors": [
        "Emile Pierret",
        "Bruno Galerne"
      ],
      "published": "2024-05-24T09:53:24Z",
      "updated": "2024-05-29T06:45:58Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15399v2",
      "landing_url": "https://arxiv.org/abs/2405.15399v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.15399"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2405.16136",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16136v1",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "published": "2024-05-25T09:10:12Z"
    },
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2405.16401",
    "anchor": "dblp_title",
    "search_term": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning.",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16401v2",
      "title": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning",
      "summary": "Vision transformers have established a precedent of patchifying images into uniformly-sized chunks before processing. We hypothesize that this design choice may limit models in learning comprehensive and compositional representations from visual data. This paper explores the notion of providing semantically-meaningful visual tokens to transformer encoders within a vision-language pre-training framework. Leveraging off-the-shelf segmentation and scene-graph models, we extract representations of instance segmentation masks (referred to as tangible tokens) and relationships and actions (referred to as intangible tokens). Subsequently, we pre-train a vision-side transformer by incorporating these newly extracted tokens and aligning the resultant embeddings with caption embeddings from a text-side encoder. To capture the structural and semantic relationships among visual tokens, we introduce additive attention weights, which are used to compute self-attention scores. Our experiments on COCO demonstrate notable improvements over ViTs in learned representation quality across text-to-image (+47%) and image-to-text retrieval (+44%) tasks. Furthermore, we showcase the advantages on compositionality benchmarks such as ARO (+18%) and Winoground (+10%).",
      "published": "2024-05-26T01:46:22Z"
    },
    "metadata": {
      "arxiv_id": "2405.16401",
      "title": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning",
      "summary": "Vision transformers have established a precedent of patchifying images into uniformly-sized chunks before processing. We hypothesize that this design choice may limit models in learning comprehensive and compositional representations from visual data. This paper explores the notion of providing semantically-meaningful visual tokens to transformer encoders within a vision-language pre-training framework. Leveraging off-the-shelf segmentation and scene-graph models, we extract representations of instance segmentation masks (referred to as tangible tokens) and relationships and actions (referred to as intangible tokens). Subsequently, we pre-train a vision-side transformer by incorporating these newly extracted tokens and aligning the resultant embeddings with caption embeddings from a text-side encoder. To capture the structural and semantic relationships among visual tokens, we introduce additive attention weights, which are used to compute self-attention scores. Our experiments on COCO demonstrate notable improvements over ViTs in learned representation quality across text-to-image (+47%) and image-to-text retrieval (+44%) tasks. Furthermore, we showcase the advantages on compositionality benchmarks such as ARO (+18%) and Winoground (+10%).",
      "authors": [
        "Neha Kalibhat",
        "Priyatham Kattakinda",
        "Sumit Nawathe",
        "Arman Zarei",
        "Nikita Seleznev",
        "Samuel Sharpe",
        "Senthil Kumar",
        "Soheil Feizi"
      ],
      "published": "2024-05-26T01:46:22Z",
      "updated": "2025-05-19T16:00:51Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16401v2",
      "landing_url": "https://arxiv.org/abs/2405.16401v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.16401"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning."
      }
    ]
  },
  {
    "arxiv_id": "2405.19600",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.19600v2",
      "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
      "summary": "The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph's spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning.",
      "published": "2024-05-30T01:30:34Z"
    },
    "metadata": {
      "arxiv_id": "2405.19600",
      "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
      "summary": "The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph's spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning.",
      "authors": [
        "Xiangru Jian",
        "Xinjian Zhao",
        "Wei Pang",
        "Chaolong Ying",
        "Yimu Wang",
        "Yaoyao Xu",
        "Tianshu Yu"
      ],
      "published": "2024-05-30T01:30:34Z",
      "updated": "2024-12-04T04:41:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19600v2",
      "landing_url": "https://arxiv.org/abs/2405.19600v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.19600"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2405.20410",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.20410v1",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "published": "2024-05-30T18:28:31Z"
    },
    "metadata": {
      "arxiv_id": "2405.20410",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "published": "2024-05-30T18:28:31Z",
      "updated": "2024-05-30T18:28:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20410v1",
      "landing_url": "https://arxiv.org/abs/2405.20410v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20410"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.00976",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.00976v2",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "published": "2024-06-03T04:16:30Z"
    },
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.02092",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02092v1",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "published": "2024-06-04T08:23:57Z"
    },
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.02940",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02940v1",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "published": "2024-06-05T04:54:49Z"
    },
    "metadata": {
      "arxiv_id": "2406.02940",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Hui Lu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-05T04:54:49Z",
      "updated": "2024-06-05T04:54:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02940v1",
      "landing_url": "https://arxiv.org/abs/2406.02940v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2406.03460",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03460v1",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "published": "2024-06-05T17:07:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.03706",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03706v1",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "published": "2024-06-06T03:06:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.04582",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04582v1",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "published": "2024-06-07T02:03:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.04582",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "authors": [
        "Xuanjun Chen",
        "Jiawei Du",
        "Haibin Wu",
        "Jyh-Shing Roger Jang",
        "Hung-yi Lee"
      ],
      "published": "2024-06-07T02:03:27Z",
      "updated": "2024-06-07T02:03:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04582v1",
      "landing_url": "https://arxiv.org/abs/2406.04582v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04582"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.04633",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04633v1",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "published": "2024-06-07T04:34:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2406.04740",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04740v1",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "published": "2024-06-07T08:40:53Z"
    },
    "metadata": {
      "arxiv_id": "2406.04740",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "authors": [
        "Yang Ma",
        "Wenchi Cheng",
        "Jingqing Wang",
        "Wei Zhang"
      ],
      "published": "2024-06-07T08:40:53Z",
      "updated": "2024-06-07T08:40:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04740v1",
      "landing_url": "https://arxiv.org/abs/2406.04740v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04740"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.05370",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05370v2",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "published": "2024-06-08T06:31:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.05370",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "authors": [
        "Sanyuan Chen",
        "Shujie Liu",
        "Long Zhou",
        "Yanqing Liu",
        "Xu Tan",
        "Jinyu Li",
        "Sheng Zhao",
        "Yao Qian",
        "Furu Wei"
      ],
      "published": "2024-06-08T06:31:03Z",
      "updated": "2024-06-17T04:39:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05370v2",
      "landing_url": "https://arxiv.org/abs/2406.05370v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05370"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.05661",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05661v4",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "published": "2024-06-09T06:30:28Z"
    },
    "metadata": {
      "arxiv_id": "2406.05661",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2024-06-09T06:30:28Z",
      "updated": "2025-02-18T10:07:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05661v4",
      "landing_url": "https://arxiv.org/abs/2406.05661v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.05661"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2406.06371",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.06371v5",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "published": "2024-06-10T15:32:42Z"
    },
    "metadata": {
      "arxiv_id": "2406.06371",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "authors": [
        "Marcely Zanon Boito",
        "Vivek Iyer",
        "Nikolaos Lagos",
        "Laurent Besacier",
        "Ioan Calapodescu"
      ],
      "published": "2024-06-10T15:32:42Z",
      "updated": "2024-11-21T10:45:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06371v5",
      "landing_url": "https://arxiv.org/abs/2406.06371v5",
      "doi": "https://doi.org/10.48550/arXiv.2406.06371"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2406.06582",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.06582v2",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "published": "2024-06-04T20:08:25Z"
    },
    "metadata": {
      "arxiv_id": "2406.06582",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "authors": [
        "Viet Anh Trinh",
        "Rosy Southwell",
        "Yiwen Guan",
        "Xinlu He",
        "Zhiyong Wang",
        "Jacob Whitehill"
      ],
      "published": "2024-06-04T20:08:25Z",
      "updated": "2024-06-25T17:44:00Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06582v2",
      "landing_url": "https://arxiv.org/abs/2406.06582v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.06582"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2406.07119",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07119v1",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "published": "2024-06-11T10:06:53Z"
    },
    "metadata": {
      "arxiv_id": "2406.07119",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "authors": [
        "Aoxiong Yin",
        "Haoyuan Li",
        "Kai Shen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2024-06-11T10:06:53Z",
      "updated": "2024-06-11T10:06:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
      "landing_url": "https://arxiv.org/abs/2406.07119v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07119"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.07846",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07846v1",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "published": "2024-06-12T03:25:18Z"
    },
    "metadata": {
      "arxiv_id": "2406.07846",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "authors": [
        "Ziqian Ning",
        "Shuai Wang",
        "Pengcheng Zhu",
        "Zhichao Wang",
        "Jixun Yao",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2024-06-12T03:25:18Z",
      "updated": "2024-06-12T03:25:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07846v1",
      "landing_url": "https://arxiv.org/abs/2406.07846v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07846"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.07855",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07855v1",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "published": "2024-06-12T04:09:44Z"
    },
    "metadata": {
      "arxiv_id": "2406.07855",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "authors": [
        "Bing Han",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Lingwei Meng",
        "Yanming Qian",
        "Yanqing Liu",
        "Sheng Zhao",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2024-06-12T04:09:44Z",
      "updated": "2024-06-12T04:09:44Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07855v1",
      "landing_url": "https://arxiv.org/abs/2406.07855v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07855"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.08112",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08112v1",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "published": "2024-06-12T11:47:23Z"
    },
    "metadata": {
      "arxiv_id": "2406.08112",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "authors": [
        "Yi Lu",
        "Yuankun Xie",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Zhiyong Wang",
        "Xin Qi",
        "Xuefei Liu",
        "Yongwei Li",
        "Yukun Liu",
        "Xiaopeng Wang",
        "Shuchen Shi"
      ],
      "published": "2024-06-12T11:47:23Z",
      "updated": "2024-06-12T11:47:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08112v1",
      "landing_url": "https://arxiv.org/abs/2406.08112v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08112"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.08336",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08336v2",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "published": "2024-06-12T15:42:21Z"
    },
    "metadata": {
      "arxiv_id": "2406.08336",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "authors": [
        "Xueyuan Chen",
        "Dongchao Yang",
        "Dingdong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "published": "2024-06-12T15:42:21Z",
      "updated": "2024-06-24T06:09:42Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08336v2",
      "landing_url": "https://arxiv.org/abs/2406.08336v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08336"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.10223",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10223v1",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "published": "2024-06-14T17:55:55Z"
    },
    "metadata": {
      "arxiv_id": "2406.10223",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "authors": [
        "Nameer Hirschkind",
        "Xiao Yu",
        "Mahesh Kumar Nandwana",
        "Joseph Liu",
        "Eloi DuBois",
        "Dao Le",
        "Nicolas Thiebaut",
        "Colin Sinclair",
        "Kyle Spence",
        "Charles Shang",
        "Zoe Abrams",
        "Morgan McGuire"
      ],
      "published": "2024-06-14T17:55:55Z",
      "updated": "2024-06-14T17:55:55Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10223v1",
      "landing_url": "https://arxiv.org/abs/2406.10223v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10223"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.10275",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10275v1",
      "title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets",
      "summary": "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",
      "published": "2024-06-11T21:30:15Z"
    },
    "metadata": {
      "arxiv_id": "2406.10275",
      "title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets",
      "summary": "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",
      "authors": [
        "Shahin Amiriparian",
        "Filip Packań",
        "Maurice Gerczuk",
        "Björn W. Schuller"
      ],
      "published": "2024-06-11T21:30:15Z",
      "updated": "2024-06-11T21:30:15Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10275v1",
      "landing_url": "https://arxiv.org/abs/2406.10275v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10275"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2406.10735",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10735v1",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "published": "2024-06-15T20:43:07Z"
    },
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.10743",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10743v1",
      "title": "Occam's Razor for Self Supervised Learning: What is Sufficient to Learn Good Representations?",
      "summary": "Deep Learning is often depicted as a trio of data-architecture-loss. Yet, recent Self Supervised Learning (SSL) solutions have introduced numerous additional design choices, e.g., a projector network, positive views, or teacher-student networks. These additions pose two challenges. First, they limit the impact of theoretical studies that often fail to incorporate all those intertwined designs. Second, they slow-down the deployment of SSL methods to new domains as numerous hyper-parameters need to be carefully tuned. In this study, we bring forward the surprising observation that--at least for pretraining datasets of up to a few hundred thousands samples--the additional designs introduced by SSL do not contribute to the quality of the learned representations. That finding not only provides legitimacy to existing theoretical studies, but also simplifies the practitioner's path to SSL deployment in numerous small and medium scale settings. Our finding answers a long-lasting question: the often-experienced sensitivity to training settings and hyper-parameters encountered in SSL come from their design, rather than the absence of supervised guidance.",
      "published": "2024-06-15T21:42:15Z"
    },
    "metadata": {
      "arxiv_id": "2406.10743",
      "title": "Occam's Razor for Self Supervised Learning: What is Sufficient to Learn Good Representations?",
      "summary": "Deep Learning is often depicted as a trio of data-architecture-loss. Yet, recent Self Supervised Learning (SSL) solutions have introduced numerous additional design choices, e.g., a projector network, positive views, or teacher-student networks. These additions pose two challenges. First, they limit the impact of theoretical studies that often fail to incorporate all those intertwined designs. Second, they slow-down the deployment of SSL methods to new domains as numerous hyper-parameters need to be carefully tuned. In this study, we bring forward the surprising observation that--at least for pretraining datasets of up to a few hundred thousands samples--the additional designs introduced by SSL do not contribute to the quality of the learned representations. That finding not only provides legitimacy to existing theoretical studies, but also simplifies the practitioner's path to SSL deployment in numerous small and medium scale settings. Our finding answers a long-lasting question: the often-experienced sensitivity to training settings and hyper-parameters encountered in SSL come from their design, rather than the absence of supervised guidance.",
      "authors": [
        "Mark Ibrahim",
        "David Klindt",
        "Randall Balestriero"
      ],
      "published": "2024-06-15T21:42:15Z",
      "updated": "2024-06-15T21:42:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10743v1",
      "landing_url": "https://arxiv.org/abs/2406.10743v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10743"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2406.11037",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11037v1",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "published": "2024-06-16T18:20:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "dblp_title",
        "search_term": "NAST: Noise Aware Speech Tokenization for Speech Language Models."
      }
    ]
  },
  {
    "arxiv_id": "2406.11175",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11175v2",
      "title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo Cancellation and Noise Suppression",
      "summary": "The proliferation of deep neural networks has spawned the rapid development of acoustic echo cancellation and noise suppression, and plenty of prior arts have been proposed, which yield promising performance. Nevertheless, they rarely consider the deployment generality in different processing scenarios, such as edge devices, and cloud processing. To this end, this paper proposes a general model, termed SMRU, to cover different application scenarios. The novelty lies in two-fold. First, a multi-scale band split layer and band merge layer are proposed to effectively fuse local frequency bands for lower complexity modeling. Besides, by simulating the multi-resolution feature modeling characteristic of the classical UNet structure, a novel recurrent-dominated UNet is devised. It consists of multiple variable frame rate blocks, each of which involves the causal time down-/up-sampling layer with varying compression ratios and the dual-path structure for inter- and intra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of MACs, and the experimental results show that the proposed approach yields competitive or even better performance over existing baselines, and has the full potential to adapt to more general scenarios with varying complexity requirements.",
      "published": "2024-06-17T03:28:08Z"
    },
    "metadata": {
      "arxiv_id": "2406.11175",
      "title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo Cancellation and Noise Suppression",
      "summary": "The proliferation of deep neural networks has spawned the rapid development of acoustic echo cancellation and noise suppression, and plenty of prior arts have been proposed, which yield promising performance. Nevertheless, they rarely consider the deployment generality in different processing scenarios, such as edge devices, and cloud processing. To this end, this paper proposes a general model, termed SMRU, to cover different application scenarios. The novelty lies in two-fold. First, a multi-scale band split layer and band merge layer are proposed to effectively fuse local frequency bands for lower complexity modeling. Besides, by simulating the multi-resolution feature modeling characteristic of the classical UNet structure, a novel recurrent-dominated UNet is devised. It consists of multiple variable frame rate blocks, each of which involves the causal time down-/up-sampling layer with varying compression ratios and the dual-path structure for inter- and intra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of MACs, and the experimental results show that the proposed approach yields competitive or even better performance over existing baselines, and has the full potential to adapt to more general scenarios with varying complexity requirements.",
      "authors": [
        "Zhihang Sun",
        "Andong Li",
        "Rilin Chen",
        "Hao Zhang",
        "Meng Yu",
        "Yi Zhou",
        "Dong Yu"
      ],
      "published": "2024-06-17T03:28:08Z",
      "updated": "2025-01-24T12:41:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11175v2",
      "landing_url": "https://arxiv.org/abs/2406.11175v2",
      "doi": "https://doi.org/10.1109/SLT61566.2024.10832279"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.11838",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11838v3",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "published": "2024-06-17T17:59:58Z"
    },
    "metadata": {
      "arxiv_id": "2406.11838",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "authors": [
        "Tianhong Li",
        "Yonglong Tian",
        "He Li",
        "Mingyang Deng",
        "Kaiming He"
      ],
      "published": "2024-06-17T17:59:58Z",
      "updated": "2024-11-01T14:45:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11838v3",
      "landing_url": "https://arxiv.org/abs/2406.11838v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.11838"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.12236",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.12236v1",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "published": "2024-06-18T03:24:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.12236",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "authors": [
        "Hanyu Meng",
        "Qiquan Zhang",
        "Xiangyu Zhang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "published": "2024-06-18T03:24:52Z",
      "updated": "2024-06-18T03:24:52Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12236v1",
      "landing_url": "https://arxiv.org/abs/2406.12236v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12236"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.13275",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13275v2",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "published": "2024-06-19T07:09:46Z"
    },
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.13431",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13431v2",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "published": "2024-06-19T10:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.13695",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13695v1",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "published": "2024-06-19T16:48:14Z"
    },
    "metadata": {
      "arxiv_id": "2406.13695",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "authors": [
        "Stefan Pasch",
        "Dimitirios Petridis",
        "Jannic Cutura"
      ],
      "published": "2024-06-19T16:48:14Z",
      "updated": "2024-06-19T16:48:14Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13695v1",
      "landing_url": "https://arxiv.org/abs/2406.13695v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13695"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2406.14017",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14017v2",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "published": "2024-06-20T06:21:56Z"
    },
    "metadata": {
      "arxiv_id": "2406.14017",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "authors": [
        "Ye Wang",
        "Jiahao Xun",
        "Minjie Hong",
        "Jieming Zhu",
        "Tao Jin",
        "Wang Lin",
        "Haoyuan Li",
        "Linjun Li",
        "Yan Xia",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "published": "2024-06-20T06:21:56Z",
      "updated": "2024-07-03T10:00:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14017v2",
      "landing_url": "https://arxiv.org/abs/2406.14017v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14017"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.14294",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14294v2",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "published": "2024-06-20T13:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.14421",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14421v1",
      "title": "Learning Binary Color Filter Arrays with Trainable Hard Thresholding",
      "summary": "Color Filter Arrays (CFA) are optical filters in digital cameras that capture specific color channels. Current commercial CFAs are hand-crafted patterns with different physical and application-specific considerations. This study proposes a binary CFA learning module based on hard thresholding with a deep learning-based demosaicing network in a joint architecture. Unlike most existing learnable CFAs that learn a channel from the whole color spectrum or linearly combine available digital colors, this method learns a binary channel selection, resulting in CFAs that are practical and physically implementable to digital cameras. The binary selection is based on adapting the hard thresholding operation into neural networks via a straight-through estimator, and therefore it is named HardMax. This paper includes the background on the CFA design problem, the description of the HardMax method, and the performance evaluation results. The evaluation of the proposed method includes tests for different demosaicing models, color configurations, filter sizes, and a comparison with existing methods in various reconstruction metrics. The proposed approach is tested with Kodak and BSDS500 datasets and provides higher reconstruction performance than hand-crafted or alternative learned binary filters.",
      "published": "2024-06-20T15:41:22Z"
    },
    "metadata": {
      "arxiv_id": "2406.14421",
      "title": "Learning Binary Color Filter Arrays with Trainable Hard Thresholding",
      "summary": "Color Filter Arrays (CFA) are optical filters in digital cameras that capture specific color channels. Current commercial CFAs are hand-crafted patterns with different physical and application-specific considerations. This study proposes a binary CFA learning module based on hard thresholding with a deep learning-based demosaicing network in a joint architecture. Unlike most existing learnable CFAs that learn a channel from the whole color spectrum or linearly combine available digital colors, this method learns a binary channel selection, resulting in CFAs that are practical and physically implementable to digital cameras. The binary selection is based on adapting the hard thresholding operation into neural networks via a straight-through estimator, and therefore it is named HardMax. This paper includes the background on the CFA design problem, the description of the HardMax method, and the performance evaluation results. The evaluation of the proposed method includes tests for different demosaicing models, color configurations, filter sizes, and a comparison with existing methods in various reconstruction metrics. The proposed approach is tested with Kodak and BSDS500 datasets and provides higher reconstruction performance than hand-crafted or alternative learned binary filters.",
      "authors": [
        "Cemre Omer Ayna",
        "Bahadir Kursat Gunturk",
        "Ali Cafer Gurbuz"
      ],
      "published": "2024-06-20T15:41:22Z",
      "updated": "2024-06-20T15:41:22Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14421v1",
      "landing_url": "https://arxiv.org/abs/2406.14421v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.14421"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2406.15752",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.15752v1",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "published": "2024-06-22T06:39:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.17310",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17310v1",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "published": "2024-06-25T06:46:47Z"
    },
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2407.03892",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03892v1",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "published": "2024-07-04T12:35:32Z"
    },
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2407.05407",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.05407v2",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "published": "2024-07-07T15:16:19Z"
    },
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2407.06667",
    "anchor": "acoustic tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.06667v2",
      "title": "Local zeta functions for a class of p-adic symmetric spaces (II)",
      "summary": "In this paper we study the zeta functions associated to the minimal spherical principal series of representations for a class of reductive p-adic symmetric spaces, which are realized as open orbits of some prehomogeneous spaces. These symmetric spaces have been studied in the paper arXiv: 2003.05764. We prove that the zeta functions satisfy a functional equation which is given explicitly (see Theorem 4.3.9 and Theorem 4.4.5). Moreover, for a subclass of these spaces, we define L-functions and epsilon-factors associated to the representations.",
      "published": "2024-07-09T08:43:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.06667",
      "title": "Local zeta functions for a class of p-adic symmetric spaces (II)",
      "summary": "In this paper we study the zeta functions associated to the minimal spherical principal series of representations for a class of reductive p-adic symmetric spaces, which are realized as open orbits of some prehomogeneous spaces. These symmetric spaces have been studied in the paper arXiv: 2003.05764. We prove that the zeta functions satisfy a functional equation which is given explicitly (see Theorem 4.3.9 and Theorem 4.4.5). Moreover, for a subclass of these spaces, we define L-functions and epsilon-factors associated to the representations.",
      "authors": [
        "Pascale Harinck",
        "Hubert Rubenthaler"
      ],
      "published": "2024-07-09T08:43:27Z",
      "updated": "2025-03-17T21:18:09Z",
      "categories": [
        "math.RT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06667v2",
      "landing_url": "https://arxiv.org/abs/2407.06667v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.06667"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2407.06945",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.06945v2",
      "title": "Adaptively Robust and Sparse K-means Clustering",
      "summary": "While K-means is known to be a standard clustering algorithm, its performance may be compromised due to the presence of outliers and high-dimensional noisy variables. This paper proposes adaptively robust and sparse K-means clustering (ARSK) to address these practical limitations of the standard K-means algorithm. For robustness, we introduce a redundant error component for each observation, and this additional parameter is penalized using a group sparse penalty. To accommodate the impact of high-dimensional noisy variables, the objective function is modified by incorporating weights and implementing a penalty to control the sparsity of the weight vector. The tuning parameters to control the robustness and sparsity are selected by Gap statistics. Through simulation experiments and real data analysis, we demonstrate the proposed method's superiority to existing algorithms in identifying clusters without outliers and informative variables simultaneously.",
      "published": "2024-07-09T15:20:41Z"
    },
    "metadata": {
      "arxiv_id": "2407.06945",
      "title": "Adaptively Robust and Sparse K-means Clustering",
      "summary": "While K-means is known to be a standard clustering algorithm, its performance may be compromised due to the presence of outliers and high-dimensional noisy variables. This paper proposes adaptively robust and sparse K-means clustering (ARSK) to address these practical limitations of the standard K-means algorithm. For robustness, we introduce a redundant error component for each observation, and this additional parameter is penalized using a group sparse penalty. To accommodate the impact of high-dimensional noisy variables, the objective function is modified by incorporating weights and implementing a penalty to control the sparsity of the weight vector. The tuning parameters to control the robustness and sparsity are selected by Gap statistics. Through simulation experiments and real data analysis, we demonstrate the proposed method's superiority to existing algorithms in identifying clusters without outliers and informative variables simultaneously.",
      "authors": [
        "Hao Li",
        "Shonosuke Sugasawa",
        "Shota Katayama"
      ],
      "published": "2024-07-09T15:20:41Z",
      "updated": "2024-11-07T15:02:32Z",
      "categories": [
        "stat.CO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06945v2",
      "landing_url": "https://arxiv.org/abs/2407.06945v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.06945"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2407.08152",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08152v2",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "published": "2024-07-11T03:10:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2407.08551",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08551v2",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "published": "2024-07-11T14:36:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.08551",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "authors": [
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Bing Han",
        "Shujie Hu",
        "Yanqing Liu",
        "Jinyu Li",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng",
        "Furu Wei"
      ],
      "published": "2024-07-11T14:36:53Z",
      "updated": "2025-05-27T05:07:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551v2",
      "landing_url": "https://arxiv.org/abs/2407.08551v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08551"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2407.09732",
    "anchor": "speech tokenization",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.09732v1",
      "title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for Speech Separation, Recognition, and Synthesis",
      "summary": "It is too early to conclude that Mamba is a better alternative to transformers for speech before comparing Mamba with transformers in terms of both performance and efficiency in multiple speech-related tasks. To reach this conclusion, we propose and evaluate three models for three tasks: Mamba-TasNet for speech separation, ConMamba for speech recognition, and VALL-M for speech synthesis. We compare them with transformers of similar sizes in performance, memory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable or higher performance than their transformer counterparts: Sepformer, Conformer, and VALL-E. They are more efficient than transformers in memory and speed for speech longer than a threshold duration, inversely related to the resolution of a speech token. Mamba for separation is the most efficient, and Mamba for recognition is the least. Further, we show that Mamba is not more efficient than transformer for speech shorter than the threshold duration and performs worse in models that require joint modeling of text and speech, such as cross or masked attention of two inputs. Therefore, we argue that the superiority of Mamba or transformer depends on particular problems and models. Code available at https://github.com/xi-j/Mamba-TasNet and https://github.com/xi-j/Mamba-ASR.",
      "published": "2024-07-13T00:35:21Z"
    },
    "metadata": {
      "arxiv_id": "2407.09732",
      "title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for Speech Separation, Recognition, and Synthesis",
      "summary": "It is too early to conclude that Mamba is a better alternative to transformers for speech before comparing Mamba with transformers in terms of both performance and efficiency in multiple speech-related tasks. To reach this conclusion, we propose and evaluate three models for three tasks: Mamba-TasNet for speech separation, ConMamba for speech recognition, and VALL-M for speech synthesis. We compare them with transformers of similar sizes in performance, memory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable or higher performance than their transformer counterparts: Sepformer, Conformer, and VALL-E. They are more efficient than transformers in memory and speed for speech longer than a threshold duration, inversely related to the resolution of a speech token. Mamba for separation is the most efficient, and Mamba for recognition is the least. Further, we show that Mamba is not more efficient than transformer for speech shorter than the threshold duration and performs worse in models that require joint modeling of text and speech, such as cross or masked attention of two inputs. Therefore, we argue that the superiority of Mamba or transformer depends on particular problems and models. Code available at https://github.com/xi-j/Mamba-TasNet and https://github.com/xi-j/Mamba-ASR.",
      "authors": [
        "Xilin Jiang",
        "Yinghao Aaron Li",
        "Adrian Nicolas Florea",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2024-07-13T00:35:21Z",
      "updated": "2024-07-13T00:35:21Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09732v1",
      "landing_url": "https://arxiv.org/abs/2407.09732v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.09732"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2407.11629",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.11629v1",
      "title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement",
      "summary": "Speaker anonymization is an effective privacy protection solution designed to conceal the speaker's identity while preserving the linguistic content and para-linguistic information of the original speech. While most prior studies focus solely on a single language, an ideal speaker anonymization system should be capable of handling multiple languages. This paper proposes MUSA, a Multi-lingual Speaker Anonymization approach that employs a serial disentanglement strategy to perform a step-by-step disentanglement from a global time-invariant representation to a temporal time-variant representation. By utilizing semantic distillation and self-supervised speaker distillation, the serial disentanglement strategy can avoid strong inductive biases and exhibit superior generalization performance across different languages. Meanwhile, we propose a straightforward anonymization strategy that employs empty embedding with zero values to simulate the speaker identity concealment process, eliminating the need for conversion to a pseudo-speaker identity and thereby reducing the complexity of speaker anonymization process. Experimental results on VoicePrivacy official datasets and multi-lingual datasets demonstrate that MUSA can effectively protect speaker privacy while preserving linguistic content and para-linguistic information.",
      "published": "2024-07-16T11:48:00Z"
    },
    "metadata": {
      "arxiv_id": "2407.11629",
      "title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement",
      "summary": "Speaker anonymization is an effective privacy protection solution designed to conceal the speaker's identity while preserving the linguistic content and para-linguistic information of the original speech. While most prior studies focus solely on a single language, an ideal speaker anonymization system should be capable of handling multiple languages. This paper proposes MUSA, a Multi-lingual Speaker Anonymization approach that employs a serial disentanglement strategy to perform a step-by-step disentanglement from a global time-invariant representation to a temporal time-variant representation. By utilizing semantic distillation and self-supervised speaker distillation, the serial disentanglement strategy can avoid strong inductive biases and exhibit superior generalization performance across different languages. Meanwhile, we propose a straightforward anonymization strategy that employs empty embedding with zero values to simulate the speaker identity concealment process, eliminating the need for conversion to a pseudo-speaker identity and thereby reducing the complexity of speaker anonymization process. Experimental results on VoicePrivacy official datasets and multi-lingual datasets demonstrate that MUSA can effectively protect speaker privacy while preserving linguistic content and para-linguistic information.",
      "authors": [
        "Jixun Yao",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Yuguang Yang",
        "Yu Pan",
        "Lei Xie"
      ],
      "published": "2024-07-16T11:48:00Z",
      "updated": "2024-07-16T11:48:00Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11629v1",
      "landing_url": "https://arxiv.org/abs/2407.11629v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.11629"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2407.12184",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.12184v1",
      "title": "The object detection method aids in image reconstruction evaluation and clinical interpretation of meniscal abnormalities",
      "summary": "This study investigates the relationship between deep learning (DL) image reconstruction quality and anomaly detection performance, and evaluates the efficacy of an artificial intelligence (AI) assistant in enhancing radiologists' interpretation of meniscal anomalies on reconstructed images. A retrospective study was conducted using an in-house reconstruction and anomaly detection pipeline to assess knee MR images from 896 patients. The original and 14 sets of DL-reconstructed images were evaluated using standard reconstruction and object detection metrics, alongside newly developed box-based reconstruction metrics. Two clinical radiologists reviewed a subset of 50 patients' images, both original and AI-assisted reconstructed, with subsequent assessment of their accuracy and performance characteristics. Results indicated that the structural similarity index (SSIM) showed a weaker correlation with anomaly detection metrics (mAP, r=0.64, p=0.01; F1 score, r=0.38, p=0.18), while box-based SSIM had a stronger association with detection performance (mAP, r=0.81, p<0.01; F1 score, r=0.65, p=0.01). Minor SSIM fluctuations did not affect detection outcomes, but significant changes reduced performance. Radiologists' AI-assisted evaluations demonstrated improved accuracy (86.0% without assistance vs. 88.3% with assistance, p<0.05) and interrater agreement (Cohen's kappa, 0.39 without assistance vs. 0.57 with assistance). An additional review led to the incorporation of 17 more lesions into the dataset. The proposed anomaly detection method shows promise in evaluating reconstruction algorithms for automated tasks and aiding radiologists in interpreting DL-reconstructed MR images.",
      "published": "2024-07-16T21:25:28Z"
    },
    "metadata": {
      "arxiv_id": "2407.12184",
      "title": "The object detection method aids in image reconstruction evaluation and clinical interpretation of meniscal abnormalities",
      "summary": "This study investigates the relationship between deep learning (DL) image reconstruction quality and anomaly detection performance, and evaluates the efficacy of an artificial intelligence (AI) assistant in enhancing radiologists' interpretation of meniscal anomalies on reconstructed images. A retrospective study was conducted using an in-house reconstruction and anomaly detection pipeline to assess knee MR images from 896 patients. The original and 14 sets of DL-reconstructed images were evaluated using standard reconstruction and object detection metrics, alongside newly developed box-based reconstruction metrics. Two clinical radiologists reviewed a subset of 50 patients' images, both original and AI-assisted reconstructed, with subsequent assessment of their accuracy and performance characteristics. Results indicated that the structural similarity index (SSIM) showed a weaker correlation with anomaly detection metrics (mAP, r=0.64, p=0.01; F1 score, r=0.38, p=0.18), while box-based SSIM had a stronger association with detection performance (mAP, r=0.81, p<0.01; F1 score, r=0.65, p=0.01). Minor SSIM fluctuations did not affect detection outcomes, but significant changes reduced performance. Radiologists' AI-assisted evaluations demonstrated improved accuracy (86.0% without assistance vs. 88.3% with assistance, p<0.05) and interrater agreement (Cohen's kappa, 0.39 without assistance vs. 0.57 with assistance). An additional review led to the incorporation of 17 more lesions into the dataset. The proposed anomaly detection method shows promise in evaluating reconstruction algorithms for automated tasks and aiding radiologists in interpreting DL-reconstructed MR images.",
      "authors": [
        "Natalia Konovalova",
        "Aniket Tolpadi",
        "Felix Liu",
        "Zehra Akkaya",
        "Felix Gassert",
        "Paula Giesler",
        "Johanna Luitjens",
        "Misung Han",
        "Emma Bahroos",
        "Sharmila Majumdar",
        "Valentina Pedoia"
      ],
      "published": "2024-07-16T21:25:28Z",
      "updated": "2024-07-16T21:25:28Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12184v1",
      "landing_url": "https://arxiv.org/abs/2407.12184v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12184"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2407.13333",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.13333v1",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "published": "2024-07-18T09:32:57Z"
    },
    "metadata": {
      "arxiv_id": "2407.13333",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "authors": [
        "Robert Sutherland",
        "George Close",
        "Thomas Hain",
        "Stefan Goetze",
        "Jon Barker"
      ],
      "published": "2024-07-18T09:32:57Z",
      "updated": "2024-07-18T09:32:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13333v1",
      "landing_url": "https://arxiv.org/abs/2407.13333v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13333"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2407.15356",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15356v1",
      "title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images",
      "summary": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of several metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: https://github.com/wangyunpengbio/X-Recon",
      "published": "2024-07-22T03:55:36Z"
    },
    "metadata": {
      "arxiv_id": "2407.15356",
      "title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images",
      "summary": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of several metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: https://github.com/wangyunpengbio/X-Recon",
      "authors": [
        "Yunpeng Wang",
        "Kang Wang",
        "Yaoyao Zhuo",
        "Weiya Shi",
        "Fei Shan",
        "Lei Liu"
      ],
      "published": "2024-07-22T03:55:36Z",
      "updated": "2024-07-22T03:55:36Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15356v1",
      "landing_url": "https://arxiv.org/abs/2407.15356v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15356"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2407.15458",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15458v4",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "published": "2024-07-22T08:14:16Z"
    },
    "metadata": {
      "arxiv_id": "2407.15458",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "authors": [
        "Wenze Ren",
        "Yi-Cheng Lin",
        "Huang-Cheng Chou",
        "Haibin Wu",
        "Yi-Chiao Wu",
        "Chi-Chun Lee",
        "Hung-yi Lee",
        "Yu Tsao"
      ],
      "published": "2024-07-22T08:14:16Z",
      "updated": "2024-07-30T12:37:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15458v4",
      "landing_url": "https://arxiv.org/abs/2407.15458v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.15458"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2407.15835",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15835v3",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "published": "2024-07-22T17:51:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2407.17060",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.17060v1",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "published": "2024-07-24T07:37:12Z"
    },
    "metadata": {
      "arxiv_id": "2407.17060",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "authors": [
        "Binzhe Li",
        "Shurun Wang",
        "Shiqi Wang",
        "Yan Ye"
      ],
      "published": "2024-07-24T07:37:12Z",
      "updated": "2024-07-24T07:37:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17060v1",
      "landing_url": "https://arxiv.org/abs/2407.17060v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.17060"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2408.00284",
    "anchor": "speech tokenization",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.00284v1",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "published": "2024-08-01T04:57:31Z"
    },
    "metadata": {
      "arxiv_id": "2408.00284",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "authors": [
        "Xinhan Di",
        "Zihao Chen",
        "Yunming Liang",
        "Junjie Zheng",
        "Yihua Wang",
        "Chaofan Ding"
      ],
      "published": "2024-08-01T04:57:31Z",
      "updated": "2024-08-01T04:57:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00284v1",
      "landing_url": "https://arxiv.org/abs/2408.00284v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.00284"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2408.01391",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.01391v2",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "published": "2024-08-02T17:01:36Z"
    },
    "metadata": {
      "arxiv_id": "2408.01391",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "authors": [
        "Shixun Wu",
        "Yitong Ding",
        "Yujia Zhai",
        "Jinyang Liu",
        "Jiajun Huang",
        "Zizhe Jian",
        "Huangliang Dai",
        "Sheng Di",
        "Bryan M. Wong",
        "Zizhong Chen",
        "Franck Cappello"
      ],
      "published": "2024-08-02T17:01:36Z",
      "updated": "2024-08-07T21:55:08Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01391v2",
      "landing_url": "https://arxiv.org/abs/2408.01391v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01391"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2408.04205",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.04205v1",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "published": "2024-08-08T04:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2408.04205",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "authors": [
        "Xinwei Chen",
        "Xiaofeng Zhong",
        "Zijian Zhang",
        "Linglong Dai",
        "Shidong Zhou"
      ],
      "published": "2024-08-08T04:05:18Z",
      "updated": "2024-08-08T04:05:18Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04205v1",
      "landing_url": "https://arxiv.org/abs/2408.04205v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04205"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2408.09483",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.09483v1",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "published": "2024-08-18T13:54:46Z"
    },
    "metadata": {
      "arxiv_id": "2408.09483",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "authors": [
        "Wei Zhao",
        "Dan Feng",
        "Wei Tong",
        "Xueliang Wei",
        "Bing Wu"
      ],
      "published": "2024-08-18T13:54:46Z",
      "updated": "2024-08-18T13:54:46Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09483v1",
      "landing_url": "https://arxiv.org/abs/2408.09483v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09483"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2408.11842",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11842v2",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "published": "2024-08-07T12:49:40Z"
    },
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2408.16373",
    "anchor": "speech tokenization",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.16373v1",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "published": "2024-08-29T09:31:06Z"
    },
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2408.17131",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17131v1",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "published": "2024-08-30T09:15:54Z"
    },
    "metadata": {
      "arxiv_id": "2408.17131",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Hong Gu",
        "Kedong Xu",
        "Kejie Huang"
      ],
      "published": "2024-08-30T09:15:54Z",
      "updated": "2024-08-30T09:15:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17131v1",
      "landing_url": "https://arxiv.org/abs/2408.17131v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2408.17175",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17175v3",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "published": "2024-08-30T10:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.17255",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17255v1",
      "title": "Self-supervised learning for crystal property prediction via denoising",
      "summary": "Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes.",
      "published": "2024-08-30T12:53:40Z"
    },
    "metadata": {
      "arxiv_id": "2408.17255",
      "title": "Self-supervised learning for crystal property prediction via denoising",
      "summary": "Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes.",
      "authors": [
        "Alexander New",
        "Nam Q. Le",
        "Michael J. Pekala",
        "Christopher D. Stiles"
      ],
      "published": "2024-08-30T12:53:40Z",
      "updated": "2024-08-30T12:53:40Z",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17255v1",
      "landing_url": "https://arxiv.org/abs/2408.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17255"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2409.00750",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00750v3",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "published": "2024-09-01T15:26:30Z"
    },
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.00942",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00942v1",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "published": "2024-09-02T05:01:41Z"
    },
    "metadata": {
      "arxiv_id": "2409.00942",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "authors": [
        "Yixuan Zhou",
        "Xing Xu",
        "Zhe Sun",
        "Jingkuan Song",
        "Andrzej Cichocki",
        "Heng Tao Shen"
      ],
      "published": "2024-09-02T05:01:41Z",
      "updated": "2024-09-02T05:01:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00942v1",
      "landing_url": "https://arxiv.org/abs/2409.00942v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.00942"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.01995",
    "anchor": "discrete speech tokens",
    "search_term": "speech token vocoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.01995v4",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "published": "2024-09-03T15:41:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.02384",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.02384v1",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "published": "2024-09-04T02:20:59Z"
    },
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2409.03283",
    "anchor": "speech tokenization",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03283v2",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "published": "2024-09-05T06:48:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.03283",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Kun Liu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie",
        "Kai-Tuo Xu"
      ],
      "published": "2024-09-05T06:48:02Z",
      "updated": "2025-04-11T07:36:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03283v2",
      "landing_url": "https://arxiv.org/abs/2409.03283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03283"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.03377",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03377v4",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "published": "2024-09-05T09:28:56Z"
    },
    "metadata": {
      "arxiv_id": "2409.03377",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "authors": [
        "Yan Ru Pei",
        "Ritik Shrivastava",
        "FNU Sidharth"
      ],
      "published": "2024-09-05T09:28:56Z",
      "updated": "2025-05-20T02:23:19Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03377v4",
      "landing_url": "https://arxiv.org/abs/2409.03377v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.03377"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2409.03393",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03393v1",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "published": "2024-09-05T09:53:53Z"
    },
    "metadata": {
      "arxiv_id": "2409.03393",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "authors": [
        "Yongyi Miao",
        "Zhongdang Li",
        "Yang Wang",
        "Die Hu",
        "Jun Yan",
        "Youfang Wang"
      ],
      "published": "2024-09-05T09:53:53Z",
      "updated": "2024-09-05T09:53:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03393v1",
      "landing_url": "https://arxiv.org/abs/2409.03393v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.03393"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.03701",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03701v2",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "published": "2024-09-05T16:57:39Z"
    },
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2409.04016",
    "anchor": "acoustic tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.04016v1",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "published": "2024-09-06T04:06:50Z"
    },
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.04173",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.04173v2",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "published": "2024-09-06T10:32:42Z"
    },
    "metadata": {
      "arxiv_id": "2409.04173",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "authors": [
        "Jixun Yao",
        "Nikita Kuzmin",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Dake Guo",
        "Kong Aik Lee",
        "Eng-Siong Chng",
        "Lei Xie"
      ],
      "published": "2024-09-06T10:32:42Z",
      "updated": "2025-02-04T08:43:31Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04173v2",
      "landing_url": "https://arxiv.org/abs/2409.04173v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.04173"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2409.05004",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.05004v2",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "published": "2024-09-08T07:24:03Z"
    },
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.06066",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.06066v3",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "published": "2024-09-09T20:58:40Z"
    },
    "metadata": {
      "arxiv_id": "2409.06066",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "authors": [
        "Marcel Gregoriadis",
        "Leonhard Balduf",
        "Björn Scheuermann",
        "Johan Pouwelse"
      ],
      "published": "2024-09-09T20:58:40Z",
      "updated": "2024-09-28T19:05:45Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06066v3",
      "landing_url": "https://arxiv.org/abs/2409.06066v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.06066"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2409.06237",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.06237v1",
      "title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion",
      "summary": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.",
      "published": "2024-09-10T06:10:33Z"
    },
    "metadata": {
      "arxiv_id": "2409.06237",
      "title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion",
      "summary": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.",
      "authors": [
        "Wei Chen",
        "Xintao Zhao",
        "Jun Chen",
        "Binzhu Sha",
        "Zhiwei Lin",
        "Zhiyong Wu"
      ],
      "published": "2024-09-10T06:10:33Z",
      "updated": "2024-09-10T06:10:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06237v1",
      "landing_url": "https://arxiv.org/abs/2409.06237v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.06237"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2409.07276",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07276v3",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "published": "2024-09-11T13:49:48Z"
    },
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.07556",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07556v2",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "published": "2024-09-11T18:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.09253",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09253v1",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "published": "2024-09-14T01:45:04Z"
    },
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.09357",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09357v1",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "published": "2024-09-14T08:09:55Z"
    },
    "metadata": {
      "arxiv_id": "2409.09357",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "published": "2024-09-14T08:09:55Z",
      "updated": "2024-09-14T08:09:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09357v1",
      "landing_url": "https://arxiv.org/abs/2409.09357v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09357"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.09921",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09921v2",
      "title": "Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation",
      "summary": "Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, ours is one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Resulting videos and code are provided at https://sites.google.com/illinois.edu/comp-teleop.",
      "published": "2024-09-16T01:39:50Z"
    },
    "metadata": {
      "arxiv_id": "2409.09921",
      "title": "Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation",
      "summary": "Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, ours is one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Resulting videos and code are provided at https://sites.google.com/illinois.edu/comp-teleop.",
      "authors": [
        "Neeloy Chakraborty",
        "Yixiao Fang",
        "Andre Schreiber",
        "Tianchen Ji",
        "Zhe Huang",
        "Aganze Mihigo",
        "Cassidy Wall",
        "Abdulrahman Almana",
        "Katherine Driggs-Campbell"
      ],
      "published": "2024-09-16T01:39:50Z",
      "updated": "2025-02-16T23:43:14Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09921v2",
      "landing_url": "https://arxiv.org/abs/2409.09921v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.09921"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.10103",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10103v1",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "published": "2024-09-16T09:07:08Z"
    },
    "metadata": {
      "arxiv_id": "2409.10103",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "authors": [
        "Ryota Komatsu",
        "Takahiro Shinozaki"
      ],
      "published": "2024-09-16T09:07:08Z",
      "updated": "2024-09-16T09:07:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10103v1",
      "landing_url": "https://arxiv.org/abs/2409.10103v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10103"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2409.10870",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10870v1",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "published": "2024-09-17T03:46:01Z"
    },
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.11003",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11003v1",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "published": "2024-09-17T09:08:43Z"
    },
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.11228",
    "anchor": "acoustic tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11228v2",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "published": "2024-09-17T14:21:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.11630",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11630v1",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "published": "2024-09-18T01:31:19Z"
    },
    "metadata": {
      "arxiv_id": "2409.11630",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-09-18T01:31:19Z",
      "updated": "2024-09-18T01:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11630v1",
      "landing_url": "https://arxiv.org/abs/2409.11630v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11630"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.12717",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12717v1",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "published": "2024-09-19T12:41:30Z"
    },
    "metadata": {
      "arxiv_id": "2409.12717",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "authors": [
        "Zhikang Niu",
        "Sanyuan Chen",
        "Long Zhou",
        "Ziyang Ma",
        "Xie Chen",
        "Shujie Liu"
      ],
      "published": "2024-09-19T12:41:30Z",
      "updated": "2024-09-19T12:41:30Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12717v1",
      "landing_url": "https://arxiv.org/abs/2409.12717v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12717"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.14085",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.14085v1",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "published": "2024-09-21T09:39:36Z"
    },
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.15897",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15897v2",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "published": "2024-09-24T09:16:11Z"
    },
    "metadata": {
      "arxiv_id": "2409.15897",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "authors": [
        "Jiatong Shi",
        "Jinchuan Tian",
        "Yihan Wu",
        "Jee-weon Jung",
        "Jia Qi Yip",
        "Yoshiki Masuyama",
        "William Chen",
        "Yuning Wu",
        "Yuxun Tang",
        "Massa Baali",
        "Dareen Alharhi",
        "Dong Zhang",
        "Ruifan Deng",
        "Tejes Srivastava",
        "Haibin Wu",
        "Alexander H. Liu",
        "Bhiksha Raj",
        "Qin Jin",
        "Ruihua Song",
        "Shinji Watanabe"
      ],
      "published": "2024-09-24T09:16:11Z",
      "updated": "2025-02-24T18:34:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15897v2",
      "landing_url": "https://arxiv.org/abs/2409.15897v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15897"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.17596",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.17596v1",
      "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
      "summary": "In recent years, live video streaming has gained widespread popularity across various social media platforms. Quality of experience (QoE), which reflects end-users' satisfaction and overall experience, plays a critical role for media service providers to optimize large-scale live compression and transmission strategies to achieve perceptually optimal rate-distortion trade-off. Although many QoE metrics for video-on-demand (VoD) have been proposed, there remain significant challenges in developing QoE metrics for live video streaming. To bridge this gap, we conduct a comprehensive study of subjective and objective QoE evaluations for live video streaming. For the subjective QoE study, we introduce the first live video streaming QoE dataset, TaoLive QoE, which consists of $42$ source videos collected from real live broadcasts and $1,155$ corresponding distorted ones degraded due to a variety of streaming distortions, including conventional streaming distortions such as compression, stalling, as well as live streaming-specific distortions like frame skipping, variable frame rate, etc. Subsequently, a human study was conducted to derive subjective QoE scores of videos in the TaoLive QoE dataset. For the objective QoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well as publicly available QoE datasets for VoD scenarios, highlighting that current models struggle to accurately assess video QoE, particularly for live content. Hence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates multi-scale semantic features and optical flow-based motion features to predicting a retrospective QoE score, eliminating reliance on statistical quality of service (QoS) features.",
      "published": "2024-09-26T07:22:38Z"
    },
    "metadata": {
      "arxiv_id": "2409.17596",
      "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
      "summary": "In recent years, live video streaming has gained widespread popularity across various social media platforms. Quality of experience (QoE), which reflects end-users' satisfaction and overall experience, plays a critical role for media service providers to optimize large-scale live compression and transmission strategies to achieve perceptually optimal rate-distortion trade-off. Although many QoE metrics for video-on-demand (VoD) have been proposed, there remain significant challenges in developing QoE metrics for live video streaming. To bridge this gap, we conduct a comprehensive study of subjective and objective QoE evaluations for live video streaming. For the subjective QoE study, we introduce the first live video streaming QoE dataset, TaoLive QoE, which consists of $42$ source videos collected from real live broadcasts and $1,155$ corresponding distorted ones degraded due to a variety of streaming distortions, including conventional streaming distortions such as compression, stalling, as well as live streaming-specific distortions like frame skipping, variable frame rate, etc. Subsequently, a human study was conducted to derive subjective QoE scores of videos in the TaoLive QoE dataset. For the objective QoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well as publicly available QoE datasets for VoD scenarios, highlighting that current models struggle to accurately assess video QoE, particularly for live content. Hence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates multi-scale semantic features and optical flow-based motion features to predicting a retrospective QoE score, eliminating reliance on statistical quality of service (QoS) features.",
      "authors": [
        "Zehao Zhu",
        "Wei Sun",
        "Jun Jia",
        "Wei Wu",
        "Sibin Deng",
        "Kai Li",
        "Ying Chen",
        "Xiongkuo Min",
        "Jia Wang",
        "Guangtao Zhai"
      ],
      "published": "2024-09-26T07:22:38Z",
      "updated": "2024-09-26T07:22:38Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.17596v1",
      "landing_url": "https://arxiv.org/abs/2409.17596v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.17596"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.19283",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.19283v2",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "published": "2024-09-28T08:36:44Z"
    },
    "metadata": {
      "arxiv_id": "2409.19283",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zhou Zhao",
        "Junyang Lin"
      ],
      "published": "2024-09-28T08:36:44Z",
      "updated": "2024-10-04T22:34:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19283v2",
      "landing_url": "https://arxiv.org/abs/2409.19283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.19283"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.00037",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00037v2",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "published": "2024-09-17T17:55:39Z"
    },
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2410.01141",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.01141v3",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "published": "2024-10-02T00:43:10Z"
    },
    "metadata": {
      "arxiv_id": "2410.01141",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "authors": [
        "Doohee You",
        "S Fraiberger"
      ],
      "published": "2024-10-02T00:43:10Z",
      "updated": "2025-06-30T18:26:08Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01141v3",
      "landing_url": "https://arxiv.org/abs/2410.01141v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.01141"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2410.03298",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.03298v1",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "published": "2024-10-04T10:21:15Z"
    },
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens."
      }
    ]
  },
  {
    "arxiv_id": "2410.04380",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04380v1",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "published": "2024-10-06T07:20:58Z"
    },
    "metadata": {
      "arxiv_id": "2410.04380",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "authors": [
        "Yuto Nishimura",
        "Takumi Hirose",
        "Masanari Ohi",
        "Hideki Nakayama",
        "Nakamasa Inoue"
      ],
      "published": "2024-10-06T07:20:58Z",
      "updated": "2024-10-06T07:20:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04380v1",
      "landing_url": "https://arxiv.org/abs/2410.04380v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04380"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.04959",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04959v4",
      "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
      "summary": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.",
      "published": "2024-10-07T11:58:56Z"
    },
    "metadata": {
      "arxiv_id": "2410.04959",
      "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
      "summary": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.",
      "authors": [
        "Emanuele Sansone",
        "Tim Lebailly",
        "Tinne Tuytelaars"
      ],
      "published": "2024-10-07T11:58:56Z",
      "updated": "2025-07-06T19:11:03Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04959v4",
      "landing_url": "https://arxiv.org/abs/2410.04959v4",
      "doi": "https://doi.org/10.48550/arXiv.2410.04959"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2410.05799",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.05799v4",
      "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
      "summary": "Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",
      "published": "2024-10-08T08:33:47Z"
    },
    "metadata": {
      "arxiv_id": "2410.05799",
      "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
      "summary": "Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",
      "authors": [
        "Qi Tang",
        "Yao Zhao",
        "Meiqin Liu",
        "Chao Yao"
      ],
      "published": "2024-10-08T08:33:47Z",
      "updated": "2024-10-26T06:11:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05799v4",
      "landing_url": "https://arxiv.org/abs/2410.05799v4",
      "doi": "https://doi.org/10.48550/arXiv.2410.05799"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2410.06016",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06016v3",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "published": "2024-10-08T13:18:24Z"
    },
    "metadata": {
      "arxiv_id": "2410.06016",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "authors": [
        "Yunkee Chae",
        "Woosung Choi",
        "Yuhta Takida",
        "Junghyun Koo",
        "Yukara Ikemiya",
        "Zhi Zhong",
        "Kin Wai Cheuk",
        "Marco A. Martínez-Ramírez",
        "Kyogu Lee",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2024-10-08T13:18:24Z",
      "updated": "2025-04-27T15:10:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06016v3",
      "landing_url": "https://arxiv.org/abs/2410.06016v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.06016"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.06424",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06424v2",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "published": "2024-10-08T23:39:34Z"
    },
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.07068",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.07068v2",
      "title": "A short proof of diffusivity for the directed polymers in the weak disorder phase",
      "summary": "We provide a new short and elementary proof of diffusivity for directed polymer in a random environment in the weak disorder phase.",
      "published": "2024-10-09T17:14:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.07068",
      "title": "A short proof of diffusivity for the directed polymers in the weak disorder phase",
      "summary": "We provide a new short and elementary proof of diffusivity for directed polymer in a random environment in the weak disorder phase.",
      "authors": [
        "Hubert Lacoin"
      ],
      "published": "2024-10-09T17:14:13Z",
      "updated": "2025-05-06T14:27:28Z",
      "categories": [
        "math.PR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07068v2",
      "landing_url": "https://arxiv.org/abs/2410.07068v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07068"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2410.07168",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.07168v2",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "published": "2024-10-09T17:59:04Z"
    },
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.08325",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08325v1",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "published": "2024-10-10T19:29:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.08469",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08469v2",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "published": "2024-10-11T02:42:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2410.10180",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.10180v1",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "published": "2024-10-14T05:58:11Z"
    },
    "metadata": {
      "arxiv_id": "2410.10180",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "authors": [
        "Mingyuan Yan",
        "Jiawei Wu",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-10-14T05:58:11Z",
      "updated": "2024-10-14T05:58:11Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10180v1",
      "landing_url": "https://arxiv.org/abs/2410.10180v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10180"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.11025",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11025v2",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "published": "2024-10-14T19:21:28Z"
    },
    "metadata": {
      "arxiv_id": "2410.11025",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "authors": [
        "Patrick O'Reilly",
        "Prem Seetharaman",
        "Jiaqi Su",
        "Zeyu Jin",
        "Bryan Pardo"
      ],
      "published": "2024-10-14T19:21:28Z",
      "updated": "2025-04-14T23:07:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11025v2",
      "landing_url": "https://arxiv.org/abs/2410.11025v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890096"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.11062",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11062v2",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "published": "2024-10-14T20:18:03Z"
    },
    "metadata": {
      "arxiv_id": "2410.11062",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "authors": [
        "Sjoerd Groot",
        "Qinyu Chen",
        "Jan C. van Gemert",
        "Chang Gao"
      ],
      "published": "2024-10-14T20:18:03Z",
      "updated": "2025-02-10T18:07:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11062v2",
      "landing_url": "https://arxiv.org/abs/2410.11062v2",
      "doi": "https://doi.org/10.1109/ISCAS56072.2025.11043389"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2410.12359",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.12359v2",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "published": "2024-10-16T08:21:37Z"
    },
    "metadata": {
      "arxiv_id": "2410.12359",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "authors": [
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-16T08:21:37Z",
      "updated": "2025-06-11T08:43:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12359v2",
      "landing_url": "https://arxiv.org/abs/2410.12359v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.12359"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.13495",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.13495v1",
      "title": "On uniqueness of the set of k-means",
      "summary": "We provide necessary and sufficient conditions for the uniqueness of the k-means set of a probability distribution. This uniqueness problem is related to the choice of k: depending on the underlying distribution, some values of this parameter could lead to multiple sets of k-means, which hampers the interpretation of the results and/or the stability of the algorithms. We give a general assessment on consistency of the empirical k-means adapted to the setting of non-uniqueness and determine the asymptotic distribution of the within cluster sum of squares (WCSS). We also provide statistical characterizations of k-means uniqueness in terms of the asymptotic behavior of the empirical WCSS. As a consequence, we derive a bootstrap test for uniqueness of the set of k-means. The results are illustrated with examples of different types of non-uniqueness and we check by simulations the performance of the proposed methodology.",
      "published": "2024-10-17T12:40:56Z"
    },
    "metadata": {
      "arxiv_id": "2410.13495",
      "title": "On uniqueness of the set of k-means",
      "summary": "We provide necessary and sufficient conditions for the uniqueness of the k-means set of a probability distribution. This uniqueness problem is related to the choice of k: depending on the underlying distribution, some values of this parameter could lead to multiple sets of k-means, which hampers the interpretation of the results and/or the stability of the algorithms. We give a general assessment on consistency of the empirical k-means adapted to the setting of non-uniqueness and determine the asymptotic distribution of the within cluster sum of squares (WCSS). We also provide statistical characterizations of k-means uniqueness in terms of the asymptotic behavior of the empirical WCSS. As a consequence, we derive a bootstrap test for uniqueness of the set of k-means. The results are illustrated with examples of different types of non-uniqueness and we check by simulations the performance of the proposed methodology.",
      "authors": [
        "Javier Cárcamo",
        "Antonio Cuevas",
        "Luis A. Rodríguez"
      ],
      "published": "2024-10-17T12:40:56Z",
      "updated": "2024-10-17T12:40:56Z",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13495v1",
      "landing_url": "https://arxiv.org/abs/2410.13495v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.13495"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2410.14269",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.14269v1",
      "title": "On time series clustering with k-means",
      "summary": "There is a long history of research into time series clustering using distance-based partitional clustering. Many of the most popular algorithms adapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in the data by specifying a time series distance function. However, these algorithms are often presented with k-means configured in various ways, altering key parameters such as the initialisation strategy. This variability makes it difficult to compare studies because k-means is known to be highly sensitive to its configuration. To address this, we propose a standard Lloyd's-based model for TSCL that adopts an end-to-end approach, incorporating a specialised distance function not only in the assignment step but also in the initialisation and stopping criteria. By doing so, we create a unified structure for comparing seven popular Lloyd's-based TSCL algorithms. This common framework enables us to more easily attribute differences in clustering performance to the distance function itself, rather than variations in the k-means configuration.",
      "published": "2024-10-18T08:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2410.14269",
      "title": "On time series clustering with k-means",
      "summary": "There is a long history of research into time series clustering using distance-based partitional clustering. Many of the most popular algorithms adapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in the data by specifying a time series distance function. However, these algorithms are often presented with k-means configured in various ways, altering key parameters such as the initialisation strategy. This variability makes it difficult to compare studies because k-means is known to be highly sensitive to its configuration. To address this, we propose a standard Lloyd's-based model for TSCL that adopts an end-to-end approach, incorporating a specialised distance function not only in the assignment step but also in the initialisation and stopping criteria. By doing so, we create a unified structure for comparing seven popular Lloyd's-based TSCL algorithms. This common framework enables us to more easily attribute differences in clustering performance to the distance function itself, rather than variations in the k-means configuration.",
      "authors": [
        "Christopher Holder",
        "Anthony Bagnall",
        "Jason Lines"
      ],
      "published": "2024-10-18T08:24:07Z",
      "updated": "2024-10-18T08:24:07Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14269v1",
      "landing_url": "https://arxiv.org/abs/2410.14269v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14269"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2410.14411",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.14411v1",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "published": "2024-10-18T12:24:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.14411",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "authors": [
        "Hubert Siuzdak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer"
      ],
      "published": "2024-10-18T12:24:05Z",
      "updated": "2024-10-18T12:24:05Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14411v1",
      "landing_url": "https://arxiv.org/abs/2410.14411v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14411"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2410.15017",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15017v2",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "published": "2024-10-19T07:14:14Z"
    },
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2410.15704",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15704v1",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "published": "2024-10-21T07:20:41Z"
    },
    "metadata": {
      "arxiv_id": "2410.15704",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "authors": [
        "Ankur Kumar"
      ],
      "published": "2024-10-21T07:20:41Z",
      "updated": "2024-10-21T07:20:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15704v1",
      "landing_url": "https://arxiv.org/abs/2410.15704v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15704"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.15764",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15764v3",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "published": "2024-10-21T08:23:31Z"
    },
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.16926",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.16926v2",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "published": "2024-10-22T11:57:32Z"
    },
    "metadata": {
      "arxiv_id": "2410.16926",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "authors": [
        "Tycho F. A. van der Ouderaa",
        "Maximilian L. Croci",
        "Agrin Hilmkil",
        "James Hensman"
      ],
      "published": "2024-10-22T11:57:32Z",
      "updated": "2024-12-04T10:52:04Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16926v2",
      "landing_url": "https://arxiv.org/abs/2410.16926v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16926"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.17081",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17081v2",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "published": "2024-10-22T15:02:37Z"
    },
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.17256",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17256v1",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "published": "2024-10-04T06:51:58Z"
    },
    "metadata": {
      "arxiv_id": "2410.17256",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "authors": [
        "Alfred K. Adzika",
        "Prudence Djagba"
      ],
      "published": "2024-10-04T06:51:58Z",
      "updated": "2024-10-04T06:51:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17256v1",
      "landing_url": "https://arxiv.org/abs/2410.17256v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17256"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2410.20573",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.20573v2",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "published": "2024-10-27T19:56:02Z"
    },
    "metadata": {
      "arxiv_id": "2410.20573",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "authors": [
        "Mohammad Hassan Vali",
        "Tom Bäckström"
      ],
      "published": "2024-10-27T19:56:02Z",
      "updated": "2025-07-02T10:27:13Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20573v2",
      "landing_url": "https://arxiv.org/abs/2410.20573v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.20573"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.21951",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.21951v2",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "published": "2024-10-29T11:12:01Z"
    },
    "metadata": {
      "arxiv_id": "2410.21951",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "authors": [
        "Bohan Li",
        "Hankun Wang",
        "Situo Zhang",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2024-10-29T11:12:01Z",
      "updated": "2025-02-10T04:22:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21951v2",
      "landing_url": "https://arxiv.org/abs/2410.21951v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21951"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.22448",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.22448v1",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "published": "2024-10-29T18:29:39Z"
    },
    "metadata": {
      "arxiv_id": "2410.22448",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "authors": [
        "Alexander H. Liu",
        "Qirui Wang",
        "Yuan Gong",
        "James Glass"
      ],
      "published": "2024-10-29T18:29:39Z",
      "updated": "2024-10-29T18:29:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22448v1",
      "landing_url": "https://arxiv.org/abs/2410.22448v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22448"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.24177",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.24177v1",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "published": "2024-10-31T17:43:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2411.01407",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.01407v1",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "published": "2024-11-03T02:14:03Z"
    },
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.02964",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.02964v2",
      "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT",
      "summary": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
      "published": "2024-11-05T10:06:40Z"
    },
    "metadata": {
      "arxiv_id": "2411.02964",
      "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT",
      "summary": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
      "authors": [
        "Pourya Jafarzadeh",
        "Amir Mohammad Rostami",
        "Padideh Choobdar"
      ],
      "published": "2024-11-05T10:06:40Z",
      "updated": "2024-11-06T14:18:15Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02964v2",
      "landing_url": "https://arxiv.org/abs/2411.02964v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.02964"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2411.03517",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.03517v2",
      "title": "Understanding Self-Supervised Learning via Gaussian Mixture Models",
      "summary": "Self-supervised learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze self-supervised learning in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla contrastive learning (specifically, the InfoNCE loss) is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We also prove a similar result for \"non-contrastive\" self-supervised learning (i.e., SimSiam loss). We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations. Finally, we corroborate our theoretical finding through synthetic data experiments.",
      "published": "2024-11-05T21:43:05Z"
    },
    "metadata": {
      "arxiv_id": "2411.03517",
      "title": "Understanding Self-Supervised Learning via Gaussian Mixture Models",
      "summary": "Self-supervised learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze self-supervised learning in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla contrastive learning (specifically, the InfoNCE loss) is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We also prove a similar result for \"non-contrastive\" self-supervised learning (i.e., SimSiam loss). We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations. Finally, we corroborate our theoretical finding through synthetic data experiments.",
      "authors": [
        "Parikshit Bansal",
        "Ali Kavis",
        "Sujay Sanghavi"
      ],
      "published": "2024-11-05T21:43:05Z",
      "updated": "2025-02-06T18:48:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03517v2",
      "landing_url": "https://arxiv.org/abs/2411.03517v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.03517"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2411.04257",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04257v3",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "published": "2024-11-06T21:00:45Z"
    },
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.04530",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04530v2",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "published": "2024-11-07T08:38:32Z"
    },
    "metadata": {
      "arxiv_id": "2411.04530",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "authors": [
        "Crystina Zhang",
        "Jing Lu",
        "Vinh Q. Tran",
        "Tal Schuster",
        "Donald Metzler",
        "Jimmy Lin"
      ],
      "published": "2024-11-07T08:38:32Z",
      "updated": "2025-11-19T00:30:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04530v2",
      "landing_url": "https://arxiv.org/abs/2411.04530v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.04530"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2411.06508",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.06508v1",
      "title": "Understanding the Role of Equivariance in Self-supervised Learning",
      "summary": "Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (\\eg colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.",
      "published": "2024-11-10T16:09:47Z"
    },
    "metadata": {
      "arxiv_id": "2411.06508",
      "title": "Understanding the Role of Equivariance in Self-supervised Learning",
      "summary": "Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (\\eg colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.",
      "authors": [
        "Yifei Wang",
        "Kaiwen Hu",
        "Sharut Gupta",
        "Ziyu Ye",
        "Yisen Wang",
        "Stefanie Jegelka"
      ],
      "published": "2024-11-10T16:09:47Z",
      "updated": "2024-11-10T16:09:47Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06508v1",
      "landing_url": "https://arxiv.org/abs/2411.06508v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06508"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2411.06968",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.06968v1",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "published": "2024-11-11T13:17:24Z"
    },
    "metadata": {
      "arxiv_id": "2411.06968",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "authors": [
        "Yoshiki Masuyama",
        "Koichi Miyazaki",
        "Masato Murata"
      ],
      "published": "2024-11-11T13:17:24Z",
      "updated": "2024-11-11T13:17:24Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06968v1",
      "landing_url": "https://arxiv.org/abs/2411.06968v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06968"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2411.08742",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.08742v1",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "published": "2024-11-13T16:20:20Z"
    },
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "dblp_title",
        "search_term": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models."
      }
    ]
  },
  {
    "arxiv_id": "2411.11123",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.11123v3",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "published": "2024-11-17T16:53:39Z"
    },
    "metadata": {
      "arxiv_id": "2411.11123",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "authors": [
        "Yu-Fei Shi",
        "Yang Ai",
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Zhen-Hua Ling"
      ],
      "published": "2024-11-17T16:53:39Z",
      "updated": "2024-12-23T12:42:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11123v3",
      "landing_url": "https://arxiv.org/abs/2411.11123v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.11123"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2411.11673",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.11673v2",
      "title": "Anomalous Spectroscopical Effects in an Antiferromagnetic Semiconductor",
      "summary": "Following the recent observation of anomalous Hall effect in antiferromagnetic hexagonal MnTe thin films, related phenomena at finite frequencies have come into focus. Magnetic circular dichroism (MCD) is the key material property here. In the x-ray range, the XMCD has already been demonstrated and used to visualise domains via photoemission electron microscopy (PEEM). Here we report on MCD in optical range and discuss its microscopic mechanism.",
      "published": "2024-11-18T15:51:50Z"
    },
    "metadata": {
      "arxiv_id": "2411.11673",
      "title": "Anomalous Spectroscopical Effects in an Antiferromagnetic Semiconductor",
      "summary": "Following the recent observation of anomalous Hall effect in antiferromagnetic hexagonal MnTe thin films, related phenomena at finite frequencies have come into focus. Magnetic circular dichroism (MCD) is the key material property here. In the x-ray range, the XMCD has already been demonstrated and used to visualise domains via photoemission electron microscopy (PEEM). Here we report on MCD in optical range and discuss its microscopic mechanism.",
      "authors": [
        "Michal Hubert",
        "Tomáš Maleček",
        "Kyo-Hoon Ahn",
        "Martin Míšek",
        "Jakub Železný",
        "František Máca",
        "Gunther Springholz",
        "Martin Veis",
        "Karel Výborný"
      ],
      "published": "2024-11-18T15:51:50Z",
      "updated": "2024-11-19T20:18:00Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11673v2",
      "landing_url": "https://arxiv.org/abs/2411.11673v2",
      "doi": "https://doi.org/10.1002/pssb.202400541"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2411.13753",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.13753v2",
      "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
      "summary": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
      "published": "2024-11-20T23:36:46Z"
    },
    "metadata": {
      "arxiv_id": "2411.13753",
      "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
      "summary": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
      "authors": [
        "Ola Shorinwa",
        "Jiankai Sun",
        "Mac Schwager"
      ],
      "published": "2024-11-20T23:36:46Z",
      "updated": "2025-03-12T02:17:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13753v2",
      "landing_url": "https://arxiv.org/abs/2411.13753v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.13753"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2411.14100",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.14100v2",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "published": "2024-11-21T13:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "dblp_title",
        "search_term": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection."
      }
    ]
  },
  {
    "arxiv_id": "2411.15197",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.15197v1",
      "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
      "summary": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.",
      "published": "2024-11-19T18:05:02Z"
    },
    "metadata": {
      "arxiv_id": "2411.15197",
      "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
      "summary": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.",
      "authors": [
        "Ziheng Sun",
        "Chris Ding",
        "Jicong Fan"
      ],
      "published": "2024-11-19T18:05:02Z",
      "updated": "2024-11-19T18:05:02Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.15197v1",
      "landing_url": "https://arxiv.org/abs/2411.15197v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.15197"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2411.16119",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16119v1",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "published": "2024-11-25T06:05:08Z"
    },
    "metadata": {
      "arxiv_id": "2411.16119",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu"
      ],
      "published": "2024-11-25T06:05:08Z",
      "updated": "2024-11-25T06:05:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16119v1",
      "landing_url": "https://arxiv.org/abs/2411.16119v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16119"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2411.16156",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16156v2",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "published": "2024-11-25T07:32:02Z"
    },
    "metadata": {
      "arxiv_id": "2411.16156",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "authors": [
        "Yicheng Feng",
        "Yijiang Li",
        "Wanpeng Zhang",
        "Hao Luo",
        "Zihao Yue",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "published": "2024-11-25T07:32:02Z",
      "updated": "2025-03-18T08:15:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16156v2",
      "landing_url": "https://arxiv.org/abs/2411.16156v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.16156"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2411.16550",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16550v1",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "published": "2024-11-25T16:32:29Z"
    },
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2411.17141",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17141v2",
      "title": "Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation",
      "summary": "Simultaneously using multimodal inputs from multiple sensors to train segmentors is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where multimodal segmentors over rely on certain modalities, causing performance drops when others are missing, common in real world applications. To this end, we develop the first framework for learning robust segmentor that can handle any combinations of visual modalities. Specifically, we first introduce a parallel multimodal learning strategy for learning a strong teacher. The cross-modal and unimodal distillation is then achieved in the multi scale representation space by transferring the feature level knowledge from multimodal to anymodal segmentors, aiming at addressing the unimodal bias and avoiding over-reliance on specific modalities. Moreover, a prediction level modality agnostic semantic distillation is proposed to achieve semantic knowledge transferring for segmentation. Extensive experiments on both synthetic and real-world multi-sensor benchmarks demonstrate that our method achieves superior performance.",
      "published": "2024-11-26T06:15:27Z"
    },
    "metadata": {
      "arxiv_id": "2411.17141",
      "title": "Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation",
      "summary": "Simultaneously using multimodal inputs from multiple sensors to train segmentors is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where multimodal segmentors over rely on certain modalities, causing performance drops when others are missing, common in real world applications. To this end, we develop the first framework for learning robust segmentor that can handle any combinations of visual modalities. Specifically, we first introduce a parallel multimodal learning strategy for learning a strong teacher. The cross-modal and unimodal distillation is then achieved in the multi scale representation space by transferring the feature level knowledge from multimodal to anymodal segmentors, aiming at addressing the unimodal bias and avoiding over-reliance on specific modalities. Moreover, a prediction level modality agnostic semantic distillation is proposed to achieve semantic knowledge transferring for segmentation. Extensive experiments on both synthetic and real-world multi-sensor benchmarks demonstrate that our method achieves superior performance.",
      "authors": [
        "Xu Zheng",
        "Haiwei Xue",
        "Jialei Chen",
        "Yibo Yan",
        "Lutao Jiang",
        "Yuanhuiyi Lyu",
        "Kailun Yang",
        "Linfeng Zhang",
        "Xuming Hu"
      ],
      "published": "2024-11-26T06:15:27Z",
      "updated": "2025-05-15T18:54:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17141v2",
      "landing_url": "https://arxiv.org/abs/2411.17141v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17141"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2411.17607",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17607v2",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "published": "2024-11-26T17:19:09Z"
    },
    "metadata": {
      "arxiv_id": "2411.17607",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "Shengmin Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-11-26T17:19:09Z",
      "updated": "2024-12-02T16:13:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17607v2",
      "landing_url": "https://arxiv.org/abs/2411.17607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17607"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.17773",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17773v2",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "published": "2024-11-26T09:36:02Z"
    },
    "metadata": {
      "arxiv_id": "2411.17773",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "authors": [
        "Minbin Huang",
        "Runhui Huang",
        "Han Shi",
        "Yimeng Chen",
        "Chuanyang Zheng",
        "Xiangguo Sun",
        "Xin Jiang",
        "Zhenguo Li",
        "Hong Cheng"
      ],
      "published": "2024-11-26T09:36:02Z",
      "updated": "2024-12-02T14:55:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17773v2",
      "landing_url": "https://arxiv.org/abs/2411.17773v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17773"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2411.17998",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17998v1",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "published": "2024-11-27T02:31:52Z"
    },
    "metadata": {
      "arxiv_id": "2411.17998",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "authors": [
        "Jia Qi Yip",
        "Chin Yuen Kwok",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2024-11-27T02:31:52Z",
      "updated": "2024-11-27T02:31:52Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17998v1",
      "landing_url": "https://arxiv.org/abs/2411.17998v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.17998"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2412.02563",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.02563v1",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "published": "2024-12-03T16:52:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.04917",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.04917v1",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "published": "2024-12-06T10:16:04Z"
    },
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2412.09607",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.09607v2",
      "title": "Spectral Image Tokenizer",
      "summary": "Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing.",
      "published": "2024-12-12T18:59:31Z"
    },
    "metadata": {
      "arxiv_id": "2412.09607",
      "title": "Spectral Image Tokenizer",
      "summary": "Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing.",
      "authors": [
        "Carlos Esteves",
        "Mohammed Suhail",
        "Ameesh Makadia"
      ],
      "published": "2024-12-12T18:59:31Z",
      "updated": "2025-06-11T17:58:34Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.09607v2",
      "landing_url": "https://arxiv.org/abs/2412.09607v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.09607"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2412.10117",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10117v3",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "published": "2024-12-13T12:59:39Z"
    },
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.10261",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10261v2",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "published": "2024-12-13T16:30:35Z"
    },
    "metadata": {
      "arxiv_id": "2412.10261",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "authors": [
        "Shuaiting Li",
        "Chengxuan Wang",
        "Juncan Deng",
        "Zeyu Wang",
        "Zewen Ye",
        "Zongsheng Wang",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "published": "2024-12-13T16:30:35Z",
      "updated": "2024-12-16T08:54:43Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10261v2",
      "landing_url": "https://arxiv.org/abs/2412.10261v2",
      "doi": "https://doi.org/10.1145/3669940.3707268"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.11102",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11102v3",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "published": "2024-12-15T07:49:31Z"
    },
    "metadata": {
      "arxiv_id": "2412.11102",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "authors": [
        "Ximing Xing",
        "Juncheng Hu",
        "Guotao Liang",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "published": "2024-12-15T07:49:31Z",
      "updated": "2025-03-25T15:35:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11102v3",
      "landing_url": "https://arxiv.org/abs/2412.11102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.11102"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.11449",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11449v1",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "published": "2024-12-16T05:03:48Z"
    },
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.12581",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.12581v2",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "published": "2024-12-17T06:20:39Z"
    },
    "metadata": {
      "arxiv_id": "2412.12581",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "authors": [
        "Haifeng Lu",
        "Jiuyi Chen",
        "Feng Liang",
        "Mingkui Tan",
        "Runhao Zeng",
        "Xiping Hu"
      ],
      "published": "2024-12-17T06:20:39Z",
      "updated": "2024-12-20T11:49:07Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12581v2",
      "landing_url": "https://arxiv.org/abs/2412.12581v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.12581"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.14169",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14169v2",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "published": "2024-12-18T18:59:53Z"
    },
    "metadata": {
      "arxiv_id": "2412.14169",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Haiwen Diao",
        "Zhengxiong Luo",
        "Yufeng Cui",
        "Huchuan Lu",
        "Shiguang Shan",
        "Yonggang Qi",
        "Xinlong Wang"
      ],
      "published": "2024-12-18T18:59:53Z",
      "updated": "2025-03-02T08:09:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14169v2",
      "landing_url": "https://arxiv.org/abs/2412.14169v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.14169"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.14643",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14643v1",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "published": "2024-12-19T08:51:57Z"
    },
    "metadata": {
      "arxiv_id": "2412.14643",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "authors": [
        "Jie Huang",
        "Ruibing Hou",
        "Jiahe Zhao",
        "Hong Chang",
        "Shiguang Shan"
      ],
      "published": "2024-12-19T08:51:57Z",
      "updated": "2024-12-19T08:51:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14643v1",
      "landing_url": "https://arxiv.org/abs/2412.14643v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14643"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.14802",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14802v1",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "published": "2024-12-19T12:48:17Z"
    },
    "metadata": {
      "arxiv_id": "2412.14802",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "authors": [
        "Egor Shibaev",
        "Denis Sushentsev",
        "Yaroslav Golubev",
        "Aleksandr Khvorov"
      ],
      "published": "2024-12-19T12:48:17Z",
      "updated": "2024-12-19T12:48:17Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14802v1",
      "landing_url": "https://arxiv.org/abs/2412.14802v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14802"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2412.15195",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15195v1",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "published": "2024-12-19T18:58:14Z"
    },
    "metadata": {
      "arxiv_id": "2412.15195",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "authors": [
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published": "2024-12-19T18:58:14Z",
      "updated": "2024-12-19T18:58:14Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15195v1",
      "landing_url": "https://arxiv.org/abs/2412.15195v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15195"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.15649",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15649v1",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "published": "2024-12-20T08:05:55Z"
    },
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.16102",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16102v3",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "published": "2024-12-20T17:43:50Z"
    },
    "metadata": {
      "arxiv_id": "2412.16102",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Hui Wang",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yuzhe Liang",
        "Ziyang Ma",
        "Yuxuan Hu",
        "Rui Zhao",
        "Jianwei Yu",
        "Yan Lu",
        "Xie Chen"
      ],
      "published": "2024-12-20T17:43:50Z",
      "updated": "2025-08-09T10:01:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16102v3",
      "landing_url": "https://arxiv.org/abs/2412.16102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16102"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2412.16626",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16626v2",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "published": "2024-12-21T13:43:51Z"
    },
    "metadata": {
      "arxiv_id": "2412.16626",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "authors": [
        "Junyu Wang",
        "Zizhen Lin",
        "Tianrui Wang",
        "Meng Ge",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "published": "2024-12-21T13:43:51Z",
      "updated": "2025-01-02T10:56:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16626v2",
      "landing_url": "https://arxiv.org/abs/2412.16626v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16626"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2412.16846",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16846v2",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "published": "2024-12-22T04:03:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.17048",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17048v1",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "published": "2024-12-22T14:59:19Z"
    },
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.17586",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17586v1",
      "title": "Enhancing Reconstruction-Based Out-of-Distribution Detection in Brain MRI with Model and Metric Ensembles",
      "summary": "Out-of-distribution (OOD) detection is crucial for safely deploying automated medical image analysis systems, as abnormal patterns in images could hamper their performance. However, OOD detection in medical imaging remains an open challenge, and we address three gaps: the underexplored potential of a simple OOD detection model, the lack of optimization of deep learning strategies specifically for OOD detection, and the selection of appropriate reconstruction metrics. In this study, we investigated the effectiveness of a reconstruction-based autoencoder for unsupervised detection of synthetic artifacts in brain MRI. We evaluated the general reconstruction capability of the model, analyzed the impact of the selected training epoch and reconstruction metrics, assessed the potential of model and/or metric ensembles, and tested the model on a dataset containing a diverse range of artifacts. Among the metrics assessed, the contrast component of SSIM and LPIPS consistently outperformed others in detecting homogeneous circular anomalies. By combining two well-converged models and using LPIPS and contrast as reconstruction metrics, we achieved a pixel-level area under the Precision-Recall curve of 0.66. Furthermore, with the more realistic OOD dataset, we observed that the detection performance varied between artifact types; local artifacts were more difficult to detect, while global artifacts showed better detection results. These findings underscore the importance of carefully selecting metrics and model configurations, and highlight the need for tailored approaches, as standard deep learning approaches do not always align with the unique needs of OOD detection.",
      "published": "2024-12-23T13:58:52Z"
    },
    "metadata": {
      "arxiv_id": "2412.17586",
      "title": "Enhancing Reconstruction-Based Out-of-Distribution Detection in Brain MRI with Model and Metric Ensembles",
      "summary": "Out-of-distribution (OOD) detection is crucial for safely deploying automated medical image analysis systems, as abnormal patterns in images could hamper their performance. However, OOD detection in medical imaging remains an open challenge, and we address three gaps: the underexplored potential of a simple OOD detection model, the lack of optimization of deep learning strategies specifically for OOD detection, and the selection of appropriate reconstruction metrics. In this study, we investigated the effectiveness of a reconstruction-based autoencoder for unsupervised detection of synthetic artifacts in brain MRI. We evaluated the general reconstruction capability of the model, analyzed the impact of the selected training epoch and reconstruction metrics, assessed the potential of model and/or metric ensembles, and tested the model on a dataset containing a diverse range of artifacts. Among the metrics assessed, the contrast component of SSIM and LPIPS consistently outperformed others in detecting homogeneous circular anomalies. By combining two well-converged models and using LPIPS and contrast as reconstruction metrics, we achieved a pixel-level area under the Precision-Recall curve of 0.66. Furthermore, with the more realistic OOD dataset, we observed that the detection performance varied between artifact types; local artifacts were more difficult to detect, while global artifacts showed better detection results. These findings underscore the importance of carefully selecting metrics and model configurations, and highlight the need for tailored approaches, as standard deep learning approaches do not always align with the unique needs of OOD detection.",
      "authors": [
        "Evi M. C. Huijben",
        "Sina Amirrajab",
        "Josien P. W. Pluim"
      ],
      "published": "2024-12-23T13:58:52Z",
      "updated": "2024-12-23T13:58:52Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17586v1",
      "landing_url": "https://arxiv.org/abs/2412.17586v1",
      "doi": "https://doi.org/10.1016/j.cmpb.2025.109045"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2412.17640",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17640v2",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "published": "2024-12-23T15:18:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.19248",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.19248v1",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "published": "2024-12-26T15:08:36Z"
    },
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2501.01046",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01046v3",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "published": "2025-01-02T04:11:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.01231",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01231v1",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "published": "2025-01-02T12:45:31Z"
    },
    "metadata": {
      "arxiv_id": "2501.01231",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2025-01-02T12:45:31Z",
      "updated": "2025-01-02T12:45:31Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01231v1",
      "landing_url": "https://arxiv.org/abs/2501.01231v1",
      "doi": "https://doi.org/10.1109/TIP.2024.352281"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.02293",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.02293v2",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "published": "2025-01-04T14:03:56Z"
    },
    "metadata": {
      "arxiv_id": "2501.02293",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "authors": [
        "Ellison Murray",
        "Morriel Kasher",
        "Predrag Spasojevic"
      ],
      "published": "2025-01-04T14:03:56Z",
      "updated": "2025-01-09T20:11:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02293v2",
      "landing_url": "https://arxiv.org/abs/2501.02293v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.02293"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2501.02350",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.02350v1",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "published": "2025-01-04T18:12:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.04379",
    "anchor": "dblp_title",
    "search_term": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition.",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.04379v1",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "published": "2025-01-08T09:45:14Z"
    },
    "metadata": {
      "arxiv_id": "2501.04379",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "authors": [
        "Huimeng Wang",
        "Xurong Xie",
        "Mengzhe Geng",
        "Shujie Hu",
        "Haoning Xu",
        "Youjun Chen",
        "Zhaoqing Li",
        "Jiajun Deng",
        "Xunying Liu"
      ],
      "published": "2025-01-08T09:45:14Z",
      "updated": "2025-01-08T09:45:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04379v1",
      "landing_url": "https://arxiv.org/abs/2501.04379v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04379"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition."
      }
    ]
  },
  {
    "arxiv_id": "2501.05587",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05587v1",
      "title": "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear Algebra",
      "summary": "K-means is a popular clustering algorithm with significant applications in numerous scientific and engineering areas. One drawback of K-means is its inability to identify non-linearly separable clusters, which may lead to inaccurate solutions in certain cases. Kernel K-means is a variant of classical K-means that can find non-linearly separable clusters. However, it scales quadratically with respect to the size of the dataset, taking several minutes to cluster even medium-sized datasets on traditional CPU-based machines. In this paper, we present a formulation of Kernel K-means using sparse-dense matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV), and we show that our formulation enables the rapid implementation of a fast GPU-based version of Kernel K-means with little programming effort. Our implementation, named Popcorn, is the first open-source GPU-based implementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x over a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a GPU implementation of Kernel K-means that does not use sparse matrix computations. Our results support the effectiveness of sparse matrices as tools for efficient parallel programming.",
      "published": "2025-01-09T21:43:16Z"
    },
    "metadata": {
      "arxiv_id": "2501.05587",
      "title": "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear Algebra",
      "summary": "K-means is a popular clustering algorithm with significant applications in numerous scientific and engineering areas. One drawback of K-means is its inability to identify non-linearly separable clusters, which may lead to inaccurate solutions in certain cases. Kernel K-means is a variant of classical K-means that can find non-linearly separable clusters. However, it scales quadratically with respect to the size of the dataset, taking several minutes to cluster even medium-sized datasets on traditional CPU-based machines. In this paper, we present a formulation of Kernel K-means using sparse-dense matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV), and we show that our formulation enables the rapid implementation of a fast GPU-based version of Kernel K-means with little programming effort. Our implementation, named Popcorn, is the first open-source GPU-based implementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x over a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a GPU implementation of Kernel K-means that does not use sparse matrix computations. Our results support the effectiveness of sparse matrices as tools for efficient parallel programming.",
      "authors": [
        "Julian Bellavita",
        "Thomas Pasquali",
        "Laura Del Rio Martin",
        "Flavio Vella",
        "Giulia Guidi"
      ],
      "published": "2025-01-09T21:43:16Z",
      "updated": "2025-01-09T21:43:16Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05587v1",
      "landing_url": "https://arxiv.org/abs/2501.05587v1",
      "doi": "https://doi.org/10.1145/3710848.3710887"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2501.05787",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05787v1",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "published": "2025-01-10T08:41:42Z"
    },
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2501.06336",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.06336v1",
      "title": "MEt3R: Measuring Multi-View Consistency in Generated Images",
      "summary": "We introduce MEt3R, a metric for multi-view consistency in generated images. Large-scale generative models for multi-view image generation are rapidly advancing the field of 3D inference from sparse observations. However, due to the nature of generative modeling, traditional reconstruction metrics are not suitable to measure the quality of generated outputs and metrics that are independent of the sampling procedure are desperately needed. In this work, we specifically address the aspect of consistency between generated multi-view images, which can be evaluated independently of the specific scene. Our approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a feed-forward manner, which are used to warp image contents from one view into the other. Then, feature maps of these images are compared to obtain a similarity score that is invariant to view-dependent effects. Using MEt3R, we evaluate the consistency of a large set of previous methods for novel view and video generation, including our open, multi-view latent diffusion model.",
      "published": "2025-01-10T20:43:33Z"
    },
    "metadata": {
      "arxiv_id": "2501.06336",
      "title": "MEt3R: Measuring Multi-View Consistency in Generated Images",
      "summary": "We introduce MEt3R, a metric for multi-view consistency in generated images. Large-scale generative models for multi-view image generation are rapidly advancing the field of 3D inference from sparse observations. However, due to the nature of generative modeling, traditional reconstruction metrics are not suitable to measure the quality of generated outputs and metrics that are independent of the sampling procedure are desperately needed. In this work, we specifically address the aspect of consistency between generated multi-view images, which can be evaluated independently of the specific scene. Our approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a feed-forward manner, which are used to warp image contents from one view into the other. Then, feature maps of these images are compared to obtain a similarity score that is invariant to view-dependent effects. Using MEt3R, we evaluate the consistency of a large set of previous methods for novel view and video generation, including our open, multi-view latent diffusion model.",
      "authors": [
        "Mohammad Asim",
        "Christopher Wewer",
        "Thomas Wimmer",
        "Bernt Schiele",
        "Jan Eric Lenssen"
      ],
      "published": "2025-01-10T20:43:33Z",
      "updated": "2025-01-10T20:43:33Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06336v1",
      "landing_url": "https://arxiv.org/abs/2501.06336v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06336"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2501.06514",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.06514v1",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "published": "2025-01-11T11:15:58Z"
    },
    "metadata": {
      "arxiv_id": "2501.06514",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "authors": [
        "Yuankun Xie",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Songjun Cao",
        "Long Ma",
        "Chenxing Li",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "published": "2025-01-11T11:15:58Z",
      "updated": "2025-01-11T11:15:58Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06514v1",
      "landing_url": "https://arxiv.org/abs/2501.06514v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06514"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.14249",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.14249v9",
      "title": "Humanity's Last Exam",
      "summary": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
      "published": "2025-01-24T05:27:46Z"
    },
    "metadata": {
      "arxiv_id": "2501.14249",
      "title": "Humanity's Last Exam",
      "summary": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
      "authors": [
        "Long Phan",
        "Alice Gatti",
        "Ziwen Han",
        "Nathaniel Li",
        "Josephina Hu",
        "Hugh Zhang",
        "Chen Bo Calvin Zhang",
        "Mohamed Shaaban",
        "John Ling",
        "Sean Shi",
        "Michael Choi",
        "Anish Agrawal",
        "Arnav Chopra",
        "Adam Khoja",
        "Ryan Kim",
        "Richard Ren",
        "Jason Hausenloy",
        "Oliver Zhang",
        "Mantas Mazeika",
        "Dmitry Dodonov",
        "Tung Nguyen",
        "Jaeho Lee",
        "Daron Anderson",
        "Mikhail Doroshenko",
        "Alun Cennyth Stokes",
        "Mobeen Mahmood",
        "Oleksandr Pokutnyi",
        "Oleg Iskra",
        "Jessica P. Wang",
        "John-Clark Levin",
        "Mstyslav Kazakov",
        "Fiona Feng",
        "Steven Y. Feng",
        "Haoran Zhao",
        "Michael Yu",
        "Varun Gangal",
        "Chelsea Zou",
        "Zihan Wang",
        "Serguei Popov",
        "Robert Gerbicz",
        "Geoff Galgon",
        "Johannes Schmitt",
        "Will Yeadon",
        "Yongki Lee",
        "Scott Sauers",
        "Alvaro Sanchez",
        "Fabian Giska",
        "Marc Roth",
        "Søren Riis",
        "Saiteja Utpala",
        "Noah Burns",
        "Gashaw M. Goshu",
        "Mohinder Maheshbhai Naiya",
        "Chidozie Agu",
        "Zachary Giboney",
        "Antrell Cheatom",
        "Francesco Fournier-Facio",
        "Sarah-Jane Crowson",
        "Lennart Finke",
        "Zerui Cheng",
        "Jennifer Zampese",
        "Ryan G. Hoerr",
        "Mark Nandor",
        "Hyunwoo Park",
        "Tim Gehrunger",
        "Jiaqi Cai",
        "Ben McCarty",
        "Alexis C Garretson",
        "Edwin Taylor",
        "Damien Sileo",
        "Qiuyu Ren",
        "Usman Qazi",
        "Lianghui Li",
        "Jungbae Nam",
        "John B. Wydallis",
        "Pavel Arkhipov",
        "Jack Wei Lun Shi",
        "Aras Bacho",
        "Chris G. Willcocks",
        "Hangrui Cao",
        "Sumeet Motwani",
        "Emily de Oliveira Santos",
        "Johannes Veith",
        "Edward Vendrow",
        "Doru Cojoc",
        "Kengo Zenitani",
        "Joshua Robinson",
        "Longke Tang",
        "Yuqi Li",
        "Joshua Vendrow",
        "Natanael Wildner Fraga",
        "Vladyslav Kuchkin",
        "Andrey Pupasov Maksimov",
        "Pierre Marion",
        "Denis Efremov",
        "Jayson Lynch",
        "Kaiqu Liang",
        "Aleksandar Mikov",
        "Andrew Gritsevskiy",
        "Julien Guillod",
        "Gözdenur Demir",
        "Dakotah Martinez",
        "Ben Pageler",
        "Kevin Zhou",
        "Saeed Soori",
        "Ori Press",
        "Henry Tang",
        "Paolo Rissone",
        "Sean R. Green",
        "Lina Brüssel",
        "Moon Twayana",
        "Aymeric Dieuleveut",
        "Joseph Marvin Imperial",
        "Ameya Prabhu",
        "Jinzhou Yang",
        "Nick Crispino",
        "Arun Rao",
        "Dimitri Zvonkine",
        "Gabriel Loiseau",
        "Mikhail Kalinin",
        "Marco Lukas",
        "Ciprian Manolescu",
        "Nate Stambaugh",
        "Subrata Mishra",
        "Tad Hogg",
        "Carlo Bosio",
        "Brian P Coppola",
        "Julian Salazar",
        "Jaehyeok Jin",
        "Rafael Sayous",
        "Stefan Ivanov",
        "Philippe Schwaller",
        "Shaipranesh Senthilkuma",
        "Andres M Bran",
        "Andres Algaba",
        "Kelsey Van den Houte",
        "Lynn Van Der Sypt",
        "Brecht Verbeken",
        "David Noever",
        "Alexei Kopylov",
        "Benjamin Myklebust",
        "Bikun Li",
        "Lisa Schut",
        "Evgenii Zheltonozhskii",
        "Qiaochu Yuan",
        "Derek Lim",
        "Richard Stanley",
        "Tong Yang",
        "John Maar",
        "Julian Wykowski",
        "Martí Oller",
        "Anmol Sahu",
        "Cesare Giulio Ardito",
        "Yuzheng Hu",
        "Ariel Ghislain Kemogne Kamdoum",
        "Alvin Jin",
        "Tobias Garcia Vilchis",
        "Yuexuan Zu",
        "Martin Lackner",
        "James Koppel",
        "Gongbo Sun",
        "Daniil S. Antonenko",
        "Steffi Chern",
        "Bingchen Zhao",
        "Pierrot Arsene",
        "Joseph M Cavanagh",
        "Daofeng Li",
        "Jiawei Shen",
        "Donato Crisostomi",
        "Wenjin Zhang",
        "Ali Dehghan",
        "Sergey Ivanov",
        "David Perrella",
        "Nurdin Kaparov",
        "Allen Zang",
        "Ilia Sucholutsky",
        "Arina Kharlamova",
        "Daniil Orel",
        "Vladislav Poritski",
        "Shalev Ben-David",
        "Zachary Berger",
        "Parker Whitfill",
        "Michael Foster",
        "Daniel Munro",
        "Linh Ho",
        "Shankar Sivarajan",
        "Dan Bar Hava",
        "Aleksey Kuchkin",
        "David Holmes",
        "Alexandra Rodriguez-Romero",
        "Frank Sommerhage",
        "Anji Zhang",
        "Richard Moat",
        "Keith Schneider",
        "Zakayo Kazibwe",
        "Don Clarke",
        "Dae Hyun Kim",
        "Felipe Meneguitti Dias",
        "Sara Fish",
        "Veit Elser",
        "Tobias Kreiman",
        "Victor Efren Guadarrama Vilchis",
        "Immo Klose",
        "Ujjwala Anantheswaran",
        "Adam Zweiger",
        "Kaivalya Rawal",
        "Jeffery Li",
        "Jeremy Nguyen",
        "Nicolas Daans",
        "Haline Heidinger",
        "Maksim Radionov",
        "Václav Rozhoň",
        "Vincent Ginis",
        "Christian Stump",
        "Niv Cohen",
        "Rafał Poświata",
        "Josef Tkadlec",
        "Alan Goldfarb",
        "Chenguang Wang",
        "Piotr Padlewski",
        "Stanislaw Barzowski",
        "Kyle Montgomery",
        "Ryan Stendall",
        "Jamie Tucker-Foltz",
        "Jack Stade",
        "T. Ryan Rogers",
        "Tom Goertzen",
        "Declan Grabb",
        "Abhishek Shukla",
        "Alan Givré",
        "John Arnold Ambay",
        "Archan Sen",
        "Muhammad Fayez Aziz",
        "Mark H Inlow",
        "Hao He",
        "Ling Zhang",
        "Younesse Kaddar",
        "Ivar Ängquist",
        "Yanxu Chen",
        "Harrison K Wang",
        "Kalyan Ramakrishnan",
        "Elliott Thornley",
        "Antonio Terpin",
        "Hailey Schoelkopf",
        "Eric Zheng",
        "Avishy Carmi",
        "Ethan D. L. Brown",
        "Kelin Zhu",
        "Max Bartolo",
        "Richard Wheeler",
        "Martin Stehberger",
        "Peter Bradshaw",
        "JP Heimonen",
        "Kaustubh Sridhar",
        "Ido Akov",
        "Jennifer Sandlin",
        "Yury Makarychev",
        "Joanna Tam",
        "Hieu Hoang",
        "David M. Cunningham",
        "Vladimir Goryachev",
        "Demosthenes Patramanis",
        "Michael Krause",
        "Andrew Redenti",
        "David Aldous",
        "Jesyin Lai",
        "Shannon Coleman",
        "Jiangnan Xu",
        "Sangwon Lee",
        "Ilias Magoulas",
        "Sandy Zhao",
        "Ning Tang",
        "Michael K. Cohen",
        "Orr Paradise",
        "Jan Hendrik Kirchner",
        "Maksym Ovchynnikov",
        "Jason O. Matos",
        "Adithya Shenoy",
        "Michael Wang",
        "Yuzhou Nie",
        "Anna Sztyber-Betley",
        "Paolo Faraboschi",
        "Robin Riblet",
        "Jonathan Crozier",
        "Shiv Halasyamani",
        "Shreyas Verma",
        "Prashant Joshi",
        "Eli Meril",
        "Ziqiao Ma",
        "Jérémy Andréoletti",
        "Raghav Singhal",
        "Jacob Platnick",
        "Volodymyr Nevirkovets",
        "Luke Basler",
        "Alexander Ivanov",
        "Seri Khoury",
        "Nils Gustafsson",
        "Marco Piccardo",
        "Hamid Mostaghimi",
        "Qijia Chen",
        "Virendra Singh",
        "Tran Quoc Khánh",
        "Paul Rosu",
        "Hannah Szlyk",
        "Zachary Brown",
        "Himanshu Narayan",
        "Aline Menezes",
        "Jonathan Roberts",
        "William Alley",
        "Kunyang Sun",
        "Arkil Patel",
        "Max Lamparth",
        "Anka Reuel",
        "Linwei Xin",
        "Hanmeng Xu",
        "Jacob Loader",
        "Freddie Martin",
        "Zixuan Wang",
        "Andrea Achilleos",
        "Thomas Preu",
        "Tomek Korbak",
        "Ida Bosio",
        "Fereshteh Kazemi",
        "Ziye Chen",
        "Biró Bálint",
        "Eve J. Y. Lo",
        "Jiaqi Wang",
        "Maria Inês S. Nunes",
        "Jeremiah Milbauer",
        "M Saiful Bari",
        "Zihao Wang",
        "Behzad Ansarinejad",
        "Yewen Sun",
        "Stephane Durand",
        "Hossam Elgnainy",
        "Guillaume Douville",
        "Daniel Tordera",
        "George Balabanian",
        "Hew Wolff",
        "Lynna Kvistad",
        "Hsiaoyun Milliron",
        "Ahmad Sakor",
        "Murat Eron",
        "Andrew Favre D. O.",
        "Shailesh Shah",
        "Xiaoxiang Zhou",
        "Firuz Kamalov",
        "Sherwin Abdoli",
        "Tim Santens",
        "Shaul Barkan",
        "Allison Tee",
        "Robin Zhang",
        "Alessandro Tomasiello",
        "G. Bruno De Luca",
        "Shi-Zhuo Looi",
        "Vinh-Kha Le",
        "Noam Kolt",
        "Jiayi Pan",
        "Emma Rodman",
        "Jacob Drori",
        "Carl J Fossum",
        "Niklas Muennighoff",
        "Milind Jagota",
        "Ronak Pradeep",
        "Honglu Fan",
        "Jonathan Eicher",
        "Michael Chen",
        "Kushal Thaman",
        "William Merrill",
        "Moritz Firsching",
        "Carter Harris",
        "Stefan Ciobâcă",
        "Jason Gross",
        "Rohan Pandey",
        "Ilya Gusev",
        "Adam Jones",
        "Shashank Agnihotri",
        "Pavel Zhelnov",
        "Mohammadreza Mofayezi",
        "Alexander Piperski",
        "David K. Zhang",
        "Kostiantyn Dobarskyi",
        "Roman Leventov",
        "Ignat Soroko",
        "Joshua Duersch",
        "Vage Taamazyan",
        "Andrew Ho",
        "Wenjie Ma",
        "William Held",
        "Ruicheng Xian",
        "Armel Randy Zebaze",
        "Mohanad Mohamed",
        "Julian Noah Leser",
        "Michelle X Yuan",
        "Laila Yacar",
        "Johannes Lengler",
        "Katarzyna Olszewska",
        "Claudio Di Fratta",
        "Edson Oliveira",
        "Joseph W. Jackson",
        "Andy Zou",
        "Muthu Chidambaram",
        "Timothy Manik",
        "Hector Haffenden",
        "Dashiell Stander",
        "Ali Dasouqi",
        "Alexander Shen",
        "Bita Golshani",
        "David Stap",
        "Egor Kretov",
        "Mikalai Uzhou",
        "Alina Borisovna Zhidkovskaya",
        "Nick Winter",
        "Miguel Orbegozo Rodriguez",
        "Robert Lauff",
        "Dustin Wehr",
        "Colin Tang",
        "Zaki Hossain",
        "Shaun Phillips",
        "Fortuna Samuele",
        "Fredrik Ekström",
        "Angela Hammon",
        "Oam Patel",
        "Faraz Farhidi",
        "George Medley",
        "Forough Mohammadzadeh",
        "Madellene Peñaflor",
        "Haile Kassahun",
        "Alena Friedrich",
        "Rayner Hernandez Perez",
        "Daniel Pyda",
        "Taom Sakal",
        "Omkar Dhamane",
        "Ali Khajegili Mirabadi",
        "Eric Hallman",
        "Kenchi Okutsu",
        "Mike Battaglia",
        "Mohammad Maghsoudimehrabani",
        "Alon Amit",
        "Dave Hulbert",
        "Roberto Pereira",
        "Simon Weber",
        "Handoko",
        "Anton Peristyy",
        "Stephen Malina",
        "Mustafa Mehkary",
        "Rami Aly",
        "Frank Reidegeld",
        "Anna-Katharina Dick",
        "Cary Friday",
        "Mukhwinder Singh",
        "Hassan Shapourian",
        "Wanyoung Kim",
        "Mariana Costa",
        "Hubeyb Gurdogan",
        "Harsh Kumar",
        "Chiara Ceconello",
        "Chao Zhuang",
        "Haon Park",
        "Micah Carroll",
        "Andrew R. Tawfeek",
        "Stefan Steinerberger",
        "Daattavya Aggarwal",
        "Michael Kirchhof",
        "Linjie Dai",
        "Evan Kim",
        "Johan Ferret",
        "Jainam Shah",
        "Yuzhou Wang",
        "Minghao Yan",
        "Krzysztof Burdzy",
        "Lixin Zhang",
        "Antonio Franca",
        "Diana T. Pham",
        "Kang Yong Loh",
        "Joshua Robinson",
        "Abram Jackson",
        "Paolo Giordano",
        "Philipp Petersen",
        "Adrian Cosma",
        "Jesus Colino",
        "Colin White",
        "Jacob Votava",
        "Vladimir Vinnikov",
        "Ethan Delaney",
        "Petr Spelda",
        "Vit Stritecky",
        "Syed M. Shahid",
        "Jean-Christophe Mourrat",
        "Lavr Vetoshkin",
        "Koen Sponselee",
        "Renas Bacho",
        "Zheng-Xin Yong",
        "Florencia de la Rosa",
        "Nathan Cho",
        "Xiuyu Li",
        "Guillaume Malod",
        "Orion Weller",
        "Guglielmo Albani",
        "Leon Lang",
        "Julien Laurendeau",
        "Dmitry Kazakov",
        "Fatimah Adesanya",
        "Julien Portier",
        "Lawrence Hollom",
        "Victor Souza",
        "Yuchen Anna Zhou",
        "Julien Degorre",
        "Yiğit Yalın",
        "Gbenga Daniel Obikoya",
        "Rai",
        "Filippo Bigi",
        "M. C. Boscá",
        "Oleg Shumar",
        "Kaniuar Bacho",
        "Gabriel Recchia",
        "Mara Popescu",
        "Nikita Shulga",
        "Ngefor Mildred Tanwie",
        "Thomas C. H. Lux",
        "Ben Rank",
        "Colin Ni",
        "Matthew Brooks",
        "Alesia Yakimchyk",
        "Huanxu",
        "Liu",
        "Stefano Cavalleri",
        "Olle Häggström",
        "Emil Verkama",
        "Joshua Newbould",
        "Hans Gundlach",
        "Leonor Brito-Santana",
        "Brian Amaro",
        "Vivek Vajipey",
        "Rynaa Grover",
        "Ting Wang",
        "Yosi Kratish",
        "Wen-Ding Li",
        "Sivakanth Gopi",
        "Andrea Caciolai",
        "Christian Schroeder de Witt",
        "Pablo Hernández-Cámara",
        "Emanuele Rodolà",
        "Jules Robins",
        "Dominic Williamson",
        "Vincent Cheng",
        "Brad Raynor",
        "Hao Qi",
        "Ben Segev",
        "Jingxuan Fan",
        "Sarah Martinson",
        "Erik Y. Wang",
        "Kaylie Hausknecht",
        "Michael P. Brenner",
        "Mao Mao",
        "Christoph Demian",
        "Peyman Kassani",
        "Xinyu Zhang",
        "David Avagian",
        "Eshawn Jessica Scipio",
        "Alon Ragoler",
        "Justin Tan",
        "Blake Sims",
        "Rebeka Plecnik",
        "Aaron Kirtland",
        "Omer Faruk Bodur",
        "D. P. Shinde",
        "Yan Carlos Leyva Labrador",
        "Zahra Adoul",
        "Mohamed Zekry",
        "Ali Karakoc",
        "Tania C. B. Santos",
        "Samir Shamseldeen",
        "Loukmane Karim",
        "Anna Liakhovitskaia",
        "Nate Resman",
        "Nicholas Farina",
        "Juan Carlos Gonzalez",
        "Gabe Maayan",
        "Earth Anderson",
        "Rodrigo De Oliveira Pena",
        "Elizabeth Kelley",
        "Hodjat Mariji",
        "Rasoul Pouriamanesh",
        "Wentao Wu",
        "Ross Finocchio",
        "Ismail Alarab",
        "Joshua Cole",
        "Danyelle Ferreira",
        "Bryan Johnson",
        "Mohammad Safdari",
        "Liangti Dai",
        "Siriphan Arthornthurasuk",
        "Isaac C. McAlister",
        "Alejandro José Moyano",
        "Alexey Pronin",
        "Jing Fan",
        "Angel Ramirez-Trinidad",
        "Yana Malysheva",
        "Daphiny Pottmaier",
        "Omid Taheri",
        "Stanley Stepanic",
        "Samuel Perry",
        "Luke Askew",
        "Raúl Adrián Huerta Rodríguez",
        "Ali M. R. Minissi",
        "Ricardo Lorena",
        "Krishnamurthy Iyer",
        "Arshad Anil Fasiludeen",
        "Ronald Clark",
        "Josh Ducey",
        "Matheus Piza",
        "Maja Somrak",
        "Eric Vergo",
        "Juehang Qin",
        "Benjámin Borbás",
        "Eric Chu",
        "Jack Lindsey",
        "Antoine Jallon",
        "I. M. J. McInnis",
        "Evan Chen",
        "Avi Semler",
        "Luk Gloor",
        "Tej Shah",
        "Marc Carauleanu",
        "Pascal Lauer",
        "Tran Đuc Huy",
        "Hossein Shahrtash",
        "Emilien Duc",
        "Lukas Lewark",
        "Assaf Brown",
        "Samuel Albanie",
        "Brian Weber",
        "Warren S. Vaz",
        "Pierre Clavier",
        "Yiyang Fan",
        "Gabriel Poesia Reis e Silva",
        "Long",
        "Lian",
        "Marcus Abramovitch",
        "Xi Jiang",
        "Sandra Mendoza",
        "Murat Islam",
        "Juan Gonzalez",
        "Vasilios Mavroudis",
        "Justin Xu",
        "Pawan Kumar",
        "Laxman Prasad Goswami",
        "Daniel Bugas",
        "Nasser Heydari",
        "Ferenc Jeanplong",
        "Thorben Jansen",
        "Antonella Pinto",
        "Archimedes Apronti",
        "Abdallah Galal",
        "Ng Ze-An",
        "Ankit Singh",
        "Tong Jiang",
        "Joan of Arc Xavier",
        "Kanu Priya Agarwal",
        "Mohammed Berkani",
        "Gang Zhang",
        "Zhehang Du",
        "Benedito Alves de Oliveira Junior",
        "Dmitry Malishev",
        "Nicolas Remy",
        "Taylor D. Hartman",
        "Tim Tarver",
        "Stephen Mensah",
        "Gautier Abou Loume",
        "Wiktor Morak",
        "Farzad Habibi",
        "Sarah Hoback",
        "Will Cai",
        "Javier Gimenez",
        "Roselynn Grace Montecillo",
        "Jakub Łucki",
        "Russell Campbell",
        "Asankhaya Sharma",
        "Khalida Meer",
        "Shreen Gul",
        "Daniel Espinosa Gonzalez",
        "Xavier Alapont",
        "Alex Hoover",
        "Gunjan Chhablani",
        "Freddie Vargus",
        "Arunim Agarwal",
        "Yibo Jiang",
        "Deepakkumar Patil",
        "David Outevsky",
        "Kevin Joseph Scaria",
        "Rajat Maheshwari",
        "Abdelkader Dendane",
        "Priti Shukla",
        "Ashley Cartwright",
        "Sergei Bogdanov",
        "Niels Mündler",
        "Sören Möller",
        "Luca Arnaboldi",
        "Kunvar Thaman",
        "Muhammad Rehan Siddiqi",
        "Prajvi Saxena",
        "Himanshu Gupta",
        "Tony Fruhauff",
        "Glen Sherman",
        "Mátyás Vincze",
        "Siranut Usawasutsakorn",
        "Dylan Ler",
        "Anil Radhakrishnan",
        "Innocent Enyekwe",
        "Sk Md Salauddin",
        "Jiang Muzhen",
        "Aleksandr Maksapetyan",
        "Vivien Rossbach",
        "Chris Harjadi",
        "Mohsen Bahaloohoreh",
        "Claire Sparrow",
        "Jasdeep Sidhu",
        "Sam Ali",
        "Song Bian",
        "John Lai",
        "Eric Singer",
        "Justine Leon Uro",
        "Greg Bateman",
        "Mohamed Sayed",
        "Ahmed Menshawy",
        "Darling Duclosel",
        "Dario Bezzi",
        "Yashaswini Jain",
        "Ashley Aaron",
        "Murat Tiryakioglu",
        "Sheeshram Siddh",
        "Keith Krenek",
        "Imad Ali Shah",
        "Jun Jin",
        "Scott Creighton",
        "Denis Peskoff",
        "Zienab EL-Wasif",
        "Ragavendran P",
        "Michael Richmond",
        "Joseph McGowan",
        "Tejal Patwardhan",
        "Hao-Yu Sun",
        "Ting Sun",
        "Nikola Zubić",
        "Samuele Sala",
        "Stephen Ebert",
        "Jean Kaddour",
        "Manuel Schottdorf",
        "Dianzhuo Wang",
        "Gerol Petruzella",
        "Alex Meiburg",
        "Tilen Medved",
        "Ali ElSheikh",
        "S Ashwin Hebbar",
        "Lorenzo Vaquero",
        "Xianjun Yang",
        "Jason Poulos",
        "Vilém Zouhar",
        "Sergey Bogdanik",
        "Mingfang Zhang",
        "Jorge Sanz-Ros",
        "David Anugraha",
        "Yinwei Dai",
        "Anh N. Nhu",
        "Xue Wang",
        "Ali Anil Demircali",
        "Zhibai Jia",
        "Yuyin Zhou",
        "Juncheng Wu",
        "Mike He",
        "Nitin Chandok",
        "Aarush Sinha",
        "Gaoxiang Luo",
        "Long Le",
        "Mickaël Noyé",
        "Michał Perełkiewicz",
        "Ioannis Pantidis",
        "Tianbo Qi",
        "Soham Sachin Purohit",
        "Letitia Parcalabescu",
        "Thai-Hoa Nguyen",
        "Genta Indra Winata",
        "Edoardo M. Ponti",
        "Hanchen Li",
        "Kaustubh Dhole",
        "Jongee Park",
        "Dario Abbondanza",
        "Yuanli Wang",
        "Anupam Nayak",
        "Diogo M. Caetano",
        "Antonio A. W. L. Wong",
        "Maria del Rio-Chanona",
        "Dániel Kondor",
        "Pieter Francois",
        "Ed Chalstrey",
        "Jakob Zsambok",
        "Dan Hoyer",
        "Jenny Reddish",
        "Jakob Hauser",
        "Francisco-Javier Rodrigo-Ginés",
        "Suchandra Datta",
        "Maxwell Shepherd",
        "Thom Kamphuis",
        "Qizheng Zhang",
        "Hyunjun Kim",
        "Ruiji Sun",
        "Jianzhu Yao",
        "Franck Dernoncourt",
        "Satyapriya Krishna",
        "Sina Rismanchian",
        "Bonan Pu",
        "Francesco Pinto",
        "Yingheng Wang",
        "Kumar Shridhar",
        "Kalon J. Overholt",
        "Glib Briia",
        "Hieu Nguyen",
        "David",
        "Soler Bartomeu",
        "Tony CY Pang",
        "Adam Wecker",
        "Yifan Xiong",
        "Fanfei Li",
        "Lukas S. Huber",
        "Joshua Jaeger",
        "Romano De Maddalena",
        "Xing Han Lù",
        "Yuhui Zhang",
        "Claas Beger",
        "Patrick Tser Jern Kon",
        "Sean Li",
        "Vivek Sanker",
        "Ming Yin",
        "Yihao Liang",
        "Xinlu Zhang",
        "Ankit Agrawal",
        "Li S. Yifei",
        "Zechen Zhang",
        "Mu Cai",
        "Yasin Sonmez",
        "Costin Cozianu",
        "Changhao Li",
        "Alex Slen",
        "Shoubin Yu",
        "Hyun Kyu Park",
        "Gabriele Sarti",
        "Marcin Briański",
        "Alessandro Stolfo",
        "Truong An Nguyen",
        "Mike Zhang",
        "Yotam Perlitz",
        "Jose Hernandez-Orallo",
        "Runjia Li",
        "Amin Shabani",
        "Felix Juefei-Xu",
        "Shikhar Dhingra",
        "Orr Zohar",
        "My Chiffon Nguyen",
        "Alexander Pondaven",
        "Abdurrahim Yilmaz",
        "Xuandong Zhao",
        "Chuanyang Jin",
        "Muyan Jiang",
        "Stefan Todoran",
        "Xinyao Han",
        "Jules Kreuer",
        "Brian Rabern",
        "Anna Plassart",
        "Martino Maggetti",
        "Luther Yap",
        "Robert Geirhos",
        "Jonathon Kean",
        "Dingsu Wang",
        "Sina Mollaei",
        "Chenkai Sun",
        "Yifan Yin",
        "Shiqi Wang",
        "Rui Li",
        "Yaowen Chang",
        "Anjiang Wei",
        "Alice Bizeul",
        "Xiaohan Wang",
        "Alexandre Oliveira Arrais",
        "Kushin Mukherjee",
        "Jorge Chamorro-Padial",
        "Jiachen Liu",
        "Xingyu Qu",
        "Junyi Guan",
        "Adam Bouyamourn",
        "Shuyu Wu",
        "Martyna Plomecka",
        "Junda Chen",
        "Mengze Tang",
        "Jiaqi Deng",
        "Shreyas Subramanian",
        "Haocheng Xi",
        "Haoxuan Chen",
        "Weizhi Zhang",
        "Yinuo Ren",
        "Haoqin Tu",
        "Sejong Kim",
        "Yushun Chen",
        "Sara Vera Marjanović",
        "Junwoo Ha",
        "Grzegorz Luczyna",
        "Jeff J. Ma",
        "Zewen Shen",
        "Dawn Song",
        "Cedegao E. Zhang",
        "Zhun Wang",
        "Gaël Gendron",
        "Yunze Xiao",
        "Leo Smucker",
        "Erica Weng",
        "Kwok Hao Lee",
        "Zhe Ye",
        "Stefano Ermon",
        "Ignacio D. Lopez-Miguel",
        "Theo Knights",
        "Anthony Gitter",
        "Namkyu Park",
        "Boyi Wei",
        "Hongzheng Chen",
        "Kunal Pai",
        "Ahmed Elkhanany",
        "Han Lin",
        "Philipp D. Siedler",
        "Jichao Fang",
        "Ritwik Mishra",
        "Károly Zsolnai-Fehér",
        "Xilin Jiang",
        "Shadab Khan",
        "Jun Yuan",
        "Rishab Kumar Jain",
        "Xi Lin",
        "Mike Peterson",
        "Zhe Wang",
        "Aditya Malusare",
        "Maosen Tang",
        "Isha Gupta",
        "Ivan Fosin",
        "Timothy Kang",
        "Barbara Dworakowska",
        "Kazuki Matsumoto",
        "Guangyao Zheng",
        "Gerben Sewuster",
        "Jorge Pretel Villanueva",
        "Ivan Rannev",
        "Igor Chernyavsky",
        "Jiale Chen",
        "Deepayan Banik",
        "Ben Racz",
        "Wenchao Dong",
        "Jianxin Wang",
        "Laila Bashmal",
        "Duarte V. Gonçalves",
        "Wei Hu",
        "Kaushik Bar",
        "Ondrej Bohdal",
        "Atharv Singh Patlan",
        "Shehzaad Dhuliawala",
        "Caroline Geirhos",
        "Julien Wist",
        "Yuval Kansal",
        "Bingsen Chen",
        "Kutay Tire",
        "Atak Talay Yücel",
        "Brandon Christof",
        "Veerupaksh Singla",
        "Zijian Song",
        "Sanxing Chen",
        "Jiaxin Ge",
        "Kaustubh Ponkshe",
        "Isaac Park",
        "Tianneng Shi",
        "Martin Q. Ma",
        "Joshua Mak",
        "Sherwin Lai",
        "Antoine Moulin",
        "Zhuo Cheng",
        "Zhanda Zhu",
        "Ziyi Zhang",
        "Vaidehi Patil",
        "Ketan Jha",
        "Qiutong Men",
        "Jiaxuan Wu",
        "Tianchi Zhang",
        "Bruno Hebling Vieira",
        "Alham Fikri Aji",
        "Jae-Won Chung",
        "Mohammed Mahfoud",
        "Ha Thi Hoang",
        "Marc Sperzel",
        "Wei Hao",
        "Kristof Meding",
        "Sihan Xu",
        "Vassilis Kostakos",
        "Davide Manini",
        "Yueying Liu",
        "Christopher Toukmaji",
        "Jay Paek",
        "Eunmi Yu",
        "Arif Engin Demircali",
        "Zhiyi Sun",
        "Ivan Dewerpe",
        "Hongsen Qin",
        "Roman Pflugfelder",
        "James Bailey",
        "Johnathan Morris",
        "Ville Heilala",
        "Sybille Rosset",
        "Zishun Yu",
        "Peter E. Chen",
        "Woongyeong Yeo",
        "Eeshaan Jain",
        "Ryan Yang",
        "Sreekar Chigurupati",
        "Julia Chernyavsky",
        "Sai Prajwal Reddy",
        "Subhashini Venugopalan",
        "Hunar Batra",
        "Core Francisco Park",
        "Hieu Tran",
        "Guilherme Maximiano",
        "Genghan Zhang",
        "Yizhuo Liang",
        "Hu Shiyu",
        "Rongwu Xu",
        "Rui Pan",
        "Siddharth Suresh",
        "Ziqi Liu",
        "Samaksh Gulati",
        "Songyang Zhang",
        "Peter Turchin",
        "Christopher W. Bartlett",
        "Christopher R. Scotese",
        "Phuong M. Cao",
        "Ben Wu",
        "Jacek Karwowski",
        "Davide Scaramuzza",
        "Aakaash Nattanmai",
        "Gordon McKellips",
        "Anish Cheraku",
        "Asim Suhail",
        "Ethan Luo",
        "Marvin Deng",
        "Jason Luo",
        "Ashley Zhang",
        "Kavin Jindel",
        "Jay Paek",
        "Kasper Halevy",
        "Allen Baranov",
        "Michael Liu",
        "Advaith Avadhanam",
        "David Zhang",
        "Vincent Cheng",
        "Brad Ma",
        "Evan Fu",
        "Liam Do",
        "Joshua Lass",
        "Hubert Yang",
        "Surya Sunkari",
        "Vishruth Bharath",
        "Violet Ai",
        "James Leung",
        "Rishit Agrawal",
        "Alan Zhou",
        "Kevin Chen",
        "Tejas Kalpathi",
        "Ziqi Xu",
        "Gavin Wang",
        "Tyler Xiao",
        "Erik Maung",
        "Sam Lee",
        "Ryan Yang",
        "Roy Yue",
        "Ben Zhao",
        "Julia Yoon",
        "Sunny Sun",
        "Aryan Singh",
        "Ethan Luo",
        "Clark Peng",
        "Tyler Osbey",
        "Taozhi Wang",
        "Daryl Echeazu",
        "Hubert Yang",
        "Timothy Wu",
        "Spandan Patel",
        "Vidhi Kulkarni",
        "Vijaykaarti Sundarapandiyan",
        "Ashley Zhang",
        "Andrew Le",
        "Zafir Nasim",
        "Srikar Yalam",
        "Ritesh Kasamsetty",
        "Soham Samal",
        "Hubert Yang",
        "David Sun",
        "Nihar Shah",
        "Abhijeet Saha",
        "Alex Zhang",
        "Leon Nguyen",
        "Laasya Nagumalli",
        "Kaixin Wang",
        "Alan Zhou",
        "Aidan Wu",
        "Jason Luo",
        "Anwith Telluri",
        "Summer Yue",
        "Alexandr Wang",
        "Dan Hendrycks"
      ],
      "published": "2025-01-24T05:27:46Z",
      "updated": "2025-09-25T19:27:10Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.14249v9",
      "landing_url": "https://arxiv.org/abs/2501.14249v9",
      "doi": "https://doi.org/10.48550/arXiv.2501.14249"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2501.17790",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.17790v1",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "published": "2025-01-29T17:31:26Z"
    },
    "metadata": {
      "arxiv_id": "2501.17790",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "authors": [
        "Chan-Jan Hsu",
        "Yi-Cheng Lin",
        "Chia-Chun Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chen-An Li",
        "Yi-Chang Chen",
        "Chien-Yu Yu",
        "Ming-Ji Lee",
        "Chien-Cheng Chen",
        "Ru-Heng Huang",
        "Hung-yi Lee",
        "Da-Shan Shiu"
      ],
      "published": "2025-01-29T17:31:26Z",
      "updated": "2025-01-29T17:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.17790v1",
      "landing_url": "https://arxiv.org/abs/2501.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.17790"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.18875",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.18875v2",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "published": "2025-01-31T04:11:34Z"
    },
    "metadata": {
      "arxiv_id": "2501.18875",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "authors": [
        "M. Hadi Sepanj",
        "Benyamin Ghojogh",
        "Paul Fieguth"
      ],
      "published": "2025-01-31T04:11:34Z",
      "updated": "2025-11-16T22:59:47Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18875v2",
      "landing_url": "https://arxiv.org/abs/2501.18875v2",
      "doi": "https://doi.org/10.1109/ACCESS.2025.3628158"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2502.02942",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02942v1",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "published": "2025-02-05T07:14:39Z"
    },
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.03128",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03128v1",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "published": "2025-02-05T12:36:21Z"
    },
    "metadata": {
      "arxiv_id": "2502.03128",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "authors": [
        "Yuancheng Wang",
        "Jiachen Zheng",
        "Junan Zhang",
        "Xueyao Zhang",
        "Huan Liao",
        "Zhizheng Wu"
      ],
      "published": "2025-02-05T12:36:21Z",
      "updated": "2025-02-05T12:36:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03128v1",
      "landing_url": "https://arxiv.org/abs/2502.03128v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03128"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.03930",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03930v4",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "published": "2025-02-06T10:09:49Z"
    },
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.04519",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.04519v2",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "published": "2025-02-06T21:40:09Z"
    },
    "metadata": {
      "arxiv_id": "2502.04519",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "authors": [
        "Zexin Cai",
        "Henry Li Xinyuan",
        "Ashi Garg",
        "Leibny Paola García-Perera",
        "Kevin Duh",
        "Sanjeev Khudanpur",
        "Matthew Wiesner",
        "Nicholas Andrews"
      ],
      "published": "2025-02-06T21:40:09Z",
      "updated": "2025-08-20T17:34:21Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04519v2",
      "landing_url": "https://arxiv.org/abs/2502.04519v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.04519"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.04770",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.04770v1",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "published": "2025-02-07T09:11:19Z"
    },
    "metadata": {
      "arxiv_id": "2502.04770",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "authors": [
        "Wolfgang Mack",
        "Ahmed Mustafa",
        "Rafał Łaganowski",
        "Samer Hijazy"
      ],
      "published": "2025-02-07T09:11:19Z",
      "updated": "2025-02-07T09:11:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04770v1",
      "landing_url": "https://arxiv.org/abs/2502.04770v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.04770"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.05236",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05236v2",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "published": "2025-02-07T06:47:11Z"
    },
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.06490",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.06490v4",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "published": "2025-02-10T14:08:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Recent Advances in Discrete Speech Tokens: A Review."
      }
    ]
  },
  {
    "arxiv_id": "2502.09520",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.09520v2",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "published": "2025-02-13T17:35:57Z"
    },
    "metadata": {
      "arxiv_id": "2502.09520",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "authors": [
        "Francesco Pezone",
        "Sergio Barbarossa",
        "Giuseppe Caire"
      ],
      "published": "2025-02-13T17:35:57Z",
      "updated": "2025-10-10T10:21:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09520v2",
      "landing_url": "https://arxiv.org/abs/2502.09520v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09520"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2502.11094",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11094v1",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "published": "2025-02-16T12:14:17Z"
    },
    "metadata": {
      "arxiv_id": "2502.11094",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "authors": [
        "Zhengyan Sheng",
        "Zhihao Du",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Yexin Yang",
        "Zhenhua Ling"
      ],
      "published": "2025-02-16T12:14:17Z",
      "updated": "2025-02-16T12:14:17Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11094v1",
      "landing_url": "https://arxiv.org/abs/2502.11094v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11094"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.11897",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11897v2",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "published": "2025-02-17T15:22:31Z"
    },
    "metadata": {
      "arxiv_id": "2502.11897",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-02-17T15:22:31Z",
      "updated": "2025-04-02T13:25:35Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11897v2",
      "landing_url": "https://arxiv.org/abs/2502.11897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.11897"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.12448",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12448v1",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "published": "2025-02-18T02:29:51Z"
    },
    "metadata": {
      "arxiv_id": "2502.12448",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-02-18T02:29:51Z",
      "updated": "2025-02-18T02:29:51Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12448v1",
      "landing_url": "https://arxiv.org/abs/2502.12448v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12448"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.16240",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16240v1",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "published": "2025-02-22T14:25:55Z"
    },
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.16474",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16474v1",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "published": "2025-02-23T07:17:28Z"
    },
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.16897",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16897v2",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "published": "2025-02-24T06:50:40Z"
    },
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.17627",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.17627v2",
      "title": "Complexity for billiards in regular N-gons",
      "summary": "We compute the complexity of the billiard language of the regular Euclidean $N$-gons (and other families of rational lattice polygons), answering a question posed by Cassaigne-Hubert-Troubetzkoy. Our key technical result is a counting result for saddle connections on lattice surfaces, when we count by combinatorial length.",
      "published": "2025-02-24T20:20:59Z"
    },
    "metadata": {
      "arxiv_id": "2502.17627",
      "title": "Complexity for billiards in regular N-gons",
      "summary": "We compute the complexity of the billiard language of the regular Euclidean $N$-gons (and other families of rational lattice polygons), answering a question posed by Cassaigne-Hubert-Troubetzkoy. Our key technical result is a counting result for saddle connections on lattice surfaces, when we count by combinatorial length.",
      "authors": [
        "Jayadev Athreya",
        "Pascal Hubert",
        "Serge Troubetzkoy"
      ],
      "published": "2025-02-24T20:20:59Z",
      "updated": "2025-06-24T13:58:02Z",
      "categories": [
        "math.DS",
        "math.CV",
        "math.GT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17627v2",
      "landing_url": "https://arxiv.org/abs/2502.17627v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.17627"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2502.18200",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.18200v2",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "published": "2025-02-25T13:41:06Z"
    },
    "metadata": {
      "arxiv_id": "2502.18200",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "authors": [
        "Jiangjing Hu",
        "Haotian Wu",
        "Wenjing Zhang",
        "Fengyu Wang",
        "Wenjun Xu",
        "Hui Gao",
        "Deniz Gündüz"
      ],
      "published": "2025-02-25T13:41:06Z",
      "updated": "2025-05-29T05:24:20Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18200v2",
      "landing_url": "https://arxiv.org/abs/2502.18200v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18200"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2502.20511",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.20511v2",
      "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
      "summary": "Accurate 3D foot reconstruction is crucial for personalized orthotics, digital healthcare, and virtual fittings. However, existing methods struggle with incomplete scans and anatomical variations, particularly in self-scanning scenarios where user mobility is limited, making it difficult to capture areas like the arch and heel. We present a novel end-to-end pipeline that refines Structure-from-Motion (SfM) reconstruction. It first resolves scan alignment ambiguities using SE(3) canonicalization with a viewpoint prediction module, then completes missing geometry through an attention-based network trained on synthetically augmented point clouds. Our approach achieves state-of-the-art performance on reconstruction metrics while preserving clinically validated anatomical fidelity. By combining synthetic training data with learned geometric priors, we enable robust foot reconstruction under real-world capture conditions, unlocking new opportunities for mobile-based 3D scanning in healthcare and retail.",
      "published": "2025-02-27T20:40:20Z"
    },
    "metadata": {
      "arxiv_id": "2502.20511",
      "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
      "summary": "Accurate 3D foot reconstruction is crucial for personalized orthotics, digital healthcare, and virtual fittings. However, existing methods struggle with incomplete scans and anatomical variations, particularly in self-scanning scenarios where user mobility is limited, making it difficult to capture areas like the arch and heel. We present a novel end-to-end pipeline that refines Structure-from-Motion (SfM) reconstruction. It first resolves scan alignment ambiguities using SE(3) canonicalization with a viewpoint prediction module, then completes missing geometry through an attention-based network trained on synthetically augmented point clouds. Our approach achieves state-of-the-art performance on reconstruction metrics while preserving clinically validated anatomical fidelity. By combining synthetic training data with learned geometric priors, we enable robust foot reconstruction under real-world capture conditions, unlocking new opportunities for mobile-based 3D scanning in healthcare and retail.",
      "authors": [
        "Kyle Fogarty",
        "Jing Yang",
        "Chayan Kumar Patodi",
        "Jack Foster",
        "Aadi Bhanti",
        "Steven Chacko",
        "Cengiz Oztireli",
        "Ujwal Bonde"
      ],
      "published": "2025-02-27T20:40:20Z",
      "updated": "2025-08-18T11:31:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.20511v2",
      "landing_url": "https://arxiv.org/abs/2502.20511v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.20511"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2503.00733",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.00733v1",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "published": "2025-03-02T05:15:40Z"
    },
    "metadata": {
      "arxiv_id": "2503.00733",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "authors": [
        "Alexander H. Liu",
        "Sang-gil Lee",
        "Chao-Han Huck Yang",
        "Yuan Gong",
        "Yu-Chiang Frank Wang",
        "James R. Glass",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "published": "2025-03-02T05:15:40Z",
      "updated": "2025-03-02T05:15:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00733v1",
      "landing_url": "https://arxiv.org/abs/2503.00733v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00733"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2503.01582",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01582v3",
      "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
      "summary": "In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used predominantly as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency and enable canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21\\% lower Chamfer distance. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13\\% improvement averaged across all reconstruction metrics, and comparable pose and size estimation accuracy, while being trained for 5$\\times$ less time. Code available at: https://github.com/snt-arg/PRENOM",
      "published": "2025-03-03T14:23:37Z"
    },
    "metadata": {
      "arxiv_id": "2503.01582",
      "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
      "summary": "In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used predominantly as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency and enable canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21\\% lower Chamfer distance. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13\\% improvement averaged across all reconstruction metrics, and comparable pose and size estimation accuracy, while being trained for 5$\\times$ less time. Code available at: https://github.com/snt-arg/PRENOM",
      "authors": [
        "Saad Ejaz",
        "Hriday Bavle",
        "Laura Ribeiro",
        "Holger Voos",
        "Jose Luis Sanchez-Lopez"
      ],
      "published": "2025-03-03T14:23:37Z",
      "updated": "2025-07-29T14:15:39Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01582v3",
      "landing_url": "https://arxiv.org/abs/2503.01582v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.01582"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2503.01710",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01710v1",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "published": "2025-03-03T16:23:10Z"
    },
    "metadata": {
      "arxiv_id": "2503.01710",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "authors": [
        "Xinsheng Wang",
        "Mingqi Jiang",
        "Ziyang Ma",
        "Ziyu Zhang",
        "Songxiang Liu",
        "Linqin Li",
        "Zheng Liang",
        "Qixi Zheng",
        "Rui Wang",
        "Xiaoqin Feng",
        "Weizhen Bian",
        "Zhen Ye",
        "Sitong Cheng",
        "Ruibin Yuan",
        "Zhixian Zhao",
        "Xinfa Zhu",
        "Jiahao Pan",
        "Liumeng Xue",
        "Pengcheng Zhu",
        "Yunlin Chen",
        "Zhifei Li",
        "Xie Chen",
        "Lei Xie",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-03-03T16:23:10Z",
      "updated": "2025-03-03T16:23:10Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01710v1",
      "landing_url": "https://arxiv.org/abs/2503.01710v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01710"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2503.02862",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.02862v1",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "published": "2025-03-04T18:40:38Z"
    },
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2503.03304",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.03304v1",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "published": "2025-03-05T09:37:14Z"
    },
    "metadata": {
      "arxiv_id": "2503.03304",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "authors": [
        "Mhd Modar Halimeh",
        "Matteo Torcoli",
        "Philipp Grundhuber",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-03-05T09:37:14Z",
      "updated": "2025-03-05T09:37:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03304v1",
      "landing_url": "https://arxiv.org/abs/2503.03304v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03304"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2503.04606",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04606v3",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "published": "2025-03-06T16:53:14Z"
    },
    "metadata": {
      "arxiv_id": "2503.04606",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "published": "2025-03-06T16:53:14Z",
      "updated": "2025-04-29T10:34:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04606v3",
      "landing_url": "https://arxiv.org/abs/2503.04606v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04606"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2503.04940",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04940v1",
      "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
      "summary": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent's involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents' architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
      "published": "2025-03-06T20:15:51Z"
    },
    "metadata": {
      "arxiv_id": "2503.04940",
      "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
      "summary": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent's involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents' architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
      "authors": [
        "Mohammad Mahdi Samiei Paqaleh",
        "Mahdieh Soleymani Baghshah"
      ],
      "published": "2025-03-06T20:15:51Z",
      "updated": "2025-03-06T20:15:51Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04940v1",
      "landing_url": "https://arxiv.org/abs/2503.04940v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.04940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2503.06764",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.06764v4",
      "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation",
      "summary": "In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel both in terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs.",
      "published": "2025-03-09T20:42:34Z"
    },
    "metadata": {
      "arxiv_id": "2503.06764",
      "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation",
      "summary": "In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel both in terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs.",
      "authors": [
        "Zisheng Chen",
        "Chunwei Wang",
        "Xiuwei Chen",
        "Hongbin Xu",
        "Runhui Huang",
        "Jun Zhou",
        "Jianhua Han",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "published": "2025-03-09T20:42:34Z",
      "updated": "2025-06-04T04:13:07Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06764v4",
      "landing_url": "https://arxiv.org/abs/2503.06764v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.06764"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2503.06921",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.06921v2",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "published": "2025-03-10T05:00:24Z"
    },
    "metadata": {
      "arxiv_id": "2503.06921",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "authors": [
        "Youngeun Kim",
        "Seunghwan Lee",
        "Aecheon Jung",
        "Bogon Ryu",
        "Sungeun Hong"
      ],
      "published": "2025-03-10T05:00:24Z",
      "updated": "2025-08-07T10:57:05Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06921v2",
      "landing_url": "https://arxiv.org/abs/2503.06921v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06921"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2503.07334",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.07334v4",
      "title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
      "summary": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM's hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at https://github.com/HKU-HealthAI/ARRA.",
      "published": "2025-03-10T13:49:28Z"
    },
    "metadata": {
      "arxiv_id": "2503.07334",
      "title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
      "summary": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM's hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at https://github.com/HKU-HealthAI/ARRA.",
      "authors": [
        "Xing Xie",
        "Jiawei Liu",
        "Ziyue Lin",
        "Huijie Fan",
        "Zhi Han",
        "Yandong Tang",
        "Liangqiong Qu"
      ],
      "published": "2025-03-10T13:49:28Z",
      "updated": "2025-11-14T10:42:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.07334v4",
      "landing_url": "https://arxiv.org/abs/2503.07334v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.07334"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2503.09509",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.09509v2",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "published": "2025-03-12T16:18:45Z"
    },
    "metadata": {
      "arxiv_id": "2503.09509",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Kedong Xu",
        "Hong Gu",
        "Kejie Huang"
      ],
      "published": "2025-03-12T16:18:45Z",
      "updated": "2025-07-30T16:58:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09509v2",
      "landing_url": "https://arxiv.org/abs/2503.09509v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.09509"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2503.10152",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.10152v1",
      "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection",
      "summary": "Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, eg, CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin. We also conduct extensive ablation studies to analyze how each component works.",
      "published": "2025-03-13T08:27:18Z"
    },
    "metadata": {
      "arxiv_id": "2503.10152",
      "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection",
      "summary": "Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, eg, CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin. We also conduct extensive ablation studies to analyze how each component works.",
      "authors": [
        "Shenghao Fu",
        "Junkai Yan",
        "Qize Yang",
        "Xihan Wei",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "published": "2025-03-13T08:27:18Z",
      "updated": "2025-03-13T08:27:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.10152v1",
      "landing_url": "https://arxiv.org/abs/2503.10152v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.10152"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2503.11315",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.11315v2",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "published": "2025-03-14T11:31:30Z"
    },
    "metadata": {
      "arxiv_id": "2503.11315",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "published": "2025-03-14T11:31:30Z",
      "updated": "2025-06-05T05:58:37Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11315v2",
      "landing_url": "https://arxiv.org/abs/2503.11315v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.11315"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2503.12115",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12115v2",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "published": "2025-03-15T12:50:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2503.12382",
    "anchor": "acoustic tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12382v1",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "published": "2025-03-16T07:03:12Z"
    },
    "metadata": {
      "arxiv_id": "2503.12382",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "authors": [
        "Kang You",
        "Tong Chen",
        "Dandan Ding",
        "M. Salman Asif",
        "Zhan Ma"
      ],
      "published": "2025-03-16T07:03:12Z",
      "updated": "2025-03-16T07:03:12Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12382v1",
      "landing_url": "https://arxiv.org/abs/2503.12382v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12382"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2503.14928",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.14928v1",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "published": "2025-03-19T06:28:17Z"
    },
    "metadata": {
      "arxiv_id": "2503.14928",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "published": "2025-03-19T06:28:17Z",
      "updated": "2025-03-19T06:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14928v1",
      "landing_url": "https://arxiv.org/abs/2503.14928v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14928"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.18769",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.18769v2",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "published": "2025-03-24T15:16:51Z"
    },
    "metadata": {
      "arxiv_id": "2503.18769",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Bui Quang Huy"
      ],
      "published": "2025-03-24T15:16:51Z",
      "updated": "2025-03-27T06:39:47Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18769v2",
      "landing_url": "https://arxiv.org/abs/2503.18769v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.18769"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2503.20499",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.20499v3",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "published": "2025-03-26T12:39:06Z"
    },
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2504.02386",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.02386v1",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "published": "2025-04-03T08:24:47Z"
    },
    "metadata": {
      "arxiv_id": "2504.02386",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "authors": [
        "Kim Sung-Bin",
        "Jeongsoo Choi",
        "Puyuan Peng",
        "Joon Son Chung",
        "Tae-Hyun Oh",
        "David Harwath"
      ],
      "published": "2025-04-03T08:24:47Z",
      "updated": "2025-04-03T08:24:47Z",
      "categories": [
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.02386v1",
      "landing_url": "https://arxiv.org/abs/2504.02386v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.02386"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2504.04318",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.04318v3",
      "title": "Variational Self-Supervised Learning",
      "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.",
      "published": "2025-04-06T01:28:50Z"
    },
    "metadata": {
      "arxiv_id": "2504.04318",
      "title": "Variational Self-Supervised Learning",
      "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.",
      "authors": [
        "Mehmet Can Yavuz",
        "Berrin Yanikoglu"
      ],
      "published": "2025-04-06T01:28:50Z",
      "updated": "2025-05-01T16:21:49Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04318v3",
      "landing_url": "https://arxiv.org/abs/2504.04318v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.04318"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2504.04633",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.04633v3",
      "title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering",
      "summary": "Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \\textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \\textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\\% with substantial improvements in overall efficiency.",
      "published": "2025-04-06T22:02:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.04633",
      "title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering",
      "summary": "Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \\textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \\textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\\% with substantial improvements in overall efficiency.",
      "authors": [
        "Yanshu Li",
        "Yi Cao",
        "Hongyang He",
        "Qisen Cheng",
        "Xiang Fu",
        "Xi Xiao",
        "Tianyang Wang",
        "Ruixiang Tang"
      ],
      "published": "2025-04-06T22:02:21Z",
      "updated": "2025-08-26T10:19:05Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04633v3",
      "landing_url": "https://arxiv.org/abs/2504.04633v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.04633"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2504.07053",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.07053v2",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "published": "2025-04-09T17:14:33Z"
    },
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2504.07213",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.07213v2",
      "title": "Evolutionary Machine Learning meets Self-Supervised Learning: a comprehensive survey",
      "summary": "The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.",
      "published": "2025-04-09T18:39:41Z"
    },
    "metadata": {
      "arxiv_id": "2504.07213",
      "title": "Evolutionary Machine Learning meets Self-Supervised Learning: a comprehensive survey",
      "summary": "The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.",
      "authors": [
        "Adriano Vinhas",
        "João Correia",
        "Penousal Machado"
      ],
      "published": "2025-04-09T18:39:41Z",
      "updated": "2025-11-04T10:48:22Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07213v2",
      "landing_url": "https://arxiv.org/abs/2504.07213v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07213"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2504.09862",
    "anchor": "semantic tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.09862v2",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "published": "2025-04-14T04:18:25Z"
    },
    "metadata": {
      "arxiv_id": "2504.09862",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "authors": [
        "Zengyuan Lai",
        "Jiarui Yang",
        "Songpengcheng Xia",
        "Lizhou Lin",
        "Lan Sun",
        "Renwen Wang",
        "Jianran Liu",
        "Qi Wu",
        "Ling Pei"
      ],
      "published": "2025-04-14T04:18:25Z",
      "updated": "2025-11-17T04:20:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09862v2",
      "landing_url": "https://arxiv.org/abs/2504.09862v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.09862"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2504.10352",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.10352v3",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "published": "2025-04-14T16:03:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.10352",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Yuxuan Hu",
        "Haibin Wu",
        "Hui Wang",
        "Jianwei Yu",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yanqing Liu",
        "Yan Lu",
        "Kai Yu",
        "Xie Chen"
      ],
      "published": "2025-04-14T16:03:21Z",
      "updated": "2025-08-05T15:33:39Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10352v3",
      "landing_url": "https://arxiv.org/abs/2504.10352v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.10352"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2504.10567",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.10567v2",
      "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models",
      "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time even on mobile devices. We also propose an omni-training objective to unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single VAE network but with enhanced quality. In addition, we propose a novel latent consistency loss that provides stable improvements in reconstruction quality. Latent consistency loss outperforms prior auxiliary losses including LPIPS, GAN and DWT in terms of both quality improvements and simplicity. H3AE achieves ultra-high compression ratios and real-time decoding speed on GPU and mobile, and outperforms prior arts in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.",
      "published": "2025-04-14T17:59:06Z"
    },
    "metadata": {
      "arxiv_id": "2504.10567",
      "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models",
      "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time even on mobile devices. We also propose an omni-training objective to unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single VAE network but with enhanced quality. In addition, we propose a novel latent consistency loss that provides stable improvements in reconstruction quality. Latent consistency loss outperforms prior auxiliary losses including LPIPS, GAN and DWT in terms of both quality improvements and simplicity. H3AE achieves ultra-high compression ratios and real-time decoding speed on GPU and mobile, and outperforms prior arts in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.",
      "authors": [
        "Yushu Wu",
        "Yanyu Li",
        "Ivan Skorokhodov",
        "Anil Kag",
        "Willi Menapace",
        "Sharath Girish",
        "Aliaksandr Siarohin",
        "Yanzhi Wang",
        "Sergey Tulyakov"
      ],
      "published": "2025-04-14T17:59:06Z",
      "updated": "2025-10-01T03:41:01Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10567v2",
      "landing_url": "https://arxiv.org/abs/2504.10567v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.10567"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2504.12339",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12339v2",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "published": "2025-04-15T01:44:56Z"
    },
    "metadata": {
      "arxiv_id": "2504.12339",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "authors": [
        "Yaodong Song",
        "Hongjie Chen",
        "Jie Lian",
        "Yuxin Zhang",
        "Guangmin Xia",
        "Zehan Li",
        "Genliang Zhao",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "published": "2025-04-15T01:44:56Z",
      "updated": "2025-05-28T14:24:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12339v2",
      "landing_url": "https://arxiv.org/abs/2504.12339v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.12339"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2504.12715",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12715v1",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "published": "2025-04-17T07:43:52Z"
    },
    "metadata": {
      "arxiv_id": "2504.12715",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "authors": [
        "Long Zeng",
        "Jianxiang Yu",
        "Jiapeng Zhu",
        "Qingsong Zhong",
        "Xiang Li"
      ],
      "published": "2025-04-17T07:43:52Z",
      "updated": "2025-04-17T07:43:52Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12715v1",
      "landing_url": "https://arxiv.org/abs/2504.12715v1",
      "doi": "https://doi.org/10.1145/3696410.3714656"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2504.14113",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.14113v1",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "published": "2025-04-19T00:13:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2504.15509",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.15509v1",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "published": "2025-04-22T01:05:32Z"
    },
    "metadata": {
      "arxiv_id": "2504.15509",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "authors": [
        "Keqi Deng",
        "Wenxi Chen",
        "Xie Chen",
        "Philip C. Woodland"
      ],
      "published": "2025-04-22T01:05:32Z",
      "updated": "2025-04-22T01:05:32Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15509v1",
      "landing_url": "https://arxiv.org/abs/2504.15509v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.15509"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2504.16654",
    "anchor": "acoustic tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.16654v5",
      "title": "The Empirical Welfare Content of International Price and Income Comparisons",
      "summary": "Multilateral index numbers are often used to make claims about welfare, such as treating PPPs as cross-country costs of living or real incomes as indicators of living standards. However, such interpretations may not be consistent with the observed data. To study this problem, I derive multilateral bounds on welfare implied by revealed preference and use these to appraise leading comparison methods. My findings support the welfare-interpretability of the contemporary indices I examine, but not of market exchange rates. When using a welfare-consistent multilateral index, the world in 2017 appears larger and more equal vis-à-vis the United States than conventional measures.",
      "published": "2025-04-23T12:22:12Z"
    },
    "metadata": {
      "arxiv_id": "2504.16654",
      "title": "The Empirical Welfare Content of International Price and Income Comparisons",
      "summary": "Multilateral index numbers are often used to make claims about welfare, such as treating PPPs as cross-country costs of living or real incomes as indicators of living standards. However, such interpretations may not be consistent with the observed data. To study this problem, I derive multilateral bounds on welfare implied by revealed preference and use these to appraise leading comparison methods. My findings support the welfare-interpretability of the contemporary indices I examine, but not of market exchange rates. When using a welfare-consistent multilateral index, the world in 2017 appears larger and more equal vis-à-vis the United States than conventional measures.",
      "authors": [
        "Hubert Wu"
      ],
      "published": "2025-04-23T12:22:12Z",
      "updated": "2025-12-09T23:58:18Z",
      "categories": [
        "econ.GN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.16654v5",
      "landing_url": "https://arxiv.org/abs/2504.16654v5",
      "doi": "https://doi.org/10.48550/arXiv.2504.16654"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2504.19046",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.19046v1",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "published": "2025-04-26T22:49:08Z"
    },
    "metadata": {
      "arxiv_id": "2504.19046",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "authors": [
        "Billel Essaid",
        "Hamza Kheddar",
        "Noureddine Batel"
      ],
      "published": "2025-04-26T22:49:08Z",
      "updated": "2025-04-26T22:49:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.19046v1",
      "landing_url": "https://arxiv.org/abs/2504.19046v1",
      "doi": "https://doi.org/10.1109/ICTIS62692.2024.10894163"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2505.05738",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.05738v2",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "published": "2025-05-09T02:34:06Z"
    },
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.06252",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.06252v3",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "published": "2025-04-30T04:16:32Z"
    },
    "metadata": {
      "arxiv_id": "2505.06252",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "authors": [
        "Zirui Wang",
        "Tingfeng Lan",
        "Zhaoyuan Su",
        "Juncheng Yang",
        "Yue Cheng"
      ],
      "published": "2025-04-30T04:16:32Z",
      "updated": "2025-11-08T18:45:50Z",
      "categories": [
        "cs.DB",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06252v3",
      "landing_url": "https://arxiv.org/abs/2505.06252v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.06252"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2505.06549",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.06549v2",
      "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems",
      "summary": "In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \\emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.",
      "published": "2025-05-10T07:31:09Z"
    },
    "metadata": {
      "arxiv_id": "2505.06549",
      "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems",
      "summary": "In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \\emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.",
      "authors": [
        "Matthias Chung",
        "Bas Peters",
        "Michael Solomon"
      ],
      "published": "2025-05-10T07:31:09Z",
      "updated": "2025-08-18T23:57:40Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06549v2",
      "landing_url": "https://arxiv.org/abs/2505.06549v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.06549"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2505.06671",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.06671v2",
      "title": "RADE: A Neural Codec for Transmitting Speech over HF Radio Channels",
      "summary": "Speech compression is commonly used to send voice over radio channels in applications such as mobile telephony and two-way push-to-talk (PTT) radio. In classical systems, the speech codec is combined with forward error correction, modulation and radio hardware. In this paper we describe an autoencoder that replaces many of the traditional signal processing elements with a neural network. The encoder takes a vocoder feature set (short term spectrum, pitch, voicing), and produces discrete time, but continuously valued quadrature amplitude modulation (QAM) symbols. We use orthogonal frequency domain multiplexing (OFDM) to send and receive these symbols over high frequency (HF) radio channels. The decoder converts received QAM symbols to vocoder features suitable for synthesis. The autoencoder has been trained to be robust to additive Gaussian noise and multipath channel impairments while simultaneously maintaining a Peak To Average Power Ratio (PAPR) of less than 1 dB. Over simulated and real world HF radio channels we have achieved output speech intelligibility that clearly surpasses existing analog and digital radio systems over a range of SNRs.",
      "published": "2025-05-10T15:16:05Z"
    },
    "metadata": {
      "arxiv_id": "2505.06671",
      "title": "RADE: A Neural Codec for Transmitting Speech over HF Radio Channels",
      "summary": "Speech compression is commonly used to send voice over radio channels in applications such as mobile telephony and two-way push-to-talk (PTT) radio. In classical systems, the speech codec is combined with forward error correction, modulation and radio hardware. In this paper we describe an autoencoder that replaces many of the traditional signal processing elements with a neural network. The encoder takes a vocoder feature set (short term spectrum, pitch, voicing), and produces discrete time, but continuously valued quadrature amplitude modulation (QAM) symbols. We use orthogonal frequency domain multiplexing (OFDM) to send and receive these symbols over high frequency (HF) radio channels. The decoder converts received QAM symbols to vocoder features suitable for synthesis. The autoencoder has been trained to be robust to additive Gaussian noise and multipath channel impairments while simultaneously maintaining a Peak To Average Power Ratio (PAPR) of less than 1 dB. Over simulated and real world HF radio channels we have achieved output speech intelligibility that clearly surpasses existing analog and digital radio systems over a range of SNRs.",
      "authors": [
        "David Rowe",
        "Jean-Marc Valin"
      ],
      "published": "2025-05-10T15:16:05Z",
      "updated": "2025-07-26T18:38:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06671v2",
      "landing_url": "https://arxiv.org/abs/2505.06671v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.06671"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2505.12053",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.12053v2",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "published": "2025-05-17T15:32:54Z"
    },
    "metadata": {
      "arxiv_id": "2505.12053",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "published": "2025-05-17T15:32:54Z",
      "updated": "2025-09-28T09:51:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "landing_url": "https://arxiv.org/abs/2505.12053v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12053"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.12477",
    "anchor": "discrete speech tokens",
    "search_term": "self supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.12477v2",
      "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
      "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction based methods. These results not only clarify the trade offs between the two paradigms but also substantiate the empirical success of joint embedding approaches on real world challenging datasets.",
      "published": "2025-05-18T15:54:55Z"
    },
    "metadata": {
      "arxiv_id": "2505.12477",
      "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
      "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction based methods. These results not only clarify the trade offs between the two paradigms but also substantiate the empirical success of joint embedding approaches on real world challenging datasets.",
      "authors": [
        "Hugues Van Assel",
        "Mark Ibrahim",
        "Tommaso Biancalani",
        "Aviv Regev",
        "Randall Balestriero"
      ],
      "published": "2025-05-18T15:54:55Z",
      "updated": "2025-10-14T16:45:33Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12477v2",
      "landing_url": "https://arxiv.org/abs/2505.12477v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12477"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2505.13775",
    "anchor": "dblp_title",
    "search_term": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens.",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13775v3",
      "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
      "summary": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
      "published": "2025-05-19T23:29:23Z"
    },
    "metadata": {
      "arxiv_id": "2505.13775",
      "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
      "summary": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
      "authors": [
        "Karthik Valmeekam",
        "Kaya Stechly",
        "Vardhan Palod",
        "Atharva Gundawar",
        "Subbarao Kambhampati"
      ],
      "published": "2025-05-19T23:29:23Z",
      "updated": "2025-11-22T07:49:57Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13775v3",
      "landing_url": "https://arxiv.org/abs/2505.13775v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.13775"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens."
      }
    ]
  },
  {
    "arxiv_id": "2505.13830",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13830v2",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "published": "2025-05-20T02:18:45Z"
    },
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2505.13997",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13997v2",
      "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning",
      "summary": "Video Class-Incremental Learning (VCIL) seeks to develop models that continuously learn new action categories over time without forgetting previously acquired knowledge. Unlike traditional Class-Incremental Learning (CIL), VCIL introduces the added complexity of spatiotemporal structures, making it particularly challenging to mitigate catastrophic forgetting while effectively capturing both frame-shared semantics and temporal dynamics. Existing approaches either rely on exemplar rehearsal, raising concerns over memory and privacy, or adapt static image-based methods that neglect temporal modeling. To address these limitations, we propose Spatiotemporal Preservation and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly disentangles and preserves spatiotemporal information. First, we introduce Frame-Shared Semantics Distillation (FSSD), which identifies semantically stable and meaningful channels by jointly considering semantic sensitivity and classification contribution. These important semantic channels are selectively regularized to maintain prior knowledge while allowing for adaptation. Second, we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which dynamically routes task-specific experts based on their temporal dynamics, enabling inference without task ID or stored exemplars. Together, StPR effectively leverages spatial semantics and temporal dynamics, achieving a unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51, and Kinetics400 show that our method outperforms existing baselines while offering improved interpretability and efficiency in VCIL. Code is available in the supplementary materials.",
      "published": "2025-05-20T06:46:51Z"
    },
    "metadata": {
      "arxiv_id": "2505.13997",
      "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning",
      "summary": "Video Class-Incremental Learning (VCIL) seeks to develop models that continuously learn new action categories over time without forgetting previously acquired knowledge. Unlike traditional Class-Incremental Learning (CIL), VCIL introduces the added complexity of spatiotemporal structures, making it particularly challenging to mitigate catastrophic forgetting while effectively capturing both frame-shared semantics and temporal dynamics. Existing approaches either rely on exemplar rehearsal, raising concerns over memory and privacy, or adapt static image-based methods that neglect temporal modeling. To address these limitations, we propose Spatiotemporal Preservation and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly disentangles and preserves spatiotemporal information. First, we introduce Frame-Shared Semantics Distillation (FSSD), which identifies semantically stable and meaningful channels by jointly considering semantic sensitivity and classification contribution. These important semantic channels are selectively regularized to maintain prior knowledge while allowing for adaptation. Second, we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which dynamically routes task-specific experts based on their temporal dynamics, enabling inference without task ID or stored exemplars. Together, StPR effectively leverages spatial semantics and temporal dynamics, achieving a unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51, and Kinetics400 show that our method outperforms existing baselines while offering improved interpretability and efficiency in VCIL. Code is available in the supplementary materials.",
      "authors": [
        "Huaijie Wang",
        "De Cheng",
        "Guozhang Li",
        "Zhipeng Xu",
        "Lingfeng He",
        "Jie Li",
        "Nannan Wang",
        "Xinbo Gao"
      ],
      "published": "2025-05-20T06:46:51Z",
      "updated": "2025-09-30T05:06:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13997v2",
      "landing_url": "https://arxiv.org/abs/2505.13997v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13997"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2505.14470",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14470v2",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "published": "2025-05-20T15:05:14Z"
    },
    "metadata": {
      "arxiv_id": "2505.14470",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "authors": [
        "Nadav Har-Tuv",
        "Or Tal",
        "Yossi Adi"
      ],
      "published": "2025-05-20T15:05:14Z",
      "updated": "2025-06-04T08:23:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14470v2",
      "landing_url": "https://arxiv.org/abs/2505.14470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14470"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.14556",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14556v1",
      "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
      "summary": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
      "published": "2025-05-20T16:14:37Z"
    },
    "metadata": {
      "arxiv_id": "2505.14556",
      "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
      "summary": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
      "authors": [
        "Marlène Careil",
        "Yohann Benchetrit",
        "Jean-Rémi King"
      ],
      "published": "2025-05-20T16:14:37Z",
      "updated": "2025-05-20T16:14:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14556v1",
      "landing_url": "https://arxiv.org/abs/2505.14556v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14556"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2505.14989",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14989v1",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "published": "2025-05-21T00:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2505.16182",
    "anchor": "dblp_title",
    "search_term": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data.",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16182v1",
      "title": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data",
      "summary": "In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as \"interlanguage speech intelligibility benefit\" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce.",
      "published": "2025-05-22T03:36:28Z"
    },
    "metadata": {
      "arxiv_id": "2505.16182",
      "title": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data",
      "summary": "In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as \"interlanguage speech intelligibility benefit\" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce.",
      "authors": [
        "Kentaro Onda",
        "Keisuke Imoto",
        "Satoru Fukayama",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2025-05-22T03:36:28Z",
      "updated": "2025-05-22T03:36:28Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16182v1",
      "landing_url": "https://arxiv.org/abs/2505.16182v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16182"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data."
      }
    ]
  },
  {
    "arxiv_id": "2505.16616",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16616v1",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "published": "2025-05-22T12:50:32Z"
    },
    "metadata": {
      "arxiv_id": "2505.16616",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "authors": [
        "Javier Perez",
        "Dimme de Groot",
        "Jorge Martinez"
      ],
      "published": "2025-05-22T12:50:32Z",
      "updated": "2025-05-22T12:50:32Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16616v1",
      "landing_url": "https://arxiv.org/abs/2505.16616v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16616"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.16687",
    "anchor": "discrete speech tokens",
    "search_term": "semantic distillation",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16687v2",
      "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
      "summary": "While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/",
      "published": "2025-05-22T13:54:09Z"
    },
    "metadata": {
      "arxiv_id": "2505.16687",
      "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
      "summary": "While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/",
      "authors": [
        "Naifu Xue",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-05-22T13:54:09Z",
      "updated": "2025-11-26T13:32:20Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16687v2",
      "landing_url": "https://arxiv.org/abs/2505.16687v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16687"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      }
    ]
  },
  {
    "arxiv_id": "2505.16845",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16845v1",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "published": "2025-05-22T16:10:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.16845",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2025-05-22T16:10:01Z",
      "updated": "2025-05-22T16:10:01Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16845v1",
      "landing_url": "https://arxiv.org/abs/2505.16845v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16845"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.17076",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17076v3",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "published": "2025-05-20T06:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.17446",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17446v2",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "published": "2025-05-23T04:03:27Z"
    },
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "dblp_title",
        "search_term": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models."
      }
    ]
  },
  {
    "arxiv_id": "2505.17477",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17477v1",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "published": "2025-05-23T04:59:27Z"
    },
    "metadata": {
      "arxiv_id": "2505.17477",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "authors": [
        "Victor OK Li",
        "Yang Han",
        "Jacqueline CK Lam",
        "Lawrence YL Cheung"
      ],
      "published": "2025-05-23T04:59:27Z",
      "updated": "2025-05-23T04:59:27Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17477v1",
      "landing_url": "https://arxiv.org/abs/2505.17477v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17477"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.17604",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17604v1",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "published": "2025-05-23T08:15:05Z"
    },
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.18231",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.18231v2",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "published": "2025-05-23T12:40:07Z"
    },
    "metadata": {
      "arxiv_id": "2505.18231",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "published": "2025-05-23T12:40:07Z",
      "updated": "2025-12-14T08:17:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "landing_url": "https://arxiv.org/abs/2505.18231v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.18231"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2505.18864",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.18864v1",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "published": "2025-05-24T20:46:36Z"
    },
    "metadata": {
      "arxiv_id": "2505.18864",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "authors": [
        "Binhao Ma",
        "Hanqing Guo",
        "Zhengping Jay Luo",
        "Rui Duan"
      ],
      "published": "2025-05-24T20:46:36Z",
      "updated": "2025-05-24T20:46:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18864v1",
      "landing_url": "https://arxiv.org/abs/2505.18864v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18864"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.19043",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19043v2",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "published": "2025-05-25T08:43:40Z"
    },
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.19462",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19462v2",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "published": "2025-05-26T03:35:44Z"
    },
    "metadata": {
      "arxiv_id": "2505.19462",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "authors": [
        "Puyuan Peng",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2025-05-26T03:35:44Z",
      "updated": "2025-05-31T22:36:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19462v2",
      "landing_url": "https://arxiv.org/abs/2505.19462v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19462"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2505.19669",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19669v2",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "published": "2025-05-26T08:25:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.19669",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "authors": [
        "Haiyang Sun",
        "Shujie Hu",
        "Shujie Liu",
        "Lingwei Meng",
        "Hui Wang",
        "Bing Han",
        "Yifan Yang",
        "Yanqing Liu",
        "Sheng Zhao",
        "Yan Lu",
        "Yanmin Qian"
      ],
      "published": "2025-05-26T08:25:01Z",
      "updated": "2025-06-02T10:03:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19669v2",
      "landing_url": "https://arxiv.org/abs/2505.19669v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19669"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.19760",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19760v2",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "published": "2025-05-26T09:43:09Z"
    },
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.20741",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.20741v1",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "published": "2025-05-27T05:31:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.20741",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "authors": [
        "Jiatong Shi",
        "Hye-Jin Shim",
        "Shinji Watanabe"
      ],
      "published": "2025-05-27T05:31:19Z",
      "updated": "2025-05-27T05:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.20741v1",
      "landing_url": "https://arxiv.org/abs/2505.20741v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.20741"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.21194",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.21194v1",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "published": "2025-05-27T13:42:33Z"
    },
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2505.22991",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.22991v1",
      "title": "Number of Clusters in a Dataset: A Regularized K-means Approach",
      "summary": "Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $λk$, where $k$ is the number of clusters and $λ$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $λ$. In this paper, we derive rigorous bounds for $λ$ assuming clusters are {\\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.",
      "published": "2025-05-29T01:58:44Z"
    },
    "metadata": {
      "arxiv_id": "2505.22991",
      "title": "Number of Clusters in a Dataset: A Regularized K-means Approach",
      "summary": "Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $λk$, where $k$ is the number of clusters and $λ$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $λ$. In this paper, we derive rigorous bounds for $λ$ assuming clusters are {\\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.",
      "authors": [
        "Behzad Kamgar-Parsi",
        "Behrooz Kamgar-Parsi"
      ],
      "published": "2025-05-29T01:58:44Z",
      "updated": "2025-05-29T01:58:44Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.22991v1",
      "landing_url": "https://arxiv.org/abs/2505.22991v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.22991"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2505.24314",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24314v1",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "published": "2025-05-30T07:53:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.24314",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "authors": [
        "Peijie Chen",
        "Wenhao Guan",
        "Kaidi Wang",
        "Weijie Wu",
        "Hukai Huang",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:53:01Z",
      "updated": "2025-05-30T07:53:01Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24314v1",
      "landing_url": "https://arxiv.org/abs/2505.24314v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24314"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.24496",
    "anchor": "discrete speech tokens",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24496v1",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "published": "2025-05-30T11:47:29Z"
    },
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "neural codec"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "self supervised learning"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "hubert"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.24518",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24518v2",
      "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
      "summary": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
      "published": "2025-05-30T12:30:04Z"
    },
    "metadata": {
      "arxiv_id": "2505.24518",
      "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
      "summary": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
      "authors": [
        "Jiatong Shi",
        "Yifan Cheng",
        "Bo-Hao Su",
        "Hye-jin Shim",
        "Jinchuan Tian",
        "Samuele Cornell",
        "Yiwen Zhao",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2025-05-30T12:30:04Z",
      "updated": "2025-10-30T15:08:00Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24518v2",
      "landing_url": "https://arxiv.org/abs/2505.24518v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.24518"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.00809",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.00809v1",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "published": "2025-06-01T03:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2506.00809",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2025-06-01T03:23:27Z",
      "updated": "2025-06-01T03:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00809v1",
      "landing_url": "https://arxiv.org/abs/2506.00809v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00809"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2506.00843",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.00843v1",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "published": "2025-06-01T05:38:39Z"
    },
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2506.01731",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.01731v1",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "published": "2025-06-02T14:42:50Z"
    },
    "metadata": {
      "arxiv_id": "2506.01731",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "authors": [
        "Anna Leschanowsky",
        "Kishor Kayyar Lakshminarayana",
        "Anjana Rajasekhar",
        "Lyonel Behringer",
        "Ibrahim Kilinc",
        "Guillaume Fuchs",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-06-02T14:42:50Z",
      "updated": "2025-06-02T14:42:50Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01731v1",
      "landing_url": "https://arxiv.org/abs/2506.01731v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01731"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.04518",
    "anchor": "speech tokenization",
    "search_term": "discrete speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.04518v2",
      "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model",
      "summary": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.",
      "published": "2025-06-04T23:53:49Z"
    },
    "metadata": {
      "arxiv_id": "2506.04518",
      "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model",
      "summary": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.",
      "authors": [
        "Haibin Wu",
        "Yuxuan Hu",
        "Ruchao Fan",
        "Xiaofei Wang",
        "Kenichi Kumatani",
        "Bo Ren",
        "Jianwei Yu",
        "Heng Lu",
        "Lijuan Wang",
        "Yao Qian",
        "Jinyu Li"
      ],
      "published": "2025-06-04T23:53:49Z",
      "updated": "2025-06-13T03:55:18Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04518v2",
      "landing_url": "https://arxiv.org/abs/2506.04518v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04518"
    },
    "queries": [
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2506.04786",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.04786v2",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "published": "2025-06-05T09:14:25Z"
    },
    "metadata": {
      "arxiv_id": "2506.04786",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "authors": [
        "Thore Gerlach",
        "Sascha Mücke",
        "Christian Bauckhage"
      ],
      "published": "2025-06-05T09:14:25Z",
      "updated": "2025-09-04T20:11:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04786v2",
      "landing_url": "https://arxiv.org/abs/2506.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04786"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2506.05432",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.05432v2",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "published": "2025-06-05T08:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2506.05432",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "authors": [
        "Yuxuan Yue",
        "Zukang Xu",
        "Zhihang Yuan",
        "Dawei Yang",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "published": "2025-06-05T08:58:58Z",
      "updated": "2025-06-26T06:17:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
      "landing_url": "https://arxiv.org/abs/2506.05432v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05432"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2506.06990",
    "anchor": "discrete speech tokens",
    "search_term": "k-means",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.06990v2",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "published": "2025-06-08T04:37:28Z"
    },
    "metadata": {
      "arxiv_id": "2506.06990",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "authors": [
        "Mingyi Li",
        "Michael R. Metel",
        "Akiko Takeda"
      ],
      "published": "2025-06-08T04:37:28Z",
      "updated": "2025-06-11T06:52:53Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06990v2",
      "landing_url": "https://arxiv.org/abs/2506.06990v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.06990"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "k-means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means"
      }
    ]
  },
  {
    "arxiv_id": "2506.07199",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.07199v1",
      "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching",
      "summary": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intricate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.",
      "published": "2025-06-08T15:47:44Z"
    },
    "metadata": {
      "arxiv_id": "2506.07199",
      "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching",
      "summary": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intricate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.",
      "authors": [
        "Ben Hayes",
        "Charalampos Saitis",
        "György Fazekas"
      ],
      "published": "2025-06-08T15:47:44Z",
      "updated": "2025-06-08T15:47:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07199v1",
      "landing_url": "https://arxiv.org/abs/2506.07199v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.07199"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2506.07863",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction metrics",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.07863v2",
      "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation",
      "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.",
      "published": "2025-06-09T15:27:03Z"
    },
    "metadata": {
      "arxiv_id": "2506.07863",
      "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation",
      "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.",
      "authors": [
        "Lev Novitskiy",
        "Viacheslav Vasilev",
        "Maria Kovaleva",
        "Vladimir Arkhipkin",
        "Denis Dimitrov"
      ],
      "published": "2025-06-09T15:27:03Z",
      "updated": "2025-12-01T18:51:34Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07863v2",
      "landing_url": "https://arxiv.org/abs/2506.07863v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07863"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction metrics"
      }
    ]
  },
  {
    "arxiv_id": "2506.09349",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09349v4",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "published": "2025-06-11T02:57:22Z"
    },
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic distillation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction metrics"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2506.09549",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09549v1",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "published": "2025-06-11T09:32:12Z"
    },
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "pesq"
      },
      {
        "anchor": "speech tokenization",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.15456",
    "anchor": "dblp_title",
    "search_term": "Factorized RVQ-GAN For Disentangled Speech Tokenization.",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.15456v1",
      "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization",
      "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks.",
      "published": "2025-06-18T13:36:34Z"
    },
    "metadata": {
      "arxiv_id": "2506.15456",
      "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization",
      "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks.",
      "authors": [
        "Sameer Khurana",
        "Dominik Klement",
        "Antoine Laurent",
        "Dominik Bobos",
        "Juraj Novosad",
        "Peter Gazdik",
        "Ellen Zhang",
        "Zili Huang",
        "Amir Hussein",
        "Ricard Marxer",
        "Yoshiki Masuyama",
        "Ryo Aihara",
        "Chiori Hori",
        "Francois G. Germain",
        "Gordon Wichern",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-18T13:36:34Z",
      "updated": "2025-06-18T13:36:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.15456v1",
      "landing_url": "https://arxiv.org/abs/2506.15456v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.15456"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Factorized RVQ-GAN For Disentangled Speech Tokenization."
      }
    ]
  },
  {
    "arxiv_id": "2506.16738",
    "anchor": "dblp_title",
    "search_term": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization.",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.16738v1",
      "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
      "summary": "With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
      "published": "2025-06-20T04:15:14Z"
    },
    "metadata": {
      "arxiv_id": "2506.16738",
      "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
      "summary": "With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
      "authors": [
        "Daejin Jo",
        "Jeeyoung Yun",
        "Byungseok Roh",
        "Sungwoong Kim"
      ],
      "published": "2025-06-20T04:15:14Z",
      "updated": "2025-06-20T04:15:14Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.16738v1",
      "landing_url": "https://arxiv.org/abs/2506.16738v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.16738"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization."
      }
    ]
  },
  {
    "arxiv_id": "2506.19028",
    "anchor": "dblp_title",
    "search_term": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective.",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.19028v5",
      "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
      "summary": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
      "published": "2025-06-23T18:31:22Z"
    },
    "metadata": {
      "arxiv_id": "2506.19028",
      "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
      "summary": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
      "authors": [
        "Weijie Xu",
        "Yiwen Wang",
        "Chi Xue",
        "Xiangkun Hu",
        "Xi Fang",
        "Guimin Dong",
        "Chandan K. Reddy"
      ],
      "published": "2025-06-23T18:31:22Z",
      "updated": "2025-10-10T17:14:10Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.19028v5",
      "landing_url": "https://arxiv.org/abs/2506.19028v5",
      "doi": "https://doi.org/10.48550/arXiv.2506.19028"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective."
      }
    ]
  },
  {
    "arxiv_id": "2507.12825",
    "anchor": "dblp_title",
    "search_term": "Autoregressive Speech Enhancement via Acoustic Tokens.",
    "search_record": {
      "id": "http://arxiv.org/abs/2507.12825v1",
      "title": "Autoregressive Speech Enhancement via Acoustic Tokens",
      "summary": "In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.",
      "published": "2025-07-17T06:32:22Z"
    },
    "metadata": {
      "arxiv_id": "2507.12825",
      "title": "Autoregressive Speech Enhancement via Acoustic Tokens",
      "summary": "In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2025-07-17T06:32:22Z",
      "updated": "2025-07-17T06:32:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.12825v1",
      "landing_url": "https://arxiv.org/abs/2507.12825v1",
      "doi": "https://doi.org/10.48550/arXiv.2507.12825"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Autoregressive Speech Enhancement via Acoustic Tokens."
      }
    ]
  },
  {
    "arxiv_id": "2508.02401",
    "anchor": "dblp_title",
    "search_term": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation.",
    "search_record": {
      "id": "http://arxiv.org/abs/2508.02401v1",
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
      "summary": "Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "published": "2025-08-04T13:26:16Z"
    },
    "metadata": {
      "arxiv_id": "2508.02401",
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
      "summary": "Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "authors": [
        "Xiaolin Lin",
        "Jingcun Wang",
        "Olga Kondrateva",
        "Yiyu Shi",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "published": "2025-08-04T13:26:16Z",
      "updated": "2025-08-04T13:26:16Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.02401v1",
      "landing_url": "https://arxiv.org/abs/2508.02401v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.02401"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation."
      }
    ]
  },
  {
    "arxiv_id": "2508.03695",
    "anchor": "dblp_title",
    "search_term": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition.",
    "search_record": {
      "id": "http://arxiv.org/abs/2508.03695v1",
      "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition",
      "summary": "Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io",
      "published": "2025-08-05T17:59:58Z"
    },
    "metadata": {
      "arxiv_id": "2508.03695",
      "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition",
      "summary": "Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io",
      "authors": [
        "Pulkit Kumar",
        "Shuaiyi Huang",
        "Matthew Walmer",
        "Sai Saketh Rambhatla",
        "Abhinav Shrivastava"
      ],
      "published": "2025-08-05T17:59:58Z",
      "updated": "2025-08-05T17:59:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.03695v1",
      "landing_url": "https://arxiv.org/abs/2508.03695v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.03695"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition."
      }
    ]
  },
  {
    "arxiv_id": "2508.17863",
    "anchor": "dblp_title",
    "search_term": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs.",
    "search_record": {
      "id": "http://arxiv.org/abs/2508.17863v1",
      "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs",
      "summary": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.",
      "published": "2025-08-25T10:16:07Z"
    },
    "metadata": {
      "arxiv_id": "2508.17863",
      "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs",
      "summary": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.",
      "authors": [
        "Dingdong Wang",
        "Junan Li",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2025-08-25T10:16:07Z",
      "updated": "2025-08-25T10:16:07Z",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.17863v1",
      "landing_url": "https://arxiv.org/abs/2508.17863v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.17863"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs."
      }
    ]
  },
  {
    "arxiv_id": "2509.04685",
    "anchor": "dblp_title",
    "search_term": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding.",
    "search_record": {
      "id": "http://arxiv.org/abs/2509.04685v3",
      "title": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding",
      "summary": "Existing speech tokenizers typically assign a fixed number of tokens per second, regardless of the varying information density or temporal fluctuations in the speech signal. This uniform token allocation mismatches the intrinsic structure of speech, where information is distributed unevenly over time. To address this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that adapts token allocation based on local feature similarity. VARSTok introduces two key innovations: (1) a temporal-aware density peak clustering algorithm that adaptively segments speech into variable-length units, and (2) a novel implicit duration coding scheme that embeds both content and temporal span into a single token index, eliminating the need for auxiliary duration predictors. Extensive experiments show that VARSTok significantly outperforms strong fixed-rate baselines. Notably, it achieves superior reconstruction naturalness while using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline. VARSTok further yields lower word error rates and improved naturalness in zero-shot text-to-speech synthesis. To the best of our knowledge, this is the first work to demonstrate that a fully dynamic, variable-frame-rate acoustic speech tokenizer can be seamlessly integrated into downstream speech language models.",
      "published": "2025-09-04T22:15:17Z"
    },
    "metadata": {
      "arxiv_id": "2509.04685",
      "title": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding",
      "summary": "Existing speech tokenizers typically assign a fixed number of tokens per second, regardless of the varying information density or temporal fluctuations in the speech signal. This uniform token allocation mismatches the intrinsic structure of speech, where information is distributed unevenly over time. To address this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that adapts token allocation based on local feature similarity. VARSTok introduces two key innovations: (1) a temporal-aware density peak clustering algorithm that adaptively segments speech into variable-length units, and (2) a novel implicit duration coding scheme that embeds both content and temporal span into a single token index, eliminating the need for auxiliary duration predictors. Extensive experiments show that VARSTok significantly outperforms strong fixed-rate baselines. Notably, it achieves superior reconstruction naturalness while using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline. VARSTok further yields lower word error rates and improved naturalness in zero-shot text-to-speech synthesis. To the best of our knowledge, this is the first work to demonstrate that a fully dynamic, variable-frame-rate acoustic speech tokenizer can be seamlessly integrated into downstream speech language models.",
      "authors": [
        "Rui-Chen Zheng",
        "Wenrui Liu",
        "Hui-Peng Du",
        "Qinglin Zhang",
        "Chong Deng",
        "Qian Chen",
        "Wen Wang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-09-04T22:15:17Z",
      "updated": "2025-11-13T18:04:16Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.04685v3",
      "landing_url": "https://arxiv.org/abs/2509.04685v3",
      "doi": "https://doi.org/10.48550/arXiv.2509.04685"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding."
      }
    ]
  },
  {
    "arxiv_id": "2509.09631",
    "anchor": "dblp_title",
    "search_term": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech.",
    "search_record": {
      "id": "http://arxiv.org/abs/2509.09631v2",
      "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech",
      "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
      "published": "2025-09-11T17:16:52Z"
    },
    "metadata": {
      "arxiv_id": "2509.09631",
      "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech",
      "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
      "authors": [
        "Ngoc-Son Nguyen",
        "Hieu-Nghia Huynh-Nguyen",
        "Thanh V. T. Tran",
        "Truong-Son Hy",
        "Van Nguyen"
      ],
      "published": "2025-09-11T17:16:52Z",
      "updated": "2025-09-12T01:59:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.09631v2",
      "landing_url": "https://arxiv.org/abs/2509.09631v2",
      "doi": "https://doi.org/10.48550/arXiv.2509.09631"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech."
      }
    ]
  },
  {
    "arxiv_id": "2509.14882",
    "anchor": "dblp_title",
    "search_term": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens.",
    "search_record": {
      "id": "http://arxiv.org/abs/2509.14882v1",
      "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens",
      "summary": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.",
      "published": "2025-09-18T12:00:07Z"
    },
    "metadata": {
      "arxiv_id": "2509.14882",
      "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens",
      "summary": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.",
      "authors": [
        "Issa Sugiura",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Ryuichiro Higashinaka"
      ],
      "published": "2025-09-18T12:00:07Z",
      "updated": "2025-09-18T12:00:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.14882v1",
      "landing_url": "https://arxiv.org/abs/2509.14882v1",
      "doi": "https://doi.org/10.48550/arXiv.2509.14882"
    },
    "queries": [
      {
        "anchor": "dblp_title",
        "search_term": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens."
      }
    ]
  }
]