[
  {
    "arxiv_id": "2206.06192",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.06192v2",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "published": "2022-06-13T14:26:40Z"
    },
    "metadata": {
      "arxiv_id": "2206.06192",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "authors": [
        "Arlo Faria",
        "Adam Janin",
        "Korbinian Riedhammer",
        "Sidhi Adkoli"
      ],
      "published": "2022-06-13T14:26:40Z",
      "updated": "2022-06-27T14:44:58Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06192v2",
      "landing_url": "https://arxiv.org/abs/2206.06192v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.06192"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2206.07086",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07086v1",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "published": "2022-06-14T18:21:01Z"
    },
    "metadata": {
      "arxiv_id": "2206.07086",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "authors": [
        "Ian Briggs",
        "Pavel Panchekha"
      ],
      "published": "2022-06-14T18:21:01Z",
      "updated": "2022-06-14T18:21:01Z",
      "categories": [
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07086v1",
      "landing_url": "https://arxiv.org/abs/2206.07086v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07086"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2206.07288",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07288v1",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "published": "2022-06-15T04:04:14Z"
    },
    "metadata": {
      "arxiv_id": "2206.07288",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "authors": [
        "Ziyi Chen",
        "Haoran Miao",
        "Pengyuan Zhang"
      ],
      "published": "2022-06-15T04:04:14Z",
      "updated": "2022-06-15T04:04:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07288v1",
      "landing_url": "https://arxiv.org/abs/2206.07288v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07288"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2206.07569",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07569v1",
      "title": "End-to-End Voice Conversion with Information Perturbation",
      "summary": "The ideal goal of voice conversion is to convert the source speaker's speech to sound naturally like the target speaker while maintaining the linguistic content and the prosody of the source speech. However, current approaches are insufficient to achieve comprehensive source prosody transfer and target speaker timbre preservation in the converted speech, and the quality of the converted speech is also unsatisfied due to the mismatch between the acoustic model and the vocoder. In this paper, we leverage the recent advances in information perturbation and propose a fully end-to-end approach to conduct high-quality voice conversion. We first adopt information perturbation to remove speaker-related information in the source speech to disentangle speaker timbre and linguistic content and thus the linguistic information is subsequently modeled by a content encoder. To better transfer the prosody of the source speech to the target, we particularly introduce a speaker-related pitch encoder which can maintain the general pitch pattern of the source speaker while flexibly modifying the pitch intensity of the generated speech. Finally, one-shot voice conversion is set up through continuous speaker space modeling. Experimental results indicate that the proposed end-to-end approach significantly outperforms the state-of-the-art models in terms of intelligibility, naturalness, and speaker similarity.",
      "published": "2022-06-15T14:38:31Z"
    },
    "metadata": {
      "arxiv_id": "2206.07569",
      "title": "End-to-End Voice Conversion with Information Perturbation",
      "summary": "The ideal goal of voice conversion is to convert the source speaker's speech to sound naturally like the target speaker while maintaining the linguistic content and the prosody of the source speech. However, current approaches are insufficient to achieve comprehensive source prosody transfer and target speaker timbre preservation in the converted speech, and the quality of the converted speech is also unsatisfied due to the mismatch between the acoustic model and the vocoder. In this paper, we leverage the recent advances in information perturbation and propose a fully end-to-end approach to conduct high-quality voice conversion. We first adopt information perturbation to remove speaker-related information in the source speech to disentangle speaker timbre and linguistic content and thus the linguistic information is subsequently modeled by a content encoder. To better transfer the prosody of the source speech to the target, we particularly introduce a speaker-related pitch encoder which can maintain the general pitch pattern of the source speaker while flexibly modifying the pitch intensity of the generated speech. Finally, one-shot voice conversion is set up through continuous speaker space modeling. Experimental results indicate that the proposed end-to-end approach significantly outperforms the state-of-the-art models in terms of intelligibility, naturalness, and speaker similarity.",
      "authors": [
        "Qicong Xie",
        "Shan Yang",
        "Yi Lei",
        "Lei Xie",
        "Dan Su"
      ],
      "published": "2022-06-15T14:38:31Z",
      "updated": "2022-06-15T14:38:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07569v1",
      "landing_url": "https://arxiv.org/abs/2206.07569v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07569"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2206.07860",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07860v1",
      "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
      "summary": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
      "published": "2022-06-16T00:33:20Z"
    },
    "metadata": {
      "arxiv_id": "2206.07860",
      "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
      "summary": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
      "authors": [
        "Li-Chin Chen",
        "Po-Hsun Chen",
        "Richard Tzong-Han Tsai",
        "Yu Tsao"
      ],
      "published": "2022-06-16T00:33:20Z",
      "updated": "2022-06-16T00:33:20Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07860v1",
      "landing_url": "https://arxiv.org/abs/2206.07860v1",
      "doi": "https://doi.org/10.1109/LSP.2022.3184636"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2206.08790",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.08790v1",
      "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
      "summary": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner.",
      "published": "2022-06-17T14:04:24Z"
    },
    "metadata": {
      "arxiv_id": "2206.08790",
      "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
      "summary": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner.",
      "authors": [
        "Marc-Antoine Georges",
        "Jean-Luc Schwartz",
        "Thomas Hueber"
      ],
      "published": "2022-06-17T14:04:24Z",
      "updated": "2022-06-17T14:04:24Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.08790v1",
      "landing_url": "https://arxiv.org/abs/2206.08790v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.08790"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2206.09103",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.09103v2",
      "title": "Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems",
      "summary": "An automatic speaker verification system aims to verify the speaker identity of a speech signal. However, a voice conversion system could manipulate a person's speech signal to make it sound like another speaker's voice and deceive the speaker verification system. Most countermeasures for voice conversion-based spoofing attacks are designed to discriminate bona fide speech from spoofed speech for speaker verification systems. In this paper, we investigate the problem of source speaker identification -- inferring the identity of the source speaker given the voice converted speech. To perform source speaker identification, we simply add voice-converted speech data with the label of source speaker identity to the genuine speech dataset during speaker embedding network training. Experimental results show the feasibility of source speaker identification when training and testing with converted speeches from the same voice conversion model(s). In addition, our results demonstrate that having more converted utterances from various voice conversion model for training helps improve the source speaker identification performance on converted utterances from unseen voice conversion models.",
      "published": "2022-06-18T03:45:34Z"
    },
    "metadata": {
      "arxiv_id": "2206.09103",
      "title": "Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems",
      "summary": "An automatic speaker verification system aims to verify the speaker identity of a speech signal. However, a voice conversion system could manipulate a person's speech signal to make it sound like another speaker's voice and deceive the speaker verification system. Most countermeasures for voice conversion-based spoofing attacks are designed to discriminate bona fide speech from spoofed speech for speaker verification systems. In this paper, we investigate the problem of source speaker identification -- inferring the identity of the source speaker given the voice converted speech. To perform source speaker identification, we simply add voice-converted speech data with the label of source speaker identity to the genuine speech dataset during speaker embedding network training. Experimental results show the feasibility of source speaker identification when training and testing with converted speeches from the same voice conversion model(s). In addition, our results demonstrate that having more converted utterances from various voice conversion model for training helps improve the source speaker identification performance on converted utterances from unseen voice conversion models.",
      "authors": [
        "Danwei Cai",
        "Zexin Cai",
        "Ming Li"
      ],
      "published": "2022-06-18T03:45:34Z",
      "updated": "2022-10-31T18:50:41Z",
      "categories": [
        "eess.AS",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09103v2",
      "landing_url": "https://arxiv.org/abs/2206.09103v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.09103"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2206.09680",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.09680v1",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "published": "2022-06-20T09:42:50Z"
    },
    "metadata": {
      "arxiv_id": "2206.09680",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "authors": [
        "Pakawat Nakwijit",
        "Matthew Purver"
      ],
      "published": "2022-06-20T09:42:50Z",
      "updated": "2022-06-20T09:42:50Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09680v1",
      "landing_url": "https://arxiv.org/abs/2206.09680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.09680"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2206.10552",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.10552v2",
      "title": "Vicinity Vision Transformer",
      "summary": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
      "published": "2022-06-21T17:33:53Z"
    },
    "metadata": {
      "arxiv_id": "2206.10552",
      "title": "Vicinity Vision Transformer",
      "summary": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
      "authors": [
        "Weixuan Sun",
        "Zhen Qin",
        "Hui Deng",
        "Jianyuan Wang",
        "Yi Zhang",
        "Kaihao Zhang",
        "Nick Barnes",
        "Stan Birchfield",
        "Lingpeng Kong",
        "Yiran Zhong"
      ],
      "published": "2022-06-21T17:33:53Z",
      "updated": "2023-07-20T08:57:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.10552v2",
      "landing_url": "https://arxiv.org/abs/2206.10552v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.10552"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2206.12351",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.12351v1",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "published": "2022-06-24T15:47:42Z"
    },
    "metadata": {
      "arxiv_id": "2206.12351",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "authors": [
        "Alex F. McKinney",
        "Chris G. Willcocks"
      ],
      "published": "2022-06-24T15:47:42Z",
      "updated": "2022-06-24T15:47:42Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12351v1",
      "landing_url": "https://arxiv.org/abs/2206.12351v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12351"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2206.12693",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.12693v1",
      "title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction",
      "summary": "This paper presents TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model. This takes advantage of the fact that if the language model will reliably and accurately predict a token anyway, then the acoustic model doesn't need to be accurate in recognizing it. We train German ASR models with 900 million parameters and show that on CommonVoice German, TEVR scores a very competitive 3.64% word error rate, which outperforms the best reported results by a relative 16.89% reduction in word error rate. We hope that releasing our fully trained speech recognition pipeline to the community will lead to privacy-preserving offline virtual assistants in the future.",
      "published": "2022-06-25T16:42:05Z"
    },
    "metadata": {
      "arxiv_id": "2206.12693",
      "title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction",
      "summary": "This paper presents TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model. This takes advantage of the fact that if the language model will reliably and accurately predict a token anyway, then the acoustic model doesn't need to be accurate in recognizing it. We train German ASR models with 900 million parameters and show that on CommonVoice German, TEVR scores a very competitive 3.64% word error rate, which outperforms the best reported results by a relative 16.89% reduction in word error rate. We hope that releasing our fully trained speech recognition pipeline to the community will lead to privacy-preserving offline virtual assistants in the future.",
      "authors": [
        "Hajo Nils Krabbenhöft",
        "Erhardt Barth"
      ],
      "published": "2022-06-25T16:42:05Z",
      "updated": "2022-06-25T16:42:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12693v1",
      "landing_url": "https://arxiv.org/abs/2206.12693v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12693"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2206.14962",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.14962v1",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "published": "2022-06-30T01:16:40Z"
    },
    "metadata": {
      "arxiv_id": "2206.14962",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "authors": [
        "Xinmeng Xu",
        "Yang Wang",
        "Jie Jia",
        "Binbin Chen",
        "Jianjun Hao"
      ],
      "published": "2022-06-30T01:16:40Z",
      "updated": "2022-06-30T01:16:40Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.14962v1",
      "landing_url": "https://arxiv.org/abs/2206.14962v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.14962"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2206.15147",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.15147v2",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "published": "2022-06-30T09:29:18Z"
    },
    "metadata": {
      "arxiv_id": "2206.15147",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "authors": [
        "Asier Gutiérrez-Fandiño",
        "David Pérez-Fernández",
        "Jordi Armengol-Estapé",
        "David Griol",
        "Zoraida Callejas"
      ],
      "published": "2022-06-30T09:29:18Z",
      "updated": "2022-07-01T08:22:32Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.15147v2",
      "landing_url": "https://arxiv.org/abs/2206.15147v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.15147"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2207.00756",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.00756v1",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "published": "2022-07-02T06:51:12Z"
    },
    "metadata": {
      "arxiv_id": "2207.00756",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "authors": [
        "Liumeng Xue",
        "Shan Yang",
        "Na Hu",
        "Dan Su",
        "Lei Xie"
      ],
      "published": "2022-07-02T06:51:12Z",
      "updated": "2022-07-02T06:51:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00756v1",
      "landing_url": "https://arxiv.org/abs/2207.00756v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00756"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2207.04356",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.04356v1",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "published": "2022-07-10T01:02:22Z"
    },
    "metadata": {
      "arxiv_id": "2207.04356",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "authors": [
        "Wen-Chin Huang",
        "Shu-Wen Yang",
        "Tomoki Hayashi",
        "Tomoki Toda"
      ],
      "published": "2022-07-10T01:02:22Z",
      "updated": "2022-07-10T01:02:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04356v1",
      "landing_url": "https://arxiv.org/abs/2207.04356v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3193761"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2207.04646",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.04646v1",
      "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
      "summary": "Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system.",
      "published": "2022-07-11T06:15:45Z"
    },
    "metadata": {
      "arxiv_id": "2207.04646",
      "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
      "summary": "Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system.",
      "authors": [
        "Yanqing Liu",
        "Ruiqing Xue",
        "Lei He",
        "Xu Tan",
        "Sheng Zhao"
      ],
      "published": "2022-07-11T06:15:45Z",
      "updated": "2022-07-11T06:15:45Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04646v1",
      "landing_url": "https://arxiv.org/abs/2207.04646v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.04646"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2207.04651",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.04651v1",
      "title": "A Lexicon and Depth-wise Separable Convolution Based Handwritten Text Recognition System",
      "summary": "Cursive handwritten text recognition is a challenging research problem in the domain of pattern recognition. The current state-of-the-art approaches include models based on convolutional recurrent neural networks and multi-dimensional long short-term memory recurrent neural networks techniques. These methods are highly computationally extensive as well model is complex at design level. In recent studies, combination of convolutional neural network and gated convolutional neural networks based models demonstrated less number of parameters in comparison to convolutional recurrent neural networks based models. In the direction to reduced the total number of parameters to be trained, in this work, we have used depthwise convolution in place of standard convolutions with a combination of gated-convolutional neural network and bidirectional gated recurrent unit to reduce the total number of parameters to be trained. Additionally, we have also included a lexicon based word beam search decoder at testing step. It also helps in improving the the overall accuracy of the model. We have obtained 3.84% character error rate and 9.40% word error rate on IAM dataset; 4.88% character error rate and 14.56% word error rate in George Washington dataset, respectively.",
      "published": "2022-07-11T06:24:26Z"
    },
    "metadata": {
      "arxiv_id": "2207.04651",
      "title": "A Lexicon and Depth-wise Separable Convolution Based Handwritten Text Recognition System",
      "summary": "Cursive handwritten text recognition is a challenging research problem in the domain of pattern recognition. The current state-of-the-art approaches include models based on convolutional recurrent neural networks and multi-dimensional long short-term memory recurrent neural networks techniques. These methods are highly computationally extensive as well model is complex at design level. In recent studies, combination of convolutional neural network and gated convolutional neural networks based models demonstrated less number of parameters in comparison to convolutional recurrent neural networks based models. In the direction to reduced the total number of parameters to be trained, in this work, we have used depthwise convolution in place of standard convolutions with a combination of gated-convolutional neural network and bidirectional gated recurrent unit to reduce the total number of parameters to be trained. Additionally, we have also included a lexicon based word beam search decoder at testing step. It also helps in improving the the overall accuracy of the model. We have obtained 3.84% character error rate and 9.40% word error rate on IAM dataset; 4.88% character error rate and 14.56% word error rate in George Washington dataset, respectively.",
      "authors": [
        "Lalita Kumari",
        "Sukhdeep Singh",
        "VVS Rathore",
        "Anuj Sharma"
      ],
      "published": "2022-07-11T06:24:26Z",
      "updated": "2022-07-11T06:24:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04651v1",
      "landing_url": "https://arxiv.org/abs/2207.04651v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.04651"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2207.05749",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.05749v1",
      "title": "Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer",
      "summary": "Pathologists have a rich vocabulary with which they can describe all the nuances of cellular morphology. In their world, there is a natural pairing of images and words. Recent advances demonstrate that machine learning models can now be trained to learn high-quality image features and represent them as discrete units of information. This enables natural language, which is also discrete, to be jointly modelled alongside the imaging, resulting in a description of the contents of the imaging. Here we present experiments in applying discrete modelling techniques to the problem domain of non-melanoma skin cancer, specifically, histological images of Intraepidermal Carcinoma (IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256) images of IEC images, we trained a sequence-to-sequence transformer to generate natural language descriptions using pathologist terminology. Combined with the idea of interactive concept vectors available by using continuous generative methods, we demonstrate an additional angle of interpretability. The result is a promising means of working towards highly expressive machine learning systems which are not only useful as predictive/classification tools, but also means to further our scientific understanding of disease.",
      "published": "2022-07-09T04:53:25Z"
    },
    "metadata": {
      "arxiv_id": "2207.05749",
      "title": "Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer",
      "summary": "Pathologists have a rich vocabulary with which they can describe all the nuances of cellular morphology. In their world, there is a natural pairing of images and words. Recent advances demonstrate that machine learning models can now be trained to learn high-quality image features and represent them as discrete units of information. This enables natural language, which is also discrete, to be jointly modelled alongside the imaging, resulting in a description of the contents of the imaging. Here we present experiments in applying discrete modelling techniques to the problem domain of non-melanoma skin cancer, specifically, histological images of Intraepidermal Carcinoma (IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256) images of IEC images, we trained a sequence-to-sequence transformer to generate natural language descriptions using pathologist terminology. Combined with the idea of interactive concept vectors available by using continuous generative methods, we demonstrate an additional angle of interpretability. The result is a promising means of working towards highly expressive machine learning systems which are not only useful as predictive/classification tools, but also means to further our scientific understanding of disease.",
      "authors": [
        "Simon M. Thomas",
        "James G. Lefevre",
        "Glenn Baxter",
        "Nicholas A. Hamilton"
      ],
      "published": "2022-07-09T04:53:25Z",
      "updated": "2022-07-09T04:53:25Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05749v1",
      "landing_url": "https://arxiv.org/abs/2207.05749v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05749"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2207.05913",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.05913v1",
      "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
      "summary": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
      "published": "2022-07-13T01:40:59Z"
    },
    "metadata": {
      "arxiv_id": "2207.05913",
      "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
      "summary": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
      "authors": [
        "Yi-Chiao Wu",
        "Patrick Lumban Tobing",
        "Kazuki Yasuhara",
        "Noriyuki Matsunaga",
        "Yamato Ohtani",
        "Tomoki Toda"
      ],
      "published": "2022-07-13T01:40:59Z",
      "updated": "2022-07-13T01:40:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05913v1",
      "landing_url": "https://arxiv.org/abs/2207.05913v1",
      "doi": "https://doi.org/10.1561/116.00000020"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2207.05920",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.05920v1",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "published": "2022-07-13T01:56:31Z"
    },
    "metadata": {
      "arxiv_id": "2207.05920",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "authors": [
        "Weiqing Wang",
        "Qingjian Lin",
        "Ming Li"
      ],
      "published": "2022-07-13T01:56:31Z",
      "updated": "2022-07-13T01:56:31Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05920v1",
      "landing_url": "https://arxiv.org/abs/2207.05920v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2207.06011",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.06011v1",
      "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
      "summary": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
      "published": "2022-07-13T07:35:23Z"
    },
    "metadata": {
      "arxiv_id": "2207.06011",
      "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
      "summary": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2022-07-13T07:35:23Z",
      "updated": "2022-07-13T07:35:23Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06011v1",
      "landing_url": "https://arxiv.org/abs/2207.06011v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06011"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2207.06057",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.06057v2",
      "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
      "summary": "Voice conversion is to generate a new speech with the source content and a target voice style. In this paper, we focus on one general setting, i.e., non-parallel many-to-many voice conversion, which is close to the real-world scenario. As the name implies, non-parallel many-to-many voice conversion does not require the paired source and reference speeches and can be applied to arbitrary voice transfer. In recent years, Generative Adversarial Networks (GANs) and other techniques such as Conditional Variational Autoencoders (CVAEs) have made considerable progress in this field. However, due to the sophistication of voice conversion, the style similarity of the converted speech is still unsatisfactory. Inspired by the inherent structure of mel-spectrogram, we propose a new voice conversion framework, i.e., Subband-based Generative Adversarial Network for Voice Conversion (SGAN-VC). SGAN-VC converts each subband content of the source speech separately by explicitly utilizing the spatial characteristics between different subbands. SGAN-VC contains one style encoder, one content encoder, and one decoder. In particular, the style encoder network is designed to learn style codes for different subbands of the target speaker. The content encoder network can capture the content information on the source speech. Finally, the decoder generates particular subband content. In addition, we propose a pitch-shift module to fine-tune the pitch of the source speaker, making the converted tone more accurate and explainable. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on VCTK Corpus and AISHELL3 datasets both qualitatively and quantitatively, whether on seen or unseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data even exceeds that of StarGANv2-VC with ASR network assistance.",
      "published": "2022-07-13T09:03:28Z"
    },
    "metadata": {
      "arxiv_id": "2207.06057",
      "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
      "summary": "Voice conversion is to generate a new speech with the source content and a target voice style. In this paper, we focus on one general setting, i.e., non-parallel many-to-many voice conversion, which is close to the real-world scenario. As the name implies, non-parallel many-to-many voice conversion does not require the paired source and reference speeches and can be applied to arbitrary voice transfer. In recent years, Generative Adversarial Networks (GANs) and other techniques such as Conditional Variational Autoencoders (CVAEs) have made considerable progress in this field. However, due to the sophistication of voice conversion, the style similarity of the converted speech is still unsatisfactory. Inspired by the inherent structure of mel-spectrogram, we propose a new voice conversion framework, i.e., Subband-based Generative Adversarial Network for Voice Conversion (SGAN-VC). SGAN-VC converts each subband content of the source speech separately by explicitly utilizing the spatial characteristics between different subbands. SGAN-VC contains one style encoder, one content encoder, and one decoder. In particular, the style encoder network is designed to learn style codes for different subbands of the target speaker. The content encoder network can capture the content information on the source speech. Finally, the decoder generates particular subband content. In addition, we propose a pitch-shift module to fine-tune the pitch of the source speaker, making the converted tone more accurate and explainable. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on VCTK Corpus and AISHELL3 datasets both qualitatively and quantitatively, whether on seen or unseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data even exceeds that of StarGANv2-VC with ASR network assistance.",
      "authors": [
        "Jian Ma",
        "Zhedong Zheng",
        "Hao Fei",
        "Feng Zheng",
        "Tat-seng Chua",
        "Yi Yang"
      ],
      "published": "2022-07-13T09:03:28Z",
      "updated": "2022-07-27T07:31:57Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06057v2",
      "landing_url": "https://arxiv.org/abs/2207.06057v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.06057"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2207.06088",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.06088v1",
      "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
      "summary": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
      "published": "2022-07-13T09:57:06Z"
    },
    "metadata": {
      "arxiv_id": "2207.06088",
      "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
      "summary": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
      "authors": [
        "Zhengxi Liu",
        "Qiao Tian",
        "Chenxu Hu",
        "Xudong Liu",
        "Menglin Wu",
        "Yuping Wang",
        "Hang Zhao",
        "Yuxuan Wang"
      ],
      "published": "2022-07-13T09:57:06Z",
      "updated": "2022-07-13T09:57:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06088v1",
      "landing_url": "https://arxiv.org/abs/2207.06088v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06088"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2207.06389",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.06389v1",
      "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
      "summary": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
      "published": "2022-07-13T17:45:43Z"
    },
    "metadata": {
      "arxiv_id": "2207.06389",
      "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
      "summary": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
      "authors": [
        "Rongjie Huang",
        "Zhou Zhao",
        "Huadai Liu",
        "Jinglin Liu",
        "Chenye Cui",
        "Yi Ren"
      ],
      "published": "2022-07-13T17:45:43Z",
      "updated": "2022-07-13T17:45:43Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06389v1",
      "landing_url": "https://arxiv.org/abs/2207.06389v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06389"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2207.07850",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.07850v1",
      "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
      "summary": "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region.",
      "published": "2022-07-16T06:04:52Z"
    },
    "metadata": {
      "arxiv_id": "2207.07850",
      "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
      "summary": "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region.",
      "authors": [
        "Viet Anh Trinh",
        "Pegah Ghahremani",
        "Brian King",
        "Jasha Droppo",
        "Andreas Stolcke",
        "Roland Maas"
      ],
      "published": "2022-07-16T06:04:52Z",
      "updated": "2022-07-16T06:04:52Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07850v1",
      "landing_url": "https://arxiv.org/abs/2207.07850v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-11063"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2207.09783",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.09783v1",
      "title": "Cancer Subtyping by Improved Transcriptomic Features Using Vector Quantized Variational Autoencoder",
      "summary": "Defining and separating cancer subtypes is essential for facilitating personalized therapy modality and prognosis of patients. The definition of subtypes has been constantly recalibrated as a result of our deepened understanding. During this recalibration, researchers often rely on clustering of cancer data to provide an intuitive visual reference that could reveal the intrinsic characteristics of subtypes. The data being clustered are often omics data such as transcriptomics that have strong correlations to the underlying biological mechanism. However, while existing studies have shown promising results, they suffer from issues associated with omics data: sample scarcity and high dimensionality. As such, existing methods often impose unrealistic assumptions to extract useful features from the data while avoiding overfitting to spurious correlations. In this paper, we propose to leverage a recent strong generative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle the data issues and extract informative latent features that are crucial to the quality of subsequent clustering by retaining only information relevant to reconstructing the input. VQ-VAE does not impose strict assumptions and hence its latent features are better representations of the input, capable of yielding superior clustering performance with any mainstream clustering method. Extensive experiments and medical analysis on multiple datasets comprising 10 distinct cancers demonstrate the VQ-VAE clustering results can significantly and robustly improve prognosis over prevalent subtyping systems.",
      "published": "2022-07-20T09:47:53Z"
    },
    "metadata": {
      "arxiv_id": "2207.09783",
      "title": "Cancer Subtyping by Improved Transcriptomic Features Using Vector Quantized Variational Autoencoder",
      "summary": "Defining and separating cancer subtypes is essential for facilitating personalized therapy modality and prognosis of patients. The definition of subtypes has been constantly recalibrated as a result of our deepened understanding. During this recalibration, researchers often rely on clustering of cancer data to provide an intuitive visual reference that could reveal the intrinsic characteristics of subtypes. The data being clustered are often omics data such as transcriptomics that have strong correlations to the underlying biological mechanism. However, while existing studies have shown promising results, they suffer from issues associated with omics data: sample scarcity and high dimensionality. As such, existing methods often impose unrealistic assumptions to extract useful features from the data while avoiding overfitting to spurious correlations. In this paper, we propose to leverage a recent strong generative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle the data issues and extract informative latent features that are crucial to the quality of subsequent clustering by retaining only information relevant to reconstructing the input. VQ-VAE does not impose strict assumptions and hence its latent features are better representations of the input, capable of yielding superior clustering performance with any mainstream clustering method. Extensive experiments and medical analysis on multiple datasets comprising 10 distinct cancers demonstrate the VQ-VAE clustering results can significantly and robustly improve prognosis over prevalent subtyping systems.",
      "authors": [
        "Zheng Chen",
        "Ziwei Yang",
        "Lingwei Zhu",
        "Guang Shi",
        "Kun Yue",
        "Takashi Matsubara",
        "Shigehiko Kanaya",
        "MD Altaf-Ul-Amin"
      ],
      "published": "2022-07-20T09:47:53Z",
      "updated": "2022-07-20T09:47:53Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.09783v1",
      "landing_url": "https://arxiv.org/abs/2207.09783v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.09783"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2207.11226",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.11226v1",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "published": "2022-07-18T07:11:28Z"
    },
    "metadata": {
      "arxiv_id": "2207.11226",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "authors": [
        "Lior Ben-Moshe",
        "Sagie Benaim",
        "Lior Wolf"
      ],
      "published": "2022-07-18T07:11:28Z",
      "updated": "2022-07-18T07:11:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.11226v1",
      "landing_url": "https://arxiv.org/abs/2207.11226v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.11226"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2207.13286",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.13286v1",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "published": "2022-07-27T04:22:29Z"
    },
    "metadata": {
      "arxiv_id": "2207.13286",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "authors": [
        "Yu-Jie Chen",
        "Shin-I Cheng",
        "Wei-Chen Chiu",
        "Hung-Yu Tseng",
        "Hsin-Ying Lee"
      ],
      "published": "2022-07-27T04:22:29Z",
      "updated": "2022-07-27T04:22:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13286v1",
      "landing_url": "https://arxiv.org/abs/2207.13286v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13286"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2207.13861",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.13861v2",
      "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
      "summary": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
      "published": "2022-07-28T02:33:57Z"
    },
    "metadata": {
      "arxiv_id": "2207.13861",
      "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
      "summary": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
      "authors": [
        "Hao Li",
        "Zhijing Yang",
        "Xiaobin Hong",
        "Ziying Zhao",
        "Junyang Chen",
        "Yukai Shi",
        "Jinshan Pan"
      ],
      "published": "2022-07-28T02:33:57Z",
      "updated": "2022-09-13T05:14:07Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13861v2",
      "landing_url": "https://arxiv.org/abs/2207.13861v2",
      "doi": "https://doi.org/10.1016/j.knosys.2022.109815"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2207.14686",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.14686v3",
      "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
      "summary": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
      "published": "2022-07-29T13:58:24Z"
    },
    "metadata": {
      "arxiv_id": "2207.14686",
      "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
      "summary": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
      "authors": [
        "Denise Moussa",
        "Anatol Maier",
        "Andreas Spruck",
        "Jürgen Seiler",
        "Christian Riess"
      ],
      "published": "2022-07-29T13:58:24Z",
      "updated": "2024-05-03T15:15:27Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.14686v3",
      "landing_url": "https://arxiv.org/abs/2207.14686v3",
      "doi": "https://doi.org/10.1109/ICIP46576.2022.9897178"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2208.03987",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.03987v4",
      "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
      "summary": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
      "published": "2022-08-08T09:08:40Z"
    },
    "metadata": {
      "arxiv_id": "2208.03987",
      "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
      "summary": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
      "authors": [
        "Di Wang",
        "Qiming Zhang",
        "Yufei Xu",
        "Jing Zhang",
        "Bo Du",
        "Dacheng Tao",
        "Liangpei Zhang"
      ],
      "published": "2022-08-08T09:08:40Z",
      "updated": "2022-12-08T13:51:33Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.03987v4",
      "landing_url": "https://arxiv.org/abs/2208.03987v4",
      "doi": "https://doi.org/10.48550/arXiv.2208.03987"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2208.04854",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.04854v1",
      "title": "Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA",
      "summary": "Convolutional Neural Networks (CNNs) reach high accuracies in various application domains, but require large amounts of computation and incur costly data movements. One method to decrease these costs while trading accuracy is weight and/or activation word-length reduction. Thereby, layer-wise mixed-precision quantization allows for more efficient results while inflating the design space. In this work, we present an in-depth quantitative methodology to efficiently explore the design space considering the limited hardware resources of a given FPGA. Our holistic exploration approach vertically traverses the various design entry levels from the architectural down to the logic level, and laterally covers optimization from processing elements to dataflow for an efficient mixed-precision CNN accelerator. Our resulting hardware accelerators implement truly mixed-precision operations that enable efficient execution of layer-wise and channel-wise quantized CNNs. Mapping feed-forward and identity-shortcut-connection mixed-precision CNNs result in competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5 accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for ResNet-152, respectively. Thereby, the required memory footprint for parameters is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",
      "published": "2022-08-09T15:32:51Z"
    },
    "metadata": {
      "arxiv_id": "2208.04854",
      "title": "Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA",
      "summary": "Convolutional Neural Networks (CNNs) reach high accuracies in various application domains, but require large amounts of computation and incur costly data movements. One method to decrease these costs while trading accuracy is weight and/or activation word-length reduction. Thereby, layer-wise mixed-precision quantization allows for more efficient results while inflating the design space. In this work, we present an in-depth quantitative methodology to efficiently explore the design space considering the limited hardware resources of a given FPGA. Our holistic exploration approach vertically traverses the various design entry levels from the architectural down to the logic level, and laterally covers optimization from processing elements to dataflow for an efficient mixed-precision CNN accelerator. Our resulting hardware accelerators implement truly mixed-precision operations that enable efficient execution of layer-wise and channel-wise quantized CNNs. Mapping feed-forward and identity-shortcut-connection mixed-precision CNNs result in competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5 accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for ResNet-152, respectively. Thereby, the required memory footprint for parameters is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",
      "authors": [
        "Cecilia Latotzke",
        "Tim Ciesielski",
        "Tobias Gemmeke"
      ],
      "published": "2022-08-09T15:32:51Z",
      "updated": "2022-08-09T15:32:51Z",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.04854v1",
      "landing_url": "https://arxiv.org/abs/2208.04854v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.04854"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2208.08757",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.08757v1",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "published": "2022-08-18T10:36:27Z"
    },
    "metadata": {
      "arxiv_id": "2208.08757",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "authors": [
        "SiCheng Yang",
        "Methawee Tantrawenith",
        "Haolin Zhuang",
        "Zhiyong Wu",
        "Aolan Sun",
        "Jianzong Wang",
        "Ning Cheng",
        "Huaizhen Tang",
        "Xintao Zhao",
        "Jie Wang",
        "Helen Meng"
      ],
      "published": "2022-08-18T10:36:27Z",
      "updated": "2022-08-18T10:36:27Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.08757v1",
      "landing_url": "https://arxiv.org/abs/2208.08757v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.08757"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2208.09030",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.09030v3",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "published": "2022-08-18T19:02:30Z"
    },
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2208.10291",
    "anchor": "acoustic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.10291v3",
      "title": "Efficient Planning in a Compact Latent Action Space",
      "summary": "Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision-making, and scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.",
      "published": "2022-08-22T13:19:02Z"
    },
    "metadata": {
      "arxiv_id": "2208.10291",
      "title": "Efficient Planning in a Compact Latent Action Space",
      "summary": "Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision-making, and scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.",
      "authors": [
        "Zhengyao Jiang",
        "Tianjun Zhang",
        "Michael Janner",
        "Yueying Li",
        "Tim Rocktäschel",
        "Edward Grefenstette",
        "Yuandong Tian"
      ],
      "published": "2022-08-22T13:19:02Z",
      "updated": "2023-01-24T11:09:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.10291v3",
      "landing_url": "https://arxiv.org/abs/2208.10291v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.10291"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2209.01978",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.01978v1",
      "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
      "summary": "Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data. However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction. Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case. Therefore, the contribution of this work is two-fold. First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction. Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker.",
      "published": "2022-09-05T14:20:42Z"
    },
    "metadata": {
      "arxiv_id": "2209.01978",
      "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
      "summary": "Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data. However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction. Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case. Therefore, the contribution of this work is two-fold. First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction. Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker.",
      "authors": [
        "Michael Kuhlmann",
        "Fritz Seebauer",
        "Janek Ebbers",
        "Petra Wagner",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-09-05T14:20:42Z",
      "updated": "2022-09-05T14:20:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01978v1",
      "landing_url": "https://arxiv.org/abs/2209.01978v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.01978"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2209.04213",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.04213v2",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "published": "2022-09-09T09:59:56Z"
    },
    "metadata": {
      "arxiv_id": "2209.04213",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "authors": [
        "Jonas Köhne",
        "Lars Henning",
        "Clemens Gühmann"
      ],
      "published": "2022-09-09T09:59:56Z",
      "updated": "2022-09-23T11:07:27Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04213v2",
      "landing_url": "https://arxiv.org/abs/2209.04213v2",
      "doi": "https://doi.org/10.1109/ACCESS.2023.3247564"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2209.04817",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.04817v1",
      "title": "Lexicon and Attention based Handwritten Text Recognition System",
      "summary": "The handwritten text recognition problem is widely studied by the researchers of computer vision community due to its scope of improvement and applicability to daily lives, It is a sub-domain of pattern recognition. Due to advancement of computational power of computers since last few decades neural networks based systems heavily contributed towards providing the state-of-the-art handwritten text recognizers. In the same direction, we have taken two state-of-the art neural networks systems and merged the attention mechanism with it. The attention technique has been widely used in the domain of neural machine translations and automatic speech recognition and now is being implemented in text recognition domain. In this study, we are able to achieve 4.15% character error rate and 9.72% word error rate on IAM dataset, 7.07% character error rate and 16.14% word error rate on GW dataset after merging the attention and word beam search decoder with existing Flor et al. architecture. To analyse further, we have also used system similar to Shi et al. neural network system with greedy decoder and observed 23.27% improvement in character error rate from the base model.",
      "published": "2022-09-11T09:26:45Z"
    },
    "metadata": {
      "arxiv_id": "2209.04817",
      "title": "Lexicon and Attention based Handwritten Text Recognition System",
      "summary": "The handwritten text recognition problem is widely studied by the researchers of computer vision community due to its scope of improvement and applicability to daily lives, It is a sub-domain of pattern recognition. Due to advancement of computational power of computers since last few decades neural networks based systems heavily contributed towards providing the state-of-the-art handwritten text recognizers. In the same direction, we have taken two state-of-the art neural networks systems and merged the attention mechanism with it. The attention technique has been widely used in the domain of neural machine translations and automatic speech recognition and now is being implemented in text recognition domain. In this study, we are able to achieve 4.15% character error rate and 9.72% word error rate on IAM dataset, 7.07% character error rate and 16.14% word error rate on GW dataset after merging the attention and word beam search decoder with existing Flor et al. architecture. To analyse further, we have also used system similar to Shi et al. neural network system with greedy decoder and observed 23.27% improvement in character error rate from the base model.",
      "authors": [
        "Lalita Kumari",
        "Sukhdeep Singh",
        "VVS Rathore",
        "Anuj Sharma"
      ],
      "published": "2022-09-11T09:26:45Z",
      "updated": "2022-09-11T09:26:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04817v1",
      "landing_url": "https://arxiv.org/abs/2209.04817v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.04817"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2209.06987",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.06987v1",
      "title": "Non-Parallel Voice Conversion for ASR Augmentation",
      "summary": "Automatic speech recognition (ASR) needs to be robust to speaker differences. Voice Conversion (VC) modifies speaker characteristics of input speech. This is an attractive feature for ASR data augmentation. In this paper, we demonstrate that voice conversion can be used as a data augmentation technique to improve ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR augmentation, it is necessary that the VC model be robust to a wide range of input speech. This motivates the use of a non-autoregressive, non-parallel VC model, and the use of a pretrained ASR encoder within the VC model. This work suggests that despite including many speakers, speaker diversity may remain a limitation to ASR quality. Finally, interrogation of our VC performance has provided useful metrics for objective evaluation of VC quality.",
      "published": "2022-09-15T00:40:35Z"
    },
    "metadata": {
      "arxiv_id": "2209.06987",
      "title": "Non-Parallel Voice Conversion for ASR Augmentation",
      "summary": "Automatic speech recognition (ASR) needs to be robust to speaker differences. Voice Conversion (VC) modifies speaker characteristics of input speech. This is an attractive feature for ASR data augmentation. In this paper, we demonstrate that voice conversion can be used as a data augmentation technique to improve ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR augmentation, it is necessary that the VC model be robust to a wide range of input speech. This motivates the use of a non-autoregressive, non-parallel VC model, and the use of a pretrained ASR encoder within the VC model. This work suggests that despite including many speakers, speaker diversity may remain a limitation to ASR quality. Finally, interrogation of our VC performance has provided useful metrics for objective evaluation of VC quality.",
      "authors": [
        "Gary Wang",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran",
        "Fadi Biadsy",
        "Yinghui Huang",
        "Jesse Emond",
        "Pedro Moreno Mengibar"
      ],
      "published": "2022-09-15T00:40:35Z",
      "updated": "2022-09-15T00:40:35Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.06987v1",
      "landing_url": "https://arxiv.org/abs/2209.06987v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.06987"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2209.07143",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.07143v1",
      "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator",
      "summary": "Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating high-fidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high-resolution video prediction on complex and large-scale datasets. Videos are available at https://sites.google.com/view/harp-videos/home.",
      "published": "2022-09-15T08:41:57Z"
    },
    "metadata": {
      "arxiv_id": "2209.07143",
      "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator",
      "summary": "Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating high-fidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high-resolution video prediction on complex and large-scale datasets. Videos are available at https://sites.google.com/view/harp-videos/home.",
      "authors": [
        "Younggyo Seo",
        "Kimin Lee",
        "Fangchen Liu",
        "Stephen James",
        "Pieter Abbeel"
      ],
      "published": "2022-09-15T08:41:57Z",
      "updated": "2022-09-15T08:41:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07143v1",
      "landing_url": "https://arxiv.org/abs/2209.07143v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07143"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2209.09735",
    "anchor": "acoustic tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.09735v1",
      "title": "Relaxed Attention for Transformer Models",
      "summary": "The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models. In this paper, we explore relaxed attention, a simple and easy-to-implement smoothing of the attention weights, yielding a two-fold improvement to the general transformer architecture: First, relaxed attention provides regularization when applied to the self-attention layers in the encoder. Second, we show that it naturally supports the integration of an external language model as it suppresses the implicitly learned internal language model by relaxing the cross attention in the decoder. We demonstrate the benefit of relaxed attention across several tasks with clear improvement in combination with recent benchmark approaches. Specifically, we exceed the former state-of-the-art performance of 26.90% word error rate on the largest public lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as we achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE$\\rightarrow$EN) machine translation task without external language models and virtually no additional model parameters. Code and models will be made publicly available.",
      "published": "2022-09-20T14:10:28Z"
    },
    "metadata": {
      "arxiv_id": "2209.09735",
      "title": "Relaxed Attention for Transformer Models",
      "summary": "The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models. In this paper, we explore relaxed attention, a simple and easy-to-implement smoothing of the attention weights, yielding a two-fold improvement to the general transformer architecture: First, relaxed attention provides regularization when applied to the self-attention layers in the encoder. Second, we show that it naturally supports the integration of an external language model as it suppresses the implicitly learned internal language model by relaxing the cross attention in the decoder. We demonstrate the benefit of relaxed attention across several tasks with clear improvement in combination with recent benchmark approaches. Specifically, we exceed the former state-of-the-art performance of 26.90% word error rate on the largest public lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as we achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE$\\rightarrow$EN) machine translation task without external language models and virtually no additional model parameters. Code and models will be made publicly available.",
      "authors": [
        "Timo Lohrenz",
        "Björn Möller",
        "Zhengyang Li",
        "Tim Fingscheidt"
      ],
      "published": "2022-09-20T14:10:28Z",
      "updated": "2022-09-20T14:10:28Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09735v1",
      "landing_url": "https://arxiv.org/abs/2209.09735v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.09735"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2209.10887",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.10887v1",
      "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
      "summary": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.",
      "published": "2022-09-22T09:43:17Z"
    },
    "metadata": {
      "arxiv_id": "2209.10887",
      "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
      "summary": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Frank K. Soong",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2022-09-22T09:43:17Z",
      "updated": "2022-09-22T09:43:17Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.10887v1",
      "landing_url": "https://arxiv.org/abs/2209.10887v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.10887"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2209.10890",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.10890v1",
      "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
      "summary": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
      "published": "2022-09-22T09:47:25Z"
    },
    "metadata": {
      "arxiv_id": "2209.10890",
      "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
      "summary": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
      "authors": [
        "Perry Lam",
        "Huayun Zhang",
        "Nancy F. Chen",
        "Berrak Sisman"
      ],
      "published": "2022-09-22T09:47:25Z",
      "updated": "2022-09-22T09:47:25Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.10890v1",
      "landing_url": "https://arxiv.org/abs/2209.10890v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10626"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2209.11750",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.11750v2",
      "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
      "summary": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
      "published": "2022-09-22T09:42:08Z"
    },
    "metadata": {
      "arxiv_id": "2209.11750",
      "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
      "summary": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
      "authors": [
        "Sannara EK",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-09-22T09:42:08Z",
      "updated": "2025-08-23T20:07:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11750v2",
      "landing_url": "https://arxiv.org/abs/2209.11750v2",
      "doi": "https://doi.org/10.1007/s00779-023-01776-3"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2209.12139",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.12139v1",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "published": "2022-09-25T04:14:26Z"
    },
    "metadata": {
      "arxiv_id": "2209.12139",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "authors": [
        "Yifan Wang",
        "Zhanxuan Mei",
        "Ioannis Katsavounidis",
        "C. -C. Jay Kuo"
      ],
      "published": "2022-09-25T04:14:26Z",
      "updated": "2022-09-25T04:14:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.12139v1",
      "landing_url": "https://arxiv.org/abs/2209.12139v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.12139"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2209.15001",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.15001v3",
      "title": "Dilated Neighborhood Attention Transformer",
      "summary": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
      "published": "2022-09-29T17:57:08Z"
    },
    "metadata": {
      "arxiv_id": "2209.15001",
      "title": "Dilated Neighborhood Attention Transformer",
      "summary": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
      "authors": [
        "Ali Hassani",
        "Humphrey Shi"
      ],
      "published": "2022-09-29T17:57:08Z",
      "updated": "2023-01-16T18:58:58Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15001v3",
      "landing_url": "https://arxiv.org/abs/2209.15001v3",
      "doi": "https://doi.org/10.48550/arXiv.2209.15001"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2209.15472",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.15472v1",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "published": "2022-09-30T13:56:25Z"
    },
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2210.01765",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.01765v4",
      "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
      "summary": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
      "published": "2022-10-04T17:30:31Z"
    },
    "metadata": {
      "arxiv_id": "2210.01765",
      "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
      "summary": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
      "authors": [
        "Shengjie Luo",
        "Tianlang Chen",
        "Yixian Xu",
        "Shuxin Zheng",
        "Tie-Yan Liu",
        "Liwei Wang",
        "Di He"
      ],
      "published": "2022-10-04T17:30:31Z",
      "updated": "2023-03-28T03:01:29Z",
      "categories": [
        "cs.LG",
        "q-bio.BM",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.01765v4",
      "landing_url": "https://arxiv.org/abs/2210.01765v4",
      "doi": "https://doi.org/10.48550/arXiv.2210.01765"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2210.04062",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.04062v3",
      "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
      "summary": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
      "published": "2022-10-08T17:15:46Z"
    },
    "metadata": {
      "arxiv_id": "2210.04062",
      "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
      "summary": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
      "authors": [
        "Chutong Meng",
        "Junyi Ao",
        "Tom Ko",
        "Mingxuan Wang",
        "Haizhou Li"
      ],
      "published": "2022-10-08T17:15:46Z",
      "updated": "2023-07-05T16:30:48Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.04062v3",
      "landing_url": "https://arxiv.org/abs/2210.04062v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.04062"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.05291",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.05291v1",
      "title": "On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding",
      "summary": "In this paper we examine the use of semantically-aligned speech representations for end-to-end spoken language understanding (SLU). We employ the recently-introduced SAMU-XLSR model, which is designed to generate a single embedding that captures the semantics at the utterance level, semantically aligned across different languages. This model combines the acoustic frame-level speech representation learning model (XLS-R) with the Language Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the SAMU-XLSR model instead of the initial XLS-R model improves significantly the performance in the framework of end-to-end SLU. Finally, we present the benefits of using this model towards language portability in SLU.",
      "published": "2022-10-11T09:40:34Z"
    },
    "metadata": {
      "arxiv_id": "2210.05291",
      "title": "On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding",
      "summary": "In this paper we examine the use of semantically-aligned speech representations for end-to-end spoken language understanding (SLU). We employ the recently-introduced SAMU-XLSR model, which is designed to generate a single embedding that captures the semantics at the utterance level, semantically aligned across different languages. This model combines the acoustic frame-level speech representation learning model (XLS-R) with the Language Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the SAMU-XLSR model instead of the initial XLS-R model improves significantly the performance in the framework of end-to-end SLU. Finally, we present the benefits of using this model towards language portability in SLU.",
      "authors": [
        "Gaëlle Laperrière",
        "Valentin Pelloin",
        "Mickaël Rouvier",
        "Themos Stafylakis",
        "Yannick Estève"
      ],
      "published": "2022-10-11T09:40:34Z",
      "updated": "2022-10-11T09:40:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.05291v1",
      "landing_url": "https://arxiv.org/abs/2210.05291v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.05291"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.06007",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.06007v2",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "published": "2022-10-12T08:23:20Z"
    },
    "metadata": {
      "arxiv_id": "2210.06007",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "authors": [
        "Yueh-Kao Wu",
        "Ching-Yu Chiu",
        "Yi-Hsuan Yang"
      ],
      "published": "2022-10-12T08:23:20Z",
      "updated": "2022-10-31T08:54:08Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06007v2",
      "landing_url": "https://arxiv.org/abs/2210.06007v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06007"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2210.07323",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.07323v3",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "published": "2022-10-13T19:46:39Z"
    },
    "metadata": {
      "arxiv_id": "2210.07323",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "authors": [
        "Ali Safaya",
        "Engin Erzin"
      ],
      "published": "2022-10-13T19:46:39Z",
      "updated": "2022-12-23T11:11:03Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.07323v3",
      "landing_url": "https://arxiv.org/abs/2210.07323v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.07323"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.08634",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.08634v2",
      "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning",
      "summary": "We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss the main findings from those submissions and the future directions of SSL research.",
      "published": "2022-10-16T20:50:04Z"
    },
    "metadata": {
      "arxiv_id": "2210.08634",
      "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning",
      "summary": "We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss the main findings from those submissions and the future directions of SSL research.",
      "authors": [
        "Tzu-hsun Feng",
        "Annie Dong",
        "Ching-Feng Yeh",
        "Shu-wen Yang",
        "Tzu-Quan Lin",
        "Jiatong Shi",
        "Kai-Wei Chang",
        "Zili Huang",
        "Haibin Wu",
        "Xuankai Chang",
        "Shinji Watanabe",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Hung-yi Lee"
      ],
      "published": "2022-10-16T20:50:04Z",
      "updated": "2022-10-29T14:00:14Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08634v2",
      "landing_url": "https://arxiv.org/abs/2210.08634v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.08634"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2210.12995",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.12995v1",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "published": "2022-10-24T07:30:42Z"
    },
    "metadata": {
      "arxiv_id": "2210.12995",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "authors": [
        "Dacheng Yin",
        "Zhiyuan Zhao",
        "Chuanxin Tang",
        "Zhiwei Xiong",
        "Chong Luo"
      ],
      "published": "2022-10-24T07:30:42Z",
      "updated": "2022-10-24T07:30:42Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12995v1",
      "landing_url": "https://arxiv.org/abs/2210.12995v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12995"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2210.13771",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.13771v1",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "published": "2022-10-25T05:12:47Z"
    },
    "metadata": {
      "arxiv_id": "2210.13771",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "authors": [
        "Hui Lu",
        "Disong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "published": "2022-10-25T05:12:47Z",
      "updated": "2022-10-25T05:12:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13771v1",
      "landing_url": "https://arxiv.org/abs/2210.13771v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13771"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2210.13805",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.13805v1",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "published": "2022-10-25T07:26:47Z"
    },
    "metadata": {
      "arxiv_id": "2210.13805",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Kexin Zhu",
        "Jing Xiao"
      ],
      "published": "2022-10-25T07:26:47Z",
      "updated": "2022-10-25T07:26:47Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13805v1",
      "landing_url": "https://arxiv.org/abs/2210.13805v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13805"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.15131",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.15131v1",
      "title": "Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations",
      "summary": "This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.",
      "published": "2022-10-27T02:32:00Z"
    },
    "metadata": {
      "arxiv_id": "2210.15131",
      "title": "Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations",
      "summary": "This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Xixin Wu",
        "Hui Lu",
        "Helen Meng"
      ],
      "published": "2022-10-27T02:32:00Z",
      "updated": "2022-10-27T02:32:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15131v1",
      "landing_url": "https://arxiv.org/abs/2210.15131v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2210.16611",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16611v2",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "published": "2022-10-29T14:22:43Z"
    },
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2210.16755",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16755v1",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "published": "2022-10-30T06:38:19Z"
    },
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2211.05239",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.05239v4",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "published": "2022-11-09T22:21:19Z"
    },
    "metadata": {
      "arxiv_id": "2211.05239",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "authors": [
        "Mark Zhao",
        "Dhruv Choudhary",
        "Devashish Tyagi",
        "Ajay Somani",
        "Max Kaplan",
        "Sung-Han Lin",
        "Sarunya Pumma",
        "Jongsoo Park",
        "Aarti Basant",
        "Niket Agarwal",
        "Carole-Jean Wu",
        "Christos Kozyrakis"
      ],
      "published": "2022-11-09T22:21:19Z",
      "updated": "2023-05-01T19:37:39Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.IR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05239v4",
      "landing_url": "https://arxiv.org/abs/2211.05239v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.05239"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2211.07321",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.07321v3",
      "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
      "summary": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
      "published": "2022-11-14T13:00:47Z"
    },
    "metadata": {
      "arxiv_id": "2211.07321",
      "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
      "summary": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Changli Tang",
        "Yujin Wang",
        "Xie Chen"
      ],
      "published": "2022-11-14T13:00:47Z",
      "updated": "2023-05-31T11:45:38Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07321v3",
      "landing_url": "https://arxiv.org/abs/2211.07321v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.07321"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2211.08849",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.08849v1",
      "title": "L2 proficiency assessment using self-supervised speech representations",
      "summary": "There has been a growing demand for automated spoken language assessment systems in recent years. A standard pipeline for this process is to start with a speech recognition system and derive features, either hand-crafted or based on deep-learning, that exploit the transcription and audio. Though these approaches can yield high performance systems, they require speech recognition systems that can be used for L2 speakers, and preferably tuned to the specific form of test being deployed. Recently a self-supervised speech representation based scheme, requiring no speech recognition, was proposed. This work extends the initial analysis conducted on this approach to a large scale proficiency test, Linguaskill, that comprises multiple parts, each designed to assess different attributes of a candidate's speaking proficiency. The performance of the self-supervised, wav2vec 2.0, system is compared to a high performance hand-crafted assessment system and a BERT-based text system both of which use speech transcriptions. Though the wav2vec 2.0 based system is found to be sensitive to the nature of the response, it can be configured to yield comparable performance to systems requiring a speech transcription, and yields gains when appropriately combined with standard approaches.",
      "published": "2022-11-16T11:47:20Z"
    },
    "metadata": {
      "arxiv_id": "2211.08849",
      "title": "L2 proficiency assessment using self-supervised speech representations",
      "summary": "There has been a growing demand for automated spoken language assessment systems in recent years. A standard pipeline for this process is to start with a speech recognition system and derive features, either hand-crafted or based on deep-learning, that exploit the transcription and audio. Though these approaches can yield high performance systems, they require speech recognition systems that can be used for L2 speakers, and preferably tuned to the specific form of test being deployed. Recently a self-supervised speech representation based scheme, requiring no speech recognition, was proposed. This work extends the initial analysis conducted on this approach to a large scale proficiency test, Linguaskill, that comprises multiple parts, each designed to assess different attributes of a candidate's speaking proficiency. The performance of the self-supervised, wav2vec 2.0, system is compared to a high performance hand-crafted assessment system and a BERT-based text system both of which use speech transcriptions. Though the wav2vec 2.0 based system is found to be sensitive to the nature of the response, it can be configured to yield comparable performance to systems requiring a speech transcription, and yields gains when appropriately combined with standard approaches.",
      "authors": [
        "Stefano Bannò",
        "Kate M. Knill",
        "Marco Matassoni",
        "Vyas Raina",
        "Mark J. F. Gales"
      ],
      "published": "2022-11-16T11:47:20Z",
      "updated": "2022-11-16T11:47:20Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08849v1",
      "landing_url": "https://arxiv.org/abs/2211.08849v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08849"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2211.09117",
    "anchor": "semantic tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09117v2",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "published": "2022-11-16T18:59:02Z"
    },
    "metadata": {
      "arxiv_id": "2211.09117",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "authors": [
        "Tianhong Li",
        "Huiwen Chang",
        "Shlok Kumar Mishra",
        "Han Zhang",
        "Dina Katabi",
        "Dilip Krishnan"
      ],
      "published": "2022-11-16T18:59:02Z",
      "updated": "2023-06-29T15:30:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09117v2",
      "landing_url": "https://arxiv.org/abs/2211.09117v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.09117"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2211.10015",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.10015v1",
      "title": "Asymptotics for The $k$-means",
      "summary": "The $k$-means is one of the most important unsupervised learning techniques in statistics and computer science. The goal is to partition a data set into many clusters, such that observations within clusters are the most homogeneous and observations between clusters are the most heterogeneous. Although it is well known, the investigation of the asymptotic properties is far behind, leading to difficulties in developing more precise $k$-means methods in practice. To address this issue, a new concept called clustering consistency is proposed. Fundamentally, the proposed clustering consistency is more appropriate than the previous criterion consistency for the clustering methods. Using this concept, a new $k$-means method is proposed. It is found that the proposed $k$-means method has lower clustering error rates and is more robust to small clusters and outliers than existing $k$-means methods. When $k$ is unknown, using the Gap statistics, the proposed method can also identify the number of clusters. This is rarely achieved by existing $k$-means methods adopted by many software packages.",
      "published": "2022-11-18T03:36:58Z"
    },
    "metadata": {
      "arxiv_id": "2211.10015",
      "title": "Asymptotics for The $k$-means",
      "summary": "The $k$-means is one of the most important unsupervised learning techniques in statistics and computer science. The goal is to partition a data set into many clusters, such that observations within clusters are the most homogeneous and observations between clusters are the most heterogeneous. Although it is well known, the investigation of the asymptotic properties is far behind, leading to difficulties in developing more precise $k$-means methods in practice. To address this issue, a new concept called clustering consistency is proposed. Fundamentally, the proposed clustering consistency is more appropriate than the previous criterion consistency for the clustering methods. Using this concept, a new $k$-means method is proposed. It is found that the proposed $k$-means method has lower clustering error rates and is more robust to small clusters and outliers than existing $k$-means methods. When $k$ is unknown, using the Gap statistics, the proposed method can also identify the number of clusters. This is rarely achieved by existing $k$-means methods adopted by many software packages.",
      "authors": [
        "Tonglin Zhang"
      ],
      "published": "2022-11-18T03:36:58Z",
      "updated": "2022-11-18T03:36:58Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.10015v1",
      "landing_url": "https://arxiv.org/abs/2211.10015v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.10015"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2211.11275",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.11275v2",
      "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
      "summary": "Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.",
      "published": "2022-11-21T09:10:10Z"
    },
    "metadata": {
      "arxiv_id": "2211.11275",
      "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
      "summary": "Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.",
      "authors": [
        "Qiushi Zhu",
        "Long Zhou",
        "Ziqiang Zhang",
        "Shujie Liu",
        "Binxing Jiao",
        "Jie Zhang",
        "Lirong Dai",
        "Daxin Jiang",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2022-11-21T09:10:10Z",
      "updated": "2023-05-19T10:03:56Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.11275v2",
      "landing_url": "https://arxiv.org/abs/2211.11275v2",
      "doi": "https://doi.org/10.1109/TMM.2023.3275873"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2211.11386",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.11386v1",
      "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
      "summary": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
      "published": "2022-11-21T11:58:25Z"
    },
    "metadata": {
      "arxiv_id": "2211.11386",
      "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
      "summary": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
      "authors": [
        "Satoshi Ikehata"
      ],
      "published": "2022-11-21T11:58:25Z",
      "updated": "2022-11-21T11:58:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.11386v1",
      "landing_url": "https://arxiv.org/abs/2211.11386v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.11386"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2211.12271",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.12271v3",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "published": "2022-11-22T13:42:53Z"
    },
    "metadata": {
      "arxiv_id": "2211.12271",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "authors": [
        "Georgios Vardakas",
        "Aristidis Likas"
      ],
      "published": "2022-11-22T13:42:53Z",
      "updated": "2023-07-14T11:39:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12271v3",
      "landing_url": "https://arxiv.org/abs/2211.12271v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.12271"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2211.14363",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.14363v1",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "published": "2022-11-25T20:09:22Z"
    },
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2211.14548",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.14548v1",
      "title": "Contextual Expressive Text-to-Speech",
      "summary": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
      "published": "2022-11-26T12:06:21Z"
    },
    "metadata": {
      "arxiv_id": "2211.14548",
      "title": "Contextual Expressive Text-to-Speech",
      "summary": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
      "authors": [
        "Jianhong Tu",
        "Zeyu Cui",
        "Xiaohuan Zhou",
        "Siqi Zheng",
        "Kai Hu",
        "Ju Fan",
        "Chang Zhou"
      ],
      "published": "2022-11-26T12:06:21Z",
      "updated": "2022-11-26T12:06:21Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14548v1",
      "landing_url": "https://arxiv.org/abs/2211.14548v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14548"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2211.15118",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.15118v2",
      "title": "A Faster $k$-means++ Algorithm",
      "summary": "$k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\\widetilde{O}(k )$ iterations, and each iteration takes $\\widetilde{O}(nd k)$ time. The overall running time is thus $\\widetilde{O}(n d k^2)$. We propose a new algorithm \\textsc{FastKmeans++} that only takes in $\\widetilde{O}(nd + nk^2)$ time, in total.",
      "published": "2022-11-28T08:17:12Z"
    },
    "metadata": {
      "arxiv_id": "2211.15118",
      "title": "A Faster $k$-means++ Algorithm",
      "summary": "$k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\\widetilde{O}(k )$ iterations, and each iteration takes $\\widetilde{O}(nd k)$ time. The overall running time is thus $\\widetilde{O}(n d k^2)$. We propose a new algorithm \\textsc{FastKmeans++} that only takes in $\\widetilde{O}(nd + nk^2)$ time, in total.",
      "authors": [
        "Jiehao Liang",
        "Somdeb Sarkhel",
        "Zhao Song",
        "Chenbo Yin",
        "Junze Yin",
        "Danyang Zhuo"
      ],
      "published": "2022-11-28T08:17:12Z",
      "updated": "2024-02-13T19:39:48Z",
      "categories": [
        "cs.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.15118v2",
      "landing_url": "https://arxiv.org/abs/2211.15118v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.15118"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2211.16112",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.16112v2",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "published": "2022-11-29T11:35:13Z"
    },
    "metadata": {
      "arxiv_id": "2211.16112",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Keisuke Kinoshita",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-11-29T11:35:13Z",
      "updated": "2023-07-21T07:28:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16112v2",
      "landing_url": "https://arxiv.org/abs/2211.16112v2",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10094784"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2212.01775",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.01775v2",
      "title": "Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech",
      "summary": "This work adapts two recent architectures of generative models and evaluates their effectiveness for the conversion of whispered speech to normal speech. We incorporate the normal target speech into the training criterion of vector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby conditioning the systems to recover voiced speech from whispered inputs. Objective and subjective quality measures indicate that both VQ-VAEs and MelGANs can be modified to perform the conversion task. We find that the proposed approaches significantly improve the Mel cepstral distortion (MCD) metric by at least 25% relative to a DiscoGAN baseline. Subjective listening tests suggest that the MelGAN-based system significantly improves naturalness, intelligibility, and voicing compared to the whispered input speech. A novel evaluation measure based on differences between latent speech representations also indicates that our MelGAN-based approach yields improvements relative to the baseline.",
      "published": "2022-12-04T09:06:18Z"
    },
    "metadata": {
      "arxiv_id": "2212.01775",
      "title": "Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech",
      "summary": "This work adapts two recent architectures of generative models and evaluates their effectiveness for the conversion of whispered speech to normal speech. We incorporate the normal target speech into the training criterion of vector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby conditioning the systems to recover voiced speech from whispered inputs. Objective and subjective quality measures indicate that both VQ-VAEs and MelGANs can be modified to perform the conversion task. We find that the proposed approaches significantly improve the Mel cepstral distortion (MCD) metric by at least 25% relative to a DiscoGAN baseline. Subjective listening tests suggest that the MelGAN-based system significantly improves naturalness, intelligibility, and voicing compared to the whispered input speech. A novel evaluation measure based on differences between latent speech representations also indicates that our MelGAN-based approach yields improvements relative to the baseline.",
      "authors": [
        "Dominik Wagner",
        "Sebastian P. Bayerl",
        "Hector A. Cordourier Maruri",
        "Tobias Bocklet"
      ],
      "published": "2022-12-04T09:06:18Z",
      "updated": "2023-01-30T10:03:00Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.01775v2",
      "landing_url": "https://arxiv.org/abs/2212.01775v2",
      "doi": "https://doi.org/10.1109/SLT54892.2023.10022796"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2212.03476",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.03476v1",
      "title": "Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information",
      "summary": "Multilingual end-to-end models have shown great improvement over monolingual systems. With the development of pre-training methods on speech, self-supervised multilingual speech representation learning like XLSR has shown success in improving the performance of multilingual automatic speech recognition (ASR). However, similar to the supervised learning, multilingual pre-training may also suffer from language interference and further affect the application of multilingual system. In this paper, we introduce several techniques for improving self-supervised multilingual pre-training by leveraging auxiliary language information, including the language adversarial training, language embedding and language adaptive training during the pre-training stage. We conduct experiments on a multilingual ASR task consisting of 16 languages. Our experimental results demonstrate 14.3% relative gain over the standard XLSR model, and 19.8% relative gain over the no pre-training multilingual model.",
      "published": "2022-12-07T06:18:59Z"
    },
    "metadata": {
      "arxiv_id": "2212.03476",
      "title": "Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information",
      "summary": "Multilingual end-to-end models have shown great improvement over monolingual systems. With the development of pre-training methods on speech, self-supervised multilingual speech representation learning like XLSR has shown success in improving the performance of multilingual automatic speech recognition (ASR). However, similar to the supervised learning, multilingual pre-training may also suffer from language interference and further affect the application of multilingual system. In this paper, we introduce several techniques for improving self-supervised multilingual pre-training by leveraging auxiliary language information, including the language adversarial training, language embedding and language adaptive training during the pre-training stage. We conduct experiments on a multilingual ASR task consisting of 16 languages. Our experimental results demonstrate 14.3% relative gain over the standard XLSR model, and 19.8% relative gain over the no pre-training multilingual model.",
      "authors": [
        "Fenglin Ding",
        "Genshun Wan",
        "Pengcheng Li",
        "Jia Pan",
        "Cong Liu"
      ],
      "published": "2022-12-07T06:18:59Z",
      "updated": "2022-12-07T06:18:59Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03476v1",
      "landing_url": "https://arxiv.org/abs/2212.03476v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03476"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2212.03482",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.03482v1",
      "title": "Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit",
      "summary": "Speech pre-training has shown great success in learning useful and general latent representations from large-scale unlabeled data. Based on a well-designed self-supervised learning pattern, pre-trained models can be used to serve lots of downstream speech tasks such as automatic speech recognition. In order to take full advantage of the labed data in low resource task, we present an improved pre-training method by introducing a supervision-enhanced acoustic unit (SEAU) pattern to intensify the expression of comtext information and ruduce the training cost. Encoder representations extracted from the SEAU pattern are used to generate more representative target units for HuBERT pre-training process. The proposed method, named SeHuBERT, achieves a relative word error rate reductions of 10.5% and 4.9% comared with the standard HuBERT on Turkmen speech recognition task with 500 hours and 100 hours fine-tuning data respectively. Extended to more languages and more data, SeHuBERT can aslo achieve a relative word error rate reductions of approximately 10% at half of the training cost compared with HuBERT.",
      "published": "2022-12-07T06:31:31Z"
    },
    "metadata": {
      "arxiv_id": "2212.03482",
      "title": "Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit",
      "summary": "Speech pre-training has shown great success in learning useful and general latent representations from large-scale unlabeled data. Based on a well-designed self-supervised learning pattern, pre-trained models can be used to serve lots of downstream speech tasks such as automatic speech recognition. In order to take full advantage of the labed data in low resource task, we present an improved pre-training method by introducing a supervision-enhanced acoustic unit (SEAU) pattern to intensify the expression of comtext information and ruduce the training cost. Encoder representations extracted from the SEAU pattern are used to generate more representative target units for HuBERT pre-training process. The proposed method, named SeHuBERT, achieves a relative word error rate reductions of 10.5% and 4.9% comared with the standard HuBERT on Turkmen speech recognition task with 500 hours and 100 hours fine-tuning data respectively. Extended to more languages and more data, SeHuBERT can aslo achieve a relative word error rate reductions of approximately 10% at half of the training cost compared with HuBERT.",
      "authors": [
        "Pengcheng Li",
        "Genshun Wan",
        "Fenglin Ding",
        "Hang Chen",
        "Jianqing Gao",
        "Jia Pan",
        "Cong Liu"
      ],
      "published": "2022-12-07T06:31:31Z",
      "updated": "2022-12-07T06:31:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03482v1",
      "landing_url": "https://arxiv.org/abs/2212.03482v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03482"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2212.04559",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.04559v1",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "published": "2022-12-08T21:00:15Z"
    },
    "metadata": {
      "arxiv_id": "2212.04559",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Takaaki Saeki",
        "Shinji Watanabe"
      ],
      "published": "2022-12-08T21:00:15Z",
      "updated": "2022-12-08T21:00:15Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04559v1",
      "landing_url": "https://arxiv.org/abs/2212.04559v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.04559"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2212.08329",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.08329v1",
      "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
      "summary": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
      "published": "2022-12-16T08:14:04Z"
    },
    "metadata": {
      "arxiv_id": "2212.08329",
      "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
      "summary": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
      "authors": [
        "Yusuke Yasuda",
        "Tomoki Toda"
      ],
      "published": "2022-12-16T08:14:04Z",
      "updated": "2022-12-16T08:14:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08329v1",
      "landing_url": "https://arxiv.org/abs/2212.08329v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08329"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2212.09058",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.09058v1",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "published": "2022-12-18T10:41:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2212.09096",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.09096v1",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "published": "2022-12-18T14:40:52Z"
    },
    "metadata": {
      "arxiv_id": "2212.09096",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "authors": [
        "Hechuan Guo",
        "Minghui Xu",
        "Jiahao Zhang",
        "Chunchi Liu",
        "Dongxiao Yu",
        "Schahram Dustdar",
        "Xiuzhen Cheng"
      ],
      "published": "2022-12-18T14:40:52Z",
      "updated": "2022-12-18T14:40:52Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09096v1",
      "landing_url": "https://arxiv.org/abs/2212.09096v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09096"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2212.10191",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.10191v1",
      "title": "Emotion Selectable End-to-End Text-based Speech Editing",
      "summary": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
      "published": "2022-12-20T12:02:40Z"
    },
    "metadata": {
      "arxiv_id": "2212.10191",
      "title": "Emotion Selectable End-to-End Text-based Speech Editing",
      "summary": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
      "authors": [
        "Tao Wang",
        "Jiangyan Yi",
        "Ruibo Fu",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Chu Yuan Zhang"
      ],
      "published": "2022-12-20T12:02:40Z",
      "updated": "2022-12-20T12:02:40Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.10191v1",
      "landing_url": "https://arxiv.org/abs/2212.10191v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.10191"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2212.11444",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.11444v1",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "published": "2022-12-22T01:26:38Z"
    },
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2212.14227",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.14227v1",
      "title": "StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models",
      "summary": "One-shot voice conversion (VC) aims to convert speech from any source speaker to an arbitrary target speaker with only a few seconds of reference speech from the target speaker. This relies heavily on disentangling the speaker's identity and speech content, a task that still remains challenging. Here, we propose a novel approach to learning disentangled speech representation by transfer learning from style-based text-to-speech (TTS) models. With cycle consistent and adversarial training, the style-based TTS models can perform transcription-guided one-shot VC with high fidelity and similarity. By learning an additional mel-spectrogram encoder through a teacher-student knowledge transfer and novel data augmentation scheme, our approach results in disentangled speech representation without needing the input text. The subjective evaluation shows that our approach can significantly outperform the previous state-of-the-art one-shot voice conversion models in both naturalness and similarity.",
      "published": "2022-12-29T08:56:20Z"
    },
    "metadata": {
      "arxiv_id": "2212.14227",
      "title": "StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models",
      "summary": "One-shot voice conversion (VC) aims to convert speech from any source speaker to an arbitrary target speaker with only a few seconds of reference speech from the target speaker. This relies heavily on disentangling the speaker's identity and speech content, a task that still remains challenging. Here, we propose a novel approach to learning disentangled speech representation by transfer learning from style-based text-to-speech (TTS) models. With cycle consistent and adversarial training, the style-based TTS models can perform transcription-guided one-shot VC with high fidelity and similarity. By learning an additional mel-spectrogram encoder through a teacher-student knowledge transfer and novel data augmentation scheme, our approach results in disentangled speech representation without needing the input text. The subjective evaluation shows that our approach can significantly outperform the previous state-of-the-art one-shot voice conversion models in both naturalness and similarity.",
      "authors": [
        "Yinghao Aaron Li",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2022-12-29T08:56:20Z",
      "updated": "2022-12-29T08:56:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.14227v1",
      "landing_url": "https://arxiv.org/abs/2212.14227v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.14227"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2212.14538",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.14538v2",
      "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
      "summary": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
      "published": "2022-12-30T03:50:38Z"
    },
    "metadata": {
      "arxiv_id": "2212.14538",
      "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
      "summary": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
      "authors": [
        "Hangyu Mao",
        "Rui Zhao",
        "Hao Chen",
        "Jianye Hao",
        "Yiqun Chen",
        "Dong Li",
        "Junge Zhang",
        "Zhen Xiao"
      ],
      "published": "2022-12-30T03:50:38Z",
      "updated": "2023-01-03T06:51:22Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.14538v2",
      "landing_url": "https://arxiv.org/abs/2212.14538v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.14538"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2301.00591",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.00591v3",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "published": "2023-01-02T10:36:40Z"
    },
    "metadata": {
      "arxiv_id": "2301.00591",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "authors": [
        "Amitay Sicherman",
        "Yossi Adi"
      ],
      "published": "2023-01-02T10:36:40Z",
      "updated": "2023-03-01T09:59:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00591v3",
      "landing_url": "https://arxiv.org/abs/2301.00591v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2301.00652",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.00652v1",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "published": "2022-12-14T06:09:08Z"
    },
    "metadata": {
      "arxiv_id": "2301.00652",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "authors": [
        "Ching-Feng Yeh",
        "Wei-Ning Hsu",
        "Paden Tomasello",
        "Abdelrahman Mohamed"
      ],
      "published": "2022-12-14T06:09:08Z",
      "updated": "2022-12-14T06:09:08Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00652v1",
      "landing_url": "https://arxiv.org/abs/2301.00652v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00652"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2301.02111",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.02111v1",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "published": "2023-01-05T15:37:15Z"
    },
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2301.04388",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.04388v3",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "published": "2023-01-11T10:20:56Z"
    },
    "metadata": {
      "arxiv_id": "2301.04388",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "authors": [
        "George Close",
        "William Ravenscroft",
        "Thomas Hain",
        "Stefan Goetze"
      ],
      "published": "2023-01-11T10:20:56Z",
      "updated": "2023-06-26T09:31:53Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04388v3",
      "landing_url": "https://arxiv.org/abs/2301.04388v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095666"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2301.06052",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.06052v4",
      "title": "T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations",
      "summary": "In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.",
      "published": "2023-01-15T09:34:42Z"
    },
    "metadata": {
      "arxiv_id": "2301.06052",
      "title": "T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations",
      "summary": "In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.",
      "authors": [
        "Jianrong Zhang",
        "Yangsong Zhang",
        "Xiaodong Cun",
        "Shaoli Huang",
        "Yong Zhang",
        "Hongwei Zhao",
        "Hongtao Lu",
        "Xi Shen"
      ],
      "published": "2023-01-15T09:34:42Z",
      "updated": "2023-09-24T17:00:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06052v4",
      "landing_url": "https://arxiv.org/abs/2301.06052v4",
      "doi": "https://doi.org/10.48550/arXiv.2301.06052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2301.09027",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.09027v1",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "published": "2023-01-22T00:18:10Z"
    },
    "metadata": {
      "arxiv_id": "2301.09027",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "authors": [
        "Amanda Shu",
        "Hamza Khalid",
        "Haohui Liu",
        "Shikhar Agnihotri",
        "Joseph Konan",
        "Ojas Bhargave"
      ],
      "published": "2023-01-22T00:18:10Z",
      "updated": "2023-01-22T00:18:10Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09027v1",
      "landing_url": "https://arxiv.org/abs/2301.09027v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.09027"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2301.09869",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.09869v2",
      "title": "Image Super-Resolution using Efficient Striped Window Transformer",
      "summary": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
      "published": "2023-01-24T09:09:35Z"
    },
    "metadata": {
      "arxiv_id": "2301.09869",
      "title": "Image Super-Resolution using Efficient Striped Window Transformer",
      "summary": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
      "authors": [
        "Jinpeng Shi",
        "Hui Li",
        "Tianle Liu",
        "Yulong Liu",
        "Mingjian Zhang",
        "Jinchen Zhu",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2023-01-24T09:09:35Z",
      "updated": "2023-03-14T07:03:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09869v2",
      "landing_url": "https://arxiv.org/abs/2301.09869v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.09869"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2301.10048",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.10048v2",
      "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
      "summary": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
      "published": "2023-01-24T14:44:44Z"
    },
    "metadata": {
      "arxiv_id": "2301.10048",
      "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
      "summary": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
      "authors": [
        "Kaidong Zhang",
        "Jialun Peng",
        "Jingjing Fu",
        "Dong Liu"
      ],
      "published": "2023-01-24T14:44:44Z",
      "updated": "2024-03-19T04:02:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.10048v2",
      "landing_url": "https://arxiv.org/abs/2301.10048v2",
      "doi": "https://doi.org/10.1109/TPAMI.2024.3361010"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2301.13662",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.13662v2",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "published": "2023-01-31T14:26:52Z"
    },
    "metadata": {
      "arxiv_id": "2301.13662",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Chao Weng",
        "Helen Meng"
      ],
      "published": "2023-01-31T14:26:52Z",
      "updated": "2023-06-25T11:42:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13662v2",
      "landing_url": "https://arxiv.org/abs/2301.13662v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.13662"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2302.02594",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.02594v1",
      "title": "Generating Subsurface Earth Models using Discrete Representation Learning and Deep Autoregressive Network",
      "summary": "Subsurface earth models (referred to as geo-models) are crucial for characterizing complex subsurface systems. Multiple-point statistics are commonly used to generate geo-models. In this paper, a deep-learning-based generative method is developed as an alternative to the traditional Geomodel generation procedure. The generative method comprises two deep-learning models, namely the hierarchical vector-quantized variational autoencoder (VQ-VAE-2) and PixelSNAIL autoregressive model. Based on the principle of neural discrete representation learning, the VQ-VAE-2 learns to massively compress the Geomodels to extract the low-dimensional, discrete latent representation corresponding to each Geomodel. Following that, PixelSNAIL uses the deep autoregressive network to learn the prior distribution of the latent codes. For the purpose of Geomodel generation, PixelSNAIL samples from the newly learned prior distribution of latent codes, and then the decoder of the VQ-VAE-2 converts the newly sampled latent code to a newly constructed geo-model. PixelSNAIL can be used for unconditional or conditional geo-model generation. In an unconditional generation, the generative workflow generates an ensemble of geo-models without any constraint. On the other hand, in the conditional geo-model generation, the generative workflow generates an ensemble of geo-models similar to a user-defined source image, which ultimately facilitates the control and manipulation of the generated geo-models. To better construct the fluvial channels in the geo-models, the perceptual loss is implemented in the VQ-VAE-2 model instead of the traditional mean squared error loss. At a specific compression ratio, the quality of multi-attribute geo-model generation is better than that of single-attribute geo-model generation.",
      "published": "2023-02-06T07:01:21Z"
    },
    "metadata": {
      "arxiv_id": "2302.02594",
      "title": "Generating Subsurface Earth Models using Discrete Representation Learning and Deep Autoregressive Network",
      "summary": "Subsurface earth models (referred to as geo-models) are crucial for characterizing complex subsurface systems. Multiple-point statistics are commonly used to generate geo-models. In this paper, a deep-learning-based generative method is developed as an alternative to the traditional Geomodel generation procedure. The generative method comprises two deep-learning models, namely the hierarchical vector-quantized variational autoencoder (VQ-VAE-2) and PixelSNAIL autoregressive model. Based on the principle of neural discrete representation learning, the VQ-VAE-2 learns to massively compress the Geomodels to extract the low-dimensional, discrete latent representation corresponding to each Geomodel. Following that, PixelSNAIL uses the deep autoregressive network to learn the prior distribution of the latent codes. For the purpose of Geomodel generation, PixelSNAIL samples from the newly learned prior distribution of latent codes, and then the decoder of the VQ-VAE-2 converts the newly sampled latent code to a newly constructed geo-model. PixelSNAIL can be used for unconditional or conditional geo-model generation. In an unconditional generation, the generative workflow generates an ensemble of geo-models without any constraint. On the other hand, in the conditional geo-model generation, the generative workflow generates an ensemble of geo-models similar to a user-defined source image, which ultimately facilitates the control and manipulation of the generated geo-models. To better construct the fluvial channels in the geo-models, the perceptual loss is implemented in the VQ-VAE-2 model instead of the traditional mean squared error loss. At a specific compression ratio, the quality of multi-attribute geo-model generation is better than that of single-attribute geo-model generation.",
      "authors": [
        "Jungang Chen",
        "Chung-Kan Huang",
        "Jose F. Delgado",
        "Siddharth Misra"
      ],
      "published": "2023-02-06T07:01:21Z",
      "updated": "2023-02-06T07:01:21Z",
      "categories": [
        "physics.geo-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.02594v1",
      "landing_url": "https://arxiv.org/abs/2302.02594v1",
      "doi": "https://doi.org/10.1007/s10596-023-10243-0"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2302.03540",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.03540v1",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "published": "2023-02-07T15:48:31Z"
    },
    "metadata": {
      "arxiv_id": "2302.03540",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2023-02-07T15:48:31Z",
      "updated": "2023-02-07T15:48:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.03540v1",
      "landing_url": "https://arxiv.org/abs/2302.03540v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.03540"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2302.04215",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.04215v1",
      "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
      "summary": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
      "published": "2023-02-08T17:34:32Z"
    },
    "metadata": {
      "arxiv_id": "2302.04215",
      "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
      "summary": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
      "authors": [
        "Li-Wei Chen",
        "Shinji Watanabe",
        "Alexander Rudnicky"
      ],
      "published": "2023-02-08T17:34:32Z",
      "updated": "2023-02-08T17:34:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04215v1",
      "landing_url": "https://arxiv.org/abs/2302.04215v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.04215"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2302.05406",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.05406v1",
      "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
      "summary": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
      "published": "2023-02-10T18:21:13Z"
    },
    "metadata": {
      "arxiv_id": "2302.05406",
      "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
      "summary": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
      "authors": [
        "Pedro Colon-Hernandez",
        "Henry Lieberman",
        "Yida Xin",
        "Claire Yin",
        "Cynthia Breazeal",
        "Peter Chin"
      ],
      "published": "2023-02-10T18:21:13Z",
      "updated": "2023-02-10T18:21:13Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05406v1",
      "landing_url": "https://arxiv.org/abs/2302.05406v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05406"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2302.05756",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.05756v1",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "published": "2023-02-11T18:33:42Z"
    },
    "metadata": {
      "arxiv_id": "2302.05756",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "authors": [
        "Cong Han",
        "Vishal Choudhari",
        "Yinghao Aaron Li",
        "Nima Mesgarani"
      ],
      "published": "2023-02-11T18:33:42Z",
      "updated": "2023-02-11T18:33:42Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05756v1",
      "landing_url": "https://arxiv.org/abs/2302.05756v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05756"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2302.05917",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.05917v2",
      "title": "Vector Quantized Wasserstein Auto-Encoder",
      "summary": "Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.",
      "published": "2023-02-12T13:51:36Z"
    },
    "metadata": {
      "arxiv_id": "2302.05917",
      "title": "Vector Quantized Wasserstein Auto-Encoder",
      "summary": "Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.",
      "authors": [
        "Tung-Long Vuong",
        "Trung Le",
        "He Zhao",
        "Chuanxia Zheng",
        "Mehrtash Harandi",
        "Jianfei Cai",
        "Dinh Phung"
      ],
      "published": "2023-02-12T13:51:36Z",
      "updated": "2023-06-17T06:52:21Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05917v2",
      "landing_url": "https://arxiv.org/abs/2302.05917v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.05917"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2302.08137",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.08137v1",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "published": "2023-02-16T08:10:41Z"
    },
    "metadata": {
      "arxiv_id": "2302.08137",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "published": "2023-02-16T08:10:41Z",
      "updated": "2023-02-16T08:10:41Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08137v1",
      "landing_url": "https://arxiv.org/abs/2302.08137v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08137"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2302.08342",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.08342v1",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "published": "2023-02-16T14:53:41Z"
    },
    "metadata": {
      "arxiv_id": "2302.08342",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "authors": [
        "Xiao-Ying Zhao",
        "Qiu-Shi Zhu",
        "Jie Zhang"
      ],
      "published": "2023-02-16T14:53:41Z",
      "updated": "2023-02-16T14:53:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08342v1",
      "landing_url": "https://arxiv.org/abs/2302.08342v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08342"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2302.10287",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.10287v1",
      "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
      "summary": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
      "published": "2023-02-01T06:09:19Z"
    },
    "metadata": {
      "arxiv_id": "2302.10287",
      "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
      "summary": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
      "authors": [
        "Kavya Gupta",
        "Sagar Verma"
      ],
      "published": "2023-02-01T06:09:19Z",
      "updated": "2023-02-01T06:09:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10287v1",
      "landing_url": "https://arxiv.org/abs/2302.10287v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.10287"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2302.12434",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.12434v1",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "published": "2023-02-24T03:33:13Z"
    },
    "metadata": {
      "arxiv_id": "2302.12434",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "authors": [
        "Jiangyi Deng",
        "Yanjiao Chen",
        "Yinan Zhong",
        "Qianhao Miao",
        "Xueluan Gong",
        "Wenyuan Xu"
      ],
      "published": "2023-02-24T03:33:13Z",
      "updated": "2023-02-24T03:33:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.12434v1",
      "landing_url": "https://arxiv.org/abs/2302.12434v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.12434"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2302.13451",
    "anchor": "speech representation",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.13451v2",
      "title": "A low latency attention module for streaming self-supervised speech representation learning",
      "summary": "The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the latency build-up problem of other streaming attention architectures, such as the masked acausal attention (MAA), guaranteeing a latency equal to one layer even when multiple layers are stacked. We present a comparative analysis between the vanilla attention, which we will refer here as acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with automatic speech recognition as downstream task. When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%). Our implementation also reduces the inference latency from 1.92 to 0.16 seconds. The proposed low-latency module preserves many of the benefits of conventional acausal transformers, but also enables latency characteristics that make it applicable to real-time streaming applications.",
      "published": "2023-02-27T00:44:22Z"
    },
    "metadata": {
      "arxiv_id": "2302.13451",
      "title": "A low latency attention module for streaming self-supervised speech representation learning",
      "summary": "The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the latency build-up problem of other streaming attention architectures, such as the masked acausal attention (MAA), guaranteeing a latency equal to one layer even when multiple layers are stacked. We present a comparative analysis between the vanilla attention, which we will refer here as acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with automatic speech recognition as downstream task. When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%). Our implementation also reduces the inference latency from 1.92 to 0.16 seconds. The proposed low-latency module preserves many of the benefits of conventional acausal transformers, but also enables latency characteristics that make it applicable to real-time streaming applications.",
      "authors": [
        "Jianbo Ma",
        "Siqi Pan",
        "Deepak Chandran",
        "Andrea Fanelli",
        "Richard Cartwright"
      ],
      "published": "2023-02-27T00:44:22Z",
      "updated": "2024-03-18T01:09:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.13451v2",
      "landing_url": "https://arxiv.org/abs/2302.13451v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.13451"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2302.13458",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.13458v1",
      "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
      "summary": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
      "published": "2023-02-27T01:12:19Z"
    },
    "metadata": {
      "arxiv_id": "2302.13458",
      "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
      "summary": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
      "authors": [
        "Yoonhyung Lee",
        "Jinhyeok Yang",
        "Kyomin Jung"
      ],
      "published": "2023-02-27T01:12:19Z",
      "updated": "2023-02-27T01:12:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.13458v1",
      "landing_url": "https://arxiv.org/abs/2302.13458v1",
      "doi": "https://doi.org/10.1109/ICASSP43922.2022.9747050"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2302.14017",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.14017v1",
      "title": "Full Stack Optimization of Transformer Inference: a Survey",
      "summary": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
      "published": "2023-02-27T18:18:13Z"
    },
    "metadata": {
      "arxiv_id": "2302.14017",
      "title": "Full Stack Optimization of Transformer Inference: a Survey",
      "summary": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
      "authors": [
        "Sehoon Kim",
        "Coleman Hooper",
        "Thanakul Wattanawong",
        "Minwoo Kang",
        "Ruohan Yan",
        "Hasan Genc",
        "Grace Dinh",
        "Qijing Huang",
        "Kurt Keutzer",
        "Michael W. Mahoney",
        "Yakun Sophia Shao",
        "Amir Gholami"
      ],
      "published": "2023-02-27T18:18:13Z",
      "updated": "2023-02-27T18:18:13Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14017v1",
      "landing_url": "https://arxiv.org/abs/2302.14017v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.14017"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2302.14337",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.14337v2",
      "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
      "summary": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
      "published": "2023-02-28T06:05:43Z"
    },
    "metadata": {
      "arxiv_id": "2302.14337",
      "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
      "summary": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2023-02-28T06:05:43Z",
      "updated": "2023-05-19T02:43:32Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14337v2",
      "landing_url": "https://arxiv.org/abs/2302.14337v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14337"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2302.14357",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.14357v2",
      "title": "A Token-Wise Beam Search Algorithm for RNN-T",
      "summary": "Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.",
      "published": "2023-02-28T07:20:49Z"
    },
    "metadata": {
      "arxiv_id": "2302.14357",
      "title": "A Token-Wise Beam Search Algorithm for RNN-T",
      "summary": "Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.",
      "authors": [
        "Gil Keren"
      ],
      "published": "2023-02-28T07:20:49Z",
      "updated": "2023-10-05T22:34:13Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14357v2",
      "landing_url": "https://arxiv.org/abs/2302.14357v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14357"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2303.00957",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.00957v1",
      "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "summary": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
      "published": "2023-03-02T04:24:29Z"
    },
    "metadata": {
      "arxiv_id": "2303.00957",
      "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "summary": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
      "authors": [
        "Changyeon Kim",
        "Jongjin Park",
        "Jinwoo Shin",
        "Honglak Lee",
        "Pieter Abbeel",
        "Kimin Lee"
      ],
      "published": "2023-03-02T04:24:29Z",
      "updated": "2023-03-02T04:24:29Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.00957v1",
      "landing_url": "https://arxiv.org/abs/2303.00957v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.00957"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2303.01261",
    "anchor": "speech representation",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.01261v3",
      "title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations",
      "summary": "We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models using only a fraction of paired data as latter.",
      "published": "2023-03-01T17:23:12Z"
    },
    "metadata": {
      "arxiv_id": "2303.01261",
      "title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations",
      "summary": "We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models using only a fraction of paired data as latter.",
      "authors": [
        "Neil Shah",
        "Saiteja Kosgi",
        "Vishal Tambrahalli",
        "Neha Sahipjohn",
        "Niranjan Pedanekar",
        "Vineet Gandhi"
      ],
      "published": "2023-03-01T17:23:12Z",
      "updated": "2023-12-17T00:06:16Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01261v3",
      "landing_url": "https://arxiv.org/abs/2303.01261v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.01261"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2303.02939",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.02939v3",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "published": "2023-03-06T07:17:15Z"
    },
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2303.03600",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.03600v1",
      "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
      "summary": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
      "published": "2023-03-07T02:31:57Z"
    },
    "metadata": {
      "arxiv_id": "2303.03600",
      "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
      "summary": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
      "authors": [
        "Jinjie Ni",
        "Yukun Ma",
        "Wen Wang",
        "Qian Chen",
        "Dianwen Ng",
        "Han Lei",
        "Trung Hieu Nguyen",
        "Chong Zhang",
        "Bin Ma",
        "Erik Cambria"
      ],
      "published": "2023-03-07T02:31:57Z",
      "updated": "2023-03-07T02:31:57Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03600v1",
      "landing_url": "https://arxiv.org/abs/2303.03600v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03600"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2303.03926",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.03926v1",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "published": "2023-03-07T14:31:55Z"
    },
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2303.04255",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.04255v1",
      "title": "Self-supervised speech representation learning for keyword-spotting with light-weight transformers",
      "summary": "Self-supervised speech representation learning (S3RL) is revolutionizing the way we leverage the ever-growing availability of data. While S3RL related studies typically use large models, we employ light-weight networks to comply with tight memory of compute-constrained devices. We demonstrate the effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers with 330k parameters and propose a mechanism to enhance utterance-wise distinction, which proves crucial for improving performance on classification tasks. On the Google speech commands v2 dataset, the proposed method applied to the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement compared to training from scratch. On an in-house KS dataset with four different keywords, it provided 6% to 23.7% relative false accept improvement at fixed false reject rate. We argue this demonstrates the applicability of S3RL approaches to light-weight models for KS and confirms S3RL is a powerful alternative to traditional supervised learning for resource-constrained applications.",
      "published": "2023-03-07T21:54:35Z"
    },
    "metadata": {
      "arxiv_id": "2303.04255",
      "title": "Self-supervised speech representation learning for keyword-spotting with light-weight transformers",
      "summary": "Self-supervised speech representation learning (S3RL) is revolutionizing the way we leverage the ever-growing availability of data. While S3RL related studies typically use large models, we employ light-weight networks to comply with tight memory of compute-constrained devices. We demonstrate the effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers with 330k parameters and propose a mechanism to enhance utterance-wise distinction, which proves crucial for improving performance on classification tasks. On the Google speech commands v2 dataset, the proposed method applied to the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement compared to training from scratch. On an in-house KS dataset with four different keywords, it provided 6% to 23.7% relative false accept improvement at fixed false reject rate. We argue this demonstrates the applicability of S3RL approaches to light-weight models for KS and confirms S3RL is a powerful alternative to traditional supervised learning for resource-constrained applications.",
      "authors": [
        "Chenyang Gao",
        "Yue Gu",
        "Francesco Caliva",
        "Yuzong Liu"
      ],
      "published": "2023-03-07T21:54:35Z",
      "updated": "2023-03-07T21:54:35Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.04255v1",
      "landing_url": "https://arxiv.org/abs/2303.04255v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.04255"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2303.06424",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.06424v2",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "published": "2023-03-11T15:20:54Z"
    },
    "metadata": {
      "arxiv_id": "2303.06424",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "authors": [
        "Jiahui Zhang",
        "Fangneng Zhan",
        "Christian Theobalt",
        "Shijian Lu"
      ],
      "published": "2023-03-11T15:20:54Z",
      "updated": "2023-10-14T06:17:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06424v2",
      "landing_url": "https://arxiv.org/abs/2303.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.06424"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2303.06705",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.06705v3",
      "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
      "summary": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
      "published": "2023-03-12T16:54:08Z"
    },
    "metadata": {
      "arxiv_id": "2303.06705",
      "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
      "summary": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
      "authors": [
        "Yuanhao Cai",
        "Hao Bian",
        "Jing Lin",
        "Haoqian Wang",
        "Radu Timofte",
        "Yulun Zhang"
      ],
      "published": "2023-03-12T16:54:08Z",
      "updated": "2023-10-26T22:19:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06705v3",
      "landing_url": "https://arxiv.org/abs/2303.06705v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06705"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2303.06982",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.06982v3",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "published": "2023-03-13T10:32:44Z"
    },
    "metadata": {
      "arxiv_id": "2303.06982",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2023-03-13T10:32:44Z",
      "updated": "2024-01-11T11:15:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06982v3",
      "landing_url": "https://arxiv.org/abs/2303.06982v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06982"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2303.07592",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.07592v1",
      "title": "Lightweight feature encoder for wake-up word detection based on self-supervised speech representation",
      "summary": "Self-supervised learning method that provides generalized speech representations has recently received increasing attention. Wav2vec 2.0 is the most famous example, showing remarkable performance in numerous downstream speech processing tasks. Despite its success, it is challenging to use it directly for wake-up word detection on mobile devices due to its expensive computational cost. In this work, we propose LiteFEW, a lightweight feature encoder for wake-up word detection that preserves the inherent ability of wav2vec 2.0 with a minimum scale. In the method, the knowledge of the pre-trained wav2vec 2.0 is compressed by introducing an auto-encoder-based dimensionality reduction technique and distilled to LiteFEW. Experimental results on the open-source \"Hey Snips\" dataset show that the proposed method applied to various model structures significantly improves the performance, achieving over 20% of relative improvements with only 64k parameters.",
      "published": "2023-03-14T02:31:44Z"
    },
    "metadata": {
      "arxiv_id": "2303.07592",
      "title": "Lightweight feature encoder for wake-up word detection based on self-supervised speech representation",
      "summary": "Self-supervised learning method that provides generalized speech representations has recently received increasing attention. Wav2vec 2.0 is the most famous example, showing remarkable performance in numerous downstream speech processing tasks. Despite its success, it is challenging to use it directly for wake-up word detection on mobile devices due to its expensive computational cost. In this work, we propose LiteFEW, a lightweight feature encoder for wake-up word detection that preserves the inherent ability of wav2vec 2.0 with a minimum scale. In the method, the knowledge of the pre-trained wav2vec 2.0 is compressed by introducing an auto-encoder-based dimensionality reduction technique and distilled to LiteFEW. Experimental results on the open-source \"Hey Snips\" dataset show that the proposed method applied to various model structures significantly improves the performance, achieving over 20% of relative improvements with only 64k parameters.",
      "authors": [
        "Hyungjun Lim",
        "Younggwan Kim",
        "Kiho Yeom",
        "Eunjoo Seo",
        "Hoodong Lee",
        "Stanley Jungkyu Choi",
        "Honglak Lee"
      ],
      "published": "2023-03-14T02:31:44Z",
      "updated": "2023-03-14T02:31:44Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.07592v1",
      "landing_url": "https://arxiv.org/abs/2303.07592v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.07592"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2303.08685",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.08685v2",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "published": "2023-03-15T15:12:36Z"
    },
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2303.09455",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.09455v1",
      "title": "Learning Cross-lingual Visual Speech Representations",
      "summary": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning.",
      "published": "2023-03-14T17:05:08Z"
    },
    "metadata": {
      "arxiv_id": "2303.09455",
      "title": "Learning Cross-lingual Visual Speech Representations",
      "summary": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning.",
      "authors": [
        "Andreas Zinonos",
        "Alexandros Haliassos",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "published": "2023-03-14T17:05:08Z",
      "updated": "2023-03-14T17:05:08Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09455v1",
      "landing_url": "https://arxiv.org/abs/2303.09455v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09455"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2303.12197",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.12197v1",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "published": "2023-03-21T21:04:03Z"
    },
    "metadata": {
      "arxiv_id": "2303.12197",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "authors": [
        "Tejas Jayashankar",
        "Jilong Wu",
        "Leda Sari",
        "David Kant",
        "Vimal Manohar",
        "Qing He"
      ],
      "published": "2023-03-21T21:04:03Z",
      "updated": "2023-03-21T21:04:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12197v1",
      "landing_url": "https://arxiv.org/abs/2303.12197v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12197"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2303.13336",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.13336v2",
      "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
      "summary": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
      "published": "2023-03-23T15:17:15Z"
    },
    "metadata": {
      "arxiv_id": "2303.13336",
      "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
      "summary": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
      "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Sheng Zheng",
        "Mengchun Zhang",
        "Maryam Qamar",
        "Sung-Ho Bae",
        "In So Kweon"
      ],
      "published": "2023-03-23T15:17:15Z",
      "updated": "2023-04-02T09:27:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.13336v2",
      "landing_url": "https://arxiv.org/abs/2303.13336v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.13336"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2304.00649",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.00649v1",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "published": "2023-04-02T23:08:11Z"
    },
    "metadata": {
      "arxiv_id": "2304.00649",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "authors": [
        "Shammur Absar Chowdhury",
        "Ahmed Ali"
      ],
      "published": "2023-04-02T23:08:11Z",
      "updated": "2023-04-02T23:08:11Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00649v1",
      "landing_url": "https://arxiv.org/abs/2304.00649v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.00649"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2304.01448",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.01448v1",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "published": "2023-04-04T01:44:24Z"
    },
    "metadata": {
      "arxiv_id": "2304.01448",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "authors": [
        "Anurag Kumar",
        "Ke Tan",
        "Zhaoheng Ni",
        "Pranay Manocha",
        "Xiaohui Zhang",
        "Ethan Henderson",
        "Buye Xu"
      ],
      "published": "2023-04-04T01:44:24Z",
      "updated": "2023-04-04T01:44:24Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01448v1",
      "landing_url": "https://arxiv.org/abs/2304.01448v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.01448"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2304.03635",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.03635v1",
      "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
      "summary": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
      "published": "2023-04-07T13:30:36Z"
    },
    "metadata": {
      "arxiv_id": "2304.03635",
      "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
      "summary": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
      "authors": [
        "Changlong Jiang",
        "Yang Xiao",
        "Cunlin Wu",
        "Mingyang Zhang",
        "Jinghong Zheng",
        "Zhiguo Cao",
        "Joey Tianyi Zhou"
      ],
      "published": "2023-04-07T13:30:36Z",
      "updated": "2023-04-07T13:30:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03635v1",
      "landing_url": "https://arxiv.org/abs/2304.03635v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03635"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2304.03940",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.03940v1",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "published": "2023-04-08T07:03:01Z"
    },
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2304.06408",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.06408v2",
      "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
      "summary": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
      "published": "2023-04-13T11:13:19Z"
    },
    "metadata": {
      "arxiv_id": "2304.06408",
      "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
      "summary": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
      "authors": [
        "Riccardo Corvi",
        "Davide Cozzolino",
        "Giovanni Poggi",
        "Koki Nagano",
        "Luisa Verdoliva"
      ],
      "published": "2023-04-13T11:13:19Z",
      "updated": "2023-06-29T15:33:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.06408v2",
      "landing_url": "https://arxiv.org/abs/2304.06408v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.06408"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2304.07240",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.07240v1",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "published": "2023-04-14T16:43:31Z"
    },
    "metadata": {
      "arxiv_id": "2304.07240",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "authors": [
        "Aaron Hurst",
        "Daniel E. Lucani",
        "Qi Zhang"
      ],
      "published": "2023-04-14T16:43:31Z",
      "updated": "2023-04-14T16:43:31Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.07240v1",
      "landing_url": "https://arxiv.org/abs/2304.07240v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.07240"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2304.09226",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.09226v1",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "published": "2023-04-18T18:26:56Z"
    },
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2304.09854",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.09854v4",
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
      "published": "2023-04-19T17:59:02Z"
    },
    "metadata": {
      "arxiv_id": "2304.09854",
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
      "authors": [
        "Xiangtai Li",
        "Henghui Ding",
        "Haobo Yuan",
        "Wenwei Zhang",
        "Jiangmiao Pang",
        "Guangliang Cheng",
        "Kai Chen",
        "Ziwei Liu",
        "Chen Change Loy"
      ],
      "published": "2023-04-19T17:59:02Z",
      "updated": "2024-08-04T04:30:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09854v4",
      "landing_url": "https://arxiv.org/abs/2304.09854v4",
      "doi": "https://doi.org/10.48550/arXiv.2304.09854"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2304.11750",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.11750v1",
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "published": "2023-04-23T21:05:33Z"
    },
    "metadata": {
      "arxiv_id": "2304.11750",
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "authors": [
        "Zhijun Liu",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2023-04-23T21:05:33Z",
      "updated": "2023-04-23T21:05:33Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11750v1",
      "landing_url": "https://arxiv.org/abs/2304.11750v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.11750"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2304.11976",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.11976v1",
      "title": "Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model",
      "summary": "This paper proposes a zero-shot text-to-speech (TTS) conditioned by a self-supervised speech-representation model acquired through self-supervised learning (SSL). Conventional methods with embedding vectors from x-vector or global style tokens still have a gap in reproducing the speaker characteristics of unseen speakers. A novel point of the proposed method is the direct use of the SSL model to obtain embedding vectors from speech representations trained with a large amount of data. We also introduce the separate conditioning of acoustic features and a phoneme duration predictor to obtain the disentangled embeddings between rhythm-based speaker characteristics and acoustic-feature-based ones. The disentangled embeddings will enable us to achieve better reproduction performance for unseen speakers and rhythm transfer conditioned by different speeches. Objective and subjective evaluations showed that the proposed method can synthesize speech with improved similarity and achieve speech-rhythm transfer.",
      "published": "2023-04-24T10:15:58Z"
    },
    "metadata": {
      "arxiv_id": "2304.11976",
      "title": "Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model",
      "summary": "This paper proposes a zero-shot text-to-speech (TTS) conditioned by a self-supervised speech-representation model acquired through self-supervised learning (SSL). Conventional methods with embedding vectors from x-vector or global style tokens still have a gap in reproducing the speaker characteristics of unseen speakers. A novel point of the proposed method is the direct use of the SSL model to obtain embedding vectors from speech representations trained with a large amount of data. We also introduce the separate conditioning of acoustic features and a phoneme duration predictor to obtain the disentangled embeddings between rhythm-based speaker characteristics and acoustic-feature-based ones. The disentangled embeddings will enable us to achieve better reproduction performance for unseen speakers and rhythm transfer conditioned by different speeches. Objective and subjective evaluations showed that the proposed method can synthesize speech with improved similarity and achieve speech-rhythm transfer.",
      "authors": [
        "Kenichi Fujita",
        "Takanori Ashihara",
        "Hiroki Kanagawa",
        "Takafumi Moriya",
        "Yusuke Ijima"
      ],
      "published": "2023-04-24T10:15:58Z",
      "updated": "2023-04-24T10:15:58Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11976v1",
      "landing_url": "https://arxiv.org/abs/2304.11976v1",
      "doi": "https://doi.org/10.1109/ICASSPW59220.2023.10193459"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2304.12404",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.12404v1",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "published": "2023-04-24T19:33:41Z"
    },
    "metadata": {
      "arxiv_id": "2304.12404",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "authors": [
        "Sandeep Mehta",
        "Darpan Shah",
        "Ravindra Kulkarni",
        "Cornelia Caragea"
      ],
      "published": "2023-04-24T19:33:41Z",
      "updated": "2023-04-24T19:33:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12404v1",
      "landing_url": "https://arxiv.org/abs/2304.12404v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12404"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2305.02528",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.02528v1",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "published": "2023-05-04T03:33:40Z"
    },
    "metadata": {
      "arxiv_id": "2305.02528",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "authors": [
        "Yaqi Shen",
        "Le Hui",
        "Jin Xie",
        "Jian Yang"
      ],
      "published": "2023-05-04T03:33:40Z",
      "updated": "2023-05-04T03:33:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02528v1",
      "landing_url": "https://arxiv.org/abs/2305.02528v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02528"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2305.03568",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.03568v3",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "published": "2023-05-05T14:19:46Z"
    },
    "metadata": {
      "arxiv_id": "2305.03568",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:19:46Z",
      "updated": "2025-05-09T08:19:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03568v3",
      "landing_url": "https://arxiv.org/abs/2305.03568v3",
      "doi": "https://doi.org/10.1016/j.cviu.2025.104362"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2305.03582",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.03582v3",
      "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
      "summary": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
      "published": "2023-05-05T14:37:26Z"
    },
    "metadata": {
      "arxiv_id": "2305.03582",
      "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
      "summary": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Laurent Girin",
        "Xavier Alameda-Pineda",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:37:26Z",
      "updated": "2024-02-20T16:18:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03582v3",
      "landing_url": "https://arxiv.org/abs/2305.03582v3",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106120"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2305.06788",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.06788v4",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "published": "2023-05-11T13:23:42Z"
    },
    "metadata": {
      "arxiv_id": "2305.06788",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "authors": [
        "Chih Wei Ling",
        "Cheuk Ting Li"
      ],
      "published": "2023-05-11T13:23:42Z",
      "updated": "2024-01-24T13:44:44Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06788v4",
      "landing_url": "https://arxiv.org/abs/2305.06788v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.06788"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2305.08286",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.08286v1",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "published": "2023-05-15T00:22:02Z"
    },
    "metadata": {
      "arxiv_id": "2305.08286",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "authors": [
        "Chia-Yi Su",
        "Aakash Bansal",
        "Vijayanta Jain",
        "Sepideh Ghanavati",
        "Collin McMillan"
      ],
      "published": "2023-05-15T00:22:02Z",
      "updated": "2023-05-15T00:22:02Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.08286v1",
      "landing_url": "https://arxiv.org/abs/2305.08286v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.08286"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2305.09636",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.09636v1",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "published": "2023-05-16T17:41:25Z"
    },
    "metadata": {
      "arxiv_id": "2305.09636",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "authors": [
        "Zalán Borsos",
        "Matt Sharifi",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-05-16T17:41:25Z",
      "updated": "2023-05-16T17:41:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.09636v1",
      "landing_url": "https://arxiv.org/abs/2305.09636v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.09636"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2305.10666",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.10666v3",
      "title": "A unified front-end framework for English text-to-speech synthesis",
      "summary": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
      "published": "2023-05-18T02:57:54Z"
    },
    "metadata": {
      "arxiv_id": "2305.10666",
      "title": "A unified front-end framework for English text-to-speech synthesis",
      "summary": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
      "authors": [
        "Zelin Ying",
        "Chen Li",
        "Yu Dong",
        "Qiuqiang Kong",
        "Qiao Tian",
        "Yuanyuan Huo",
        "Yuxuan Wang"
      ],
      "published": "2023-05-18T02:57:54Z",
      "updated": "2024-03-25T10:59:04Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10666v3",
      "landing_url": "https://arxiv.org/abs/2305.10666v3",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447144"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2305.10684",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.10684v1",
      "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
      "summary": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these variations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining.",
      "published": "2023-05-18T03:54:10Z"
    },
    "metadata": {
      "arxiv_id": "2305.10684",
      "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
      "summary": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these variations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining.",
      "authors": [
        "Avani Tanna",
        "Michael Saxon",
        "Amr El Abbadi",
        "William Yang Wang"
      ],
      "published": "2023-05-18T03:54:10Z",
      "updated": "2023-05-18T03:54:10Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10684v1",
      "landing_url": "https://arxiv.org/abs/2305.10684v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.10684"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2305.11403",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.11403v5",
      "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
      "summary": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
      "published": "2023-05-19T03:19:38Z"
    },
    "metadata": {
      "arxiv_id": "2305.11403",
      "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
      "summary": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
      "authors": [
        "Ling Zheng",
        "Jinchen Zhu",
        "Jinpeng Shi",
        "Shizhuang Weng"
      ],
      "published": "2023-05-19T03:19:38Z",
      "updated": "2023-06-19T06:56:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11403v5",
      "landing_url": "https://arxiv.org/abs/2305.11403v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.11403"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2305.11490",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.11490v5",
      "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
      "summary": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
      "published": "2023-05-19T07:44:39Z"
    },
    "metadata": {
      "arxiv_id": "2305.11490",
      "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
      "summary": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
      "authors": [
        "Suhyeon Lee",
        "Won Jun Kim",
        "Jinho Chang",
        "Jong Chul Ye"
      ],
      "published": "2023-05-19T07:44:39Z",
      "updated": "2024-03-18T03:41:09Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11490v5",
      "landing_url": "https://arxiv.org/abs/2305.11490v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.11490"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2305.11795",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.11795v1",
      "title": "A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images",
      "summary": "The highly realistic image quality achieved by current image generative models has many academic and industrial applications. To limit the use of such models to benign applications, though, it is necessary that tools to conclusively detect whether an image has been generated synthetically or not are developed. For this reason, several detectors have been developed providing excellent performance in computer vision applications, however, they can not be applied as they are to multispectral satellite images, and hence new models must be trained. In general, two-class classifiers can achieve very good detection accuracies, however they are not able to generalise to image domains and generative models architectures different than those used during training. For this reason, in this paper, we propose a one-class classifier based on Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the limitations of two-class classifiers. First, we emphasize the generalization problem that binary classifiers suffer from by training and testing an EfficientNet-B4 architecture on multiple multispectral datasets. Then we show that, since the VQ-VAE 2 based classifier is trained only on pristine images, it is able to detect images belonging to different domains and generated by architectures that have not been used during training. Last, we compare the two classifiers head-to-head on the same generated datasets, highlighting the superiori generalization capabilities of the VQ-VAE 2-based detector.",
      "published": "2023-05-19T16:30:50Z"
    },
    "metadata": {
      "arxiv_id": "2305.11795",
      "title": "A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images",
      "summary": "The highly realistic image quality achieved by current image generative models has many academic and industrial applications. To limit the use of such models to benign applications, though, it is necessary that tools to conclusively detect whether an image has been generated synthetically or not are developed. For this reason, several detectors have been developed providing excellent performance in computer vision applications, however, they can not be applied as they are to multispectral satellite images, and hence new models must be trained. In general, two-class classifiers can achieve very good detection accuracies, however they are not able to generalise to image domains and generative models architectures different than those used during training. For this reason, in this paper, we propose a one-class classifier based on Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the limitations of two-class classifiers. First, we emphasize the generalization problem that binary classifiers suffer from by training and testing an EfficientNet-B4 architecture on multiple multispectral datasets. Then we show that, since the VQ-VAE 2 based classifier is trained only on pristine images, it is able to detect images belonging to different domains and generated by architectures that have not been used during training. Last, we compare the two classifiers head-to-head on the same generated datasets, highlighting the superiori generalization capabilities of the VQ-VAE 2-based detector.",
      "authors": [
        "Lydia Abady",
        "Giovanna Maria Dimitri",
        "Mauro Barni"
      ],
      "published": "2023-05-19T16:30:50Z",
      "updated": "2023-05-19T16:30:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11795v1",
      "landing_url": "https://arxiv.org/abs/2305.11795v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11795"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2305.11926",
    "anchor": "speech representation",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.11926v1",
      "title": "MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting",
      "summary": "We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts/",
      "published": "2023-05-19T13:43:36Z"
    },
    "metadata": {
      "arxiv_id": "2305.11926",
      "title": "MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting",
      "summary": "We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts/",
      "authors": [
        "Neil Shah",
        "Vishal Tambrahalli",
        "Saiteja Kosgi",
        "Niranjan Pedanekar",
        "Vineet Gandhi"
      ],
      "published": "2023-05-19T13:43:36Z",
      "updated": "2023-05-19T13:43:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11926v1",
      "landing_url": "https://arxiv.org/abs/2305.11926v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11926"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2305.12200",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.12200v1",
      "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
      "summary": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
      "published": "2023-05-20T14:24:45Z"
    },
    "metadata": {
      "arxiv_id": "2305.12200",
      "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
      "summary": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
      "authors": [
        "Yuyue Wang",
        "Huan Xiao",
        "Yihan Wu",
        "Ruihua Song"
      ],
      "published": "2023-05-20T14:24:45Z",
      "updated": "2023-05-20T14:24:45Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12200v1",
      "landing_url": "https://arxiv.org/abs/2305.12200v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12200"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2305.12425",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.12425v2",
      "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
      "summary": "Voice conversion is an increasingly popular technology, and the growing number of real-time applications requires models with streaming conversion capabilities. Unlike typical (non-streaming) voice conversion, which can leverage the entire utterance as full context, streaming voice conversion faces significant challenges due to the missing future information, resulting in degraded intelligibility, speaker similarity, and sound quality. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion. Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252.8 ms.",
      "published": "2023-05-21T10:45:48Z"
    },
    "metadata": {
      "arxiv_id": "2305.12425",
      "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
      "summary": "Voice conversion is an increasingly popular technology, and the growing number of real-time applications requires models with streaming conversion capabilities. Unlike typical (non-streaming) voice conversion, which can leverage the entire utterance as full context, streaming voice conversion faces significant challenges due to the missing future information, resulting in degraded intelligibility, speaker similarity, and sound quality. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion. Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252.8 ms.",
      "authors": [
        "Ziqian Ning",
        "Yuepeng Jiang",
        "Pengcheng Zhu",
        "Jixun Yao",
        "Shuai Wang",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2023-05-21T10:45:48Z",
      "updated": "2023-05-31T01:17:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12425v2",
      "landing_url": "https://arxiv.org/abs/2305.12425v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.12425"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2305.13009",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13009v3",
      "title": "Textually Pretrained Speech Language Models",
      "summary": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
      "published": "2023-05-22T13:12:16Z"
    },
    "metadata": {
      "arxiv_id": "2305.13009",
      "title": "Textually Pretrained Speech Language Models",
      "summary": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
      "authors": [
        "Michael Hassid",
        "Tal Remez",
        "Tu Anh Nguyen",
        "Itai Gat",
        "Alexis Conneau",
        "Felix Kreuk",
        "Jade Copet",
        "Alexandre Defossez",
        "Gabriel Synnaeve",
        "Emmanuel Dupoux",
        "Roy Schwartz",
        "Yossi Adi"
      ],
      "published": "2023-05-22T13:12:16Z",
      "updated": "2024-01-30T11:52:51Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13009v3",
      "landing_url": "https://arxiv.org/abs/2305.13009v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.13009"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2305.13651",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13651v2",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "published": "2023-05-23T03:49:41Z"
    },
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2305.13686",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13686v1",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "published": "2023-05-23T04:48:51Z"
    },
    "metadata": {
      "arxiv_id": "2305.13686",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2023-05-23T04:48:51Z",
      "updated": "2023-05-23T04:48:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13686v1",
      "landing_url": "https://arxiv.org/abs/2305.13686v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1441"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2305.13905",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13905v1",
      "title": "EfficientSpeech: An On-Device Text to Speech Model",
      "summary": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
      "published": "2023-05-23T10:28:41Z"
    },
    "metadata": {
      "arxiv_id": "2305.13905",
      "title": "EfficientSpeech: An On-Device Text to Speech Model",
      "summary": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
      "authors": [
        "Rowel Atienza"
      ],
      "published": "2023-05-23T10:28:41Z",
      "updated": "2023-05-23T10:28:41Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13905v1",
      "landing_url": "https://arxiv.org/abs/2305.13905v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.13905"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2305.14858",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.14858v2",
      "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
      "summary": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
      "published": "2023-05-24T08:08:26Z"
    },
    "metadata": {
      "arxiv_id": "2305.14858",
      "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
      "summary": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
      "authors": [
        "Zixuan Jiang",
        "Jiaqi Gu",
        "Hanqing Zhu",
        "David Z. Pan"
      ],
      "published": "2023-05-24T08:08:26Z",
      "updated": "2023-10-26T04:32:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14858v2",
      "landing_url": "https://arxiv.org/abs/2305.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.14858"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2305.15719",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15719v1",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "published": "2023-05-25T05:02:35Z"
    },
    "metadata": {
      "arxiv_id": "2305.15719",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Qiao Tian",
        "Tang Li",
        "Zongyu Yin",
        "Siyuan Feng",
        "Ming Tu",
        "Yuliang Ji",
        "Rui Xia",
        "Mingbo Ma",
        "Xuchen Song",
        "Jitong Chen",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2023-05-25T05:02:35Z",
      "updated": "2023-05-25T05:02:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15719v1",
      "landing_url": "https://arxiv.org/abs/2305.15719v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15719"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2305.16753",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.16753v1",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "published": "2023-05-26T09:06:04Z"
    },
    "metadata": {
      "arxiv_id": "2305.16753",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "authors": [
        "Enoch Hsin-Ho Huang",
        "Rong Chao",
        "Yu Tsao",
        "Chao-Min Wu"
      ],
      "published": "2023-05-26T09:06:04Z",
      "updated": "2023-05-26T09:06:04Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16753v1",
      "landing_url": "https://arxiv.org/abs/2305.16753v1",
      "doi": "https://doi.org/10.1109/TCDS.2023.3275587"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2305.17310",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17310v1",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "published": "2023-05-27T00:05:39Z"
    },
    "metadata": {
      "arxiv_id": "2305.17310",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "authors": [
        "Igor Nunes",
        "Mike Heddes",
        "Pere Vergés",
        "Danny Abraham",
        "Alexander Veidenbaum",
        "Alexandru Nicolau",
        "Tony Givargis"
      ],
      "published": "2023-05-27T00:05:39Z",
      "updated": "2023-05-27T00:05:39Z",
      "categories": [
        "cs.SI",
        "cs.DS",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17310v1",
      "landing_url": "https://arxiv.org/abs/2305.17310v1",
      "doi": "https://doi.org/10.1145/3580305.3599314"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2305.18415",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18415v3",
      "title": "Geometric Algebra Transformer",
      "summary": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
      "published": "2023-05-28T18:48:50Z"
    },
    "metadata": {
      "arxiv_id": "2305.18415",
      "title": "Geometric Algebra Transformer",
      "summary": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
      "authors": [
        "Johann Brehmer",
        "Pim de Haan",
        "Sönke Behrends",
        "Taco Cohen"
      ],
      "published": "2023-05-28T18:48:50Z",
      "updated": "2023-11-20T08:31:51Z",
      "categories": [
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18415v3",
      "landing_url": "https://arxiv.org/abs/2305.18415v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.18415"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2305.18739",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18739v1",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "published": "2023-05-30T04:26:48Z"
    },
    "metadata": {
      "arxiv_id": "2305.18739",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "authors": [
        "Jaeuk Byun",
        "Youna Ji",
        "Soo Whan Chung",
        "Soyeon Choe",
        "Min Seok Choi"
      ],
      "published": "2023-05-30T04:26:48Z",
      "updated": "2023-05-30T04:26:48Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18739v1",
      "landing_url": "https://arxiv.org/abs/2305.18739v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095881"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2305.18769",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18769v1",
      "title": "DualVAE: Controlling Colours of Generated and Real Images",
      "summary": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity mapping that identifies structural features. The disentangled representation is obtained by two novel mechanisms:\n  (i) a dual branch architecture that separates image colour attributes from geometric attributes, and (ii) a new ELBO that trains the combined colour and geometry representations. DualVAE can control the colour of generated images, and recolour existing images by transferring the colour latent representation obtained from an exemplar image. We demonstrate that DualVAE generates images with FID nearly two times better than VQ-GAN on a diverse collection of datasets, including animated faces, logos and artistic landscapes.",
      "published": "2023-05-30T06:04:30Z"
    },
    "metadata": {
      "arxiv_id": "2305.18769",
      "title": "DualVAE: Controlling Colours of Generated and Real Images",
      "summary": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity mapping that identifies structural features. The disentangled representation is obtained by two novel mechanisms:\n  (i) a dual branch architecture that separates image colour attributes from geometric attributes, and (ii) a new ELBO that trains the combined colour and geometry representations. DualVAE can control the colour of generated images, and recolour existing images by transferring the colour latent representation obtained from an exemplar image. We demonstrate that DualVAE generates images with FID nearly two times better than VQ-GAN on a diverse collection of datasets, including animated faces, logos and artistic landscapes.",
      "authors": [
        "Keerth Rathakumar",
        "David Liebowitz",
        "Christian Walder",
        "Kristen Moore",
        "Salil S. Kanhere"
      ],
      "published": "2023-05-30T06:04:30Z",
      "updated": "2023-05-30T06:04:30Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18769v1",
      "landing_url": "https://arxiv.org/abs/2305.18769v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.18769"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2305.18975",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18975v1",
      "title": "Voice Conversion With Just Nearest Neighbors",
      "summary": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity -- making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
      "published": "2023-05-30T12:19:07Z"
    },
    "metadata": {
      "arxiv_id": "2305.18975",
      "title": "Voice Conversion With Just Nearest Neighbors",
      "summary": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity -- making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
      "authors": [
        "Matthew Baas",
        "Benjamin van Niekerk",
        "Herman Kamper"
      ],
      "published": "2023-05-30T12:19:07Z",
      "updated": "2023-05-30T12:19:07Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18975v1",
      "landing_url": "https://arxiv.org/abs/2305.18975v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.18975"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2305.19269",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.19269v1",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "published": "2023-05-30T17:59:26Z"
    },
    "metadata": {
      "arxiv_id": "2305.19269",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Luping Liu",
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "published": "2023-05-30T17:59:26Z",
      "updated": "2023-05-30T17:59:26Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19269v1",
      "landing_url": "https://arxiv.org/abs/2305.19269v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19269"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2305.19533",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.19533v3",
      "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
      "summary": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
      "published": "2023-05-31T03:37:11Z"
    },
    "metadata": {
      "arxiv_id": "2305.19533",
      "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
      "summary": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
      "authors": [
        "Hanqing Zhu",
        "Jiaqi Gu",
        "Hanrui Wang",
        "Zixuan Jiang",
        "Zhekai Zhang",
        "Rongxing Tang",
        "Chenghao Feng",
        "Song Han",
        "Ray T. Chen",
        "David Z. Pan"
      ],
      "published": "2023-05-31T03:37:11Z",
      "updated": "2023-12-31T23:22:41Z",
      "categories": [
        "cs.ET",
        "cs.AR",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19533v3",
      "landing_url": "https://arxiv.org/abs/2305.19533v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.19533"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2305.19750",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.19750v1",
      "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
      "summary": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
      "published": "2023-05-31T11:33:18Z"
    },
    "metadata": {
      "arxiv_id": "2305.19750",
      "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
      "summary": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
      "authors": [
        "Tobias Bollinger",
        "Jan Deriu",
        "Manfred Vogel"
      ],
      "published": "2023-05-31T11:33:18Z",
      "updated": "2023-05-31T11:33:18Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19750v1",
      "landing_url": "https://arxiv.org/abs/2305.19750v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19750"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2305.19957",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.19957v2",
      "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
      "summary": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
      "published": "2023-05-31T15:44:00Z"
    },
    "metadata": {
      "arxiv_id": "2305.19957",
      "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
      "summary": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
      "authors": [
        "Maoyuan Ye",
        "Jing Zhang",
        "Shanshan Zhao",
        "Juhua Liu",
        "Tongliang Liu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "published": "2023-05-31T15:44:00Z",
      "updated": "2024-03-18T13:30:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19957v2",
      "landing_url": "https://arxiv.org/abs/2305.19957v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.19957"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2306.00331",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.00331v1",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "published": "2023-06-01T04:19:57Z"
    },
    "metadata": {
      "arxiv_id": "2306.00331",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "authors": [
        "Pin-Jui Ku",
        "Chao-Han Huck Yang",
        "Sabato Marco Siniscalchi",
        "Chin-Hui Lee"
      ],
      "published": "2023-06-01T04:19:57Z",
      "updated": "2023-06-01T04:19:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00331v1",
      "landing_url": "https://arxiv.org/abs/2306.00331v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1084"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2306.01303",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01303v1",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "published": "2023-06-02T07:03:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.01303",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "authors": [
        "Haoyu Wang",
        "Siyuan Wang",
        "Wei-Qiang Zhang",
        "Jinfeng Bai"
      ],
      "published": "2023-06-02T07:03:06Z",
      "updated": "2023-06-02T07:03:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01303v1",
      "landing_url": "https://arxiv.org/abs/2306.01303v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01303"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.04374",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.04374v1",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "published": "2023-06-07T12:14:16Z"
    },
    "metadata": {
      "arxiv_id": "2306.04374",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "authors": [
        "Shikhar Vashishth",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Ankur Bapna",
        "Min Ma",
        "Wei Han",
        "Vera Axelrod",
        "Partha Talukdar"
      ],
      "published": "2023-06-07T12:14:16Z",
      "updated": "2023-06-07T12:14:16Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04374v1",
      "landing_url": "https://arxiv.org/abs/2306.04374v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.04374"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.06246",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06246v1",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "published": "2023-06-09T20:42:11Z"
    },
    "metadata": {
      "arxiv_id": "2306.06246",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "authors": [
        "Tianyu Huang",
        "Chung Hoon Hong",
        "Carl Wivagg",
        "Kanna Shimizu"
      ],
      "published": "2023-06-09T20:42:11Z",
      "updated": "2023-06-09T20:42:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06246v1",
      "landing_url": "https://arxiv.org/abs/2306.06246v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06246"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2306.06652",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06652v1",
      "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
      "summary": "Electrolarynx is a commonly used assistive device to help patients with removed vocal cords regain their ability to speak. Although the electrolarynx can generate excitation signals like the vocal cords, the naturalness and intelligibility of electrolaryngeal (EL) speech are very different from those of natural (NL) speech. Many deep-learning-based models have been applied to electrolaryngeal speech voice conversion (ELVC) for converting EL speech to NL speech. In this study, we propose a multimodal voice conversion (VC) model that integrates acoustic and visual information into a unified network. We compared different pre-trained models as visual feature extractors and evaluated the effectiveness of these features in the ELVC task. The experimental results demonstrate that the proposed multimodal VC model outperforms single-modal models in both objective and subjective metrics, suggesting that the integration of visual information can significantly improve the quality of ELVC.",
      "published": "2023-06-11T11:25:17Z"
    },
    "metadata": {
      "arxiv_id": "2306.06652",
      "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
      "summary": "Electrolarynx is a commonly used assistive device to help patients with removed vocal cords regain their ability to speak. Although the electrolarynx can generate excitation signals like the vocal cords, the naturalness and intelligibility of electrolaryngeal (EL) speech are very different from those of natural (NL) speech. Many deep-learning-based models have been applied to electrolaryngeal speech voice conversion (ELVC) for converting EL speech to NL speech. In this study, we propose a multimodal voice conversion (VC) model that integrates acoustic and visual information into a unified network. We compared different pre-trained models as visual feature extractors and evaluated the effectiveness of these features in the ELVC task. The experimental results demonstrate that the proposed multimodal VC model outperforms single-modal models in both objective and subjective metrics, suggesting that the integration of visual information can significantly improve the quality of ELVC.",
      "authors": [
        "Yung-Lun Chien",
        "Hsin-Hao Chen",
        "Ming-Chi Yen",
        "Shu-Wei Tsai",
        "Hsin-Min Wang",
        "Yu Tsao",
        "Tai-Shih Chi"
      ],
      "published": "2023-06-11T11:25:17Z",
      "updated": "2023-06-11T11:25:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06652v1",
      "landing_url": "https://arxiv.org/abs/2306.06652v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06652"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2306.07547",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.07547v6",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "published": "2023-06-13T05:38:34Z"
    },
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.08920",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.08920v1",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "published": "2023-06-15T07:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2306.09452",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.09452v1",
      "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
      "summary": "Second-pass rescoring is employed in most state-of-the-art speech recognition systems. Recently, BERT based models have gained popularity for re-ranking the n-best hypothesis by exploiting the knowledge from masked language model pre-training. Further, fine-tuning with discriminative loss such as minimum word error rate (MWER) has shown to perform better than likelihood-based loss. Streaming applications with low latency requirements impose significant constraints on the size of the models, thereby limiting the word error rate (WER) performance gains. In this paper, we propose effective strategies for distilling from large models discriminatively trained with the MWER objective. We experiment on Librispeech and production scale internal dataset for voice-assistant. Our results demonstrate relative improvements of upto 7% WER over student models trained with MWER. We also show that the proposed distillation can reduce the WER gap between the student and the teacher by 62% upto 100%.",
      "published": "2023-06-15T19:15:14Z"
    },
    "metadata": {
      "arxiv_id": "2306.09452",
      "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
      "summary": "Second-pass rescoring is employed in most state-of-the-art speech recognition systems. Recently, BERT based models have gained popularity for re-ranking the n-best hypothesis by exploiting the knowledge from masked language model pre-training. Further, fine-tuning with discriminative loss such as minimum word error rate (MWER) has shown to perform better than likelihood-based loss. Streaming applications with low latency requirements impose significant constraints on the size of the models, thereby limiting the word error rate (WER) performance gains. In this paper, we propose effective strategies for distilling from large models discriminatively trained with the MWER objective. We experiment on Librispeech and production scale internal dataset for voice-assistant. Our results demonstrate relative improvements of upto 7% WER over student models trained with MWER. We also show that the proposed distillation can reduce the WER gap between the student and the teacher by 62% upto 100%.",
      "authors": [
        "Prashanth Gurunath Shivakumar",
        "Jari Kolehmainen",
        "Yile Gu",
        "Ankur Gandhe",
        "Ariya Rastrow",
        "Ivan Bulyko"
      ],
      "published": "2023-06-15T19:15:14Z",
      "updated": "2023-06-15T19:15:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.09452v1",
      "landing_url": "https://arxiv.org/abs/2306.09452v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.09452"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.10521",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.10521v2",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "published": "2023-06-18T10:59:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.12020",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.12020v1",
      "title": "Visual-Aware Text-to-Speech",
      "summary": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
      "published": "2023-06-21T05:11:39Z"
    },
    "metadata": {
      "arxiv_id": "2306.12020",
      "title": "Visual-Aware Text-to-Speech",
      "summary": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
      "authors": [
        "Mohan Zhou",
        "Yalong Bai",
        "Wei Zhang",
        "Ting Yao",
        "Tiejun Zhao",
        "Tao Mei"
      ],
      "published": "2023-06-21T05:11:39Z",
      "updated": "2023-06-21T05:11:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12020v1",
      "landing_url": "https://arxiv.org/abs/2306.12020v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095084"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2306.12785",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.12785v1",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "published": "2023-06-22T10:29:24Z"
    },
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2306.14422",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.14422v2",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "published": "2023-06-26T05:04:58Z"
    },
    "metadata": {
      "arxiv_id": "2306.14422",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "authors": [
        "Wen-Chin Huang",
        "Lester Phillip Violeta",
        "Songxiang Liu",
        "Jiatong Shi",
        "Tomoki Toda"
      ],
      "published": "2023-06-26T05:04:58Z",
      "updated": "2023-07-06T08:17:31Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14422v2",
      "landing_url": "https://arxiv.org/abs/2306.14422v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14422"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2306.15354",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.15354v3",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "published": "2023-06-27T10:09:43Z"
    },
    "metadata": {
      "arxiv_id": "2306.15354",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "published": "2023-06-27T10:09:43Z",
      "updated": "2023-09-25T02:36:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15354v3",
      "landing_url": "https://arxiv.org/abs/2306.15354v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.15354"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2306.15687",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.15687v2",
      "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
      "summary": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
      "published": "2023-06-23T16:23:24Z"
    },
    "metadata": {
      "arxiv_id": "2306.15687",
      "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
      "summary": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
      "authors": [
        "Matthew Le",
        "Apoorv Vyas",
        "Bowen Shi",
        "Brian Karrer",
        "Leda Sari",
        "Rashel Moritz",
        "Mary Williamson",
        "Vimal Manohar",
        "Yossi Adi",
        "Jay Mahadeokar",
        "Wei-Ning Hsu"
      ],
      "published": "2023-06-23T16:23:24Z",
      "updated": "2023-10-19T13:23:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15687v2",
      "landing_url": "https://arxiv.org/abs/2306.15687v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15687"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2306.16317",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.16317v2",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "published": "2023-06-28T15:49:20Z"
    },
    "metadata": {
      "arxiv_id": "2306.16317",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "authors": [
        "Joshua A. Grochow",
        "Youming Qiao"
      ],
      "published": "2023-06-28T15:49:20Z",
      "updated": "2024-04-12T13:10:08Z",
      "categories": [
        "cs.CC",
        "cs.DS",
        "math.AG",
        "math.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.16317v2",
      "landing_url": "https://arxiv.org/abs/2306.16317v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.16317"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2307.00024",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00024v1",
      "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
      "summary": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
      "published": "2023-06-28T19:34:16Z"
    },
    "metadata": {
      "arxiv_id": "2307.00024",
      "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
      "summary": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
      "authors": [
        "Daria Diatlova",
        "Vitaly Shutov"
      ],
      "published": "2023-06-28T19:34:16Z",
      "updated": "2023-06-28T19:34:16Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00024v1",
      "landing_url": "https://arxiv.org/abs/2307.00024v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00024"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2307.00393",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00393v1",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "published": "2023-07-01T17:44:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.00393",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "authors": [
        "Houjian Guo",
        "Chaoran Liu",
        "Carlos Toshinori Ishi",
        "Hiroshi Ishiguro"
      ],
      "published": "2023-07-01T17:44:18Z",
      "updated": "2023-07-01T17:44:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00393v1",
      "landing_url": "https://arxiv.org/abs/2307.00393v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00393"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2307.00453",
    "anchor": "speech representation",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00453v1",
      "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
      "summary": "Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.",
      "published": "2023-07-02T02:21:29Z"
    },
    "metadata": {
      "arxiv_id": "2307.00453",
      "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
      "summary": "Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.",
      "authors": [
        "Anshu Bhatia",
        "Sanchit Sinha",
        "Saket Dingliwal",
        "Karthik Gopalakrishnan",
        "Sravan Bodapati",
        "Katrin Kirchhoff"
      ],
      "published": "2023-07-02T02:21:29Z",
      "updated": "2023-07-02T02:21:29Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00453v1",
      "landing_url": "https://arxiv.org/abs/2307.00453v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00453"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.00729",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00729v1",
      "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
      "summary": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
      "published": "2023-07-03T03:21:23Z"
    },
    "metadata": {
      "arxiv_id": "2307.00729",
      "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
      "summary": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
      "authors": [
        "Sheng Zhao",
        "Qilong Yuan",
        "Yibo Duan",
        "Zhuoyue Chen"
      ],
      "published": "2023-07-03T03:21:23Z",
      "updated": "2023-07-03T03:21:23Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00729v1",
      "landing_url": "https://arxiv.org/abs/2307.00729v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00729"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2307.00782",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00782v2",
      "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
      "summary": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
      "published": "2023-07-03T06:55:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.00782",
      "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
      "summary": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
      "authors": [
        "Yujia Xiao",
        "Shaofei Zhang",
        "Xi Wang",
        "Xu Tan",
        "Lei He",
        "Sheng Zhao",
        "Frank K. Soong",
        "Tan Lee"
      ],
      "published": "2023-07-03T06:55:03Z",
      "updated": "2023-10-07T08:32:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00782v2",
      "landing_url": "https://arxiv.org/abs/2307.00782v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-122"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2307.01323",
    "anchor": "speech representation",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.01323v1",
      "title": "Semantic enrichment towards efficient speech representations",
      "summary": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
      "published": "2023-07-03T19:52:56Z"
    },
    "metadata": {
      "arxiv_id": "2307.01323",
      "title": "Semantic enrichment towards efficient speech representations",
      "summary": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
      "authors": [
        "Gaëlle Laperrière",
        "Ha Nguyen",
        "Sahar Ghannay",
        "Bassam Jabaian",
        "Yannick Estève"
      ],
      "published": "2023-07-03T19:52:56Z",
      "updated": "2023-07-03T19:52:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.01323v1",
      "landing_url": "https://arxiv.org/abs/2307.01323v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-2234"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2307.02720",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.02720v1",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "published": "2023-07-06T02:03:31Z"
    },
    "metadata": {
      "arxiv_id": "2307.02720",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "authors": [
        "Gene-Ping Yang",
        "Yue Gu",
        "Qingming Tang",
        "Dongsu Du",
        "Yuzong Liu"
      ],
      "published": "2023-07-06T02:03:31Z",
      "updated": "2023-07-06T02:03:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02720v1",
      "landing_url": "https://arxiv.org/abs/2307.02720v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.02720"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.04179",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.04179v1",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "published": "2023-07-09T14:04:58Z"
    },
    "metadata": {
      "arxiv_id": "2307.04179",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "authors": [
        "Wen-Yuan Ting",
        "Syu-Siang Wang",
        "Yu Tsao",
        "Borching Su"
      ],
      "published": "2023-07-09T14:04:58Z",
      "updated": "2023-07-09T14:04:58Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04179v1",
      "landing_url": "https://arxiv.org/abs/2307.04179v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.04179"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2307.04686",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.04686v2",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "published": "2023-07-10T16:42:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.06040",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.06040v1",
      "title": "Rhythm Modeling for Voice Conversion",
      "summary": "Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.",
      "published": "2023-07-12T09:35:16Z"
    },
    "metadata": {
      "arxiv_id": "2307.06040",
      "title": "Rhythm Modeling for Voice Conversion",
      "summary": "Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.",
      "authors": [
        "Benjamin van Niekerk",
        "Marc-André Carbonneau",
        "Herman Kamper"
      ],
      "published": "2023-07-12T09:35:16Z",
      "updated": "2023-07-12T09:35:16Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06040v1",
      "landing_url": "https://arxiv.org/abs/2307.06040v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06040"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2307.06832",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.06832v1",
      "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
      "summary": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
      "published": "2023-07-13T15:54:32Z"
    },
    "metadata": {
      "arxiv_id": "2307.06832",
      "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
      "summary": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
      "authors": [
        "Jari Kolehmainen",
        "Yile Gu",
        "Aditya Gourav",
        "Prashanth Gurunath Shivakumar",
        "Ankur Gandhe",
        "Ariya Rastrow",
        "Ivan Bulyko"
      ],
      "published": "2023-07-13T15:54:32Z",
      "updated": "2023-07-13T15:54:32Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06832v1",
      "landing_url": "https://arxiv.org/abs/2307.06832v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06832"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.07062",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.07062v1",
      "title": "Controllable Emphasis with zero data for text-to-speech",
      "summary": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
      "published": "2023-07-13T21:06:23Z"
    },
    "metadata": {
      "arxiv_id": "2307.07062",
      "title": "Controllable Emphasis with zero data for text-to-speech",
      "summary": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
      "authors": [
        "Arnaud Joly",
        "Marco Nicolis",
        "Ekaterina Peterova",
        "Alessandro Lombardi",
        "Ammar Abbas",
        "Arent van Korlaar",
        "Aman Hussain",
        "Parul Sharma",
        "Alexis Moinet",
        "Mateusz Lajszczak",
        "Penny Karanasou",
        "Antonio Bonafonte",
        "Thomas Drugman",
        "Elena Sokolova"
      ],
      "published": "2023-07-13T21:06:23Z",
      "updated": "2023-07-13T21:06:23Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07062v1",
      "landing_url": "https://arxiv.org/abs/2307.07062v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.07062"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2307.07940",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.07940v2",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "published": "2023-07-16T04:20:26Z"
    },
    "metadata": {
      "arxiv_id": "2307.07940",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "authors": [
        "Atsushi Shirafuji",
        "Yutaka Watanobe"
      ],
      "published": "2023-07-16T04:20:26Z",
      "updated": "2023-09-11T09:42:37Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07940v2",
      "landing_url": "https://arxiv.org/abs/2307.07940v2",
      "doi": "https://doi.org/10.1145/3634814.3634828"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2307.09435",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.09435v1",
      "title": "SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs",
      "summary": "In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.",
      "published": "2023-07-18T17:09:15Z"
    },
    "metadata": {
      "arxiv_id": "2307.09435",
      "title": "SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs",
      "summary": "In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.",
      "authors": [
        "Yinghao Aaron Li",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2023-07-18T17:09:15Z",
      "updated": "2023-07-18T17:09:15Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09435v1",
      "landing_url": "https://arxiv.org/abs/2307.09435v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09435"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2307.09735",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.09735v1",
      "title": "Reduction of the secret key length in the perfect cipher by data compression and randomisation",
      "summary": "Perfect ciphers have been a very attractive cryptographic tool ever since C. Shannon described them. Note that, by definition, if a perfect cipher is used, no one can get any information about the encrypted message without knowing the secret key. We consider the problem of reducing the key length of perfect ciphers, because in many applications the length of the secret key is a crucial parameter. This paper describes a simple method of key length reduction. This method gives a perfect cipher and is based on the use of data compression and randomisation, and the average key length can be made close to Shannon entropy (which is the key length limit). It should be noted that the method can effectively use readily available data compressors (archivers).",
      "published": "2023-07-19T03:08:44Z"
    },
    "metadata": {
      "arxiv_id": "2307.09735",
      "title": "Reduction of the secret key length in the perfect cipher by data compression and randomisation",
      "summary": "Perfect ciphers have been a very attractive cryptographic tool ever since C. Shannon described them. Note that, by definition, if a perfect cipher is used, no one can get any information about the encrypted message without knowing the secret key. We consider the problem of reducing the key length of perfect ciphers, because in many applications the length of the secret key is a crucial parameter. This paper describes a simple method of key length reduction. This method gives a perfect cipher and is based on the use of data compression and randomisation, and the average key length can be made close to Shannon entropy (which is the key length limit). It should be noted that the method can effectively use readily available data compressors (archivers).",
      "authors": [
        "Boris Ryabko"
      ],
      "published": "2023-07-19T03:08:44Z",
      "updated": "2023-07-19T03:08:44Z",
      "categories": [
        "cs.CR",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09735v1",
      "landing_url": "https://arxiv.org/abs/2307.09735v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09735"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2307.10982",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.10982v2",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "published": "2023-07-20T16:09:57Z"
    },
    "metadata": {
      "arxiv_id": "2307.10982",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "authors": [
        "Anjali Raj",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Min Ma",
        "Shikhar Vashishth"
      ],
      "published": "2023-07-20T16:09:57Z",
      "updated": "2023-09-25T12:49:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10982v2",
      "landing_url": "https://arxiv.org/abs/2307.10982v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.10982"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.11394",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.11394v3",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "published": "2023-07-21T07:22:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.11394",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-07-21T07:22:18Z",
      "updated": "2024-01-25T19:48:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11394v3",
      "landing_url": "https://arxiv.org/abs/2307.11394v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.11394"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.11794",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.11794v1",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "published": "2023-07-21T02:49:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.11794",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "authors": [
        "Yangpeng Huang",
        "Naixing Feng",
        "Yijun Cai"
      ],
      "published": "2023-07-21T02:49:03Z",
      "updated": "2023-07-21T02:49:03Z",
      "categories": [
        "physics.optics",
        "cs.LG",
        "physics.app-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11794v1",
      "landing_url": "https://arxiv.org/abs/2307.11794v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.11794"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2307.12052",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.12052v1",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "published": "2023-07-22T11:27:05Z"
    },
    "metadata": {
      "arxiv_id": "2307.12052",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "authors": [
        "Mallikarjun Reddy Dorsala",
        "V. N. Sastry",
        "Sudhakar Chapram"
      ],
      "published": "2023-07-22T11:27:05Z",
      "updated": "2023-07-22T11:27:05Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12052v1",
      "landing_url": "https://arxiv.org/abs/2307.12052v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2307.13685",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.13685v1",
      "title": "Noisy k-means++ Revisited",
      "summary": "The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem. While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation.\n  In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact.\n  Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$. In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.",
      "published": "2023-07-25T17:45:41Z"
    },
    "metadata": {
      "arxiv_id": "2307.13685",
      "title": "Noisy k-means++ Revisited",
      "summary": "The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem. While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation.\n  In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact.\n  Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$. In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.",
      "authors": [
        "Christoph Grunau",
        "Ahmet Alper Özüdoğru",
        "Václav Rozhoň"
      ],
      "published": "2023-07-25T17:45:41Z",
      "updated": "2023-07-25T17:45:41Z",
      "categories": [
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.13685v1",
      "landing_url": "https://arxiv.org/abs/2307.13685v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.13685"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2307.15139",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.15139v1",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "published": "2023-07-27T18:31:04Z"
    },
    "metadata": {
      "arxiv_id": "2307.15139",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "authors": [
        "Chuanxia Zheng",
        "Andrea Vedaldi"
      ],
      "published": "2023-07-27T18:31:04Z",
      "updated": "2023-07-27T18:31:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15139v1",
      "landing_url": "https://arxiv.org/abs/2307.15139v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.15139"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2307.16332",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.16332v1",
      "title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text",
      "summary": "In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.",
      "published": "2023-07-30T22:36:22Z"
    },
    "metadata": {
      "arxiv_id": "2307.16332",
      "title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text",
      "summary": "In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.",
      "authors": [
        "Eric Sun",
        "Jinyu Li",
        "Jian Xue",
        "Yifan Gong"
      ],
      "published": "2023-07-30T22:36:22Z",
      "updated": "2023-07-30T22:36:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16332v1",
      "landing_url": "https://arxiv.org/abs/2307.16332v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.16332"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.16679",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.16679v1",
      "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
      "summary": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
      "published": "2023-07-31T13:57:04Z"
    },
    "metadata": {
      "arxiv_id": "2307.16679",
      "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
      "summary": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
      "authors": [
        "Guangyan Zhang",
        "Thomas Merritt",
        "Manuel Sam Ribeiro",
        "Biel Tura-Vecino",
        "Kayoko Yanagisawa",
        "Kamil Pokora",
        "Abdelhamid Ezzerg",
        "Sebastian Cygert",
        "Ammar Abbas",
        "Piotr Bilinski",
        "Roberto Barra-Chicote",
        "Daniel Korzekwa",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2023-07-31T13:57:04Z",
      "updated": "2023-07-31T13:57:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16679v1",
      "landing_url": "https://arxiv.org/abs/2307.16679v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.16679"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2308.00129",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00129v1",
      "title": "Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods",
      "summary": "This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.",
      "published": "2023-07-25T20:38:55Z"
    },
    "metadata": {
      "arxiv_id": "2308.00129",
      "title": "Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods",
      "summary": "This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.",
      "authors": [
        "Qingming Tang"
      ],
      "published": "2023-07-25T20:38:55Z",
      "updated": "2023-07-25T20:38:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00129v1",
      "landing_url": "https://arxiv.org/abs/2308.00129v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00129"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2308.00721",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00721v4",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "published": "2023-07-31T03:56:46Z"
    },
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2308.01926",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.01926v1",
      "title": "Are Easy Data Easy (for K-Means)",
      "summary": "This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.",
      "published": "2023-08-02T09:40:19Z"
    },
    "metadata": {
      "arxiv_id": "2308.01926",
      "title": "Are Easy Data Easy (for K-Means)",
      "summary": "This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.",
      "authors": [
        "Mieczysław A. Kłopotek"
      ],
      "published": "2023-08-02T09:40:19Z",
      "updated": "2023-08-02T09:40:19Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.01926v1",
      "landing_url": "https://arxiv.org/abs/2308.01926v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.01926"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2308.03332",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.03332v1",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "published": "2023-08-07T06:26:53Z"
    },
    "metadata": {
      "arxiv_id": "2308.03332",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "authors": [
        "Rawad Melhem",
        "Assef Jafar",
        "Riad Hamadeh"
      ],
      "published": "2023-08-07T06:26:53Z",
      "updated": "2023-08-07T06:26:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.03332v1",
      "landing_url": "https://arxiv.org/abs/2308.03332v1",
      "doi": "https://doi.org/10.11916/j.issn.1005-9113.2019044"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2308.05502",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.05502v2",
      "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
      "summary": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
      "published": "2023-08-10T11:14:22Z"
    },
    "metadata": {
      "arxiv_id": "2308.05502",
      "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
      "summary": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
      "authors": [
        "Candida M. Greco",
        "Andrea Tagarelli"
      ],
      "published": "2023-08-10T11:14:22Z",
      "updated": "2024-02-03T09:54:51Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.NE",
        "physics.soc-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05502v2",
      "landing_url": "https://arxiv.org/abs/2308.05502v2",
      "doi": "https://doi.org/10.1007/s10506-023-09374-7"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2308.06873",
    "anchor": "acoustic tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.06873v2",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "published": "2023-08-14T01:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2308.11084",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.11084v1",
      "title": "PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion",
      "summary": "Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.",
      "published": "2023-08-21T23:37:45Z"
    },
    "metadata": {
      "arxiv_id": "2308.11084",
      "title": "PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion",
      "summary": "Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.",
      "authors": [
        "Yimin Deng",
        "Huaizhen Tang",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-08-21T23:37:45Z",
      "updated": "2023-08-21T23:37:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.11084v1",
      "landing_url": "https://arxiv.org/abs/2308.11084v1",
      "doi": "https://doi.org/10.1145/3581783.3613800"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2308.13367",
    "anchor": "acoustic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.13367v1",
      "title": "Burnt area extraction from high-resolution satellite images based on anomaly detection",
      "summary": "Wildfire detection using satellite images is a widely studied task in remote sensing with many applications to fire delineation and mapping. Recently, deep learning methods have become a scalable solution to automate this task, especially in the field of unsupervised learning where no training data is available. This is particularly important in the context of emergency risk monitoring where fast and effective detection is needed, generally based on high-resolution satellite data. Among various approaches, Anomaly Detection (AD) appears to be highly potential thanks to its broad applications in computer vision, medical imaging, as well as remote sensing. In this work, we build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE), a popular reconstruction-based AD method with discrete latent spaces, to perform unsupervised burnt area extraction. We integrate VQ-VAE into an end-to-end framework with an intensive post-processing step using dedicated vegetation, water and brightness indexes. Our experiments conducted on high-resolution SPOT-6/7 images provide promising results of the proposed technique, showing its high potential in future research on unsupervised burnt area extraction.",
      "published": "2023-08-25T13:25:27Z"
    },
    "metadata": {
      "arxiv_id": "2308.13367",
      "title": "Burnt area extraction from high-resolution satellite images based on anomaly detection",
      "summary": "Wildfire detection using satellite images is a widely studied task in remote sensing with many applications to fire delineation and mapping. Recently, deep learning methods have become a scalable solution to automate this task, especially in the field of unsupervised learning where no training data is available. This is particularly important in the context of emergency risk monitoring where fast and effective detection is needed, generally based on high-resolution satellite data. Among various approaches, Anomaly Detection (AD) appears to be highly potential thanks to its broad applications in computer vision, medical imaging, as well as remote sensing. In this work, we build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE), a popular reconstruction-based AD method with discrete latent spaces, to perform unsupervised burnt area extraction. We integrate VQ-VAE into an end-to-end framework with an intensive post-processing step using dedicated vegetation, water and brightness indexes. Our experiments conducted on high-resolution SPOT-6/7 images provide promising results of the proposed technique, showing its high potential in future research on unsupervised burnt area extraction.",
      "authors": [
        "Oscar David Rafael Narvaez Luces",
        "Minh-Tan Pham",
        "Quentin Poterek",
        "Rémi Braun"
      ],
      "published": "2023-08-25T13:25:27Z",
      "updated": "2023-08-25T13:25:27Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.13367v1",
      "landing_url": "https://arxiv.org/abs/2308.13367v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.13367"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2308.14319",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.14319v1",
      "title": "Voice Conversion with Denoising Diffusion Probabilistic GAN Models",
      "summary": "Voice conversion is a method that allows for the transformation of speaking style while maintaining the integrity of linguistic information. There are many researchers using deep generative models for voice conversion tasks. Generative Adversarial Networks (GANs) can quickly generate high-quality samples, but the generated samples lack diversity. The samples generated by the Denoising Diffusion Probabilistic Models (DDPMs) are better than GANs in terms of mode coverage and sample diversity. But the DDPMs have high computational costs and the inference speed is slower than GANs. In order to make GANs and DDPMs more practical we proposes DiffGAN-VC, a variant of GANs and DDPMS, to achieve non-parallel many-to-many voice conversion (VC). We use large steps to achieve denoising, and also introduce a multimodal conditional GANs to model the denoising diffusion generative adversarial network. According to both objective and subjective evaluation experiments, DiffGAN-VC has been shown to achieve high voice quality on non-parallel data sets. Compared with the CycleGAN-VC method, DiffGAN-VC achieves speaker similarity, naturalness and higher sound quality.",
      "published": "2023-08-28T05:53:06Z"
    },
    "metadata": {
      "arxiv_id": "2308.14319",
      "title": "Voice Conversion with Denoising Diffusion Probabilistic GAN Models",
      "summary": "Voice conversion is a method that allows for the transformation of speaking style while maintaining the integrity of linguistic information. There are many researchers using deep generative models for voice conversion tasks. Generative Adversarial Networks (GANs) can quickly generate high-quality samples, but the generated samples lack diversity. The samples generated by the Denoising Diffusion Probabilistic Models (DDPMs) are better than GANs in terms of mode coverage and sample diversity. But the DDPMs have high computational costs and the inference speed is slower than GANs. In order to make GANs and DDPMs more practical we proposes DiffGAN-VC, a variant of GANs and DDPMS, to achieve non-parallel many-to-many voice conversion (VC). We use large steps to achieve denoising, and also introduce a multimodal conditional GANs to model the denoising diffusion generative adversarial network. According to both objective and subjective evaluation experiments, DiffGAN-VC has been shown to achieve high voice quality on non-parallel data sets. Compared with the CycleGAN-VC method, DiffGAN-VC achieves speaker similarity, naturalness and higher sound quality.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-08-28T05:53:06Z",
      "updated": "2023-08-28T05:53:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.14319v1",
      "landing_url": "https://arxiv.org/abs/2308.14319v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.14319"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2308.16552",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.16552v1",
      "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
      "summary": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
      "published": "2023-08-31T08:43:52Z"
    },
    "metadata": {
      "arxiv_id": "2308.16552",
      "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
      "summary": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
      "authors": [
        "Yang Liu",
        "Xiaoyun Zhong",
        "Shiyao Zhai",
        "Zhicheng Du",
        "Zhenyuan Gao",
        "Qiming Huang",
        "Canyang Zhang",
        "Bin Jiang",
        "Vijay Kumar Pandey",
        "Sanyang Han",
        "Runming Wang",
        "Yuxing Han",
        "Peiwu Qin"
      ],
      "published": "2023-08-31T08:43:52Z",
      "updated": "2023-08-31T08:43:52Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16552v1",
      "landing_url": "https://arxiv.org/abs/2308.16552v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.16552"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2308.16692",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.16692v2",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "published": "2023-08-31T12:53:09Z"
    },
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2309.00126",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00126v1",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "published": "2023-08-31T20:25:44Z"
    },
    "metadata": {
      "arxiv_id": "2309.00126",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Jiawen Kang",
        "Yujia Xiao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-08-31T20:25:44Z",
      "updated": "2023-08-31T20:25:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00126v1",
      "landing_url": "https://arxiv.org/abs/2309.00126v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.00126"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2309.00169",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00169v3",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "published": "2023-08-31T23:26:10Z"
    },
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2309.01692",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.01692v1",
      "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
      "summary": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
      "published": "2023-09-04T16:09:28Z"
    },
    "metadata": {
      "arxiv_id": "2309.01692",
      "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
      "summary": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
      "authors": [
        "Xin Lai",
        "Yuhui Yuan",
        "Ruihang Chu",
        "Yukang Chen",
        "Han Hu",
        "Jiaya Jia"
      ],
      "published": "2023-09-04T16:09:28Z",
      "updated": "2023-09-04T16:09:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01692v1",
      "landing_url": "https://arxiv.org/abs/2309.01692v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.01692"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2309.02432",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.02432v1",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "published": "2023-09-05T17:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2309.02710",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.02710v1",
      "title": "Improved Outlier Robust Seeding for k-means",
      "summary": "The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold.\n  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.\n  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \\cite{Charikar}, \\cite{KrishnaswamyLS18} and \\textit{robust $k$-means++} \\cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\\cite{tkmeanspp}, and robust $k$-means++~\\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \\cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \\cite{feldman2007ptas,langberg2010universal,feldman2011unified}.",
      "published": "2023-09-06T04:46:01Z"
    },
    "metadata": {
      "arxiv_id": "2309.02710",
      "title": "Improved Outlier Robust Seeding for k-means",
      "summary": "The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold.\n  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.\n  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \\cite{Charikar}, \\cite{KrishnaswamyLS18} and \\textit{robust $k$-means++} \\cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\\cite{tkmeanspp}, and robust $k$-means++~\\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \\cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \\cite{feldman2007ptas,langberg2010universal,feldman2011unified}.",
      "authors": [
        "Amit Deshpande",
        "Rameshwar Pratap"
      ],
      "published": "2023-09-06T04:46:01Z",
      "updated": "2023-09-06T04:46:01Z",
      "categories": [
        "cs.LG",
        "cs.CG",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02710v1",
      "landing_url": "https://arxiv.org/abs/2309.02710v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02710"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2309.04156",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.04156v2",
      "title": "Cross-Utterance Conditioned VAE for Speech Generation",
      "summary": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
      "published": "2023-09-08T06:48:41Z"
    },
    "metadata": {
      "arxiv_id": "2309.04156",
      "title": "Cross-Utterance Conditioned VAE for Speech Generation",
      "summary": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
      "authors": [
        "Yang Li",
        "Cheng Yu",
        "Guangzhi Sun",
        "Weiqin Zu",
        "Zheng Tian",
        "Ying Wen",
        "Wei Pan",
        "Chao Zhang",
        "Jun Wang",
        "Yang Yang",
        "Fanglei Sun"
      ],
      "published": "2023-09-08T06:48:41Z",
      "updated": "2024-09-19T13:41:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04156v2",
      "landing_url": "https://arxiv.org/abs/2309.04156v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.04156"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2309.04949",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.04949v1",
      "title": "A multiple k-means cluster ensemble framework for clustering citation trajectories",
      "summary": "Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles from the Microsoft Academic Graph data are considered for clustering short term (10 year) and long term (30 year) trajectories, respectively. It has linear run time. Four distinct trajectories are obtained Early Rise Rapid Decline (2.2%), Early Rise Slow Decline (45%), Delayed Rise No Decline (53%), and Delayed Rise Slow Decline (0.8%). Individual trajectory differences for two different spans are studied. Most papers exhibit Early Rise Slow Decline and Delayed Rise No Decline patterns. The growth and decay times, cumulative citation distribution, and peak characteristics of individual trajectories are redefined empirically. A detailed comparative study reveals our proposed methodology can detect all distinct trajectory classes.",
      "published": "2023-09-10T07:10:31Z"
    },
    "metadata": {
      "arxiv_id": "2309.04949",
      "title": "A multiple k-means cluster ensemble framework for clustering citation trajectories",
      "summary": "Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles from the Microsoft Academic Graph data are considered for clustering short term (10 year) and long term (30 year) trajectories, respectively. It has linear run time. Four distinct trajectories are obtained Early Rise Rapid Decline (2.2%), Early Rise Slow Decline (45%), Delayed Rise No Decline (53%), and Delayed Rise Slow Decline (0.8%). Individual trajectory differences for two different spans are studied. Most papers exhibit Early Rise Slow Decline and Delayed Rise No Decline patterns. The growth and decay times, cumulative citation distribution, and peak characteristics of individual trajectories are redefined empirically. A detailed comparative study reveals our proposed methodology can detect all distinct trajectory classes.",
      "authors": [
        "Joyita Chakraborty",
        "Dinesh K. Pradhan",
        "Subrata Nandi"
      ],
      "published": "2023-09-10T07:10:31Z",
      "updated": "2023-09-10T07:10:31Z",
      "categories": [
        "cs.SI",
        "cs.DB",
        "cs.DL",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04949v1",
      "landing_url": "https://arxiv.org/abs/2309.04949v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.04949"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2309.05027",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.05027v3",
      "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "published": "2023-09-10T13:47:39Z"
    },
    "metadata": {
      "arxiv_id": "2309.05027",
      "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Ziyang Ma",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-09-10T13:47:39Z",
      "updated": "2024-09-01T14:57:31Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05027v3",
      "landing_url": "https://arxiv.org/abs/2309.05027v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.05027"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2309.05224",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.05224v1",
      "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
      "summary": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
      "published": "2023-09-11T04:03:43Z"
    },
    "metadata": {
      "arxiv_id": "2309.05224",
      "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
      "summary": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
      "authors": [
        "Krisna Pinasthika",
        "Blessius Sheldo Putra Laksono",
        "Riyandi Banovbi Putera Irsal",
        "Syifa Hukma Shabiyya",
        "Novanto Yudistira"
      ],
      "published": "2023-09-11T04:03:43Z",
      "updated": "2023-09-11T04:03:43Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05224v1",
      "landing_url": "https://arxiv.org/abs/2309.05224v1",
      "doi": "https://doi.org/10.1016/j.neucom.2024.127433"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2309.06787",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.06787v1",
      "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
      "summary": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
      "published": "2023-09-13T08:22:38Z"
    },
    "metadata": {
      "arxiv_id": "2309.06787",
      "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
      "summary": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
      "authors": [
        "Zhichao Wu",
        "Qiulin Li",
        "Sixing Liu",
        "Qun Yang"
      ],
      "published": "2023-09-13T08:22:38Z",
      "updated": "2023-09-13T08:22:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06787v1",
      "landing_url": "https://arxiv.org/abs/2309.06787v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06787"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2309.07478",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07478v1",
      "title": "Direct Text to Speech Translation System using Acoustic Units",
      "summary": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
      "published": "2023-09-14T07:35:14Z"
    },
    "metadata": {
      "arxiv_id": "2309.07478",
      "title": "Direct Text to Speech Translation System using Acoustic Units",
      "summary": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
      "authors": [
        "Victoria Mingote",
        "Pablo Gimeno",
        "Luis Vicente",
        "Sameer Khurana",
        "Antoine Laurent",
        "Jarod Duret"
      ],
      "published": "2023-09-14T07:35:14Z",
      "updated": "2023-09-14T07:35:14Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07478v1",
      "landing_url": "https://arxiv.org/abs/2309.07478v1",
      "doi": "https://doi.org/10.1109/LSP.2023.3313513"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2309.07937",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07937v3",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "published": "2023-09-14T03:13:18Z"
    },
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.08166",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.08166v2",
      "title": "Residual Speaker Representation for One-Shot Voice Conversion",
      "summary": "Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach that leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. Introducing multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available.",
      "published": "2023-09-15T05:27:21Z"
    },
    "metadata": {
      "arxiv_id": "2309.08166",
      "title": "Residual Speaker Representation for One-Shot Voice Conversion",
      "summary": "Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach that leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. Introducing multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available.",
      "authors": [
        "Le Xu",
        "Jiangyan Yi",
        "Tao Wang",
        "Yong Ren",
        "Rongxiu Zhong",
        "Zhengqi Wen",
        "Jianhua Tao"
      ],
      "published": "2023-09-15T05:27:21Z",
      "updated": "2024-08-11T16:40:07Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08166v2",
      "landing_url": "https://arxiv.org/abs/2309.08166v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.08166"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2309.09470",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09470v1",
      "title": "Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment",
      "summary": "This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.",
      "published": "2023-09-18T04:08:02Z"
    },
    "metadata": {
      "arxiv_id": "2309.09470",
      "title": "Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment",
      "summary": "This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.",
      "authors": [
        "Zheng-Yan Sheng",
        "Yang Ai",
        "Yan-Nian Chen",
        "Zhen-Hua Ling"
      ],
      "published": "2023-09-18T04:08:02Z",
      "updated": "2023-09-18T04:08:02Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09470v1",
      "landing_url": "https://arxiv.org/abs/2309.09470v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09470"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2309.09630",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09630v1",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "published": "2023-09-18T10:05:41Z"
    },
    "metadata": {
      "arxiv_id": "2309.09630",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "authors": [
        "Julitta Bartolewska",
        "Stanisław Kacprzak",
        "Konrad Kowalczyk"
      ],
      "published": "2023-09-18T10:05:41Z",
      "updated": "2023-09-18T10:05:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09630v1",
      "landing_url": "https://arxiv.org/abs/2309.09630v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10632"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2309.10379",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10379v1",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "published": "2023-09-19T07:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2309.10379",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "authors": [
        "Jiahui Pan",
        "Shulin He",
        "Tianci Wu",
        "Hui Zhang",
        "Xueliang Zhang"
      ],
      "published": "2023-09-19T07:27:38Z",
      "updated": "2023-09-19T07:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10379v1",
      "landing_url": "https://arxiv.org/abs/2309.10379v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10379"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2309.10818",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10818v3",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "published": "2023-09-19T17:59:54Z"
    },
    "metadata": {
      "arxiv_id": "2309.10818",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "authors": [
        "Zhiqiang Shen",
        "Tianhua Tao",
        "Liqun Ma",
        "Willie Neiswanger",
        "Zhengzhong Liu",
        "Hongyi Wang",
        "Bowen Tan",
        "Joel Hestness",
        "Natalia Vassilieva",
        "Daria Soboleva",
        "Eric Xing"
      ],
      "published": "2023-09-19T17:59:54Z",
      "updated": "2024-05-09T13:56:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10818v3",
      "landing_url": "https://arxiv.org/abs/2309.10818v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.10818"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2309.11641",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11641v2",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "published": "2023-09-20T21:11:36Z"
    },
    "metadata": {
      "arxiv_id": "2309.11641",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "authors": [
        "Angello Hoyos",
        "Mariano Rivera"
      ],
      "published": "2023-09-20T21:11:36Z",
      "updated": "2024-02-08T20:52:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11641v2",
      "landing_url": "https://arxiv.org/abs/2309.11641v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.11641"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2309.11977",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11977v3",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "published": "2023-09-21T11:22:22Z"
    },
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2309.14324",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14324v2",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "published": "2023-09-25T17:52:09Z"
    },
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2309.15505",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.15505v2",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "published": "2023-09-27T09:13:40Z"
    },
    "metadata": {
      "arxiv_id": "2309.15505",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "authors": [
        "Fabian Mentzer",
        "David Minnen",
        "Eirikur Agustsson",
        "Michael Tschannen"
      ],
      "published": "2023-09-27T09:13:40Z",
      "updated": "2023-10-12T07:55:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15505v2",
      "landing_url": "https://arxiv.org/abs/2309.15505v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.15505"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2309.16384",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.16384v2",
      "title": "Multi-Swap $k$-Means++",
      "summary": "The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.",
      "published": "2023-09-28T12:31:35Z"
    },
    "metadata": {
      "arxiv_id": "2309.16384",
      "title": "Multi-Swap $k$-Means++",
      "summary": "The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.",
      "authors": [
        "Lorenzo Beretta",
        "Vincent Cohen-Addad",
        "Silvio Lattanzi",
        "Nikos Parotsidis"
      ],
      "published": "2023-09-28T12:31:35Z",
      "updated": "2024-10-25T18:14:44Z",
      "categories": [
        "cs.CG",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16384v2",
      "landing_url": "https://arxiv.org/abs/2309.16384v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16384"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2309.16482",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.16482v2",
      "title": "Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization",
      "summary": "We propose a modular pipeline for the single-channel separation, recognition, and diarization of meeting-style recordings and evaluate it on the Libri-CSS dataset. Using a Continuous Speech Separation (CSS) system with a TF-GridNet separation architecture, followed by a speaker-agnostic speech recognizer, we achieve state-of-the-art recognition performance in terms of Optimal Reference Combination Word Error Rate (ORC WER). Then, a d-vector-based diarization module is employed to extract speaker embeddings from the enhanced signals and to assign the CSS outputs to the correct speaker. Here, we propose a syntactically informed diarization using sentence- and word-level boundaries of the ASR module to support speaker turn detection. This results in a state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) for the full meeting recognition pipeline.",
      "published": "2023-09-28T14:45:46Z"
    },
    "metadata": {
      "arxiv_id": "2309.16482",
      "title": "Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization",
      "summary": "We propose a modular pipeline for the single-channel separation, recognition, and diarization of meeting-style recordings and evaluate it on the Libri-CSS dataset. Using a Continuous Speech Separation (CSS) system with a TF-GridNet separation architecture, followed by a speaker-agnostic speech recognizer, we achieve state-of-the-art recognition performance in terms of Optimal Reference Combination Word Error Rate (ORC WER). Then, a d-vector-based diarization module is employed to extract speaker embeddings from the enhanced signals and to assign the CSS outputs to the correct speaker. Here, we propose a syntactically informed diarization using sentence- and word-level boundaries of the ASR module to support speaker turn detection. This results in a state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) for the full meeting recognition pipeline.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Tobias Cord-Landwehr",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-09-28T14:45:46Z",
      "updated": "2024-05-06T07:09:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16482v2",
      "landing_url": "https://arxiv.org/abs/2309.16482v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16482"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2310.01381",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.01381v3",
      "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
      "summary": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
      "published": "2023-10-02T17:42:22Z"
    },
    "metadata": {
      "arxiv_id": "2310.01381",
      "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
      "summary": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
      "authors": [
        "Roi Benita",
        "Michael Elad",
        "Joseph Keshet"
      ],
      "published": "2023-10-02T17:42:22Z",
      "updated": "2024-03-10T22:31:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01381v3",
      "landing_url": "https://arxiv.org/abs/2310.01381v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.01381"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2310.03639",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.03639v2",
      "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages",
      "summary": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
      "published": "2023-10-05T16:11:14Z"
    },
    "metadata": {
      "arxiv_id": "2310.03639",
      "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages",
      "summary": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
      "authors": [
        "Chih-Chen Chen",
        "William Chen",
        "Rodolfo Zevallos",
        "John E. Ortega"
      ],
      "published": "2023-10-05T16:11:14Z",
      "updated": "2023-10-08T23:28:50Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03639v2",
      "landing_url": "https://arxiv.org/abs/2310.03639v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.03639"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.04358",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.04358v1",
      "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
      "summary": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
      "published": "2023-10-06T16:28:07Z"
    },
    "metadata": {
      "arxiv_id": "2310.04358",
      "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
      "summary": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
      "authors": [
        "Ziyun Cui",
        "Wen Wu",
        "Wei-Qiang Zhang",
        "Ji Wu",
        "Chao Zhang"
      ],
      "published": "2023-10-06T16:28:07Z",
      "updated": "2023-10-06T16:28:07Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04358v1",
      "landing_url": "https://arxiv.org/abs/2310.04358v1",
      "doi": "https://doi.org/10.1109/ASRU57964.2023.10389785"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2310.05203",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.05203v1",
      "title": "A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023",
      "summary": "This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.",
      "published": "2023-10-08T15:30:44Z"
    },
    "metadata": {
      "arxiv_id": "2310.05203",
      "title": "A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023",
      "summary": "This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.",
      "authors": [
        "Ryuichi Yamamoto",
        "Reo Yoneyama",
        "Lester Phillip Violeta",
        "Wen-Chin Huang",
        "Tomoki Toda"
      ],
      "published": "2023-10-08T15:30:44Z",
      "updated": "2023-10-08T15:30:44Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05203v1",
      "landing_url": "https://arxiv.org/abs/2310.05203v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.05203"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2310.07161",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.07161v3",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "published": "2023-10-11T03:19:22Z"
    },
    "metadata": {
      "arxiv_id": "2310.07161",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "authors": [
        "Joseph Konan",
        "Shikhar Agnihotri",
        "Ojas Bhargave",
        "Shuo Han",
        "Yunyang Zeng",
        "Ankit Shah",
        "Bhiksha Raj"
      ],
      "published": "2023-10-11T03:19:22Z",
      "updated": "2024-08-01T11:37:16Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07161v3",
      "landing_url": "https://arxiv.org/abs/2310.07161v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.07161"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.07246",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.07246v2",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "published": "2023-10-11T07:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2310.07246",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "authors": [
        "Xinfa Zhu",
        "Yuanjun Lv",
        "Yi Lei",
        "Tao Li",
        "Wendi He",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-10-11T07:23:27Z",
      "updated": "2023-10-12T05:49:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07246v2",
      "landing_url": "https://arxiv.org/abs/2310.07246v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.07246"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.08104",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08104v1",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "published": "2023-10-12T08:00:25Z"
    },
    "metadata": {
      "arxiv_id": "2310.08104",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "authors": [
        "Matthew Baas",
        "Herman Kamper"
      ],
      "published": "2023-10-12T08:00:25Z",
      "updated": "2023-10-12T08:00:25Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08104v1",
      "landing_url": "https://arxiv.org/abs/2310.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08104"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2310.08225",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08225v2",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "published": "2023-10-12T11:17:40Z"
    },
    "metadata": {
      "arxiv_id": "2310.08225",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "authors": [
        "Chanho Park",
        "Chengsong Lu",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2023-10-12T11:17:40Z",
      "updated": "2025-01-29T11:28:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08225v2",
      "landing_url": "https://arxiv.org/abs/2310.08225v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.08225"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2310.08696",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08696v1",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "published": "2023-10-12T20:02:07Z"
    },
    "metadata": {
      "arxiv_id": "2310.08696",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "authors": [
        "Weiqing Wang",
        "Ming Li"
      ],
      "published": "2023-10-12T20:02:07Z",
      "updated": "2023-10-12T20:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08696v1",
      "landing_url": "https://arxiv.org/abs/2310.08696v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08696"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2310.08981",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08981v3",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "published": "2023-10-13T09:57:09Z"
    },
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2310.09382",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09382v1",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "published": "2023-10-13T20:03:18Z"
    },
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2310.09653",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09653v2",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "published": "2023-10-14T19:51:17Z"
    },
    "metadata": {
      "arxiv_id": "2310.09653",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Rafael Valle",
        "Boris Ginsburg",
        "Rishabh Ranjan",
        "Shlomo Dubnov",
        "Farinaz Koushanfar",
        "Julian McAuley"
      ],
      "published": "2023-10-14T19:51:17Z",
      "updated": "2024-05-03T16:45:39Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09653v2",
      "landing_url": "https://arxiv.org/abs/2310.09653v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.09653"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2310.10922",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10922v1",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "published": "2023-10-17T01:31:59Z"
    },
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2310.11541",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.11541v1",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "published": "2023-10-17T19:27:23Z"
    },
    "metadata": {
      "arxiv_id": "2310.11541",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "authors": [
        "Noé Tits"
      ],
      "published": "2023-10-17T19:27:23Z",
      "updated": "2023-10-17T19:27:23Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11541v1",
      "landing_url": "https://arxiv.org/abs/2310.11541v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.11541"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.14044",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14044v2",
      "title": "Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models",
      "summary": "Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion.",
      "published": "2023-10-21T15:41:50Z"
    },
    "metadata": {
      "arxiv_id": "2310.14044",
      "title": "Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models",
      "summary": "Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion.",
      "authors": [
        "Jincheng Zhang",
        "György Fazekas",
        "Charalampos Saitis"
      ],
      "published": "2023-10-21T15:41:50Z",
      "updated": "2024-09-03T19:14:25Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14044v2",
      "landing_url": "https://arxiv.org/abs/2310.14044v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14044"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2310.14580",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14580v4",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "published": "2023-10-23T05:38:41Z"
    },
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2310.14837",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14837v1",
      "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
      "summary": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
      "published": "2023-10-23T11:57:44Z"
    },
    "metadata": {
      "arxiv_id": "2310.14837",
      "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
      "summary": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
      "authors": [
        "Daniel Biermann",
        "Fabrizio Palumbo",
        "Morten Goodwin",
        "Ole-Christoffer Granmo"
      ],
      "published": "2023-10-23T11:57:44Z",
      "updated": "2023-10-23T11:57:44Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14837v1",
      "landing_url": "https://arxiv.org/abs/2310.14837v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.14837"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2310.14858",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14858v2",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "published": "2023-10-23T12:28:21Z"
    },
    "metadata": {
      "arxiv_id": "2310.14858",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "authors": [
        "Patrick Holzer",
        "Tania Jacob",
        "Shubham Kavane"
      ],
      "published": "2023-10-23T12:28:21Z",
      "updated": "2023-11-17T10:35:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14858v2",
      "landing_url": "https://arxiv.org/abs/2310.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14858"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2310.15399",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.15399v3",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "published": "2023-10-23T23:01:33Z"
    },
    "metadata": {
      "arxiv_id": "2310.15399",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "authors": [
        "Ayako Yamamoto",
        "Toshio Irino",
        "Fuki Miyazaki",
        "Honoka Tamaru"
      ],
      "published": "2023-10-23T23:01:33Z",
      "updated": "2024-03-14T02:14:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15399v3",
      "landing_url": "https://arxiv.org/abs/2310.15399v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.15399"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.16550",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.16550v1",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "published": "2023-10-25T11:04:32Z"
    },
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.16621",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.16621v1",
      "title": "ArTST: Arabic Text and Speech Transformer",
      "summary": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
      "published": "2023-10-25T13:20:54Z"
    },
    "metadata": {
      "arxiv_id": "2310.16621",
      "title": "ArTST: Arabic Text and Speech Transformer",
      "summary": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
      "authors": [
        "Hawau Olamide Toyin",
        "Amirbek Djanibekov",
        "Ajinkya Kulkarni",
        "Hanan Aldarmaki"
      ],
      "published": "2023-10-25T13:20:54Z",
      "updated": "2023-10-25T13:20:54Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16621v1",
      "landing_url": "https://arxiv.org/abs/2310.16621v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16621"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2310.17142",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.17142v1",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "published": "2023-10-26T04:29:27Z"
    },
    "metadata": {
      "arxiv_id": "2310.17142",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "authors": [
        "Sania Gul",
        "Muhammad Salman Khan",
        "Muhammad Fazeel"
      ],
      "published": "2023-10-26T04:29:27Z",
      "updated": "2023-10-26T04:29:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.17142v1",
      "landing_url": "https://arxiv.org/abs/2310.17142v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.17142"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2310.18967",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.18967v1",
      "title": "Pressure influence on excitonic luminescence of CsPbBr3 perovskite",
      "summary": "This study investigates the effect of hydrostatic pressure on the luminescence properties of CsPbBr3 single crystals at 12 K. The luminescence at the edge of the band gap reveals a structure attributed to free excitons, phonon replica of the free excitons, and Rashba excitons. Changes in the relative intensity of the free and Rashba excitons were observed with increasing pressure, caused by changes in the probability of nonradiative deexcitation. At pressures around 3 GPa, luminescence completely fades away. The red shift of the energy position of the maximum luminescence of free and Rashba excitons in pressure ranges of 0-1.3 GPa is attributed to the length reduction of Pb-Br bonds in [PbBr6]4- octahedra, while the high-energy shift of the Rashba excitons at pressures above 1.3 GPa is due to [PbBr6]4- octahedra rotation and changes in the Pb-Br_Pb angle.",
      "published": "2023-10-29T10:17:41Z"
    },
    "metadata": {
      "arxiv_id": "2310.18967",
      "title": "Pressure influence on excitonic luminescence of CsPbBr3 perovskite",
      "summary": "This study investigates the effect of hydrostatic pressure on the luminescence properties of CsPbBr3 single crystals at 12 K. The luminescence at the edge of the band gap reveals a structure attributed to free excitons, phonon replica of the free excitons, and Rashba excitons. Changes in the relative intensity of the free and Rashba excitons were observed with increasing pressure, caused by changes in the probability of nonradiative deexcitation. At pressures around 3 GPa, luminescence completely fades away. The red shift of the energy position of the maximum luminescence of free and Rashba excitons in pressure ranges of 0-1.3 GPa is attributed to the length reduction of Pb-Br bonds in [PbBr6]4- octahedra, while the high-energy shift of the Rashba excitons at pressures above 1.3 GPa is due to [PbBr6]4- octahedra rotation and changes in the Pb-Br_Pb angle.",
      "authors": [
        "Lev Ivan Bulyk",
        "Taras Demkiv",
        "Oleh Antonyak",
        "Yaroslav M. Chornodolskyy",
        "Roman Gamernyk",
        "Andrzej Suchocki",
        "Anatolii Voloshinovskii"
      ],
      "published": "2023-10-29T10:17:41Z",
      "updated": "2023-10-29T10:17:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.18967v1",
      "landing_url": "https://arxiv.org/abs/2310.18967v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.18967"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2311.00873",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.00873v1",
      "title": "Low-latency Real-time Voice Conversion on CPU",
      "summary": "We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\\textbf{L}$ow-latency $\\textbf{L}$ow-resource $\\textbf{V}$oice $\\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.",
      "published": "2023-11-01T21:57:52Z"
    },
    "metadata": {
      "arxiv_id": "2311.00873",
      "title": "Low-latency Real-time Voice Conversion on CPU",
      "summary": "We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\\textbf{L}$ow-latency $\\textbf{L}$ow-resource $\\textbf{V}$oice $\\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.",
      "authors": [
        "Konstantine Sadov",
        "Matthew Hutter",
        "Asara Near"
      ],
      "published": "2023-11-01T21:57:52Z",
      "updated": "2023-11-01T21:57:52Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00873v1",
      "landing_url": "https://arxiv.org/abs/2311.00873v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00873"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2311.00945",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.00945v1",
      "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
      "summary": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "published": "2023-11-02T02:22:21Z"
    },
    "metadata": {
      "arxiv_id": "2311.00945",
      "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
      "summary": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "authors": [
        "Yuan Gao",
        "Nobuyuki Morioka",
        "Yu Zhang",
        "Nanxin Chen"
      ],
      "published": "2023-11-02T02:22:21Z",
      "updated": "2023-11-02T02:22:21Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00945v1",
      "landing_url": "https://arxiv.org/abs/2311.00945v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00945"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2311.01635",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.01635v1",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "published": "2023-11-02T23:12:42Z"
    },
    "metadata": {
      "arxiv_id": "2311.01635",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "authors": [
        "Cheng Luo",
        "Tianle Zhong",
        "Geoffrey Fox"
      ],
      "published": "2023-11-02T23:12:42Z",
      "updated": "2023-11-02T23:12:42Z",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01635v1",
      "landing_url": "https://arxiv.org/abs/2311.01635v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01635"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2311.02898",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.02898v2",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "published": "2023-11-06T06:13:39Z"
    },
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2311.03389",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03389v4",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "published": "2023-11-04T04:54:17Z"
    },
    "metadata": {
      "arxiv_id": "2311.03389",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-11-04T04:54:17Z",
      "updated": "2025-01-11T06:05:41Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03389v4",
      "landing_url": "https://arxiv.org/abs/2311.03389v4",
      "doi": "https://doi.org/10.48550/arXiv.2311.03389"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2311.03792",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03792v1",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "published": "2023-11-07T08:20:06Z"
    },
    "metadata": {
      "arxiv_id": "2311.03792",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "authors": [
        "Jakir Hasan",
        "Shrestha Datta",
        "Ameya Debnath"
      ],
      "published": "2023-11-07T08:20:06Z",
      "updated": "2023-11-07T08:20:06Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03792v1",
      "landing_url": "https://arxiv.org/abs/2311.03792v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03792"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2311.04534",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.04534v2",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "published": "2023-11-08T08:45:14Z"
    },
    "metadata": {
      "arxiv_id": "2311.04534",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "published": "2023-11-08T08:45:14Z",
      "updated": "2024-02-05T02:42:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04534v2",
      "landing_url": "https://arxiv.org/abs/2311.04534v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.04534"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2311.08104",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.08104v1",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "published": "2023-11-14T12:03:46Z"
    },
    "metadata": {
      "arxiv_id": "2311.08104",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "authors": [
        "Anders R. Bargum",
        "Stefania Serafin",
        "Cumhur Erkut"
      ],
      "published": "2023-11-14T12:03:46Z",
      "updated": "2023-11-14T12:03:46Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08104v1",
      "landing_url": "https://arxiv.org/abs/2311.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08104"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2311.08670",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.08670v1",
      "title": "CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation",
      "summary": "Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.",
      "published": "2023-11-15T03:29:31Z"
    },
    "metadata": {
      "arxiv_id": "2311.08670",
      "title": "CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation",
      "summary": "Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.",
      "authors": [
        "Yimin Deng",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-11-15T03:29:31Z",
      "updated": "2023-11-15T03:29:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08670v1",
      "landing_url": "https://arxiv.org/abs/2311.08670v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08670"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2311.10525",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10525v1",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "published": "2023-11-17T13:45:31Z"
    },
    "metadata": {
      "arxiv_id": "2311.10525",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "authors": [
        "Junliang Wang",
        "Qinghua Zhang",
        "Guanhua Zhu",
        "Guoxi Sun"
      ],
      "published": "2023-11-17T13:45:31Z",
      "updated": "2023-11-17T13:45:31Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10525v1",
      "landing_url": "https://arxiv.org/abs/2311.10525v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.10525"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2311.13588",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.13588v1",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "published": "2023-11-22T18:49:00Z"
    },
    "metadata": {
      "arxiv_id": "2311.13588",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "authors": [
        "Wei Qiu",
        "Marcin Copik",
        "Yun Wang",
        "Alexandru Calotoiu",
        "Torsten Hoefler"
      ],
      "published": "2023-11-22T18:49:00Z",
      "updated": "2023-11-22T18:49:00Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.13588v1",
      "landing_url": "https://arxiv.org/abs/2311.13588v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.13588"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2311.17264",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.17264v1",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "published": "2023-11-28T22:54:33Z"
    },
    "metadata": {
      "arxiv_id": "2311.17264",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "authors": [
        "Marina Zhang",
        "Owen Vallis",
        "Aysegul Bumin",
        "Tanay Vakharia",
        "Elie Bursztein"
      ],
      "published": "2023-11-28T22:54:33Z",
      "updated": "2023-11-28T22:54:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17264v1",
      "landing_url": "https://arxiv.org/abs/2311.17264v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17264"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2312.02116",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.02116v4",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "published": "2023-12-04T18:48:02Z"
    },
    "metadata": {
      "arxiv_id": "2312.02116",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "authors": [
        "Michael Tschannen",
        "Cian Eastwood",
        "Fabian Mentzer"
      ],
      "published": "2023-12-04T18:48:02Z",
      "updated": "2024-07-17T16:32:09Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02116v4",
      "landing_url": "https://arxiv.org/abs/2312.02116v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.02116"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2312.02147",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.02147v2",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "published": "2023-12-04T18:59:20Z"
    },
    "metadata": {
      "arxiv_id": "2312.02147",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "authors": [
        "Sucheng Ren",
        "Zeyu Wang",
        "Hongru Zhu",
        "Junfei Xiao",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "published": "2023-12-04T18:59:20Z",
      "updated": "2024-07-05T05:07:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02147v2",
      "landing_url": "https://arxiv.org/abs/2312.02147v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.02147"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2312.03406",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.03406v4",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "published": "2023-12-06T10:42:40Z"
    },
    "metadata": {
      "arxiv_id": "2312.03406",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "authors": [
        "Chao Chen",
        "Tian Zhou",
        "Yanjun Zhao",
        "Hui Liu",
        "Liang Sun",
        "Rong Jin"
      ],
      "published": "2023-12-06T10:42:40Z",
      "updated": "2025-05-18T09:11:15Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03406v4",
      "landing_url": "https://arxiv.org/abs/2312.03406v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.03406"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2312.04919",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.04919v2",
      "title": "Neural Concatenative Singing Voice Conversion: Rethinking Concatenation-Based Approach for One-Shot Singing Voice Conversion",
      "summary": "Any-to-any singing voice conversion (SVC) is confronted with the challenge of ``timbre leakage'' issue caused by inadequate disentanglement between the content and the speaker timbre. To address this issue, this study introduces NeuCoSVC, a novel neural concatenative SVC framework. It consists of a self-supervised learning (SSL) representation extractor, a neural harmonic signal generator, and a waveform synthesizer. The SSL extractor condenses audio into fixed-dimensional SSL features, while the harmonic signal generator leverages linear time-varying filters to produce both raw and filtered harmonic signals for pitch information. The synthesizer reconstructs waveforms using SSL features, harmonic signals, and loudness information. During inference, voice conversion is performed by substituting source SSL features with their nearest counterparts from a matching pool which comprises SSL features extracted from the reference audio, while preserving raw harmonic signals and loudness from the source audio. By directly utilizing SSL features from the reference audio, the proposed framework effectively resolves the ``timbre leakage\" issue caused by previous disentanglement-based approaches. Experimental results demonstrate that the proposed NeuCoSVC system outperforms the disentanglement-based speaker embedding approach in one-shot SVC across intra-language, cross-language, and cross-domain evaluations.",
      "published": "2023-12-08T09:35:08Z"
    },
    "metadata": {
      "arxiv_id": "2312.04919",
      "title": "Neural Concatenative Singing Voice Conversion: Rethinking Concatenation-Based Approach for One-Shot Singing Voice Conversion",
      "summary": "Any-to-any singing voice conversion (SVC) is confronted with the challenge of ``timbre leakage'' issue caused by inadequate disentanglement between the content and the speaker timbre. To address this issue, this study introduces NeuCoSVC, a novel neural concatenative SVC framework. It consists of a self-supervised learning (SSL) representation extractor, a neural harmonic signal generator, and a waveform synthesizer. The SSL extractor condenses audio into fixed-dimensional SSL features, while the harmonic signal generator leverages linear time-varying filters to produce both raw and filtered harmonic signals for pitch information. The synthesizer reconstructs waveforms using SSL features, harmonic signals, and loudness information. During inference, voice conversion is performed by substituting source SSL features with their nearest counterparts from a matching pool which comprises SSL features extracted from the reference audio, while preserving raw harmonic signals and loudness from the source audio. By directly utilizing SSL features from the reference audio, the proposed framework effectively resolves the ``timbre leakage\" issue caused by previous disentanglement-based approaches. Experimental results demonstrate that the proposed NeuCoSVC system outperforms the disentanglement-based speaker embedding approach in one-shot SVC across intra-language, cross-language, and cross-domain evaluations.",
      "authors": [
        "Binzhu Sha",
        "Xu Li",
        "Zhiyong Wu",
        "Ying Shan",
        "Helen Meng"
      ],
      "published": "2023-12-08T09:35:08Z",
      "updated": "2024-01-08T13:36:36Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04919v2",
      "landing_url": "https://arxiv.org/abs/2312.04919v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.04919"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2312.07759",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.07759v2",
      "title": "IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable k-Means",
      "summary": "Compressing large neural networks with minimal performance loss is crucial to enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight quantization method that uses an attention-based clustering algorithm called differentiable $k$-means (DKM). Despite achieving state-of-the-art results, DKM's performance is constrained by its heavy memory dependency. We propose an implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$ be the number of weight-vectors, and $b$ be the number of bits per cluster address. IDKM reduces the overall memory complexity of a single $k$-means layer from $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for which the time complexity of the gradient calculation is independent of $t$ as well. We provide a proof of concept of our methods by showing that, under the same settings, IDKM achieves comparable performance to DKM with less compute time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural network, Resnet18, on hardware where DKM cannot train at all.",
      "published": "2023-12-12T22:02:57Z"
    },
    "metadata": {
      "arxiv_id": "2312.07759",
      "title": "IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable k-Means",
      "summary": "Compressing large neural networks with minimal performance loss is crucial to enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight quantization method that uses an attention-based clustering algorithm called differentiable $k$-means (DKM). Despite achieving state-of-the-art results, DKM's performance is constrained by its heavy memory dependency. We propose an implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$ be the number of weight-vectors, and $b$ be the number of bits per cluster address. IDKM reduces the overall memory complexity of a single $k$-means layer from $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for which the time complexity of the gradient calculation is independent of $t$ as well. We provide a proof of concept of our methods by showing that, under the same settings, IDKM achieves comparable performance to DKM with less compute time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural network, Resnet18, on hardware where DKM cannot train at all.",
      "authors": [
        "Sean Jaffe",
        "Ambuj K. Singh",
        "Francesco Bullo"
      ],
      "published": "2023-12-12T22:02:57Z",
      "updated": "2023-12-15T21:46:10Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07759v2",
      "landing_url": "https://arxiv.org/abs/2312.07759v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.07759"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2312.08309",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08309v1",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "published": "2023-12-13T17:27:17Z"
    },
    "metadata": {
      "arxiv_id": "2312.08309",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "authors": [
        "Sabbir Ahmed",
        "Md Nahiduzzaman",
        "Tariqul Islam",
        "Faisal Haque Bappy",
        "Tarannum Shaila Zaman",
        "Raiful Hasan"
      ],
      "published": "2023-12-13T17:27:17Z",
      "updated": "2023-12-13T17:27:17Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08309v1",
      "landing_url": "https://arxiv.org/abs/2312.08309v1",
      "doi": "https://doi.org/10.1109/CCNC51664.2024.10454894"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2312.08676",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08676v2",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "published": "2023-12-14T06:26:55Z"
    },
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2312.09128",
    "anchor": "semantic tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09128v2",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "published": "2023-12-14T17:01:02Z"
    },
    "metadata": {
      "arxiv_id": "2312.09128",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "authors": [
        "Ting Pan",
        "Lulu Tang",
        "Xinlong Wang",
        "Shiguang Shan"
      ],
      "published": "2023-12-14T17:01:02Z",
      "updated": "2024-07-17T04:34:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09128v2",
      "landing_url": "https://arxiv.org/abs/2312.09128v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09128"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2312.09469",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09469v1",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "published": "2023-09-29T18:35:52Z"
    },
    "metadata": {
      "arxiv_id": "2312.09469",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "authors": [
        "Isotta Landi",
        "Eugenia Alleva",
        "Alissa A. Valentine",
        "Lauren A. Lepow",
        "Alexander W. Charney"
      ],
      "published": "2023-09-29T18:35:52Z",
      "updated": "2023-09-29T18:35:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09469v1",
      "landing_url": "https://arxiv.org/abs/2312.09469v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09469"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2312.09727",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09727v1",
      "title": "LiteVSR: Efficient Visual Speech Recognition by Learning from Speech Representations of Unlabeled Data",
      "summary": "This paper proposes a novel, resource-efficient approach to Visual Speech Recognition (VSR) leveraging speech representations produced by any trained Automatic Speech Recognition (ASR) model. Moving away from the resource-intensive trends prevalent in recent literature, our method distills knowledge from a trained Conformer-based ASR model, achieving competitive performance on standard VSR benchmarks with significantly less resource utilization. Using unlabeled audio-visual data only, our baseline model achieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test benchmarks, respectively. After fine-tuning the model with limited labeled data, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can be trained on a single consumer-grade GPU within a few days and is capable of performing real-time end-to-end VSR on dated hardware, suggesting a path towards more accessible and resource-efficient VSR methodologies.",
      "published": "2023-12-15T12:04:24Z"
    },
    "metadata": {
      "arxiv_id": "2312.09727",
      "title": "LiteVSR: Efficient Visual Speech Recognition by Learning from Speech Representations of Unlabeled Data",
      "summary": "This paper proposes a novel, resource-efficient approach to Visual Speech Recognition (VSR) leveraging speech representations produced by any trained Automatic Speech Recognition (ASR) model. Moving away from the resource-intensive trends prevalent in recent literature, our method distills knowledge from a trained Conformer-based ASR model, achieving competitive performance on standard VSR benchmarks with significantly less resource utilization. Using unlabeled audio-visual data only, our baseline model achieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test benchmarks, respectively. After fine-tuning the model with limited labeled data, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can be trained on a single consumer-grade GPU within a few days and is capable of performing real-time end-to-end VSR on dated hardware, suggesting a path towards more accessible and resource-efficient VSR methodologies.",
      "authors": [
        "Hendrik Laux",
        "Emil Mededovic",
        "Ahmed Hallawa",
        "Lukas Martin",
        "Arne Peine",
        "Anke Schmeink"
      ],
      "published": "2023-12-15T12:04:24Z",
      "updated": "2023-12-15T12:04:24Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09727v1",
      "landing_url": "https://arxiv.org/abs/2312.09727v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09727"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2312.09747",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09747v2",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "published": "2023-12-15T12:36:05Z"
    },
    "metadata": {
      "arxiv_id": "2312.09747",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "YuanJun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "published": "2023-12-15T12:36:05Z",
      "updated": "2024-01-07T09:02:52Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09747v2",
      "landing_url": "https://arxiv.org/abs/2312.09747v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09747"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2312.09911",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09911v3",
      "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
      "summary": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
      "published": "2023-12-15T16:23:21Z"
    },
    "metadata": {
      "arxiv_id": "2312.09911",
      "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
      "summary": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
      "authors": [
        "Xueyao Zhang",
        "Liumeng Xue",
        "Yicheng Gu",
        "Yuancheng Wang",
        "Jiaqi Li",
        "Haorui He",
        "Chaoren Wang",
        "Songting Liu",
        "Xi Chen",
        "Junan Zhang",
        "Zihao Fang",
        "Haopeng Chen",
        "Tze Ying Tang",
        "Lexiao Zou",
        "Mingxuan Wang",
        "Jun Han",
        "Kai Chen",
        "Haizhou Li",
        "Zhizheng Wu"
      ],
      "published": "2023-12-15T16:23:21Z",
      "updated": "2024-09-16T11:35:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09911v3",
      "landing_url": "https://arxiv.org/abs/2312.09911v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.09911"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2312.11532",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.11532v2",
      "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
      "summary": "This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clovaai/TVQ-VAE.",
      "published": "2023-12-15T15:01:10Z"
    },
    "metadata": {
      "arxiv_id": "2312.11532",
      "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
      "summary": "This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clovaai/TVQ-VAE.",
      "authors": [
        "YoungJoon Yoo",
        "Jongwon Choi"
      ],
      "published": "2023-12-15T15:01:10Z",
      "updated": "2024-01-21T09:30:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.11532v2",
      "landing_url": "https://arxiv.org/abs/2312.11532v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.11532"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2312.12181",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.12181v1",
      "title": "StyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based Pre-training for Expressive Audiobook Speech Synthesis",
      "summary": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios.",
      "published": "2023-12-19T14:13:26Z"
    },
    "metadata": {
      "arxiv_id": "2312.12181",
      "title": "StyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based Pre-training for Expressive Audiobook Speech Synthesis",
      "summary": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios.",
      "authors": [
        "Xueyuan Chen",
        "Xi Wang",
        "Shaofei Zhang",
        "Lei He",
        "Zhiyong Wu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-12-19T14:13:26Z",
      "updated": "2023-12-19T14:13:26Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12181v1",
      "landing_url": "https://arxiv.org/abs/2312.12181v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.12181"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2312.12917",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.12917v1",
      "title": "Sign Language Production with Latent Motion Transformer",
      "summary": "Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we've developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video's hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL.",
      "published": "2023-12-20T10:53:06Z"
    },
    "metadata": {
      "arxiv_id": "2312.12917",
      "title": "Sign Language Production with Latent Motion Transformer",
      "summary": "Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we've developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video's hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL.",
      "authors": [
        "Pan Xie",
        "Taiyi Peng",
        "Yao Du",
        "Qipeng Zhang"
      ],
      "published": "2023-12-20T10:53:06Z",
      "updated": "2023-12-20T10:53:06Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12917v1",
      "landing_url": "https://arxiv.org/abs/2312.12917v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.12917"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2312.17255",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.17255v1",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "published": "2023-12-20T00:25:55Z"
    },
    "metadata": {
      "arxiv_id": "2312.17255",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "authors": [
        "Oscar Chang",
        "Dung N. Tran",
        "Kazuhito Koishida"
      ],
      "published": "2023-12-20T00:25:55Z",
      "updated": "2023-12-20T00:25:55Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.17255v1",
      "landing_url": "https://arxiv.org/abs/2312.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.17255"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2401.00365",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.00365v2",
      "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
      "summary": "Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances codebook usage and improves reconstruction performance. We also validated HQ-VAE in terms of its applicability to a different modality with an audio dataset.",
      "published": "2023-12-31T01:39:38Z"
    },
    "metadata": {
      "arxiv_id": "2401.00365",
      "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
      "summary": "Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances codebook usage and improves reconstruction performance. We also validated HQ-VAE in terms of its applicability to a different modality with an audio dataset.",
      "authors": [
        "Yuhta Takida",
        "Yukara Ikemiya",
        "Takashi Shibuya",
        "Kazuki Shimada",
        "Woosung Choi",
        "Chieh-Hsin Lai",
        "Naoki Murata",
        "Toshimitsu Uesaka",
        "Kengo Uchida",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2023-12-31T01:39:38Z",
      "updated": "2024-03-28T06:38:55Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.00365v2",
      "landing_url": "https://arxiv.org/abs/2401.00365v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.00365"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2401.01498",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01498v1",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "published": "2024-01-03T02:03:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2401.01755",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01755v1",
      "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
      "summary": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
      "published": "2024-01-03T14:17:35Z"
    },
    "metadata": {
      "arxiv_id": "2401.01755",
      "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
      "summary": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
      "authors": [
        "Muyang Du",
        "Chuan Liu",
        "Junjie Lai"
      ],
      "published": "2024-01-03T14:17:35Z",
      "updated": "2024-01-03T14:17:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01755v1",
      "landing_url": "https://arxiv.org/abs/2401.01755v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01755"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2401.02839",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.02839v1",
      "title": "Pheme: Efficient and Conversational Speech Generation",
      "summary": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
      "published": "2024-01-05T14:47:20Z"
    },
    "metadata": {
      "arxiv_id": "2401.02839",
      "title": "Pheme: Efficient and Conversational Speech Generation",
      "summary": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
      "authors": [
        "Paweł Budzianowski",
        "Taras Sereda",
        "Tomasz Cichy",
        "Ivan Vulić"
      ],
      "published": "2024-01-05T14:47:20Z",
      "updated": "2024-01-05T14:47:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02839v1",
      "landing_url": "https://arxiv.org/abs/2401.02839v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.02839"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2401.03198",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03198v1",
      "title": "Learning-Augmented K-Means Clustering Using Dimensional Reduction",
      "summary": "Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower cost results compared to running it without PCA. \"Principal component analysis (PCA) is the problem of fitting a low-dimensional affine subspace to a set of data points in a high-dimensional space. PCA is well-established in the literature and has become one of the most useful tools for data modeling, compression, and visualization.\"",
      "published": "2024-01-06T12:02:33Z"
    },
    "metadata": {
      "arxiv_id": "2401.03198",
      "title": "Learning-Augmented K-Means Clustering Using Dimensional Reduction",
      "summary": "Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower cost results compared to running it without PCA. \"Principal component analysis (PCA) is the problem of fitting a low-dimensional affine subspace to a set of data points in a high-dimensional space. PCA is well-established in the literature and has become one of the most useful tools for data modeling, compression, and visualization.\"",
      "authors": [
        "Issam K. O Jabari",
        "Shofiyah",
        "Pradiptya Kahvi S",
        "Novi Nur Putriwijaya",
        "Novanto Yudistira"
      ],
      "published": "2024-01-06T12:02:33Z",
      "updated": "2024-01-06T12:02:33Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03198v1",
      "landing_url": "https://arxiv.org/abs/2401.03198v1",
      "doi": "https://doi.org/10.1145/3626641.3627239"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2401.03468",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03468v1",
      "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation",
      "summary": "Self-supervised speech pre-training methods have developed rapidly in recent years, which show to be very effective for many near-field single-channel speech tasks. However, far-field multichannel speech processing is suffering from the scarcity of labeled multichannel data and complex ambient noises. The efficacy of self-supervised learning for far-field multichannel and multi-modal speech processing has not been well explored. Considering that visual information helps to improve speech recognition performance in noisy scenes, in this work we propose a multichannel multi-modal speech self-supervised learning framework AV-wav2vec2, which utilizes video and multichannel audio data as inputs. First, we propose a multi-path structure to process multichannel audio streams and a visual stream in parallel, with intra- and inter-channel contrastive losses as training targets to fully exploit the spatiotemporal information in multichannel speech data. Second, based on contrastive learning, we use additional single-channel audio data, which is trained jointly to improve the performance of speech representation. Finally, we use a Chinese multichannel multi-modal dataset in real scenarios to validate the effectiveness of the proposed method on audio-visual speech recognition (AVSR), automatic speech recognition (ASR), visual speech recognition (VSR) and audio-visual speaker diarization (AVSD) tasks.",
      "published": "2024-01-07T12:27:18Z"
    },
    "metadata": {
      "arxiv_id": "2401.03468",
      "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation",
      "summary": "Self-supervised speech pre-training methods have developed rapidly in recent years, which show to be very effective for many near-field single-channel speech tasks. However, far-field multichannel speech processing is suffering from the scarcity of labeled multichannel data and complex ambient noises. The efficacy of self-supervised learning for far-field multichannel and multi-modal speech processing has not been well explored. Considering that visual information helps to improve speech recognition performance in noisy scenes, in this work we propose a multichannel multi-modal speech self-supervised learning framework AV-wav2vec2, which utilizes video and multichannel audio data as inputs. First, we propose a multi-path structure to process multichannel audio streams and a visual stream in parallel, with intra- and inter-channel contrastive losses as training targets to fully exploit the spatiotemporal information in multichannel speech data. Second, based on contrastive learning, we use additional single-channel audio data, which is trained jointly to improve the performance of speech representation. Finally, we use a Chinese multichannel multi-modal dataset in real scenarios to validate the effectiveness of the proposed method on audio-visual speech recognition (AVSR), automatic speech recognition (ASR), visual speech recognition (VSR) and audio-visual speaker diarization (AVSD) tasks.",
      "authors": [
        "Qiushi Zhu",
        "Jie Zhang",
        "Yu Gu",
        "Yuchen Hu",
        "Lirong Dai"
      ],
      "published": "2024-01-07T12:27:18Z",
      "updated": "2024-01-07T12:27:18Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03468v1",
      "landing_url": "https://arxiv.org/abs/2401.03468v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03468"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2401.03908",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03908v1",
      "title": "Classification of circular polarization Stokes profiles in a sunspot using k-means clustering",
      "summary": "The magnetic and velocity fields in sunspots are highly structured on small spatial scales which are encoded in the Stokes profiles. Our aim is to identify Stokes profiles in a sunspot which exhibit spectral characteristics that deviate from those associated with the Evershed flow and their spatial distribution. We employ a k-means clustering routine to classify Stokes V spectra in the penumbra of a sunspot. 75% of the penumbral region is dominated by profiles comprising two, nearly anti-symmetric lobes, while 21% of the area is occupied by three-lobed profiles that represent the Evershed flow returning to the photosphere. 4% of the area is dominated by four profile groups - Group 1: three-lobed profiles in which both the rest and strong downflowing component have the same polarity as the sunspot and seen exclusively in the light bridge. Group 2: single, red-lobed profiles over an area of about 2% seen at the outer penumbra in discrete patches that possibly signify the downflowing leg of an Omega-loop. Group 3: three-lobed/highly asymmetric profiles, where the rest and strong downflowing component have a polarity opposite the sunspot. These occupy 1.4% of the penumbral area over conspicuous, elongated structures or isolated patches in the outer penumbra and penumbra-QS boundary. Group 4: three lobed-profiles, in which the rest component has the same polarity as the sunspot and a weaker, upflowing component with an opposite polarity. These profiles are located near the entrance of the light bridge and are found in only 0.12% of the penumbral area. These minority groups of profiles could be related to dynamic phenomena that could affect the overlying chromosphere. The simplicity and speed of k-means can be utilized to identify such anomalous profiles in larger data sets to ascertain their temporal evolution and the physical processes responsible for these inhomogeneities.",
      "published": "2024-01-08T14:10:43Z"
    },
    "metadata": {
      "arxiv_id": "2401.03908",
      "title": "Classification of circular polarization Stokes profiles in a sunspot using k-means clustering",
      "summary": "The magnetic and velocity fields in sunspots are highly structured on small spatial scales which are encoded in the Stokes profiles. Our aim is to identify Stokes profiles in a sunspot which exhibit spectral characteristics that deviate from those associated with the Evershed flow and their spatial distribution. We employ a k-means clustering routine to classify Stokes V spectra in the penumbra of a sunspot. 75% of the penumbral region is dominated by profiles comprising two, nearly anti-symmetric lobes, while 21% of the area is occupied by three-lobed profiles that represent the Evershed flow returning to the photosphere. 4% of the area is dominated by four profile groups - Group 1: three-lobed profiles in which both the rest and strong downflowing component have the same polarity as the sunspot and seen exclusively in the light bridge. Group 2: single, red-lobed profiles over an area of about 2% seen at the outer penumbra in discrete patches that possibly signify the downflowing leg of an Omega-loop. Group 3: three-lobed/highly asymmetric profiles, where the rest and strong downflowing component have a polarity opposite the sunspot. These occupy 1.4% of the penumbral area over conspicuous, elongated structures or isolated patches in the outer penumbra and penumbra-QS boundary. Group 4: three lobed-profiles, in which the rest component has the same polarity as the sunspot and a weaker, upflowing component with an opposite polarity. These profiles are located near the entrance of the light bridge and are found in only 0.12% of the penumbral area. These minority groups of profiles could be related to dynamic phenomena that could affect the overlying chromosphere. The simplicity and speed of k-means can be utilized to identify such anomalous profiles in larger data sets to ascertain their temporal evolution and the physical processes responsible for these inhomogeneities.",
      "authors": [
        "Rohan Eugene Louis",
        "Shibu K. Mathew",
        "A. Raja Bayanna"
      ],
      "published": "2024-01-08T14:10:43Z",
      "updated": "2024-01-08T14:10:43Z",
      "categories": [
        "astro-ph.SR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03908v1",
      "landing_url": "https://arxiv.org/abs/2401.03908v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03908"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2401.04511",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04511v1",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "published": "2024-01-09T12:10:04Z"
    },
    "metadata": {
      "arxiv_id": "2401.04511",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "published": "2024-01-09T12:10:04Z",
      "updated": "2024-01-09T12:10:04Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04511v1",
      "landing_url": "https://arxiv.org/abs/2401.04511v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04511"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2401.04964",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04964v2",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "published": "2024-01-10T07:11:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.04964",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "authors": [
        "Bo Wang",
        "Xiran Xu",
        "Zechen Zhang",
        "Haolin Zhu",
        "YuJie Yan",
        "Xihong Wu",
        "Jing Chen"
      ],
      "published": "2024-01-10T07:11:36Z",
      "updated": "2024-02-01T04:51:22Z",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04964v2",
      "landing_url": "https://arxiv.org/abs/2401.04964v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04964"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2401.05111",
    "anchor": "speech representation",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.05111v1",
      "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters",
      "summary": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
      "published": "2024-01-10T12:21:21Z"
    },
    "metadata": {
      "arxiv_id": "2401.05111",
      "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters",
      "summary": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
      "authors": [
        "Kenichi Fujita",
        "Hiroshi Sato",
        "Takanori Ashihara",
        "Hiroki Kanagawa",
        "Marc Delcroix",
        "Takafumi Moriya",
        "Yusuke Ijima"
      ],
      "published": "2024-01-10T12:21:21Z",
      "updated": "2024-01-10T12:21:21Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05111v1",
      "landing_url": "https://arxiv.org/abs/2401.05111v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.05111"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2401.05379",
    "anchor": "acoustic tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.05379v2",
      "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform",
      "summary": "This study presents a comprehensive evaluation of tools available on the HuggingFace platform for two pivotal applications in artificial intelligence: image segmentation and voice conversion. The primary objective was to identify the top three tools within each category and subsequently install and configure these tools on Linux systems. We leveraged the power of pre-trained segmentation models such as SAM and DETR Model with ResNet-50 backbone for image segmentation, and the so-vits-svc-fork model for voice conversion. This paper delves into the methodologies and challenges encountered during the implementation process, and showcases the successful combination of video segmentation and voice conversion in a unified project named AutoVisual Fusion Suite.",
      "published": "2023-12-17T16:38:41Z"
    },
    "metadata": {
      "arxiv_id": "2401.05379",
      "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform",
      "summary": "This study presents a comprehensive evaluation of tools available on the HuggingFace platform for two pivotal applications in artificial intelligence: image segmentation and voice conversion. The primary objective was to identify the top three tools within each category and subsequently install and configure these tools on Linux systems. We leveraged the power of pre-trained segmentation models such as SAM and DETR Model with ResNet-50 backbone for image segmentation, and the so-vits-svc-fork model for voice conversion. This paper delves into the methodologies and challenges encountered during the implementation process, and showcases the successful combination of video segmentation and voice conversion in a unified project named AutoVisual Fusion Suite.",
      "authors": [
        "Amirreza Hashemi"
      ],
      "published": "2023-12-17T16:38:41Z",
      "updated": "2024-01-12T10:06:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05379v2",
      "landing_url": "https://arxiv.org/abs/2401.05379v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.05379"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2401.05883",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.05883v3",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "published": "2024-01-11T12:43:26Z"
    },
    "metadata": {
      "arxiv_id": "2401.05883",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "authors": [
        "Xianming Li",
        "Jing Li"
      ],
      "published": "2024-01-11T12:43:26Z",
      "updated": "2024-10-03T03:34:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05883v3",
      "landing_url": "https://arxiv.org/abs/2401.05883v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.05883"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2401.07333",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.07333v1",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "published": "2024-01-14T17:43:55Z"
    },
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2401.08833",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.08833v1",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "published": "2024-01-16T21:13:22Z"
    },
    "metadata": {
      "arxiv_id": "2401.08833",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "authors": [
        "Alexander H. Liu",
        "Sung-Lin Yeh",
        "James Glass"
      ],
      "published": "2024-01-16T21:13:22Z",
      "updated": "2024-01-16T21:13:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08833v1",
      "landing_url": "https://arxiv.org/abs/2401.08833v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.08833"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2401.11857",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.11857v1",
      "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
      "summary": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
      "published": "2024-01-22T11:26:59Z"
    },
    "metadata": {
      "arxiv_id": "2401.11857",
      "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
      "summary": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
      "authors": [
        "Shihao Chen",
        "Liping Chen",
        "Jie Zhang",
        "KongAik Lee",
        "Zhenhua Ling",
        "Lirong Dai"
      ],
      "published": "2024-01-22T11:26:59Z",
      "updated": "2024-01-22T11:26:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11857v1",
      "landing_url": "https://arxiv.org/abs/2401.11857v1",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447699"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2401.13527",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.13527v2",
      "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
      "summary": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
      "published": "2024-01-24T15:25:01Z"
    },
    "metadata": {
      "arxiv_id": "2401.13527",
      "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
      "summary": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
      "authors": [
        "Dong Zhang",
        "Xin Zhang",
        "Jun Zhan",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2024-01-24T15:25:01Z",
      "updated": "2024-01-25T17:24:52Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13527v2",
      "landing_url": "https://arxiv.org/abs/2401.13527v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.13527"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2401.13891",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.13891v1",
      "title": "Text to speech synthesis",
      "summary": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
      "published": "2024-01-25T02:13:45Z"
    },
    "metadata": {
      "arxiv_id": "2401.13891",
      "title": "Text to speech synthesis",
      "summary": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
      "authors": [
        "Harini s",
        "Manoj G M"
      ],
      "published": "2024-01-25T02:13:45Z",
      "updated": "2024-01-25T02:13:45Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13891v1",
      "landing_url": "https://arxiv.org/abs/2401.13891v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13891"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2401.14008",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.14008v1",
      "title": "Massive Unsourced Random Access for Near-Field Communications",
      "summary": "This paper investigates the unsourced random access (URA) problem with a massive multiple-input multiple-output receiver that serves wireless devices in the near-field of radiation. We employ an uncoupled transmission protocol without appending redundancies to the slot-wise encoded messages. To exploit the channel sparsity for block length reduction while facing the collapsed sparse structure in the angular domain of near-field channels, we propose a sparse channel sampling method that divides the angle-distance (polar) domain based on the maximum permissible coherence. Decoding starts with retrieving active codewords and channels from each slot. We address the issue by leveraging the structured channel sparsity in the spatial and polar domains and propose a novel turbo-based recovery algorithm. Furthermore, we investigate an off-grid compressed sensing method to refine discretely estimated channel parameters over the continuum that improves the detection performance. Afterward, without the assistance of redundancies, we recouple the separated messages according to the similarity of the users' channel information and propose a modified K-medoids method to handle the constraints and collisions involved in channel clustering. Simulations reveal that via exploiting the channel sparsity, the proposed URA scheme achieves high spectral efficiency and surpasses existing multi-slot-based schemes. Moreover, with more measurements provided by the overcomplete channel sampling, the near-field-suited scheme outperforms its counterpart of the far-field.",
      "published": "2024-01-25T08:17:51Z"
    },
    "metadata": {
      "arxiv_id": "2401.14008",
      "title": "Massive Unsourced Random Access for Near-Field Communications",
      "summary": "This paper investigates the unsourced random access (URA) problem with a massive multiple-input multiple-output receiver that serves wireless devices in the near-field of radiation. We employ an uncoupled transmission protocol without appending redundancies to the slot-wise encoded messages. To exploit the channel sparsity for block length reduction while facing the collapsed sparse structure in the angular domain of near-field channels, we propose a sparse channel sampling method that divides the angle-distance (polar) domain based on the maximum permissible coherence. Decoding starts with retrieving active codewords and channels from each slot. We address the issue by leveraging the structured channel sparsity in the spatial and polar domains and propose a novel turbo-based recovery algorithm. Furthermore, we investigate an off-grid compressed sensing method to refine discretely estimated channel parameters over the continuum that improves the detection performance. Afterward, without the assistance of redundancies, we recouple the separated messages according to the similarity of the users' channel information and propose a modified K-medoids method to handle the constraints and collisions involved in channel clustering. Simulations reveal that via exploiting the channel sparsity, the proposed URA scheme achieves high spectral efficiency and surpasses existing multi-slot-based schemes. Moreover, with more measurements provided by the overcomplete channel sampling, the near-field-suited scheme outperforms its counterpart of the far-field.",
      "authors": [
        "Xinyu Xie",
        "Yongpeng Wu",
        "Jianping An",
        "Derrick Wing Kwan Ng",
        "Chengwen Xing",
        "Wenjun Zhang"
      ],
      "published": "2024-01-25T08:17:51Z",
      "updated": "2024-01-25T08:17:51Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14008v1",
      "landing_url": "https://arxiv.org/abs/2401.14008v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.14008"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2401.15579",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.15579v1",
      "title": "MunTTS: A Text-to-Speech System for Mundari",
      "summary": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
      "published": "2024-01-28T06:27:17Z"
    },
    "metadata": {
      "arxiv_id": "2401.15579",
      "title": "MunTTS: A Text-to-Speech System for Mundari",
      "summary": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
      "authors": [
        "Varun Gumma",
        "Rishav Hada",
        "Aditya Yadavalli",
        "Pamir Gogoi",
        "Ishani Mondal",
        "Vivek Seshadri",
        "Kalika Bali"
      ],
      "published": "2024-01-28T06:27:17Z",
      "updated": "2024-01-28T06:27:17Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15579v1",
      "landing_url": "https://arxiv.org/abs/2401.15579v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.15579"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2401.16812",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.16812v3",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "published": "2024-01-30T08:26:28Z"
    },
    "metadata": {
      "arxiv_id": "2401.16812",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "authors": [
        "Takaaki Saeki",
        "Soumi Maiti",
        "Shinnosuke Takamichi",
        "Shinji Watanabe",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-01-30T08:26:28Z",
      "updated": "2024-09-01T14:34:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.16812v3",
      "landing_url": "https://arxiv.org/abs/2401.16812v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.16812"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2402.01708",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01708v2",
      "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
      "summary": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
      "published": "2024-01-25T11:47:06Z"
    },
    "metadata": {
      "arxiv_id": "2402.01708",
      "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
      "summary": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
      "authors": [
        "Wiebke Hutiri",
        "Oresiti Papakyriakopoulos",
        "Alice Xiang"
      ],
      "published": "2024-01-25T11:47:06Z",
      "updated": "2024-05-15T15:26:42Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01708v2",
      "landing_url": "https://arxiv.org/abs/2402.01708v2",
      "doi": "https://doi.org/10.1145/3630106.3658911"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2402.02302",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.02302v1",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "published": "2024-02-03T23:54:03Z"
    },
    "metadata": {
      "arxiv_id": "2402.02302",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "authors": [
        "Nay San",
        "Georgios Paraskevopoulos",
        "Aryaman Arora",
        "Xiluo He",
        "Prabhjot Kaur",
        "Oliver Adams",
        "Dan Jurafsky"
      ],
      "published": "2024-02-03T23:54:03Z",
      "updated": "2024-02-03T23:54:03Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02302v1",
      "landing_url": "https://arxiv.org/abs/2402.02302v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.02302"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2402.03158",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03158v2",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "published": "2024-02-05T16:27:59Z"
    },
    "metadata": {
      "arxiv_id": "2402.03158",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "authors": [
        "Ran Ben-Basat",
        "Yaniv Ben-Itzhak",
        "Michael Mitzenmacher",
        "Shay Vargaftik"
      ],
      "published": "2024-02-05T16:27:59Z",
      "updated": "2025-07-31T13:53:50Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03158v2",
      "landing_url": "https://arxiv.org/abs/2402.03158v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03158"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2402.03407",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03407v1",
      "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
      "summary": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
      "published": "2024-02-05T15:08:19Z"
    },
    "metadata": {
      "arxiv_id": "2402.03407",
      "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
      "summary": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
      "authors": [
        "Álvaro Martín-Cortinas",
        "Daniel Sáez-Trigueros",
        "Iván Vallés-Pérez",
        "Biel Tura-Vecino",
        "Piotr Biliński",
        "Mateusz Lajszczak",
        "Grzegorz Beringer",
        "Roberto Barra-Chicote",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2024-02-05T15:08:19Z",
      "updated": "2024-02-05T15:08:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03407v1",
      "landing_url": "https://arxiv.org/abs/2402.03407v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2402.04563",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.04563v1",
      "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
      "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "published": "2024-02-07T03:43:56Z"
    },
    "metadata": {
      "arxiv_id": "2402.04563",
      "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
      "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "authors": [
        "Saebom Leem",
        "Hyunseok Seo"
      ],
      "published": "2024-02-07T03:43:56Z",
      "updated": "2024-02-07T03:43:56Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04563v1",
      "landing_url": "https://arxiv.org/abs/2402.04563v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04563"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2402.05706",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.05706v3",
      "title": "Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation",
      "summary": "Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naver-ai/usdm.",
      "published": "2024-02-08T14:35:09Z"
    },
    "metadata": {
      "arxiv_id": "2402.05706",
      "title": "Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation",
      "summary": "Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naver-ai/usdm.",
      "authors": [
        "Heeseung Kim",
        "Soonshin Seo",
        "Kyeongseok Jeong",
        "Ohsung Kwon",
        "Soyoon Kim",
        "Jungwhan Kim",
        "Jaehong Lee",
        "Eunwoo Song",
        "Myungwoo Oh",
        "Jung-Woo Ha",
        "Sungroh Yoon",
        "Kang Min Yoo"
      ],
      "published": "2024-02-08T14:35:09Z",
      "updated": "2024-11-28T01:10:49Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05706v3",
      "landing_url": "https://arxiv.org/abs/2402.05706v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.05706"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2402.05964",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.05964v2",
      "title": "A Survey on Transformer Compression",
      "summary": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
      "published": "2024-02-05T12:16:28Z"
    },
    "metadata": {
      "arxiv_id": "2402.05964",
      "title": "A Survey on Transformer Compression",
      "summary": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
      "authors": [
        "Yehui Tang",
        "Yunhe Wang",
        "Jianyuan Guo",
        "Zhijun Tu",
        "Kai Han",
        "Hailin Hu",
        "Dacheng Tao"
      ],
      "published": "2024-02-05T12:16:28Z",
      "updated": "2024-04-07T13:03:58Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05964v2",
      "landing_url": "https://arxiv.org/abs/2402.05964v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.05964"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2402.06492",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.06492v1",
      "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
      "summary": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
      "published": "2024-02-09T15:53:15Z"
    },
    "metadata": {
      "arxiv_id": "2402.06492",
      "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
      "summary": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
      "authors": [
        "Yichen Jiang",
        "Xiang Zhou",
        "Mohit Bansal"
      ],
      "published": "2024-02-09T15:53:15Z",
      "updated": "2024-02-09T15:53:15Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06492v1",
      "landing_url": "https://arxiv.org/abs/2402.06492v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.06492"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2402.08093",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.08093v2",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "published": "2024-02-12T22:21:30Z"
    },
    "metadata": {
      "arxiv_id": "2402.08093",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "authors": [
        "Mateusz Łajszczak",
        "Guillermo Cámbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "Álvaro Martín-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "Alexis Moinet",
        "Sri Karlapati",
        "Ewa Muszyńska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad López Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
      ],
      "published": "2024-02-12T22:21:30Z",
      "updated": "2024-02-15T18:57:26Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08093v2",
      "landing_url": "https://arxiv.org/abs/2402.08093v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.08093"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2402.09378",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.09378v2",
      "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
      "summary": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
      "published": "2024-02-14T18:24:41Z"
    },
    "metadata": {
      "arxiv_id": "2402.09378",
      "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
      "summary": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
      "authors": [
        "Shengpeng Ji",
        "Ziyue Jiang",
        "Hanting Wang",
        "Jialong Zuo",
        "Zhou Zhao"
      ],
      "published": "2024-02-14T18:24:41Z",
      "updated": "2024-06-02T16:11:18Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09378v2",
      "landing_url": "https://arxiv.org/abs/2402.09378v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.09378"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2402.11363",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.11363v3",
      "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
      "summary": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
      "published": "2024-02-17T19:04:23Z"
    },
    "metadata": {
      "arxiv_id": "2402.11363",
      "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
      "summary": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
      "authors": [
        "Shiva Ebrahimi",
        "Xuan Guo"
      ],
      "published": "2024-02-17T19:04:23Z",
      "updated": "2024-06-26T07:45:33Z",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.11363v3",
      "landing_url": "https://arxiv.org/abs/2402.11363v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.11363"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2402.12208",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.12208v4",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "published": "2024-02-19T15:12:12Z"
    },
    "metadata": {
      "arxiv_id": "2402.12208",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "authors": [
        "Shengpeng Ji",
        "Minghui Fang",
        "Jialong Zuo",
        "Ziyue Jiang",
        "Dingdong Wang",
        "Hanting Wang",
        "Hai Huang",
        "Zhou Zhao"
      ],
      "published": "2024-02-19T15:12:12Z",
      "updated": "2025-06-04T05:50:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12208v4",
      "landing_url": "https://arxiv.org/abs/2402.12208v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.12208"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2402.13572",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.13572v2",
      "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
      "summary": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
      "published": "2024-02-21T07:07:54Z"
    },
    "metadata": {
      "arxiv_id": "2402.13572",
      "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
      "summary": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
      "authors": [
        "Yihang Gao",
        "Chuanyang Zheng",
        "Enze Xie",
        "Han Shi",
        "Tianyang Hu",
        "Yu Li",
        "Michael K. Ng",
        "Zhenguo Li",
        "Zhaoqiang Liu"
      ],
      "published": "2024-02-21T07:07:54Z",
      "updated": "2025-01-10T09:11:39Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13572v2",
      "landing_url": "https://arxiv.org/abs/2402.13572v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.13572"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2403.02002",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.02002v2",
      "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
      "summary": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
      "published": "2024-03-04T12:53:15Z"
    },
    "metadata": {
      "arxiv_id": "2403.02002",
      "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
      "summary": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
      "authors": [
        "Sho Inoue",
        "Kun Zhou",
        "Shuai Wang",
        "Haizhou Li"
      ],
      "published": "2024-03-04T12:53:15Z",
      "updated": "2024-09-29T15:39:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.02002v2",
      "landing_url": "https://arxiv.org/abs/2403.02002v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.02002"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2403.05010",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.05010v3",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "published": "2024-03-08T03:16:47Z"
    },
    "metadata": {
      "arxiv_id": "2403.05010",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "published": "2024-03-08T03:16:47Z",
      "updated": "2024-10-07T02:08:05Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.05010v3",
      "landing_url": "https://arxiv.org/abs/2403.05010v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.05010"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2403.06536",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.06536v1",
      "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
      "summary": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
      "published": "2024-03-11T09:23:20Z"
    },
    "metadata": {
      "arxiv_id": "2403.06536",
      "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
      "summary": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
      "authors": [
        "Jinchen Zhu",
        "Mingjian Zhang",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2024-03-11T09:23:20Z",
      "updated": "2024-03-11T09:23:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06536v1",
      "landing_url": "https://arxiv.org/abs/2403.06536v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06536"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2403.07355",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.07355v2",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "published": "2024-03-12T06:28:41Z"
    },
    "metadata": {
      "arxiv_id": "2403.07355",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "authors": [
        "Junyong Shin",
        "Yujin Kang",
        "Yo-Seb Jeon"
      ],
      "published": "2024-03-12T06:28:41Z",
      "updated": "2024-03-13T02:29:29Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07355v2",
      "landing_url": "https://arxiv.org/abs/2403.07355v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07355"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2403.08206",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.08206v2",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "published": "2024-03-13T03:03:15Z"
    },
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2403.09673",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.09673v2",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "published": "2024-02-04T12:18:51Z"
    },
    "metadata": {
      "arxiv_id": "2403.09673",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Jue Wang",
        "Yufei Huang",
        "Lirong Wu",
        "Stan Z. Li"
      ],
      "published": "2024-02-04T12:18:51Z",
      "updated": "2024-03-19T05:29:23Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09673v2",
      "landing_url": "https://arxiv.org/abs/2403.09673v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.09673"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2403.14402",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.14402v2",
      "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
      "summary": "Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.",
      "published": "2024-03-21T13:52:17Z"
    },
    "metadata": {
      "arxiv_id": "2403.14402",
      "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
      "summary": "Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.",
      "authors": [
        "HyoJung Han",
        "Mohamed Anwar",
        "Juan Pino",
        "Wei-Ning Hsu",
        "Marine Carpuat",
        "Bowen Shi",
        "Changhan Wang"
      ],
      "published": "2024-03-21T13:52:17Z",
      "updated": "2024-08-12T13:16:48Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14402v2",
      "landing_url": "https://arxiv.org/abs/2403.14402v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14402"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2403.14562",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.14562v2",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "published": "2024-03-21T17:06:17Z"
    },
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2403.17378",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.17378v1",
      "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
      "summary": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
      "published": "2024-03-26T04:53:15Z"
    },
    "metadata": {
      "arxiv_id": "2403.17378",
      "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
      "summary": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
      "authors": [
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-03-26T04:53:15Z",
      "updated": "2024-03-26T04:53:15Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17378v1",
      "landing_url": "https://arxiv.org/abs/2403.17378v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17378"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2403.19461",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.19461v2",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "published": "2024-03-28T14:32:57Z"
    },
    "metadata": {
      "arxiv_id": "2403.19461",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "authors": [
        "Simon Idoko",
        "Basant Sharma",
        "Arun Kumar Singh"
      ],
      "published": "2024-03-28T14:32:57Z",
      "updated": "2024-04-25T14:29:08Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19461v2",
      "landing_url": "https://arxiv.org/abs/2403.19461v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.19461"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2404.00685",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.00685v2",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "published": "2024-03-31T13:30:12Z"
    },
    "metadata": {
      "arxiv_id": "2404.00685",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "authors": [
        "Santiago Cuervo",
        "Ricard Marxer"
      ],
      "published": "2024-03-31T13:30:12Z",
      "updated": "2024-04-16T06:46:18Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00685v2",
      "landing_url": "https://arxiv.org/abs/2404.00685v2",
      "doi": "https://doi.org/10.18653/v1/2024.emnlp-main.21"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2404.00856",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.00856v1",
      "title": "Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling",
      "summary": "Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light's phonetic ABX task and SUPERB's speaker identification task.",
      "published": "2024-04-01T01:49:09Z"
    },
    "metadata": {
      "arxiv_id": "2404.00856",
      "title": "Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling",
      "summary": "Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light's phonetic ABX task and SUPERB's speaker identification task.",
      "authors": [
        "Injune Hwang",
        "Kyogu Lee"
      ],
      "published": "2024-04-01T01:49:09Z",
      "updated": "2024-04-01T01:49:09Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00856v1",
      "landing_url": "https://arxiv.org/abs/2404.00856v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.00856"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2404.03204",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.03204v3",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "published": "2024-04-04T05:15:07Z"
    },
    "metadata": {
      "arxiv_id": "2404.03204",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "authors": [
        "Detai Xin",
        "Xu Tan",
        "Kai Shen",
        "Zeqian Ju",
        "Dongchao Yang",
        "Yuancheng Wang",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari",
        "Shujie Liu",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-04-04T05:15:07Z",
      "updated": "2024-05-19T21:34:28Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03204v3",
      "landing_url": "https://arxiv.org/abs/2404.03204v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.03204"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2404.03663",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.03663v1",
      "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
      "summary": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
      "published": "2024-02-15T13:26:18Z"
    },
    "metadata": {
      "arxiv_id": "2404.03663",
      "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
      "summary": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
      "authors": [
        "Man Yao",
        "Jiakui Hu",
        "Tianxiang Hu",
        "Yifan Xu",
        "Zhaokun Zhou",
        "Yonghong Tian",
        "Bo Xu",
        "Guoqi Li"
      ],
      "published": "2024-02-15T13:26:18Z",
      "updated": "2024-02-15T13:26:18Z",
      "categories": [
        "cs.NE",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03663v1",
      "landing_url": "https://arxiv.org/abs/2404.03663v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.03663"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2404.04879",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04879v2",
      "title": "Semantic Region Aware Autonomous Exploration for Multi-Type Map Construction in Unknown Indoor Environments",
      "summary": "Mainstream autonomous exploration methods usually perform excessively-repeated explorations for the same region, leading to long exploration time and exploration trajectory in complex scenes. To handle this issue, we propose a novel semantic region aware autonomous exploration method, the core idea of which is considering the information of semantic regions to optimize the autonomous navigation strategy. Our method enables the mobile robot to fully explore the current semantic region before moving to the next region, contributing to avoid excessively-repeated explorations and accelerate the exploration speed. In addition, compared with existing au?tonomous exploration methods that usually construct the single-type map, our method allows to construct four types of maps including point cloud map, occupancy grid map, topological map, and semantic map. The experiment results demonstrate that our method achieves the highest 50.7% exploration time reduction and 48.1% exploration trajectory length reduction while maintaining >98% exploration rate when comparing with the classical RRT (Rapid-exploration Random Tree) based autonomous exploration method.",
      "published": "2024-04-07T08:49:09Z"
    },
    "metadata": {
      "arxiv_id": "2404.04879",
      "title": "Semantic Region Aware Autonomous Exploration for Multi-Type Map Construction in Unknown Indoor Environments",
      "summary": "Mainstream autonomous exploration methods usually perform excessively-repeated explorations for the same region, leading to long exploration time and exploration trajectory in complex scenes. To handle this issue, we propose a novel semantic region aware autonomous exploration method, the core idea of which is considering the information of semantic regions to optimize the autonomous navigation strategy. Our method enables the mobile robot to fully explore the current semantic region before moving to the next region, contributing to avoid excessively-repeated explorations and accelerate the exploration speed. In addition, compared with existing au?tonomous exploration methods that usually construct the single-type map, our method allows to construct four types of maps including point cloud map, occupancy grid map, topological map, and semantic map. The experiment results demonstrate that our method achieves the highest 50.7% exploration time reduction and 48.1% exploration trajectory length reduction while maintaining >98% exploration rate when comparing with the classical RRT (Rapid-exploration Random Tree) based autonomous exploration method.",
      "authors": [
        "Jianfang Mao"
      ],
      "published": "2024-04-07T08:49:09Z",
      "updated": "2024-10-10T11:56:00Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04879v2",
      "landing_url": "https://arxiv.org/abs/2404.04879v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04879"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2404.05657",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.05657v1",
      "title": "MLP Can Be A Good Transformer Learner",
      "summary": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
      "published": "2024-04-08T16:40:15Z"
    },
    "metadata": {
      "arxiv_id": "2404.05657",
      "title": "MLP Can Be A Good Transformer Learner",
      "summary": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
      "authors": [
        "Sihao Lin",
        "Pumeng Lyu",
        "Dongrui Liu",
        "Tao Tang",
        "Xiaodan Liang",
        "Andy Song",
        "Xiaojun Chang"
      ],
      "published": "2024-04-08T16:40:15Z",
      "updated": "2024-04-08T16:40:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05657v1",
      "landing_url": "https://arxiv.org/abs/2404.05657v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.05657"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2404.06079",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.06079v2",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "published": "2024-04-09T07:37:41Z"
    },
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2404.06690",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.06690v3",
      "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
      "summary": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
      "published": "2024-04-10T02:32:58Z"
    },
    "metadata": {
      "arxiv_id": "2404.06690",
      "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
      "summary": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
      "authors": [
        "Leying Zhang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Midia Yousefi",
        "Yanmin Qian",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "published": "2024-04-10T02:32:58Z",
      "updated": "2024-12-15T16:30:54Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06690v3",
      "landing_url": "https://arxiv.org/abs/2404.06690v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.06690"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2404.10153",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.10153v1",
      "title": "How Users Experience Closed Captions on Live Television: Quality Metrics Remain a Challenge",
      "summary": "This paper presents a mixed methods study on how deaf, hard of hearing and hearing viewers perceive live TV caption quality with captioned video stimuli designed to mirror TV captioning experiences. To assess caption quality, we used four commonly-used quality metrics focusing on accuracy: word error rate, weighted word error rate, automated caption evaluation (ACE), and its successor ACE2. We calculated the correlation between the four quality metrics and viewer ratings for subjective quality and found that the correlation was weak, revealing that other factors besides accuracy affect user ratings. Additionally, even high-quality captions are perceived to have problems, despite controlling for confounding factors. Qualitative analysis of viewer comments revealed three major factors affecting their experience: Errors within captions, difficulty in following captions, and caption appearance. The findings raise questions as to how objective caption quality metrics can be reconciled with the user experience across a diverse spectrum of viewers.",
      "published": "2024-04-15T21:55:29Z"
    },
    "metadata": {
      "arxiv_id": "2404.10153",
      "title": "How Users Experience Closed Captions on Live Television: Quality Metrics Remain a Challenge",
      "summary": "This paper presents a mixed methods study on how deaf, hard of hearing and hearing viewers perceive live TV caption quality with captioned video stimuli designed to mirror TV captioning experiences. To assess caption quality, we used four commonly-used quality metrics focusing on accuracy: word error rate, weighted word error rate, automated caption evaluation (ACE), and its successor ACE2. We calculated the correlation between the four quality metrics and viewer ratings for subjective quality and found that the correlation was weak, revealing that other factors besides accuracy affect user ratings. Additionally, even high-quality captions are perceived to have problems, despite controlling for confounding factors. Qualitative analysis of viewer comments revealed three major factors affecting their experience: Errors within captions, difficulty in following captions, and caption appearance. The findings raise questions as to how objective caption quality metrics can be reconciled with the user experience across a diverse spectrum of viewers.",
      "authors": [
        "Mariana Arroyo Chavez",
        "Molly Feanny",
        "Matthew Seita",
        "Bernard Thompson",
        "Keith Delk",
        "Skyler Officer",
        "Abraham Glasser",
        "Raja Kushalnagar",
        "Christian Vogler"
      ],
      "published": "2024-04-15T21:55:29Z",
      "updated": "2024-04-15T21:55:29Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.10153v1",
      "landing_url": "https://arxiv.org/abs/2404.10153v1",
      "doi": "https://doi.org/10.1145/3613904.3641988"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2404.12062",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.12062v1",
      "title": "MIDGET: Music Conditioned 3D Dance Generation",
      "summary": "In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.",
      "published": "2024-04-18T10:20:37Z"
    },
    "metadata": {
      "arxiv_id": "2404.12062",
      "title": "MIDGET: Music Conditioned 3D Dance Generation",
      "summary": "In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.",
      "authors": [
        "Jinwu Wang",
        "Wei Mao",
        "Miaomiao Liu"
      ],
      "published": "2024-04-18T10:20:37Z",
      "updated": "2024-04-18T10:20:37Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.GR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.12062v1",
      "landing_url": "https://arxiv.org/abs/2404.12062v1",
      "doi": "https://doi.org/10.1007/978-981-99-8388-9_23"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2404.14774",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.14774v2",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "published": "2024-04-23T06:29:48Z"
    },
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2404.16123",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16123v1",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "published": "2024-04-24T18:28:17Z"
    },
    "metadata": {
      "arxiv_id": "2404.16123",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "authors": [
        "Eric Slyman",
        "Stefan Lee",
        "Scott Cohen",
        "Kushal Kafle"
      ],
      "published": "2024-04-24T18:28:17Z",
      "updated": "2024-04-24T18:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16123v1",
      "landing_url": "https://arxiv.org/abs/2404.16123v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16123"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2404.16743",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16743v2",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "published": "2024-04-25T16:57:05Z"
    },
    "metadata": {
      "arxiv_id": "2404.16743",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "authors": [
        "Chanho Park",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2024-04-25T16:57:05Z",
      "updated": "2024-04-26T11:11:02Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16743v2",
      "landing_url": "https://arxiv.org/abs/2404.16743v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.16743"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2404.19441",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.19441v3",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "published": "2024-04-30T10:44:33Z"
    },
    "metadata": {
      "arxiv_id": "2404.19441",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "authors": [
        "Yuzhe Gu",
        "Enmao Diao"
      ],
      "published": "2024-04-30T10:44:33Z",
      "updated": "2024-10-03T12:23:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19441v3",
      "landing_url": "https://arxiv.org/abs/2404.19441v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.19441"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2405.00930",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.00930v2",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "published": "2024-05-02T01:11:15Z"
    },
    "metadata": {
      "arxiv_id": "2405.00930",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "authors": [
        "Pengcheng Li",
        "Jianzong Wang",
        "Xulong Zhang",
        "Yong Zhang",
        "Jing Xiao",
        "Ning Cheng"
      ],
      "published": "2024-05-02T01:11:15Z",
      "updated": "2024-11-24T09:30:29Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00930v2",
      "landing_url": "https://arxiv.org/abs/2405.00930v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.00930"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2405.01242",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.01242v3",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "published": "2024-05-02T12:45:48Z"
    },
    "metadata": {
      "arxiv_id": "2405.01242",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "authors": [
        "Yueyuan Sui",
        "Minghui Zhao",
        "Junxi Xia",
        "Xiaofan Jiang",
        "Stephen Xia"
      ],
      "published": "2024-05-02T12:45:48Z",
      "updated": "2024-05-29T15:46:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.01242v3",
      "landing_url": "https://arxiv.org/abs/2405.01242v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.01242"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2405.02330",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02330v1",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "published": "2024-04-25T13:49:50Z"
    },
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2405.02682",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02682v1",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "published": "2024-05-04T14:48:19Z"
    },
    "metadata": {
      "arxiv_id": "2405.02682",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "authors": [
        "Md Washik Al Azad",
        "Spyridon Mastorakis"
      ],
      "published": "2024-05-04T14:48:19Z",
      "updated": "2024-05-04T14:48:19Z",
      "categories": [
        "cs.DC",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02682v1",
      "landing_url": "https://arxiv.org/abs/2405.02682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02682"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2405.03110",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03110v1",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "published": "2024-05-06T02:06:26Z"
    },
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2405.06266",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.06266v1",
      "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
      "summary": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
      "published": "2024-05-10T06:37:07Z"
    },
    "metadata": {
      "arxiv_id": "2405.06266",
      "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
      "summary": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
      "authors": [
        "Jianli Xiao",
        "Baichao Long"
      ],
      "published": "2024-05-10T06:37:07Z",
      "updated": "2024-05-10T06:37:07Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06266v1",
      "landing_url": "https://arxiv.org/abs/2405.06266v1",
      "doi": "https://doi.org/10.1016/j.ins.2024.120648"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.06535",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.06535v1",
      "title": "Controllable Image Generation With Composed Parallel Token Prediction",
      "summary": "Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation). Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity. To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space. Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fréchet Inception Distance (FID) scores. Our method attains an average generation accuracy of $80.71\\%$ across the studied settings. Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting. We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation.",
      "published": "2024-05-10T15:27:35Z"
    },
    "metadata": {
      "arxiv_id": "2405.06535",
      "title": "Controllable Image Generation With Composed Parallel Token Prediction",
      "summary": "Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation). Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity. To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space. Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fréchet Inception Distance (FID) scores. Our method attains an average generation accuracy of $80.71\\%$ across the studied settings. Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting. We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation.",
      "authors": [
        "Jamie Stirling",
        "Noura Al-Moubayed"
      ],
      "published": "2024-05-10T15:27:35Z",
      "updated": "2024-05-10T15:27:35Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06535v1",
      "landing_url": "https://arxiv.org/abs/2405.06535v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.06535"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2405.06573",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.06573v2",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "published": "2024-05-10T16:18:49Z"
    },
    "metadata": {
      "arxiv_id": "2405.06573",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "authors": [
        "Rong Chao",
        "Wen-Huang Cheng",
        "Moreno La Quatra",
        "Sabato Marco Siniscalchi",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Yu Tsao"
      ],
      "published": "2024-05-10T16:18:49Z",
      "updated": "2025-10-07T07:07:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06573v2",
      "landing_url": "https://arxiv.org/abs/2405.06573v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.06573"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2405.07682",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.07682v1",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "published": "2024-05-13T12:14:54Z"
    },
    "metadata": {
      "arxiv_id": "2405.07682",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "authors": [
        "Jianyi Chen",
        "Wei Xue",
        "Xu Tan",
        "Zhen Ye",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "published": "2024-05-13T12:14:54Z",
      "updated": "2024-05-13T12:14:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07682v1",
      "landing_url": "https://arxiv.org/abs/2405.07682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.07682"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2405.09508",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09508v2",
      "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
      "summary": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
      "published": "2024-05-15T17:01:02Z"
    },
    "metadata": {
      "arxiv_id": "2405.09508",
      "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
      "summary": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
      "authors": [
        "Demi Zhang",
        "Bushi Xiao",
        "Chao Gao",
        "Sangpil Youm",
        "Bonnie J Dorr"
      ],
      "published": "2024-05-15T17:01:02Z",
      "updated": "2024-10-15T20:24:00Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09508v2",
      "landing_url": "https://arxiv.org/abs/2405.09508v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.09508"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.09768",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09768v1",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "published": "2024-05-16T02:18:41Z"
    },
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2405.09789",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09789v1",
      "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
      "summary": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
      "published": "2024-05-16T03:26:06Z"
    },
    "metadata": {
      "arxiv_id": "2405.09789",
      "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
      "summary": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
      "authors": [
        "Wentao Jiang",
        "Jing Zhang",
        "Di Wang",
        "Qiming Zhang",
        "Zengmao Wang",
        "Bo Du"
      ],
      "published": "2024-05-16T03:26:06Z",
      "updated": "2024-05-16T03:26:06Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09789v1",
      "landing_url": "https://arxiv.org/abs/2405.09789v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09789"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.15310",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.15310v5",
      "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
      "summary": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
      "published": "2024-05-24T07:52:53Z"
    },
    "metadata": {
      "arxiv_id": "2405.15310",
      "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
      "summary": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
      "authors": [
        "Duke Nguyen",
        "Du Yin",
        "Aditya Joshi",
        "Flora Salim"
      ],
      "published": "2024-05-24T07:52:53Z",
      "updated": "2025-09-23T03:21:45Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15310v5",
      "landing_url": "https://arxiv.org/abs/2405.15310v5",
      "doi": "https://doi.org/10.1145/3768161"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.15405",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.15405v1",
      "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
      "summary": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
      "published": "2024-05-24T10:13:49Z"
    },
    "metadata": {
      "arxiv_id": "2405.15405",
      "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
      "summary": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
      "authors": [
        "Barış Büyüktaş",
        "Kenneth Weitzel",
        "Sebastian Völkers",
        "Felix Zailskas",
        "Begüm Demir"
      ],
      "published": "2024-05-24T10:13:49Z",
      "updated": "2024-05-24T10:13:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15405v1",
      "landing_url": "https://arxiv.org/abs/2405.15405v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15405"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.15562",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.15562v1",
      "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
      "summary": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
      "published": "2024-05-24T13:49:31Z"
    },
    "metadata": {
      "arxiv_id": "2405.15562",
      "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
      "summary": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
      "authors": [
        "Gao Tianci"
      ],
      "published": "2024-05-24T13:49:31Z",
      "updated": "2024-05-24T13:49:31Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15562v1",
      "landing_url": "https://arxiv.org/abs/2405.15562v1",
      "doi": "https://doi.org/10.1007/978-3-032-04758-8_3"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2405.16136",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16136v1",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "published": "2024-05-25T09:10:12Z"
    },
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2405.20410",
    "anchor": "acoustic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.20410v1",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "published": "2024-05-30T18:28:31Z"
    },
    "metadata": {
      "arxiv_id": "2405.20410",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "published": "2024-05-30T18:28:31Z",
      "updated": "2024-05-30T18:28:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20410v1",
      "landing_url": "https://arxiv.org/abs/2405.20410v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20410"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2406.00976",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.00976v2",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "published": "2024-06-03T04:16:30Z"
    },
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.02092",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02092v1",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "published": "2024-06-04T08:23:57Z"
    },
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2406.02430",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02430v1",
      "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
      "summary": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
      "published": "2024-06-04T15:48:29Z"
    },
    "metadata": {
      "arxiv_id": "2406.02430",
      "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
      "summary": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
      "authors": [
        "Philip Anastassiou",
        "Jiawei Chen",
        "Jitong Chen",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Ziyi Chen",
        "Jian Cong",
        "Lelai Deng",
        "Chuang Ding",
        "Lu Gao",
        "Mingqing Gong",
        "Peisong Huang",
        "Qingqing Huang",
        "Zhiying Huang",
        "Yuanyuan Huo",
        "Dongya Jia",
        "Chumin Li",
        "Feiya Li",
        "Hui Li",
        "Jiaxin Li",
        "Xiaoyang Li",
        "Xingxing Li",
        "Lin Liu",
        "Shouda Liu",
        "Sichao Liu",
        "Xudong Liu",
        "Yuchen Liu",
        "Zhengxi Liu",
        "Lu Lu",
        "Junjie Pan",
        "Xin Wang",
        "Yuping Wang",
        "Yuxuan Wang",
        "Zhen Wei",
        "Jian Wu",
        "Chao Yao",
        "Yifeng Yang",
        "Yuanhao Yi",
        "Junteng Zhang",
        "Qidi Zhang",
        "Shuo Zhang",
        "Wenjie Zhang",
        "Yang Zhang",
        "Zilin Zhao",
        "Dejian Zhong",
        "Xiaobin Zhuang"
      ],
      "published": "2024-06-04T15:48:29Z",
      "updated": "2024-06-04T15:48:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02430v1",
      "landing_url": "https://arxiv.org/abs/2406.02430v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02430"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2406.02739",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02739v1",
      "title": "Local Search k-means++ with Foresight",
      "summary": "Since its introduction in 1957, Lloyd's algorithm for $k$-means clustering has been extensively studied and has undergone several improvements. While in its original form it does not guarantee any approximation factor at all, Arthur and Vassilvitskii (SODA 2007) proposed $k$-means++ which enhances Lloyd's algorithm by a seeding method which guarantees a $\\mathcal{O}(\\log k)$-approximation in expectation. More recently, Lattanzi and Sohler (ICML 2019) proposed LS++ which further improves the solution quality of $k$-means++ by local search techniques to obtain a $\\mathcal{O}(1)$-approximation. On the practical side, the greedy variant of $k$-means++ is often used although its worst-case behaviour is provably worse than for the standard $k$-means++ variant.\n  We investigate how to improve LS++ further in practice. We study two options for improving the practical performance: (a) Combining LS++ with greedy $k$-means++ instead of $k$-means++, and (b) Improving LS++ by better entangling it with Lloyd's algorithm. Option (a) worsens the theoretical guarantees of $k$-means++ but improves the practical quality also in combination with LS++ as we confirm in our experiments. Option (b) is our new algorithm, Foresight LS++. We experimentally show that FLS++ improves upon the solution quality of LS++. It retains its asymptotic runtime and its worst-case approximation bounds.",
      "published": "2024-06-04T19:42:15Z"
    },
    "metadata": {
      "arxiv_id": "2406.02739",
      "title": "Local Search k-means++ with Foresight",
      "summary": "Since its introduction in 1957, Lloyd's algorithm for $k$-means clustering has been extensively studied and has undergone several improvements. While in its original form it does not guarantee any approximation factor at all, Arthur and Vassilvitskii (SODA 2007) proposed $k$-means++ which enhances Lloyd's algorithm by a seeding method which guarantees a $\\mathcal{O}(\\log k)$-approximation in expectation. More recently, Lattanzi and Sohler (ICML 2019) proposed LS++ which further improves the solution quality of $k$-means++ by local search techniques to obtain a $\\mathcal{O}(1)$-approximation. On the practical side, the greedy variant of $k$-means++ is often used although its worst-case behaviour is provably worse than for the standard $k$-means++ variant.\n  We investigate how to improve LS++ further in practice. We study two options for improving the practical performance: (a) Combining LS++ with greedy $k$-means++ instead of $k$-means++, and (b) Improving LS++ by better entangling it with Lloyd's algorithm. Option (a) worsens the theoretical guarantees of $k$-means++ but improves the practical quality also in combination with LS++ as we confirm in our experiments. Option (b) is our new algorithm, Foresight LS++. We experimentally show that FLS++ improves upon the solution quality of LS++. It retains its asymptotic runtime and its worst-case approximation bounds.",
      "authors": [
        "Theo Conrads",
        "Lukas Drexler",
        "Joshua Könen",
        "Daniel R. Schmidt",
        "Melanie Schmidt"
      ],
      "published": "2024-06-04T19:42:15Z",
      "updated": "2024-06-04T19:42:15Z",
      "categories": [
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02739v1",
      "landing_url": "https://arxiv.org/abs/2406.02739v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02739"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2406.02940",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02940v1",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "published": "2024-06-05T04:54:49Z"
    },
    "metadata": {
      "arxiv_id": "2406.02940",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Hui Lu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-05T04:54:49Z",
      "updated": "2024-06-05T04:54:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02940v1",
      "landing_url": "https://arxiv.org/abs/2406.02940v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2406.03044",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03044v4",
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "summary": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
      "published": "2024-06-05T08:15:09Z"
    },
    "metadata": {
      "arxiv_id": "2406.03044",
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "summary": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
      "authors": [
        "Geeling Chau",
        "Christopher Wang",
        "Sabera Talukder",
        "Vighnesh Subramaniam",
        "Saraswati Soedarmadji",
        "Yisong Yue",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "published": "2024-06-05T08:15:09Z",
      "updated": "2025-03-28T06:43:28Z",
      "categories": [
        "cs.LG",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03044v4",
      "landing_url": "https://arxiv.org/abs/2406.03044v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.03044"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2406.03460",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03460v1",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "published": "2024-06-05T17:07:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.03706",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03706v1",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "published": "2024-06-06T03:06:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2406.04633",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04633v1",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "published": "2024-06-07T04:34:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2406.04740",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04740v1",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "published": "2024-06-07T08:40:53Z"
    },
    "metadata": {
      "arxiv_id": "2406.04740",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "authors": [
        "Yang Ma",
        "Wenchi Cheng",
        "Jingqing Wang",
        "Wei Zhang"
      ],
      "published": "2024-06-07T08:40:53Z",
      "updated": "2024-06-07T08:40:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04740v1",
      "landing_url": "https://arxiv.org/abs/2406.04740v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04740"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.05551",
    "anchor": "speech representation",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05551v1",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "published": "2024-06-08T18:57:13Z"
    },
    "metadata": {
      "arxiv_id": "2406.05551",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
      ],
      "published": "2024-06-08T18:57:13Z",
      "updated": "2024-06-08T18:57:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05551v1",
      "landing_url": "https://arxiv.org/abs/2406.05551v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05551"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2406.06251",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.06251v1",
      "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
      "summary": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
      "published": "2024-06-10T13:31:18Z"
    },
    "metadata": {
      "arxiv_id": "2406.06251",
      "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
      "summary": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
      "authors": [
        "Chung-Ming Chien",
        "Andros Tjandra",
        "Apoorv Vyas",
        "Matt Le",
        "Bowen Shi",
        "Wei-Ning Hsu"
      ],
      "published": "2024-06-10T13:31:18Z",
      "updated": "2024-06-10T13:31:18Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06251v1",
      "landing_url": "https://arxiv.org/abs/2406.06251v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.06251"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2406.06582",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.06582v2",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "published": "2024-06-04T20:08:25Z"
    },
    "metadata": {
      "arxiv_id": "2406.06582",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "authors": [
        "Viet Anh Trinh",
        "Rosy Southwell",
        "Yiwen Guan",
        "Xinlu He",
        "Zhiyong Wang",
        "Jacob Whitehill"
      ],
      "published": "2024-06-04T20:08:25Z",
      "updated": "2024-06-25T17:44:00Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06582v2",
      "landing_url": "https://arxiv.org/abs/2406.06582v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.06582"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2406.07119",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07119v1",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "published": "2024-06-11T10:06:53Z"
    },
    "metadata": {
      "arxiv_id": "2406.07119",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "authors": [
        "Aoxiong Yin",
        "Haoyuan Li",
        "Kai Shen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2024-06-11T10:06:53Z",
      "updated": "2024-06-11T10:06:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
      "landing_url": "https://arxiv.org/abs/2406.07119v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07119"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.07696",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07696v1",
      "title": "Sustainable self-supervised learning for speech representations",
      "summary": "Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible. In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption. Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs. The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations. It pretrains using a single GPU in less than a day. On top of that, it improves the error rate performance of the baseline in downstream task evaluations. When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement.",
      "published": "2024-06-11T20:21:36Z"
    },
    "metadata": {
      "arxiv_id": "2406.07696",
      "title": "Sustainable self-supervised learning for speech representations",
      "summary": "Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible. In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption. Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs. The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations. It pretrains using a single GPU in less than a day. On top of that, it improves the error rate performance of the baseline in downstream task evaluations. When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement.",
      "authors": [
        "Luis Lugo",
        "Valentin Vielzeuf"
      ],
      "published": "2024-06-11T20:21:36Z",
      "updated": "2024-06-11T20:21:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07696v1",
      "landing_url": "https://arxiv.org/abs/2406.07696v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07696"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.07846",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07846v1",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "published": "2024-06-12T03:25:18Z"
    },
    "metadata": {
      "arxiv_id": "2406.07846",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "authors": [
        "Ziqian Ning",
        "Shuai Wang",
        "Pengcheng Zhu",
        "Zhichao Wang",
        "Jixun Yao",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2024-06-12T03:25:18Z",
      "updated": "2024-06-12T03:25:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07846v1",
      "landing_url": "https://arxiv.org/abs/2406.07846v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07846"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2406.07855",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.07855v1",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "published": "2024-06-12T04:09:44Z"
    },
    "metadata": {
      "arxiv_id": "2406.07855",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "authors": [
        "Bing Han",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Lingwei Meng",
        "Yanming Qian",
        "Yanqing Liu",
        "Sheng Zhao",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2024-06-12T04:09:44Z",
      "updated": "2024-06-12T04:09:44Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07855v1",
      "landing_url": "https://arxiv.org/abs/2406.07855v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07855"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2406.08266",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08266v2",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "published": "2024-06-12T14:34:41Z"
    },
    "metadata": {
      "arxiv_id": "2406.08266",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "authors": [
        "Hengyu Li",
        "Kangdi Mei",
        "Zhaoci Liu",
        "Yang Ai",
        "Liping Chen",
        "Jie Zhang",
        "Zhenhua Ling"
      ],
      "published": "2024-06-12T14:34:41Z",
      "updated": "2024-06-13T06:26:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08266v2",
      "landing_url": "https://arxiv.org/abs/2406.08266v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08266"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.08353",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08353v3",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "published": "2024-06-12T15:59:25Z"
    },
    "metadata": {
      "arxiv_id": "2406.08353",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "published": "2024-06-12T15:59:25Z",
      "updated": "2025-03-23T16:45:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08353v3",
      "landing_url": "https://arxiv.org/abs/2406.08353v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.08353"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.10108",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10108v1",
      "title": "Precipitation Nowcasting Using Physics Informed Discriminator Generative Models",
      "summary": "Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.",
      "published": "2024-06-14T15:12:53Z"
    },
    "metadata": {
      "arxiv_id": "2406.10108",
      "title": "Precipitation Nowcasting Using Physics Informed Discriminator Generative Models",
      "summary": "Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.",
      "authors": [
        "Junzhe Yin",
        "Cristian Meo",
        "Ankush Roy",
        "Zeineh Bou Cher",
        "Yanbo Wang",
        "Ruben Imhoff",
        "Remko Uijlenhoet",
        "Justin Dauwels"
      ],
      "published": "2024-06-14T15:12:53Z",
      "updated": "2024-06-14T15:12:53Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10108v1",
      "landing_url": "https://arxiv.org/abs/2406.10108v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10108"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2406.10223",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10223v1",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "published": "2024-06-14T17:55:55Z"
    },
    "metadata": {
      "arxiv_id": "2406.10223",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "authors": [
        "Nameer Hirschkind",
        "Xiao Yu",
        "Mahesh Kumar Nandwana",
        "Joseph Liu",
        "Eloi DuBois",
        "Dao Le",
        "Nicolas Thiebaut",
        "Colin Sinclair",
        "Kyle Spence",
        "Charles Shang",
        "Zoe Abrams",
        "Morgan McGuire"
      ],
      "published": "2024-06-14T17:55:55Z",
      "updated": "2024-06-14T17:55:55Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10223v1",
      "landing_url": "https://arxiv.org/abs/2406.10223v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10223"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.10735",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10735v1",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "published": "2024-06-15T20:43:07Z"
    },
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.11037",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11037v1",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "published": "2024-06-16T18:20:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2406.11693",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11693v1",
      "title": "Wake dynamics of wind turbines in unsteady streamwise flow conditions",
      "summary": "The unsteady flow physics of wind-turbine wakes under dynamic forcing conditions are critical to the modeling and control of wind farms for optimal power density. Unsteady forcing in the streamwise direction may be generated by unsteady inflow conditions in the atmospheric boundary layer, dynamic induction control of the turbine, or streamwise surge motions of a floating offshore wind turbine due to floating-platform oscillations. This study seeks to identify the dominant flow mechanisms in unsteady wakes forced by a periodic upstream inflow condition. A theoretical framework for the problem is derived, which describes traveling-wave undulations in the wake radius and streamwise velocity. These dynamics encourage the aggregation of tip vortices into large structures that are advected along in the wake. Flow measurements in the wake of a periodically surging turbine were obtained in an optically accessible towing-tank facility, with an average diameter-based Reynolds number of 300,000 and with surge-velocity amplitudes of up to 40% of the mean inflow velocity. Qualitative agreement between trends in the measurements and model predictions is observed, supporting the validity of the theoretical analyses. The experiments also demonstrate large enhancements in the recovery of the wake relative to the steady-flow case, with wake-length reductions of up to 46.5% and improvements in the available power at 10 diameters downstream of up to 15.7%. These results provide fundamental insights into the dynamics of unsteady wakes and serve as additional evidence that unsteady fluid mechanics can be leveraged to increase the power density of wind farms.",
      "published": "2024-06-17T16:10:50Z"
    },
    "metadata": {
      "arxiv_id": "2406.11693",
      "title": "Wake dynamics of wind turbines in unsteady streamwise flow conditions",
      "summary": "The unsteady flow physics of wind-turbine wakes under dynamic forcing conditions are critical to the modeling and control of wind farms for optimal power density. Unsteady forcing in the streamwise direction may be generated by unsteady inflow conditions in the atmospheric boundary layer, dynamic induction control of the turbine, or streamwise surge motions of a floating offshore wind turbine due to floating-platform oscillations. This study seeks to identify the dominant flow mechanisms in unsteady wakes forced by a periodic upstream inflow condition. A theoretical framework for the problem is derived, which describes traveling-wave undulations in the wake radius and streamwise velocity. These dynamics encourage the aggregation of tip vortices into large structures that are advected along in the wake. Flow measurements in the wake of a periodically surging turbine were obtained in an optically accessible towing-tank facility, with an average diameter-based Reynolds number of 300,000 and with surge-velocity amplitudes of up to 40% of the mean inflow velocity. Qualitative agreement between trends in the measurements and model predictions is observed, supporting the validity of the theoretical analyses. The experiments also demonstrate large enhancements in the recovery of the wake relative to the steady-flow case, with wake-length reductions of up to 46.5% and improvements in the available power at 10 diameters downstream of up to 15.7%. These results provide fundamental insights into the dynamics of unsteady wakes and serve as additional evidence that unsteady fluid mechanics can be leveraged to increase the power density of wind farms.",
      "authors": [
        "Nathaniel J. Wei",
        "Adnan El Makdah",
        "JiaCheng Hu",
        "Frieder Kaiser",
        "David E. Rival",
        "John O. Dabiri"
      ],
      "published": "2024-06-17T16:10:50Z",
      "updated": "2024-06-17T16:10:50Z",
      "categories": [
        "physics.flu-dyn"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11693v1",
      "landing_url": "https://arxiv.org/abs/2406.11693v1",
      "doi": "https://doi.org/10.1017/jfm.2024.999"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2406.11838",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11838v3",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "published": "2024-06-17T17:59:58Z"
    },
    "metadata": {
      "arxiv_id": "2406.11838",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "authors": [
        "Tianhong Li",
        "Yonglong Tian",
        "He Li",
        "Mingyang Deng",
        "Kaiming He"
      ],
      "published": "2024-06-17T17:59:58Z",
      "updated": "2024-11-01T14:45:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11838v3",
      "landing_url": "https://arxiv.org/abs/2406.11838v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.11838"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2406.12236",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.12236v1",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "published": "2024-06-18T03:24:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.12236",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "authors": [
        "Hanyu Meng",
        "Qiquan Zhang",
        "Xiangyu Zhang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "published": "2024-06-18T03:24:52Z",
      "updated": "2024-06-18T03:24:52Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12236v1",
      "landing_url": "https://arxiv.org/abs/2406.12236v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12236"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.12428",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.12428v2",
      "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
      "summary": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
      "published": "2024-06-18T09:23:54Z"
    },
    "metadata": {
      "arxiv_id": "2406.12428",
      "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
      "summary": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
      "authors": [
        "Kentaro Mitsui",
        "Koh Mitsuda",
        "Toshiaki Wakatsuki",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2024-06-18T09:23:54Z",
      "updated": "2024-10-03T05:17:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12428v2",
      "landing_url": "https://arxiv.org/abs/2406.12428v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.12428"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2406.13275",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13275v2",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "published": "2024-06-19T07:09:46Z"
    },
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.13431",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13431v2",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "published": "2024-06-19T10:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.13695",
    "anchor": "acoustic tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13695v1",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "published": "2024-06-19T16:48:14Z"
    },
    "metadata": {
      "arxiv_id": "2406.13695",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "authors": [
        "Stefan Pasch",
        "Dimitirios Petridis",
        "Jannic Cutura"
      ],
      "published": "2024-06-19T16:48:14Z",
      "updated": "2024-06-19T16:48:14Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13695v1",
      "landing_url": "https://arxiv.org/abs/2406.13695v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13695"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2406.14017",
    "anchor": "semantic tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14017v2",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "published": "2024-06-20T06:21:56Z"
    },
    "metadata": {
      "arxiv_id": "2406.14017",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "authors": [
        "Ye Wang",
        "Jiahao Xun",
        "Minjie Hong",
        "Jieming Zhu",
        "Tao Jin",
        "Wang Lin",
        "Haoyuan Li",
        "Linjun Li",
        "Yan Xia",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "published": "2024-06-20T06:21:56Z",
      "updated": "2024-07-03T10:00:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14017v2",
      "landing_url": "https://arxiv.org/abs/2406.14017v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14017"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2406.14294",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14294v2",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "published": "2024-06-20T13:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2406.15752",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.15752v1",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "published": "2024-06-22T06:39:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2406.16120",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.16120v1",
      "title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss",
      "summary": "Contextualized end-to-end automatic speech recognition has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network.",
      "published": "2024-06-23T14:22:59Z"
    },
    "metadata": {
      "arxiv_id": "2406.16120",
      "title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss",
      "summary": "Contextualized end-to-end automatic speech recognition has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network.",
      "authors": [
        "Muhammad Shakeel",
        "Yui Sudo",
        "Yifan Peng",
        "Shinji Watanabe"
      ],
      "published": "2024-06-23T14:22:59Z",
      "updated": "2024-06-23T14:22:59Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16120v1",
      "landing_url": "https://arxiv.org/abs/2406.16120v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-1257"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.17310",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17310v1",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "published": "2024-06-25T06:46:47Z"
    },
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2406.17722",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17722v1",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "published": "2024-06-25T17:10:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.17722",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "authors": [
        "Kentaro Seki",
        "Shinnosuke Takamichi",
        "Norihiro Takamune",
        "Yuki Saito",
        "Kanami Imamura",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-06-25T17:10:39Z",
      "updated": "2024-06-25T17:10:39Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17722v1",
      "landing_url": "https://arxiv.org/abs/2406.17722v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17722"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2406.17957",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17957v1",
      "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
      "summary": "Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens. Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.",
      "published": "2024-06-25T22:18:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.17957",
      "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
      "summary": "Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens. Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Subhankar Ghosh",
        "Jason Li",
        "Rafael Valle",
        "Rohan Badlani",
        "Boris Ginsburg"
      ],
      "published": "2024-06-25T22:18:52Z",
      "updated": "2024-06-25T22:18:52Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17957v1",
      "landing_url": "https://arxiv.org/abs/2406.17957v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17957"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2407.00261",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.00261v2",
      "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
      "summary": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
      "published": "2024-06-28T23:20:57Z"
    },
    "metadata": {
      "arxiv_id": "2407.00261",
      "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
      "summary": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
      "authors": [
        "Yubo Huang",
        "Jia Wang",
        "Peipei Li",
        "Liuyu Xiang",
        "Peigang Li",
        "Zhaofeng He"
      ],
      "published": "2024-06-28T23:20:57Z",
      "updated": "2024-09-20T23:54:19Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00261v2",
      "landing_url": "https://arxiv.org/abs/2407.00261v2",
      "doi": "https://doi.org/10.1109/ICME55011.2023.00094"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2407.01290",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.01290v2",
      "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
      "summary": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
      "published": "2024-07-01T13:44:38Z"
    },
    "metadata": {
      "arxiv_id": "2407.01290",
      "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
      "summary": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
      "authors": [
        "Menglin Yang",
        "Harshit Verma",
        "Delvin Ce Zhang",
        "Jiahong Liu",
        "Irwin King",
        "Rex Ying"
      ],
      "published": "2024-07-01T13:44:38Z",
      "updated": "2025-08-24T12:17:27Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01290v2",
      "landing_url": "https://arxiv.org/abs/2407.01290v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.01290"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2407.03892",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03892v1",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "published": "2024-07-04T12:35:32Z"
    },
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2407.04291",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.04291v3",
      "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
      "summary": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
      "published": "2024-07-05T06:54:24Z"
    },
    "metadata": {
      "arxiv_id": "2407.04291",
      "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
      "summary": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
      "authors": [
        "Ismail Rasim Ulgen",
        "John H. L. Hansen",
        "Carlos Busso",
        "Berrak Sisman"
      ],
      "published": "2024-07-05T06:54:24Z",
      "updated": "2025-09-18T20:22:33Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04291v3",
      "landing_url": "https://arxiv.org/abs/2407.04291v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.04291"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2407.04939",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.04939v1",
      "title": "Balance of Number of Embedding and their Dimensions in Vector Quantization",
      "summary": "The dimensionality of the embedding and the number of available embeddings ( also called codebook size) are critical factors influencing the performance of Vector Quantization(VQ), a discretization process used in many models such as the Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study examines the balance between the codebook sizes and dimensions of embeddings in VQ, while maintaining their product constant. Traditionally, these hyper parameters are static during training; however, our findings indicate that augmenting the codebook size while simultaneously reducing the embedding dimension can significantly boost the effectiveness of the VQ-VAE. As a result, the strategic selection of codebook size and embedding dimensions, while preserving the capacity of the discrete codebook space, is critically important. To address this, we propose a novel adaptive dynamic quantization approach, underpinned by the Gumbel-Softmax mechanism, which allows the model to autonomously determine the optimal codebook configuration for each data instance. This dynamic discretizer gives the VQ-VAE remarkable flexibility. Thorough empirical evaluations across multiple benchmark datasets validate the notable performance enhancements achieved by our approach, highlighting the significant potential of adaptive dynamic quantization to improve model performance.",
      "published": "2024-07-06T03:07:31Z"
    },
    "metadata": {
      "arxiv_id": "2407.04939",
      "title": "Balance of Number of Embedding and their Dimensions in Vector Quantization",
      "summary": "The dimensionality of the embedding and the number of available embeddings ( also called codebook size) are critical factors influencing the performance of Vector Quantization(VQ), a discretization process used in many models such as the Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study examines the balance between the codebook sizes and dimensions of embeddings in VQ, while maintaining their product constant. Traditionally, these hyper parameters are static during training; however, our findings indicate that augmenting the codebook size while simultaneously reducing the embedding dimension can significantly boost the effectiveness of the VQ-VAE. As a result, the strategic selection of codebook size and embedding dimensions, while preserving the capacity of the discrete codebook space, is critically important. To address this, we propose a novel adaptive dynamic quantization approach, underpinned by the Gumbel-Softmax mechanism, which allows the model to autonomously determine the optimal codebook configuration for each data instance. This dynamic discretizer gives the VQ-VAE remarkable flexibility. Thorough empirical evaluations across multiple benchmark datasets validate the notable performance enhancements achieved by our approach, highlighting the significant potential of adaptive dynamic quantization to improve model performance.",
      "authors": [
        "Hang Chen",
        "Sankepally Sainath Reddy",
        "Ziwei Chen",
        "Dianbo Liu"
      ],
      "published": "2024-07-06T03:07:31Z",
      "updated": "2024-07-06T03:07:31Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04939v1",
      "landing_url": "https://arxiv.org/abs/2407.04939v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.04939"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2407.05361",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.05361v3",
      "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
      "summary": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
      "published": "2024-07-07T13:24:54Z"
    },
    "metadata": {
      "arxiv_id": "2407.05361",
      "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
      "summary": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-07-07T13:24:54Z",
      "updated": "2024-09-07T15:08:24Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05361v3",
      "landing_url": "https://arxiv.org/abs/2407.05361v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.05361"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2407.05407",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.05407v2",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "published": "2024-07-07T15:16:19Z"
    },
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2407.08152",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08152v2",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "published": "2024-07-11T03:10:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2407.08551",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08551v2",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "published": "2024-07-11T14:36:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.08551",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "authors": [
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Bing Han",
        "Shujie Hu",
        "Yanqing Liu",
        "Jinyu Li",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng",
        "Furu Wei"
      ],
      "published": "2024-07-11T14:36:53Z",
      "updated": "2025-05-27T05:07:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551v2",
      "landing_url": "https://arxiv.org/abs/2407.08551v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08551"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2407.10949",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.10949v2",
      "title": "Representing Rule-based Chatbots with Transformers",
      "summary": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
      "published": "2024-07-15T17:45:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.10949",
      "title": "Representing Rule-based Chatbots with Transformers",
      "summary": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
      "authors": [
        "Dan Friedman",
        "Abhishek Panigrahi",
        "Danqi Chen"
      ],
      "published": "2024-07-15T17:45:53Z",
      "updated": "2025-02-12T15:18:32Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10949v2",
      "landing_url": "https://arxiv.org/abs/2407.10949v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.10949"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2407.12208",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.12208v1",
      "title": "Computing $k$-means in mixed precision",
      "summary": "The $k$-means algorithm is one of the most popular and critical techniques in data mining and machine learning, and it has achieved significant success in numerous science and engineering domains. Computing $k$-means to a global optimum is NP-hard in Euclidean space, yet there are a variety of efficient heuristic algorithms, such as Lloyd's algorithm, that converge to a local optimum with superpolynomial complexity in the worst case. Motivated by the emergence and prominence of mixed precision capabilities in hardware, a current trend is to develop low and mixed precision variants of algorithms in order to improve the runtime and energy consumption. In this paper we study the numerical stability of Lloyd's $k$-means algorithm, and, in particular, we confirm the stability of the widely used distance computation formula. We propose a mixed-precision framework for $k$-means computation and investigate the effects of low-precision distance computation within the framework. Through extensive simulations on various data clustering and image segmentation tasks, we verify the applicability and robustness of the mixed precision $k$-means method. We find that, in $k$-means computation, normalized data is more tolerant to the reduction of precision in the distance computation, while for nonnormalized data more care is needed in the use of reduced precision, mainly to avoid overflow. Our study demonstrates the potential for the use of mixed precision to accelerate the $k$-means computation and offers some insights into other distance-based machine learning methods.",
      "published": "2024-07-16T22:48:35Z"
    },
    "metadata": {
      "arxiv_id": "2407.12208",
      "title": "Computing $k$-means in mixed precision",
      "summary": "The $k$-means algorithm is one of the most popular and critical techniques in data mining and machine learning, and it has achieved significant success in numerous science and engineering domains. Computing $k$-means to a global optimum is NP-hard in Euclidean space, yet there are a variety of efficient heuristic algorithms, such as Lloyd's algorithm, that converge to a local optimum with superpolynomial complexity in the worst case. Motivated by the emergence and prominence of mixed precision capabilities in hardware, a current trend is to develop low and mixed precision variants of algorithms in order to improve the runtime and energy consumption. In this paper we study the numerical stability of Lloyd's $k$-means algorithm, and, in particular, we confirm the stability of the widely used distance computation formula. We propose a mixed-precision framework for $k$-means computation and investigate the effects of low-precision distance computation within the framework. Through extensive simulations on various data clustering and image segmentation tasks, we verify the applicability and robustness of the mixed precision $k$-means method. We find that, in $k$-means computation, normalized data is more tolerant to the reduction of precision in the distance computation, while for nonnormalized data more care is needed in the use of reduced precision, mainly to avoid overflow. Our study demonstrates the potential for the use of mixed precision to accelerate the $k$-means computation and offers some insights into other distance-based machine learning methods.",
      "authors": [
        "Erin Carson",
        "Xinye Chen",
        "Xiaobo Liu"
      ],
      "published": "2024-07-16T22:48:35Z",
      "updated": "2024-07-16T22:48:35Z",
      "categories": [
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12208v1",
      "landing_url": "https://arxiv.org/abs/2407.12208v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12208"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2407.12707",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.12707v3",
      "title": "TTSDS -- Text-to-Speech Distribution Score",
      "summary": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
      "published": "2024-07-17T16:30:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.12707",
      "title": "TTSDS -- Text-to-Speech Distribution Score",
      "summary": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
      "authors": [
        "Christoph Minixhofer",
        "Ondřej Klejch",
        "Peter Bell"
      ],
      "published": "2024-07-17T16:30:27Z",
      "updated": "2024-12-02T03:45:42Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12707v3",
      "landing_url": "https://arxiv.org/abs/2407.12707v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.12707"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2407.13333",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.13333v1",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "published": "2024-07-18T09:32:57Z"
    },
    "metadata": {
      "arxiv_id": "2407.13333",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "authors": [
        "Robert Sutherland",
        "George Close",
        "Thomas Hain",
        "Stefan Goetze",
        "Jon Barker"
      ],
      "published": "2024-07-18T09:32:57Z",
      "updated": "2024-07-18T09:32:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13333v1",
      "landing_url": "https://arxiv.org/abs/2407.13333v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13333"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2407.15835",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15835v3",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "published": "2024-07-22T17:51:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2407.17060",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.17060v1",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "published": "2024-07-24T07:37:12Z"
    },
    "metadata": {
      "arxiv_id": "2407.17060",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "authors": [
        "Binzhe Li",
        "Shurun Wang",
        "Shiqi Wang",
        "Yan Ye"
      ],
      "published": "2024-07-24T07:37:12Z",
      "updated": "2024-07-24T07:37:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17060v1",
      "landing_url": "https://arxiv.org/abs/2407.17060v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.17060"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2407.19051",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.19051v1",
      "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
      "summary": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
      "published": "2024-07-26T19:13:11Z"
    },
    "metadata": {
      "arxiv_id": "2407.19051",
      "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
      "summary": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
      "authors": [
        "Bruna Bazaluk",
        "Mosab Hamdan",
        "Mustafa Ghaleb",
        "Mohammed S. M. Gismalla",
        "Flavio S. Correa da Silva",
        "Daniel Macêdo Batista"
      ],
      "published": "2024-07-26T19:13:11Z",
      "updated": "2024-07-26T19:13:11Z",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19051v1",
      "landing_url": "https://arxiv.org/abs/2407.19051v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.19051"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2407.21061",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.21061v1",
      "title": "Improving noisy student training for low-resource languages in End-to-End ASR using CycleGAN and inter-domain losses",
      "summary": "Training a semi-supervised end-to-end speech recognition system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired speech-text and unlabeled speech, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic speech recognition where there are limited paired speech-text, unlabeled speech (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning \"CycleGAN and inter-domain losses\" solely with external text. Secondly, we enhance \"CycleGAN and inter-domain losses\" by incorporating automatic hyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\" Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.",
      "published": "2024-07-26T10:57:06Z"
    },
    "metadata": {
      "arxiv_id": "2407.21061",
      "title": "Improving noisy student training for low-resource languages in End-to-End ASR using CycleGAN and inter-domain losses",
      "summary": "Training a semi-supervised end-to-end speech recognition system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired speech-text and unlabeled speech, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic speech recognition where there are limited paired speech-text, unlabeled speech (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning \"CycleGAN and inter-domain losses\" solely with external text. Secondly, we enhance \"CycleGAN and inter-domain losses\" by incorporating automatic hyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\" Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.",
      "authors": [
        "Chia-Yu Li",
        "Ngoc Thang Vu"
      ],
      "published": "2024-07-26T10:57:06Z",
      "updated": "2024-07-26T10:57:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.21061v1",
      "landing_url": "https://arxiv.org/abs/2407.21061v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.21061"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2408.00284",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.00284v1",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "published": "2024-08-01T04:57:31Z"
    },
    "metadata": {
      "arxiv_id": "2408.00284",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "authors": [
        "Xinhan Di",
        "Zihao Chen",
        "Yunming Liang",
        "Junjie Zheng",
        "Yihua Wang",
        "Chaofan Ding"
      ],
      "published": "2024-08-01T04:57:31Z",
      "updated": "2024-08-01T04:57:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00284v1",
      "landing_url": "https://arxiv.org/abs/2408.00284v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.00284"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.01391",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.01391v2",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "published": "2024-08-02T17:01:36Z"
    },
    "metadata": {
      "arxiv_id": "2408.01391",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "authors": [
        "Shixun Wu",
        "Yitong Ding",
        "Yujia Zhai",
        "Jinyang Liu",
        "Jiajun Huang",
        "Zizhe Jian",
        "Huangliang Dai",
        "Sheng Di",
        "Bryan M. Wong",
        "Zizhong Chen",
        "Franck Cappello"
      ],
      "published": "2024-08-02T17:01:36Z",
      "updated": "2024-08-07T21:55:08Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01391v2",
      "landing_url": "https://arxiv.org/abs/2408.01391v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01391"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2408.03558",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.03558v1",
      "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
      "summary": "In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We take style images from the WikiArt dataset and content images from the COCO dataset. Experimental results demonstrate that D$^2$Styler produces high-quality style-transferred images and outperforms twelve existing methods on nearly all the metrics. The qualitative results and ablation studies provide further insights into the efficacy of our technique. The code is available at https://github.com/Onkarsus13/D2Styler.",
      "published": "2024-08-07T05:47:06Z"
    },
    "metadata": {
      "arxiv_id": "2408.03558",
      "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
      "summary": "In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We take style images from the WikiArt dataset and content images from the COCO dataset. Experimental results demonstrate that D$^2$Styler produces high-quality style-transferred images and outperforms twelve existing methods on nearly all the metrics. The qualitative results and ablation studies provide further insights into the efficacy of our technique. The code is available at https://github.com/Onkarsus13/D2Styler.",
      "authors": [
        "Onkar Susladkar",
        "Gayatri Deshmukh",
        "Sparsh Mittal",
        "Parth Shastri"
      ],
      "published": "2024-08-07T05:47:06Z",
      "updated": "2024-08-07T05:47:06Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03558v1",
      "landing_url": "https://arxiv.org/abs/2408.03558v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.03558"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2408.03887",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.03887v1",
      "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
      "summary": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
      "published": "2024-08-06T07:04:59Z"
    },
    "metadata": {
      "arxiv_id": "2408.03887",
      "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
      "summary": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
      "authors": [
        "Hawraz A. Ahmad",
        "Tarik A. Rashid"
      ],
      "published": "2024-08-06T07:04:59Z",
      "updated": "2024-08-06T07:04:59Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03887v1",
      "landing_url": "https://arxiv.org/abs/2408.03887v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.03887"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2408.04205",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.04205v1",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "published": "2024-08-08T04:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2408.04205",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "authors": [
        "Xinwei Chen",
        "Xiaofeng Zhong",
        "Zijian Zhang",
        "Linglong Dai",
        "Shidong Zhou"
      ],
      "published": "2024-08-08T04:05:18Z",
      "updated": "2024-08-08T04:05:18Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04205v1",
      "landing_url": "https://arxiv.org/abs/2408.04205v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04205"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2408.04505",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.04505v1",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "published": "2024-08-08T15:03:45Z"
    },
    "metadata": {
      "arxiv_id": "2408.04505",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "authors": [
        "Nurettin Turan",
        "Michael Baur",
        "Jianqing Li",
        "Wolfgang Utschick"
      ],
      "published": "2024-08-08T15:03:45Z",
      "updated": "2024-08-08T15:03:45Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04505v1",
      "landing_url": "https://arxiv.org/abs/2408.04505v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04505"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2408.04708",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.04708v1",
      "title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency",
      "summary": "Voice conversion aims to modify the source speaker's voice to resemble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take inspiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).",
      "published": "2024-08-08T18:12:51Z"
    },
    "metadata": {
      "arxiv_id": "2408.04708",
      "title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency",
      "summary": "Voice conversion aims to modify the source speaker's voice to resemble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take inspiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).",
      "authors": [
        "Jiawei Huang",
        "Chen Zhang",
        "Yi Ren",
        "Ziyue Jiang",
        "Zhenhui Ye",
        "Jinglin Liu",
        "Jinzheng He",
        "Xiang Yin",
        "Zhou Zhao"
      ],
      "published": "2024-08-08T18:12:51Z",
      "updated": "2024-08-08T18:12:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04708v1",
      "landing_url": "https://arxiv.org/abs/2408.04708v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04708"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2408.09483",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.09483v1",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "published": "2024-08-18T13:54:46Z"
    },
    "metadata": {
      "arxiv_id": "2408.09483",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "authors": [
        "Wei Zhao",
        "Dan Feng",
        "Wei Tong",
        "Xueliang Wei",
        "Bing Wu"
      ],
      "published": "2024-08-18T13:54:46Z",
      "updated": "2024-08-18T13:54:46Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09483v1",
      "landing_url": "https://arxiv.org/abs/2408.09483v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09483"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2408.09802",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.09802v1",
      "title": "Hear Your Face: Face-based voice conversion with F0 estimation",
      "summary": "This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency.",
      "published": "2024-08-19T08:47:03Z"
    },
    "metadata": {
      "arxiv_id": "2408.09802",
      "title": "Hear Your Face: Face-based voice conversion with F0 estimation",
      "summary": "This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency.",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Injune Hwang",
        "Kyogu Lee"
      ],
      "published": "2024-08-19T08:47:03Z",
      "updated": "2024-08-19T08:47:03Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09802v1",
      "landing_url": "https://arxiv.org/abs/2408.09802v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09802"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2408.11842",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11842v2",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "published": "2024-08-07T12:49:40Z"
    },
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2408.11858",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11858v1",
      "title": "Convexity-based Pruning of Speech Representation Models",
      "summary": "Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.",
      "published": "2024-08-16T09:04:54Z"
    },
    "metadata": {
      "arxiv_id": "2408.11858",
      "title": "Convexity-based Pruning of Speech Representation Models",
      "summary": "Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.",
      "authors": [
        "Teresa Dorszewski",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-08-16T09:04:54Z",
      "updated": "2024-08-16T09:04:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11858v1",
      "landing_url": "https://arxiv.org/abs/2408.11858v1",
      "doi": "https://doi.org/10.1109/MLSP58920.2024.10734716"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2408.13893",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.13893v2",
      "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
      "summary": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
      "published": "2024-08-25T17:07:39Z"
    },
    "metadata": {
      "arxiv_id": "2408.13893",
      "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
      "summary": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
      "authors": [
        "Dongchao Yang",
        "Rongjie Huang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-08-25T17:07:39Z",
      "updated": "2024-08-28T07:16:37Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13893v2",
      "landing_url": "https://arxiv.org/abs/2408.13893v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.13893"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2408.15616",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.15616v1",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "published": "2024-08-28T08:14:51Z"
    },
    "metadata": {
      "arxiv_id": "2408.15616",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "authors": [
        "Korbinian Kuhn",
        "Verena Kersken",
        "Gottfried Zimmermann"
      ],
      "published": "2024-08-28T08:14:51Z",
      "updated": "2024-08-28T08:14:51Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15616v1",
      "landing_url": "https://arxiv.org/abs/2408.15616v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-32"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2408.15676",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.15676v1",
      "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
      "summary": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
      "published": "2024-08-28T09:57:17Z"
    },
    "metadata": {
      "arxiv_id": "2408.15676",
      "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
      "summary": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
      "authors": [
        "Yixuan Zhou",
        "Xiaoyu Qin",
        "Zeyu Jin",
        "Shuoyi Zhou",
        "Shun Lei",
        "Songtao Zhou",
        "Zhiyong Wu",
        "Jia Jia"
      ],
      "published": "2024-08-28T09:57:17Z",
      "updated": "2024-08-28T09:57:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15676v1",
      "landing_url": "https://arxiv.org/abs/2408.15676v1",
      "doi": "https://doi.org/10.1145/3664647.3681680"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2408.16373",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.16373v1",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "published": "2024-08-29T09:31:06Z"
    },
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.16546",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.16546v1",
      "title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates",
      "summary": "Voice conversion has gained increasing popularity within the field of audio manipulation and speech synthesis. Often, the main objective is to transfer the input identity to that of a target speaker without changing its linguistic content. While current work provides high-fidelity solutions they rarely focus on model simplicity, high-sampling rate environments or stream-ability. By incorporating speech representation learning into a generative timbre transfer model, traditionally created for musical purposes, we investigate the realm of voice conversion generated directly in the time domain at high sampling rates. More specifically, we guide the latent space of a baseline model towards linguistically relevant representations and condition it on external speaker information. Through objective and subjective assessments, we demonstrate that the proposed solution can attain levels of naturalness, quality, and intelligibility comparable to those of a state-of-the-art solution for seen speakers, while significantly decreasing inference time. However, despite the presence of target speaker characteristics in the converted output, the actual similarity to unseen speakers remains a challenge.",
      "published": "2024-08-29T14:09:37Z"
    },
    "metadata": {
      "arxiv_id": "2408.16546",
      "title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates",
      "summary": "Voice conversion has gained increasing popularity within the field of audio manipulation and speech synthesis. Often, the main objective is to transfer the input identity to that of a target speaker without changing its linguistic content. While current work provides high-fidelity solutions they rarely focus on model simplicity, high-sampling rate environments or stream-ability. By incorporating speech representation learning into a generative timbre transfer model, traditionally created for musical purposes, we investigate the realm of voice conversion generated directly in the time domain at high sampling rates. More specifically, we guide the latent space of a baseline model towards linguistically relevant representations and condition it on external speaker information. Through objective and subjective assessments, we demonstrate that the proposed solution can attain levels of naturalness, quality, and intelligibility comparable to those of a state-of-the-art solution for seen speakers, while significantly decreasing inference time. However, despite the presence of target speaker characteristics in the converted output, the actual similarity to unseen speakers remains a challenge.",
      "authors": [
        "Anders R. Bargum",
        "Simon Lajboschitz",
        "Cumhur Erkut"
      ],
      "published": "2024-08-29T14:09:37Z",
      "updated": "2024-08-29T14:09:37Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16546v1",
      "landing_url": "https://arxiv.org/abs/2408.16546v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16546"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2408.17131",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17131v1",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "published": "2024-08-30T09:15:54Z"
    },
    "metadata": {
      "arxiv_id": "2408.17131",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Hong Gu",
        "Kedong Xu",
        "Kejie Huang"
      ],
      "published": "2024-08-30T09:15:54Z",
      "updated": "2024-08-30T09:15:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17131v1",
      "landing_url": "https://arxiv.org/abs/2408.17131v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2408.17175",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17175v3",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "published": "2024-08-30T10:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.00750",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00750v3",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "published": "2024-09-01T15:26:30Z"
    },
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2409.00942",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00942v1",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "published": "2024-09-02T05:01:41Z"
    },
    "metadata": {
      "arxiv_id": "2409.00942",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "authors": [
        "Yixuan Zhou",
        "Xing Xu",
        "Zhe Sun",
        "Jingkuan Song",
        "Andrzej Cichocki",
        "Heng Tao Shen"
      ],
      "published": "2024-09-02T05:01:41Z",
      "updated": "2024-09-02T05:01:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00942v1",
      "landing_url": "https://arxiv.org/abs/2409.00942v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.00942"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.01217",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.01217v1",
      "title": "A multilingual training strategy for low resource Text to Speech",
      "summary": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
      "published": "2024-09-02T12:53:01Z"
    },
    "metadata": {
      "arxiv_id": "2409.01217",
      "title": "A multilingual training strategy for low resource Text to Speech",
      "summary": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
      "authors": [
        "Asma Amalas",
        "Mounir Ghogho",
        "Mohamed Chetouani",
        "Rachid Oulad Haj Thami"
      ],
      "published": "2024-09-02T12:53:01Z",
      "updated": "2024-09-02T12:53:01Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01217v1",
      "landing_url": "https://arxiv.org/abs/2409.01217v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01217"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2409.02384",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.02384v1",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "published": "2024-09-04T02:20:59Z"
    },
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2409.03283",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03283v2",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "published": "2024-09-05T06:48:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.03283",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Kun Liu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie",
        "Kai-Tuo Xu"
      ],
      "published": "2024-09-05T06:48:02Z",
      "updated": "2025-04-11T07:36:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03283v2",
      "landing_url": "https://arxiv.org/abs/2409.03283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03283"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2409.03377",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03377v4",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "published": "2024-09-05T09:28:56Z"
    },
    "metadata": {
      "arxiv_id": "2409.03377",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "authors": [
        "Yan Ru Pei",
        "Ritik Shrivastava",
        "FNU Sidharth"
      ],
      "published": "2024-09-05T09:28:56Z",
      "updated": "2025-05-20T02:23:19Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03377v4",
      "landing_url": "https://arxiv.org/abs/2409.03377v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.03377"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2409.03393",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03393v1",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "published": "2024-09-05T09:53:53Z"
    },
    "metadata": {
      "arxiv_id": "2409.03393",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "authors": [
        "Yongyi Miao",
        "Zhongdang Li",
        "Yang Wang",
        "Die Hu",
        "Jun Yan",
        "Youfang Wang"
      ],
      "published": "2024-09-05T09:53:53Z",
      "updated": "2024-09-05T09:53:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03393v1",
      "landing_url": "https://arxiv.org/abs/2409.03393v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.03393"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.03701",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03701v2",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "published": "2024-09-05T16:57:39Z"
    },
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2409.04016",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.04016v1",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "published": "2024-09-06T04:06:50Z"
    },
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2409.05004",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.05004v2",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "published": "2024-09-08T07:24:03Z"
    },
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2409.06066",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.06066v3",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "published": "2024-09-09T20:58:40Z"
    },
    "metadata": {
      "arxiv_id": "2409.06066",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "authors": [
        "Marcel Gregoriadis",
        "Leonhard Balduf",
        "Björn Scheuermann",
        "Johan Pouwelse"
      ],
      "published": "2024-09-09T20:58:40Z",
      "updated": "2024-09-28T19:05:45Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06066v3",
      "landing_url": "https://arxiv.org/abs/2409.06066v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.06066"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2409.07151",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07151v2",
      "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
      "summary": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
      "published": "2024-09-11T09:55:57Z"
    },
    "metadata": {
      "arxiv_id": "2409.07151",
      "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
      "summary": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
      "authors": [
        "Tien-Hong Lo",
        "Meng-Ting Tsai",
        "Yao-Ting Sung",
        "Berlin Chen"
      ],
      "published": "2024-09-11T09:55:57Z",
      "updated": "2025-07-26T06:14:20Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07151v2",
      "landing_url": "https://arxiv.org/abs/2409.07151v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07151"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2409.07276",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07276v3",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "published": "2024-09-11T13:49:48Z"
    },
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.07556",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07556v2",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "published": "2024-09-11T18:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2409.07966",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07966v4",
      "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
      "summary": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",
      "published": "2024-09-12T11:53:05Z"
    },
    "metadata": {
      "arxiv_id": "2409.07966",
      "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
      "summary": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",
      "authors": [
        "Sichun Wu",
        "Kazi Injamamul Haque",
        "Zerrin Yumak"
      ],
      "published": "2024-09-12T11:53:05Z",
      "updated": "2025-02-16T14:23:08Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07966v4",
      "landing_url": "https://arxiv.org/abs/2409.07966v4",
      "doi": "https://doi.org/10.1145/3677388.3696320"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2409.08039",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.08039v2",
      "title": "Zero-Shot Sing Voice Conversion: built upon clustering-based phoneme representations",
      "summary": "This study presents an innovative Zero-Shot any-to-any Singing Voice Conversion (SVC) method, leveraging a novel clustering-based phoneme representation to effectively separate content, timbre, and singing style. This approach enables precise voice characteristic manipulation. We discovered that datasets with fewer recordings per artist are more susceptible to timbre leakage. Extensive testing on over 10,000 hours of singing and user feedback revealed our model significantly improves sound quality and timbre accuracy, aligning with our objectives and advancing voice conversion technology. Furthermore, this research advances zero-shot SVC and sets the stage for future work on discrete speech representation, emphasizing the preservation of rhyme.",
      "published": "2024-09-12T13:42:04Z"
    },
    "metadata": {
      "arxiv_id": "2409.08039",
      "title": "Zero-Shot Sing Voice Conversion: built upon clustering-based phoneme representations",
      "summary": "This study presents an innovative Zero-Shot any-to-any Singing Voice Conversion (SVC) method, leveraging a novel clustering-based phoneme representation to effectively separate content, timbre, and singing style. This approach enables precise voice characteristic manipulation. We discovered that datasets with fewer recordings per artist are more susceptible to timbre leakage. Extensive testing on over 10,000 hours of singing and user feedback revealed our model significantly improves sound quality and timbre accuracy, aligning with our objectives and advancing voice conversion technology. Furthermore, this research advances zero-shot SVC and sets the stage for future work on discrete speech representation, emphasizing the preservation of rhyme.",
      "authors": [
        "Wangjin Zhou",
        "Fengrun Zhang",
        "Yiming Liu",
        "Wenhao Guan",
        "Yi Zhao",
        "Tatsuya Kawahara"
      ],
      "published": "2024-09-12T13:42:04Z",
      "updated": "2024-10-14T08:05:40Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08039v2",
      "landing_url": "https://arxiv.org/abs/2409.08039v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08039"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2409.08711",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.08711v2",
      "title": "Text-To-Speech Synthesis In The Wild",
      "summary": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
      "published": "2024-09-13T10:58:55Z"
    },
    "metadata": {
      "arxiv_id": "2409.08711",
      "title": "Text-To-Speech Synthesis In The Wild",
      "summary": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
      "authors": [
        "Jee-weon Jung",
        "Wangyou Zhang",
        "Soumi Maiti",
        "Yihan Wu",
        "Xin Wang",
        "Ji-Hoon Kim",
        "Yuta Matsunaga",
        "Seyun Um",
        "Jinchuan Tian",
        "Hye-jin Shim",
        "Nicholas Evans",
        "Joon Son Chung",
        "Shinnosuke Takamichi",
        "Shinji Watanabe"
      ],
      "published": "2024-09-13T10:58:55Z",
      "updated": "2025-06-01T09:29:36Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08711v2",
      "landing_url": "https://arxiv.org/abs/2409.08711v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08711"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2409.09253",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09253v1",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "published": "2024-09-14T01:45:04Z"
    },
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.09357",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09357v1",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "published": "2024-09-14T08:09:55Z"
    },
    "metadata": {
      "arxiv_id": "2409.09357",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "published": "2024-09-14T08:09:55Z",
      "updated": "2024-09-14T08:09:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09357v1",
      "landing_url": "https://arxiv.org/abs/2409.09357v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09357"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.09733",
    "anchor": "speech representation",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09733v5",
      "title": "Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms",
      "summary": "Multimodal schizophrenia assessment systems have gained traction over the last few years. This work introduces a schizophrenia assessment system to discern between prominent symptom classes of schizophrenia and predict an overall schizophrenia severity score. We develop a Vector Quantized Variational Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to produce task-agnostic speech representations from vocal Tract Variables (TVs) and Facial Action Units (FAUs). These representations are then used in a Multi-Task Learning (MTL) based downstream prediction model to obtain class labels and an overall severity score. The proposed framework outperforms the previous works on the multi-class classification task across all evaluation metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy). Additionally, it estimates the schizophrenia severity score, a task not addressed by earlier approaches.",
      "published": "2024-09-15T13:45:04Z"
    },
    "metadata": {
      "arxiv_id": "2409.09733",
      "title": "Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms",
      "summary": "Multimodal schizophrenia assessment systems have gained traction over the last few years. This work introduces a schizophrenia assessment system to discern between prominent symptom classes of schizophrenia and predict an overall schizophrenia severity score. We develop a Vector Quantized Variational Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to produce task-agnostic speech representations from vocal Tract Variables (TVs) and Facial Action Units (FAUs). These representations are then used in a Multi-Task Learning (MTL) based downstream prediction model to obtain class labels and an overall severity score. The proposed framework outperforms the previous works on the multi-class classification task across all evaluation metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy). Additionally, it estimates the schizophrenia severity score, a task not addressed by earlier approaches.",
      "authors": [
        "Gowtham Premananth",
        "Carol Espy-Wilson"
      ],
      "published": "2024-09-15T13:45:04Z",
      "updated": "2024-11-17T16:48:38Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09733v5",
      "landing_url": "https://arxiv.org/abs/2409.09733v5",
      "doi": "https://doi.org/10.48550/arXiv.2409.09733"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2409.10870",
    "anchor": "acoustic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10870v1",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "published": "2024-09-17T03:46:01Z"
    },
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2409.11003",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11003v1",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "published": "2024-09-17T09:08:43Z"
    },
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2409.11184",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11184v1",
      "title": "LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling",
      "summary": "Learning compact and meaningful latent space representations has been shown to be very useful in generative modeling tasks for visual data. One particular example is applying Vector Quantization (VQ) in variational autoencoders (VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance in many modern generative modeling applications. Quantizing the latent space has been justified by the assumption that the data themselves are inherently discrete in the latent space (like pixel values). In this paper, we propose an alternative representation of the latent space by relaxing the structural assumption than the VQ formulation. Specifically, we assume that the latent space can be approximated by a union of subspaces model corresponding to a dictionary-based representation under a sparsity constraint. The dictionary is learned/updated during the training process. We apply this approach to look at two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs with Generative Adversarial Networks (DL-GANs). We show empirically that our more latent space is more expressive and has leads to better representations than the VQ approach in terms of reconstruction quality at the expense of a small computational overhead for the latent space computation. Our results thus suggest that the true benefit of the VQ approach might not be from discretization of the latent space, but rather the lossy compression of the latent space. We confirm this hypothesis by showing that our sparse representations also address the codebook collapse issue as found common in VQ-family models.",
      "published": "2024-09-16T08:20:58Z"
    },
    "metadata": {
      "arxiv_id": "2409.11184",
      "title": "LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling",
      "summary": "Learning compact and meaningful latent space representations has been shown to be very useful in generative modeling tasks for visual data. One particular example is applying Vector Quantization (VQ) in variational autoencoders (VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance in many modern generative modeling applications. Quantizing the latent space has been justified by the assumption that the data themselves are inherently discrete in the latent space (like pixel values). In this paper, we propose an alternative representation of the latent space by relaxing the structural assumption than the VQ formulation. Specifically, we assume that the latent space can be approximated by a union of subspaces model corresponding to a dictionary-based representation under a sparsity constraint. The dictionary is learned/updated during the training process. We apply this approach to look at two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs with Generative Adversarial Networks (DL-GANs). We show empirically that our more latent space is more expressive and has leads to better representations than the VQ approach in terms of reconstruction quality at the expense of a small computational overhead for the latent space computation. Our results thus suggest that the true benefit of the VQ approach might not be from discretization of the latent space, but rather the lossy compression of the latent space. We confirm this hypothesis by showing that our sparse representations also address the codebook collapse issue as found common in VQ-family models.",
      "authors": [
        "Xin Li",
        "Anand Sarwate"
      ],
      "published": "2024-09-16T08:20:58Z",
      "updated": "2024-09-16T08:20:58Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11184v1",
      "landing_url": "https://arxiv.org/abs/2409.11184v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11184"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2409.11742",
    "anchor": "speech representation",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11742v2",
      "title": "Simulating Native Speaker Shadowing for Nonnative Speech Assessment with Latent Speech Representations",
      "summary": "Evaluating speech intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic speech recognition (ASR) as intelligibility scores. However, this approach has significant limitations due to notable differences between human speech recognition (HSR) and ASR. A promising alternative is to involve a native (L1) speaker in shadowing what nonnative (L2) speakers say. Breakdowns or mispronunciations in the L1 speaker's shadowing utterance can serve as indicators for assessing L2 speech intelligibility. In this study, we propose a speech generation system that simulates the L1 shadowing process using voice conversion (VC) techniques and latent speech representations. Our experimental results demonstrate that this method effectively replicates the L1 shadowing process, offering an innovative tool to evaluate L2 speech intelligibility. Notably, systems that utilize self-supervised speech representations (S3R) show a higher degree of similarity to real L1 shadowing utterances in both linguistic accuracy and naturalness.",
      "published": "2024-09-18T06:54:46Z"
    },
    "metadata": {
      "arxiv_id": "2409.11742",
      "title": "Simulating Native Speaker Shadowing for Nonnative Speech Assessment with Latent Speech Representations",
      "summary": "Evaluating speech intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic speech recognition (ASR) as intelligibility scores. However, this approach has significant limitations due to notable differences between human speech recognition (HSR) and ASR. A promising alternative is to involve a native (L1) speaker in shadowing what nonnative (L2) speakers say. Breakdowns or mispronunciations in the L1 speaker's shadowing utterance can serve as indicators for assessing L2 speech intelligibility. In this study, we propose a speech generation system that simulates the L1 shadowing process using voice conversion (VC) techniques and latent speech representations. Our experimental results demonstrate that this method effectively replicates the L1 shadowing process, offering an innovative tool to evaluate L2 speech intelligibility. Notably, systems that utilize self-supervised speech representations (S3R) show a higher degree of similarity to real L1 shadowing utterances in both linguistic accuracy and naturalness.",
      "authors": [
        "Haopeng Geng",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2024-09-18T06:54:46Z",
      "updated": "2024-09-19T01:19:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11742v2",
      "landing_url": "https://arxiv.org/abs/2409.11742v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11742"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.11883",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11883v1",
      "title": "Upgrading edges in the maximal covering location problem",
      "summary": "We study the upgrading version of the maximal covering location problem with edge length modifications on networks. This problem aims at locating p facilities on the vertices (of the network) so as to maximise coverage, considering that the length of the edges can be reduced at a cost, subject to a given budget. Hence, we have to decide on: the optimal location of p facilities and the optimal edge length reductions.\n  This problem is NP-hard on general graphs. To solve it, we propose three different mixed-integer formulations and a preprocessing phase for fixing variables and removing some of the constraints. Moreover, we strengthen the proposed formulations including valid inequalities. Finally, we compare the three formulations and their corresponding improvements by testing their performance over different datasets.",
      "published": "2024-09-18T11:30:00Z"
    },
    "metadata": {
      "arxiv_id": "2409.11883",
      "title": "Upgrading edges in the maximal covering location problem",
      "summary": "We study the upgrading version of the maximal covering location problem with edge length modifications on networks. This problem aims at locating p facilities on the vertices (of the network) so as to maximise coverage, considering that the length of the edges can be reduced at a cost, subject to a given budget. Hence, we have to decide on: the optimal location of p facilities and the optimal edge length reductions.\n  This problem is NP-hard on general graphs. To solve it, we propose three different mixed-integer formulations and a preprocessing phase for fixing variables and removing some of the constraints. Moreover, we strengthen the proposed formulations including valid inequalities. Finally, we compare the three formulations and their corresponding improvements by testing their performance over different datasets.",
      "authors": [
        "Marta Baldomero-Naranjo",
        "Jörg Kalcsics",
        "Alfredo Marín",
        "Antonio M. Rodríguez-Chía"
      ],
      "published": "2024-09-18T11:30:00Z",
      "updated": "2024-09-18T11:30:00Z",
      "categories": [
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11883v1",
      "landing_url": "https://arxiv.org/abs/2409.11883v1",
      "doi": "https://doi.org/10.1016/j.ejor.2022.02.001"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2409.12176",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12176v1",
      "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
      "summary": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
      "published": "2024-09-10T11:50:35Z"
    },
    "metadata": {
      "arxiv_id": "2409.12176",
      "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
      "summary": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
      "authors": [
        "Podakanti Satyajith Chary"
      ],
      "published": "2024-09-10T11:50:35Z",
      "updated": "2024-09-10T11:50:35Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12176v1",
      "landing_url": "https://arxiv.org/abs/2409.12176v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12176"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2409.12717",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12717v1",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "published": "2024-09-19T12:41:30Z"
    },
    "metadata": {
      "arxiv_id": "2409.12717",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "authors": [
        "Zhikang Niu",
        "Sanyuan Chen",
        "Long Zhou",
        "Ziyang Ma",
        "Xie Chen",
        "Shujie Liu"
      ],
      "published": "2024-09-19T12:41:30Z",
      "updated": "2024-09-19T12:41:30Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12717v1",
      "landing_url": "https://arxiv.org/abs/2409.12717v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12717"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.14919",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.14919v1",
      "title": "Voice Conversion-based Privacy through Adversarial Information Hiding",
      "summary": "Privacy-preserving voice conversion aims to remove only the attributes of speech audio that convey identity information, keeping other speech characteristics intact. This paper presents a mechanism for privacy-preserving voice conversion that allows controlling the leakage of identity-bearing information using adversarial information hiding. This enables a deliberate trade-off between maintaining source-speech characteristics and modification of speaker identity. As such, the approach improves on voice-conversion techniques like CycleGAN and StarGAN, which were not designed for privacy, meaning that converted speech may leak personal information in unpredictable ways. Our approach is also more flexible than ASR-TTS voice conversion pipelines, which by design discard all prosodic information linked to textual content. Evaluations show that the proposed system successfully modifies perceived speaker identity whilst well maintaining source lexical content.",
      "published": "2024-09-23T11:16:49Z"
    },
    "metadata": {
      "arxiv_id": "2409.14919",
      "title": "Voice Conversion-based Privacy through Adversarial Information Hiding",
      "summary": "Privacy-preserving voice conversion aims to remove only the attributes of speech audio that convey identity information, keeping other speech characteristics intact. This paper presents a mechanism for privacy-preserving voice conversion that allows controlling the leakage of identity-bearing information using adversarial information hiding. This enables a deliberate trade-off between maintaining source-speech characteristics and modification of speaker identity. As such, the approach improves on voice-conversion techniques like CycleGAN and StarGAN, which were not designed for privacy, meaning that converted speech may leak personal information in unpredictable ways. Our approach is also more flexible than ASR-TTS voice conversion pipelines, which by design discard all prosodic information linked to textual content. Evaluations show that the proposed system successfully modifies perceived speaker identity whilst well maintaining source lexical content.",
      "authors": [
        "Jacob J Webber",
        "Oliver Watts",
        "Gustav Eje Henter",
        "Jennifer Williams",
        "Simon King"
      ],
      "published": "2024-09-23T11:16:49Z",
      "updated": "2024-09-23T11:16:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14919v1",
      "landing_url": "https://arxiv.org/abs/2409.14919v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14919"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2409.15741",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15741v1",
      "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
      "summary": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
      "published": "2024-09-24T04:55:17Z"
    },
    "metadata": {
      "arxiv_id": "2409.15741",
      "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
      "summary": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
      "authors": [
        "Zhiyong Chen",
        "Xinnuo Li",
        "Zhiqi Ai",
        "Shugong Xu"
      ],
      "published": "2024-09-24T04:55:17Z",
      "updated": "2024-09-24T04:55:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15741v1",
      "landing_url": "https://arxiv.org/abs/2409.15741v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15741"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2409.15882",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15882v1",
      "title": "Exploring VQ-VAE with Prosody Parameters for Speaker Anonymization",
      "summary": "Human speech conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these speech components. This approach is designed to disentangle these components to specifically target and modify the speaker identity while preserving the linguistic and emotionalcontent. To do so, three separate branches compute embeddings for content, prosody, and speaker identity respectively. During synthesis, taking these embeddings, the decoder of the proposed architecture is conditioned on both speaker and prosody information, allowing for capturing more nuanced emotional states and precise adjustments to speaker identification. Findings indicate that this method outperforms most baseline techniques in preserving emotional information. However, it exhibits more limited performance on other voice privacy tasks, emphasizing the need for further improvements.",
      "published": "2024-09-24T08:55:10Z"
    },
    "metadata": {
      "arxiv_id": "2409.15882",
      "title": "Exploring VQ-VAE with Prosody Parameters for Speaker Anonymization",
      "summary": "Human speech conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these speech components. This approach is designed to disentangle these components to specifically target and modify the speaker identity while preserving the linguistic and emotionalcontent. To do so, three separate branches compute embeddings for content, prosody, and speaker identity respectively. During synthesis, taking these embeddings, the decoder of the proposed architecture is conditioned on both speaker and prosody information, allowing for capturing more nuanced emotional states and precise adjustments to speaker identification. Findings indicate that this method outperforms most baseline techniques in preserving emotional information. However, it exhibits more limited performance on other voice privacy tasks, emphasizing the need for further improvements.",
      "authors": [
        "Sotheara Leang",
        "Anderson Augusma",
        "Eric Castelli",
        "Frédérique Letué",
        "Sethserey Sam",
        "Dominique Vaufreydaz"
      ],
      "published": "2024-09-24T08:55:10Z",
      "updated": "2024-09-24T08:55:10Z",
      "categories": [
        "cs.CV",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15882v1",
      "landing_url": "https://arxiv.org/abs/2409.15882v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15882"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2409.16302",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.16302v2",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "published": "2024-09-10T11:00:24Z"
    },
    "metadata": {
      "arxiv_id": "2409.16302",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "authors": [
        "Teresa Dorszewski",
        "Albert Kjøller Jacobsen",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-09-10T11:00:24Z",
      "updated": "2025-01-17T12:27:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16302v2",
      "landing_url": "https://arxiv.org/abs/2409.16302v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.16302"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.18042",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.18042v4",
      "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
      "summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
      "published": "2024-09-26T16:44:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.18042",
      "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
      "summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
      "authors": [
        "Kai Chen",
        "Yunhao Gou",
        "Runhui Huang",
        "Zhili Liu",
        "Daxin Tan",
        "Jing Xu",
        "Chunwei Wang",
        "Yi Zhu",
        "Yihan Zeng",
        "Kuo Yang",
        "Dingdong Wang",
        "Kun Xiang",
        "Haoyuan Li",
        "Haoli Bai",
        "Jianhua Han",
        "Xiaohui Li",
        "Weike Jin",
        "Nian Xie",
        "Yu Zhang",
        "James T. Kwok",
        "Hengshuang Zhao",
        "Xiaodan Liang",
        "Dit-Yan Yeung",
        "Xiao Chen",
        "Zhenguo Li",
        "Wei Zhang",
        "Qun Liu",
        "Jun Yao",
        "Lanqing Hong",
        "Lu Hou",
        "Hang Xu"
      ],
      "published": "2024-09-26T16:44:02Z",
      "updated": "2025-03-20T08:47:39Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18042v4",
      "landing_url": "https://arxiv.org/abs/2409.18042v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.18042"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2410.00037",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00037v2",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "published": "2024-09-17T17:55:39Z"
    },
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2410.01141",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.01141v3",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "published": "2024-10-02T00:43:10Z"
    },
    "metadata": {
      "arxiv_id": "2410.01141",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "authors": [
        "Doohee You",
        "S Fraiberger"
      ],
      "published": "2024-10-02T00:43:10Z",
      "updated": "2025-06-30T18:26:08Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01141v3",
      "landing_url": "https://arxiv.org/abs/2410.01141v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.01141"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2410.01912",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.01912v1",
      "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
      "summary": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
      "published": "2024-10-02T18:10:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.01912",
      "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
      "summary": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
      "authors": [
        "Liang Chen",
        "Sinan Tan",
        "Zefan Cai",
        "Weichu Xie",
        "Haozhe Zhao",
        "Yichi Zhang",
        "Junyang Lin",
        "Jinze Bai",
        "Tianyu Liu",
        "Baobao Chang"
      ],
      "published": "2024-10-02T18:10:05Z",
      "updated": "2024-10-02T18:10:05Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01912v1",
      "landing_url": "https://arxiv.org/abs/2410.01912v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.01912"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2410.03298",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.03298v1",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "published": "2024-10-04T10:21:15Z"
    },
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2410.04690",
    "anchor": "semantic tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04690v1",
      "title": "SegINR: Segment-wise Implicit Neural Representation for Sequence Alignment in Neural Text-to-Speech",
      "summary": "We present SegINR, a novel approach to neural Text-to-Speech (TTS) that addresses sequence alignment without relying on an auxiliary duration predictor and complex autoregressive (AR) or non-autoregressive (NAR) frame-level sequence modeling. SegINR simplifies the process by converting text sequences directly into frame-level features. It leverages an optimal text encoder to extract embeddings, transforming each into a segment of frame-level features using a conditional implicit neural representation (INR). This method, named segment-wise INR (SegINR), models temporal dynamics within each segment and autonomously defines segment boundaries, reducing computational costs. We integrate SegINR into a two-stage TTS framework, using it for semantic token prediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate that SegINR outperforms conventional methods in speech quality with computational efficiency.",
      "published": "2024-10-07T02:04:58Z"
    },
    "metadata": {
      "arxiv_id": "2410.04690",
      "title": "SegINR: Segment-wise Implicit Neural Representation for Sequence Alignment in Neural Text-to-Speech",
      "summary": "We present SegINR, a novel approach to neural Text-to-Speech (TTS) that addresses sequence alignment without relying on an auxiliary duration predictor and complex autoregressive (AR) or non-autoregressive (NAR) frame-level sequence modeling. SegINR simplifies the process by converting text sequences directly into frame-level features. It leverages an optimal text encoder to extract embeddings, transforming each into a segment of frame-level features using a conditional implicit neural representation (INR). This method, named segment-wise INR (SegINR), models temporal dynamics within each segment and autonomously defines segment boundaries, reducing computational costs. We integrate SegINR into a two-stage TTS framework, using it for semantic token prediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate that SegINR outperforms conventional methods in speech quality with computational efficiency.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-10-07T02:04:58Z",
      "updated": "2024-10-07T02:04:58Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04690v1",
      "landing_url": "https://arxiv.org/abs/2410.04690v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04690"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2410.05902",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.05902v1",
      "title": "Mini-Batch Kernel $k$-means",
      "summary": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetildeΩ(\\max \\{γ^{4}, γ^{2}\\} \\cdot ε^{-2})$, the algorithm terminates in $O(γ^2/ε)$ iterations with high probability, where $γ$ bounds the norm of points in feature space and $ε$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $γ=1$. Taking $ε= O(1)$ and $b=Θ(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time.",
      "published": "2024-10-08T10:59:14Z"
    },
    "metadata": {
      "arxiv_id": "2410.05902",
      "title": "Mini-Batch Kernel $k$-means",
      "summary": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetildeΩ(\\max \\{γ^{4}, γ^{2}\\} \\cdot ε^{-2})$, the algorithm terminates in $O(γ^2/ε)$ iterations with high probability, where $γ$ bounds the norm of points in feature space and $ε$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $γ=1$. Taking $ε= O(1)$ and $b=Θ(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time.",
      "authors": [
        "Ben Jourdan",
        "Gregory Schwartzman"
      ],
      "published": "2024-10-08T10:59:14Z",
      "updated": "2024-10-08T10:59:14Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05902v1",
      "landing_url": "https://arxiv.org/abs/2410.05902v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.05902"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2410.06016",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06016v3",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "published": "2024-10-08T13:18:24Z"
    },
    "metadata": {
      "arxiv_id": "2410.06016",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "authors": [
        "Yunkee Chae",
        "Woosung Choi",
        "Yuhta Takida",
        "Junghyun Koo",
        "Yukara Ikemiya",
        "Zhi Zhong",
        "Kin Wai Cheuk",
        "Marco A. Martínez-Ramírez",
        "Kyogu Lee",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2024-10-08T13:18:24Z",
      "updated": "2025-04-27T15:10:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06016v3",
      "landing_url": "https://arxiv.org/abs/2410.06016v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.06016"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.06424",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06424v2",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "published": "2024-10-08T23:39:34Z"
    },
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2410.07168",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.07168v2",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "published": "2024-10-09T17:59:04Z"
    },
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.08325",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08325v1",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "published": "2024-10-10T19:29:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2410.08469",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08469v2",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "published": "2024-10-11T02:42:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2410.10180",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.10180v1",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "published": "2024-10-14T05:58:11Z"
    },
    "metadata": {
      "arxiv_id": "2410.10180",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "authors": [
        "Mingyuan Yan",
        "Jiawei Wu",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-10-14T05:58:11Z",
      "updated": "2024-10-14T05:58:11Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10180v1",
      "landing_url": "https://arxiv.org/abs/2410.10180v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10180"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.11062",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11062v2",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "published": "2024-10-14T20:18:03Z"
    },
    "metadata": {
      "arxiv_id": "2410.11062",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "authors": [
        "Sjoerd Groot",
        "Qinyu Chen",
        "Jan C. van Gemert",
        "Chang Gao"
      ],
      "published": "2024-10-14T20:18:03Z",
      "updated": "2025-02-10T18:07:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11062v2",
      "landing_url": "https://arxiv.org/abs/2410.11062v2",
      "doi": "https://doi.org/10.1109/ISCAS56072.2025.11043389"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2410.12359",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.12359v2",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "published": "2024-10-16T08:21:37Z"
    },
    "metadata": {
      "arxiv_id": "2410.12359",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "authors": [
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-16T08:21:37Z",
      "updated": "2025-06-11T08:43:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12359v2",
      "landing_url": "https://arxiv.org/abs/2410.12359v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.12359"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.15017",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15017v2",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "published": "2024-10-19T07:14:14Z"
    },
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2410.15704",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15704v1",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "published": "2024-10-21T07:20:41Z"
    },
    "metadata": {
      "arxiv_id": "2410.15704",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "authors": [
        "Ankur Kumar"
      ],
      "published": "2024-10-21T07:20:41Z",
      "updated": "2024-10-21T07:20:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15704v1",
      "landing_url": "https://arxiv.org/abs/2410.15704v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15704"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.15764",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15764v3",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "published": "2024-10-21T08:23:31Z"
    },
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.16926",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.16926v2",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "published": "2024-10-22T11:57:32Z"
    },
    "metadata": {
      "arxiv_id": "2410.16926",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "authors": [
        "Tycho F. A. van der Ouderaa",
        "Maximilian L. Croci",
        "Agrin Hilmkil",
        "James Hensman"
      ],
      "published": "2024-10-22T11:57:32Z",
      "updated": "2024-12-04T10:52:04Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16926v2",
      "landing_url": "https://arxiv.org/abs/2410.16926v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16926"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.17081",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17081v2",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "published": "2024-10-22T15:02:37Z"
    },
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2410.17256",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17256v1",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "published": "2024-10-04T06:51:58Z"
    },
    "metadata": {
      "arxiv_id": "2410.17256",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "authors": [
        "Alfred K. Adzika",
        "Prudence Djagba"
      ],
      "published": "2024-10-04T06:51:58Z",
      "updated": "2024-10-04T06:51:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17256v1",
      "landing_url": "https://arxiv.org/abs/2410.17256v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17256"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2410.18283",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.18283v1",
      "title": "Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals",
      "summary": "Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.",
      "published": "2024-10-23T21:17:45Z"
    },
    "metadata": {
      "arxiv_id": "2410.18283",
      "title": "Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals",
      "summary": "Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.",
      "authors": [
        "Srihari Kamesh Kompella",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "published": "2024-10-23T21:17:45Z",
      "updated": "2024-10-23T21:17:45Z",
      "categories": [
        "cs.LG",
        "cs.NI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18283v1",
      "landing_url": "https://arxiv.org/abs/2410.18283v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.18283"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2410.19199",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.19199v1",
      "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
      "summary": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
      "published": "2024-10-24T23:18:02Z"
    },
    "metadata": {
      "arxiv_id": "2410.19199",
      "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
      "summary": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
      "authors": [
        "Suparna De",
        "Ionut Bostan",
        "Nishanth Sastry"
      ],
      "published": "2024-10-24T23:18:02Z",
      "updated": "2024-10-24T23:18:02Z",
      "categories": [
        "cs.SI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19199v1",
      "landing_url": "https://arxiv.org/abs/2410.19199v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.19199"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2410.20336",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.20336v1",
      "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
      "summary": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
      "published": "2024-10-27T04:28:57Z"
    },
    "metadata": {
      "arxiv_id": "2410.20336",
      "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
      "summary": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
      "authors": [
        "Maohao Shen",
        "Shun Zhang",
        "Jilong Wu",
        "Zhiping Xiu",
        "Ehab AlBadawy",
        "Yiting Lu",
        "Mike Seltzer",
        "Qing He"
      ],
      "published": "2024-10-27T04:28:57Z",
      "updated": "2024-10-27T04:28:57Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20336v1",
      "landing_url": "https://arxiv.org/abs/2410.20336v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.20336"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2410.20573",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.20573v2",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "published": "2024-10-27T19:56:02Z"
    },
    "metadata": {
      "arxiv_id": "2410.20573",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "authors": [
        "Mohammad Hassan Vali",
        "Tom Bäckström"
      ],
      "published": "2024-10-27T19:56:02Z",
      "updated": "2025-07-02T10:27:13Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20573v2",
      "landing_url": "https://arxiv.org/abs/2410.20573v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.20573"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.21951",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.21951v2",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "published": "2024-10-29T11:12:01Z"
    },
    "metadata": {
      "arxiv_id": "2410.21951",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "authors": [
        "Bohan Li",
        "Hankun Wang",
        "Situo Zhang",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2024-10-29T11:12:01Z",
      "updated": "2025-02-10T04:22:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21951v2",
      "landing_url": "https://arxiv.org/abs/2410.21951v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21951"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2410.24177",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.24177v1",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "published": "2024-10-31T17:43:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2411.01274",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.01274v1",
      "title": "Efficient Collaborative Navigation through Perception Fusion for Multi-Robots in Unknown Environments",
      "summary": "For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.",
      "published": "2024-11-02T14:53:26Z"
    },
    "metadata": {
      "arxiv_id": "2411.01274",
      "title": "Efficient Collaborative Navigation through Perception Fusion for Multi-Robots in Unknown Environments",
      "summary": "For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.",
      "authors": [
        "Qingquan Lin",
        "Weining Lu",
        "Litong Meng",
        "Chenxi Li",
        "Bin Liang"
      ],
      "published": "2024-11-02T14:53:26Z",
      "updated": "2024-11-02T14:53:26Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01274v1",
      "landing_url": "https://arxiv.org/abs/2411.01274v1",
      "doi": "https://doi.org/10.1016/j.neucom.2025.132444"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2411.01407",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.01407v1",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "published": "2024-11-03T02:14:03Z"
    },
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.04257",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04257v3",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "published": "2024-11-06T21:00:45Z"
    },
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.04530",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04530v2",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "published": "2024-11-07T08:38:32Z"
    },
    "metadata": {
      "arxiv_id": "2411.04530",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "authors": [
        "Crystina Zhang",
        "Jing Lu",
        "Vinh Q. Tran",
        "Tal Schuster",
        "Donald Metzler",
        "Jimmy Lin"
      ],
      "published": "2024-11-07T08:38:32Z",
      "updated": "2025-11-19T00:30:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04530v2",
      "landing_url": "https://arxiv.org/abs/2411.04530v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.04530"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2411.06968",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.06968v1",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "published": "2024-11-11T13:17:24Z"
    },
    "metadata": {
      "arxiv_id": "2411.06968",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "authors": [
        "Yoshiki Masuyama",
        "Koichi Miyazaki",
        "Masato Murata"
      ],
      "published": "2024-11-11T13:17:24Z",
      "updated": "2024-11-11T13:17:24Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06968v1",
      "landing_url": "https://arxiv.org/abs/2411.06968v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06968"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.08742",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.08742v1",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "published": "2024-11-13T16:20:20Z"
    },
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2411.09431",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.09431v1",
      "title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data",
      "summary": "Recent research has shown that state-of-the-art (SotA) Automatic Speech Recognition (ASR) systems, such as Whisper, often exhibit predictive biases that disproportionately affect various demographic groups. This study focuses on identifying the performance disparities of Whisper models on Dutch speech data from the Common Voice dataset and the Dutch National Public Broadcasting organisation. We analyzed the word error rate, character error rate and a BERT-based semantic similarity across gender groups. We used the moral framework of Weerts et al. (2022) to assess quality of service harms and fairness, and to provide a nuanced discussion on the implications of these biases, particularly for automatic subtitling. Our findings reveal substantial disparities in word error rate (WER) among gender groups across all model sizes, with bias identified through statistical testing.",
      "published": "2024-11-14T13:29:09Z"
    },
    "metadata": {
      "arxiv_id": "2411.09431",
      "title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data",
      "summary": "Recent research has shown that state-of-the-art (SotA) Automatic Speech Recognition (ASR) systems, such as Whisper, often exhibit predictive biases that disproportionately affect various demographic groups. This study focuses on identifying the performance disparities of Whisper models on Dutch speech data from the Common Voice dataset and the Dutch National Public Broadcasting organisation. We analyzed the word error rate, character error rate and a BERT-based semantic similarity across gender groups. We used the moral framework of Weerts et al. (2022) to assess quality of service harms and fairness, and to provide a nuanced discussion on the implications of these biases, particularly for automatic subtitling. Our findings reveal substantial disparities in word error rate (WER) among gender groups across all model sizes, with bias identified through statistical testing.",
      "authors": [
        "Rik Raes",
        "Saskia Lensink",
        "Mykola Pechenizkiy"
      ],
      "published": "2024-11-14T13:29:09Z",
      "updated": "2024-11-14T13:29:09Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09431v1",
      "landing_url": "https://arxiv.org/abs/2411.09431v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09431"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2411.09943",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.09943v1",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "published": "2024-11-15T04:43:44Z"
    },
    "metadata": {
      "arxiv_id": "2411.09943",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "authors": [
        "Songting Liu"
      ],
      "published": "2024-11-15T04:43:44Z",
      "updated": "2024-11-15T04:43:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09943v1",
      "landing_url": "https://arxiv.org/abs/2411.09943v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09943"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2411.10293",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.10293v3",
      "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
      "summary": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
      "published": "2024-11-15T15:51:25Z"
    },
    "metadata": {
      "arxiv_id": "2411.10293",
      "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
      "summary": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
      "authors": [
        "Ryoma Yataka",
        "Adriano Cardace",
        "Pu Perry Wang",
        "Petros Boufounos",
        "Ryuhei Takahashi"
      ],
      "published": "2024-11-15T15:51:25Z",
      "updated": "2025-01-17T19:06:26Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "math.DG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10293v3",
      "landing_url": "https://arxiv.org/abs/2411.10293v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.10293"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2411.14100",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.14100v2",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "published": "2024-11-21T13:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2411.14642",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.14642v1",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "published": "2024-11-22T00:21:39Z"
    },
    "metadata": {
      "arxiv_id": "2411.14642",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "authors": [
        "Armani Rodriguez",
        "Silvija Kokalj-Filipovic"
      ],
      "published": "2024-11-22T00:21:39Z",
      "updated": "2024-11-22T00:21:39Z",
      "categories": [
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14642v1",
      "landing_url": "https://arxiv.org/abs/2411.14642v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.14642"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2411.16119",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16119v1",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "published": "2024-11-25T06:05:08Z"
    },
    "metadata": {
      "arxiv_id": "2411.16119",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu"
      ],
      "published": "2024-11-25T06:05:08Z",
      "updated": "2024-11-25T06:05:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16119v1",
      "landing_url": "https://arxiv.org/abs/2411.16119v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16119"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2411.16156",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16156v2",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "published": "2024-11-25T07:32:02Z"
    },
    "metadata": {
      "arxiv_id": "2411.16156",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "authors": [
        "Yicheng Feng",
        "Yijiang Li",
        "Wanpeng Zhang",
        "Hao Luo",
        "Zihao Yue",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "published": "2024-11-25T07:32:02Z",
      "updated": "2025-03-18T08:15:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16156v2",
      "landing_url": "https://arxiv.org/abs/2411.16156v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.16156"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2411.16550",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16550v1",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "published": "2024-11-25T16:32:29Z"
    },
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2411.16971",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16971v1",
      "title": "Generative vs. Predictive Models in Massive MIMO Channel Prediction",
      "summary": "Massive MIMO (mMIMO) systems are essential for 5G/6G networks to meet high throughput and reliability demands, with machine learning (ML)-based techniques, particularly autoencoders (AEs), showing promise for practical deployment. However, standard AEs struggle under noisy channel conditions, limiting their effectiveness. This work introduces a Vector Quantization-based generative AE model (VQ-VAE) for robust mMIMO cross-antenna channel prediction. We compare Generative and Predictive AE-based models, demonstrating that Generative models outperform Predictive ones, especially in noisy environments. The proposed VQ-VAE achieves up to 15 [dB] NMSE gains over standard AEs and about 9 [dB] over VAEs. Additionally, we present a complexity analysis of AE-based models alongside a diffusion model, highlighting the trade-off between accuracy and computational efficiency.",
      "published": "2024-11-25T22:43:22Z"
    },
    "metadata": {
      "arxiv_id": "2411.16971",
      "title": "Generative vs. Predictive Models in Massive MIMO Channel Prediction",
      "summary": "Massive MIMO (mMIMO) systems are essential for 5G/6G networks to meet high throughput and reliability demands, with machine learning (ML)-based techniques, particularly autoencoders (AEs), showing promise for practical deployment. However, standard AEs struggle under noisy channel conditions, limiting their effectiveness. This work introduces a Vector Quantization-based generative AE model (VQ-VAE) for robust mMIMO cross-antenna channel prediction. We compare Generative and Predictive AE-based models, demonstrating that Generative models outperform Predictive ones, especially in noisy environments. The proposed VQ-VAE achieves up to 15 [dB] NMSE gains over standard AEs and about 9 [dB] over VAEs. Additionally, we present a complexity analysis of AE-based models alongside a diffusion model, highlighting the trade-off between accuracy and computational efficiency.",
      "authors": [
        "Ju-Hyung Lee",
        "Joohan Lee",
        "Andreas F. Molisch"
      ],
      "published": "2024-11-25T22:43:22Z",
      "updated": "2024-11-25T22:43:22Z",
      "categories": [
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16971v1",
      "landing_url": "https://arxiv.org/abs/2411.16971v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16971"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2411.17100",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17100v2",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "published": "2024-11-26T04:37:11Z"
    },
    "metadata": {
      "arxiv_id": "2411.17100",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "authors": [
        "Yifan Yang",
        "Jianheng Zhuo",
        "Zengrui Jin",
        "Ziyang Ma",
        "Xiaoyu Yang",
        "Zengwei Yao",
        "Liyong Guo",
        "Wei Kang",
        "Fangjun Kuang",
        "Long Lin",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2024-11-26T04:37:11Z",
      "updated": "2025-03-22T04:46:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17100v2",
      "landing_url": "https://arxiv.org/abs/2411.17100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17100"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2411.17607",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17607v2",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "published": "2024-11-26T17:19:09Z"
    },
    "metadata": {
      "arxiv_id": "2411.17607",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "Shengmin Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-11-26T17:19:09Z",
      "updated": "2024-12-02T16:13:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17607v2",
      "landing_url": "https://arxiv.org/abs/2411.17607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17607"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2411.17666",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17666v2",
      "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
      "summary": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
      "published": "2024-11-26T18:29:11Z"
    },
    "metadata": {
      "arxiv_id": "2411.17666",
      "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
      "summary": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
      "authors": [
        "Hyunji Lee",
        "Danni Liu",
        "Supriti Sinhamahapatra",
        "Jan Niehues"
      ],
      "published": "2024-11-26T18:29:11Z",
      "updated": "2025-02-20T18:04:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17666v2",
      "landing_url": "https://arxiv.org/abs/2411.17666v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17666"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2411.17773",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17773v2",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "published": "2024-11-26T09:36:02Z"
    },
    "metadata": {
      "arxiv_id": "2411.17773",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "authors": [
        "Minbin Huang",
        "Runhui Huang",
        "Han Shi",
        "Yimeng Chen",
        "Chuanyang Zheng",
        "Xiangguo Sun",
        "Xin Jiang",
        "Zhenguo Li",
        "Hong Cheng"
      ],
      "published": "2024-11-26T09:36:02Z",
      "updated": "2024-12-02T14:55:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17773v2",
      "landing_url": "https://arxiv.org/abs/2411.17773v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17773"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2411.17998",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17998v1",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "published": "2024-11-27T02:31:52Z"
    },
    "metadata": {
      "arxiv_id": "2411.17998",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "authors": [
        "Jia Qi Yip",
        "Chin Yuen Kwok",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2024-11-27T02:31:52Z",
      "updated": "2024-11-27T02:31:52Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17998v1",
      "landing_url": "https://arxiv.org/abs/2411.17998v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.17998"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2412.01762",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.01762v1",
      "title": "XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive Generation",
      "summary": "Image tokenizers play a critical role in shaping the performance of subsequent generative models. Since the introduction of VQ-GAN, discrete image tokenization has undergone remarkable advancements. Improvements in architecture, quantization techniques, and training recipes have significantly enhanced both image reconstruction and the downstream generation quality. In this paper, we present XQ-GAN, an image tokenization framework designed for both image reconstruction and generation tasks. Our framework integrates state-of-the-art quantization techniques, including vector quantization (VQ), residual quantization (RQ), multi-scale residual quantization (MSVQ), product quantization (PQ), lookup-free quantization (LFQ), and binary spherical quantization (BSQ), within a highly flexible and customizable training environment. On the standard ImageNet 256x256 benchmark, our released model achieves an rFID of 0.64, significantly surpassing MAGVIT-v2 (0.9 rFID) and VAR (0.9 rFID). Furthermore, we demonstrate that using XQ-GAN as a tokenizer improves gFID metrics alongside rFID. For instance, with the same VAR architecture, XQ-GAN+VAR achieves a gFID of 2.6, outperforming VAR's 3.3 gFID by a notable margin. To support further research, we provide pre-trained weights of different image tokenizers for the community to directly train the subsequent generative models on it or fine-tune for specialized tasks.",
      "published": "2024-12-02T17:58:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.01762",
      "title": "XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive Generation",
      "summary": "Image tokenizers play a critical role in shaping the performance of subsequent generative models. Since the introduction of VQ-GAN, discrete image tokenization has undergone remarkable advancements. Improvements in architecture, quantization techniques, and training recipes have significantly enhanced both image reconstruction and the downstream generation quality. In this paper, we present XQ-GAN, an image tokenization framework designed for both image reconstruction and generation tasks. Our framework integrates state-of-the-art quantization techniques, including vector quantization (VQ), residual quantization (RQ), multi-scale residual quantization (MSVQ), product quantization (PQ), lookup-free quantization (LFQ), and binary spherical quantization (BSQ), within a highly flexible and customizable training environment. On the standard ImageNet 256x256 benchmark, our released model achieves an rFID of 0.64, significantly surpassing MAGVIT-v2 (0.9 rFID) and VAR (0.9 rFID). Furthermore, we demonstrate that using XQ-GAN as a tokenizer improves gFID metrics alongside rFID. For instance, with the same VAR architecture, XQ-GAN+VAR achieves a gFID of 2.6, outperforming VAR's 3.3 gFID by a notable margin. To support further research, we provide pre-trained weights of different image tokenizers for the community to directly train the subsequent generative models on it or fine-tune for specialized tasks.",
      "authors": [
        "Xiang Li",
        "Kai Qiu",
        "Hao Chen",
        "Jason Kuen",
        "Jiuxiang Gu",
        "Jindong Wang",
        "Zhe Lin",
        "Bhiksha Raj"
      ],
      "published": "2024-12-02T17:58:06Z",
      "updated": "2024-12-02T17:58:06Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01762v1",
      "landing_url": "https://arxiv.org/abs/2412.01762v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.01762"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2412.02563",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.02563v1",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "published": "2024-12-03T16:52:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2412.02612",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.02612v1",
      "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot",
      "summary": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.",
      "published": "2024-12-03T17:41:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.02612",
      "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot",
      "summary": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Kedong Wang",
        "Shengmin Jiang",
        "Lei Zhao",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-12-03T17:41:24Z",
      "updated": "2024-12-03T17:41:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02612v1",
      "landing_url": "https://arxiv.org/abs/2412.02612v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02612"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2412.03074",
    "anchor": "speech representation",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.03074v1",
      "title": "Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model",
      "summary": "We examine the text-free speech representations of raw audio obtained from a self-supervised learning (SSL) model by analyzing the synthesized speech using the SSL representations instead of conventional text representations. Since raw audio does not have paired speech representations as transcribed texts do, obtaining speech representations from unpaired speech is crucial for augmenting available datasets for speech synthesis. Specifically, the proposed speech synthesis is conducted using discrete symbol representations from the SSL model in comparison with text representations, and analytical examinations of the synthesized speech have been carried out. The results empirically show that using text representations is advantageous for preserving semantic information, while using discrete symbol representations is superior for preserving acoustic content, including prosodic and intonational information.",
      "published": "2024-12-04T06:52:03Z"
    },
    "metadata": {
      "arxiv_id": "2412.03074",
      "title": "Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model",
      "summary": "We examine the text-free speech representations of raw audio obtained from a self-supervised learning (SSL) model by analyzing the synthesized speech using the SSL representations instead of conventional text representations. Since raw audio does not have paired speech representations as transcribed texts do, obtaining speech representations from unpaired speech is crucial for augmenting available datasets for speech synthesis. Specifically, the proposed speech synthesis is conducted using discrete symbol representations from the SSL model in comparison with text representations, and analytical examinations of the synthesized speech have been carried out. The results empirically show that using text representations is advantageous for preserving semantic information, while using discrete symbol representations is superior for preserving acoustic content, including prosodic and intonational information.",
      "authors": [
        "Joonyong Park",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2024-12-04T06:52:03Z",
      "updated": "2024-12-04T06:52:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03074v1",
      "landing_url": "https://arxiv.org/abs/2412.03074v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.03074"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2412.04917",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.04917v1",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "published": "2024-12-06T10:16:04Z"
    },
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2412.07783",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.07783v3",
      "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
      "summary": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
      "published": "2024-11-25T12:20:07Z"
    },
    "metadata": {
      "arxiv_id": "2412.07783",
      "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
      "summary": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
      "authors": [
        "Patrick Styll",
        "Dowon Kim",
        "Jiook Cha"
      ],
      "published": "2024-11-25T12:20:07Z",
      "updated": "2025-01-30T10:33:33Z",
      "categories": [
        "q-bio.NC",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.07783v3",
      "landing_url": "https://arxiv.org/abs/2412.07783v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.07783"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2412.08117",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.08117v1",
      "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
      "summary": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
      "published": "2024-12-11T05:55:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.08117",
      "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
      "summary": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
      "authors": [
        "Haowei Lou",
        "Helen Paik",
        "Pari Delir Haghighi",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2024-12-11T05:55:06Z",
      "updated": "2024-12-11T05:55:06Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08117v1",
      "landing_url": "https://arxiv.org/abs/2412.08117v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2412.10117",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10117v3",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "published": "2024-12-13T12:59:39Z"
    },
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.10261",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10261v2",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "published": "2024-12-13T16:30:35Z"
    },
    "metadata": {
      "arxiv_id": "2412.10261",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "authors": [
        "Shuaiting Li",
        "Chengxuan Wang",
        "Juncan Deng",
        "Zeyu Wang",
        "Zewen Ye",
        "Zongsheng Wang",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "published": "2024-12-13T16:30:35Z",
      "updated": "2024-12-16T08:54:43Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10261v2",
      "landing_url": "https://arxiv.org/abs/2412.10261v2",
      "doi": "https://doi.org/10.1145/3669940.3707268"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.11102",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11102v3",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "published": "2024-12-15T07:49:31Z"
    },
    "metadata": {
      "arxiv_id": "2412.11102",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "authors": [
        "Ximing Xing",
        "Juncheng Hu",
        "Guotao Liang",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "published": "2024-12-15T07:49:31Z",
      "updated": "2025-03-25T15:35:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11102v3",
      "landing_url": "https://arxiv.org/abs/2412.11102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.11102"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2412.11449",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11449v1",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "published": "2024-12-16T05:03:48Z"
    },
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2412.12581",
    "anchor": "semantic tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.12581v2",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "published": "2024-12-17T06:20:39Z"
    },
    "metadata": {
      "arxiv_id": "2412.12581",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "authors": [
        "Haifeng Lu",
        "Jiuyi Chen",
        "Feng Liang",
        "Mingkui Tan",
        "Runhao Zeng",
        "Xiping Hu"
      ],
      "published": "2024-12-17T06:20:39Z",
      "updated": "2024-12-20T11:49:07Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12581v2",
      "landing_url": "https://arxiv.org/abs/2412.12581v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.12581"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2412.14169",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14169v2",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "published": "2024-12-18T18:59:53Z"
    },
    "metadata": {
      "arxiv_id": "2412.14169",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Haiwen Diao",
        "Zhengxiong Luo",
        "Yufeng Cui",
        "Huchuan Lu",
        "Shiguang Shan",
        "Yonggang Qi",
        "Xinlong Wang"
      ],
      "published": "2024-12-18T18:59:53Z",
      "updated": "2025-03-02T08:09:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14169v2",
      "landing_url": "https://arxiv.org/abs/2412.14169v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.14169"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.14643",
    "anchor": "semantic tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14643v1",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "published": "2024-12-19T08:51:57Z"
    },
    "metadata": {
      "arxiv_id": "2412.14643",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "authors": [
        "Jie Huang",
        "Ruibing Hou",
        "Jiahe Zhao",
        "Hong Chang",
        "Shiguang Shan"
      ],
      "published": "2024-12-19T08:51:57Z",
      "updated": "2024-12-19T08:51:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14643v1",
      "landing_url": "https://arxiv.org/abs/2412.14643v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14643"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2412.14802",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.14802v1",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "published": "2024-12-19T12:48:17Z"
    },
    "metadata": {
      "arxiv_id": "2412.14802",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "authors": [
        "Egor Shibaev",
        "Denis Sushentsev",
        "Yaroslav Golubev",
        "Aleksandr Khvorov"
      ],
      "published": "2024-12-19T12:48:17Z",
      "updated": "2024-12-19T12:48:17Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14802v1",
      "landing_url": "https://arxiv.org/abs/2412.14802v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14802"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2412.15195",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15195v1",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "published": "2024-12-19T18:58:14Z"
    },
    "metadata": {
      "arxiv_id": "2412.15195",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "authors": [
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published": "2024-12-19T18:58:14Z",
      "updated": "2024-12-19T18:58:14Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15195v1",
      "landing_url": "https://arxiv.org/abs/2412.15195v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15195"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.15649",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15649v1",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "published": "2024-12-20T08:05:55Z"
    },
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2412.16102",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16102v3",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "published": "2024-12-20T17:43:50Z"
    },
    "metadata": {
      "arxiv_id": "2412.16102",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Hui Wang",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yuzhe Liang",
        "Ziyang Ma",
        "Yuxuan Hu",
        "Rui Zhao",
        "Jianwei Yu",
        "Yan Lu",
        "Xie Chen"
      ],
      "published": "2024-12-20T17:43:50Z",
      "updated": "2025-08-09T10:01:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16102v3",
      "landing_url": "https://arxiv.org/abs/2412.16102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16102"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2412.16626",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16626v2",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "published": "2024-12-21T13:43:51Z"
    },
    "metadata": {
      "arxiv_id": "2412.16626",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "authors": [
        "Junyu Wang",
        "Zizhen Lin",
        "Tianrui Wang",
        "Meng Ge",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "published": "2024-12-21T13:43:51Z",
      "updated": "2025-01-02T10:56:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16626v2",
      "landing_url": "https://arxiv.org/abs/2412.16626v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16626"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2412.16846",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16846v2",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "published": "2024-12-22T04:03:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.16919",
    "anchor": "acoustic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16919v3",
      "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
      "summary": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
      "published": "2024-12-22T08:28:20Z"
    },
    "metadata": {
      "arxiv_id": "2412.16919",
      "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
      "summary": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
      "authors": [
        "Xuying Zhang",
        "Yutong Liu",
        "Yangguang Li",
        "Renrui Zhang",
        "Yufei Liu",
        "Kai Wang",
        "Wanli Ouyang",
        "Zhiwei Xiong",
        "Peng Gao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "published": "2024-12-22T08:28:20Z",
      "updated": "2025-08-09T03:33:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16919v3",
      "landing_url": "https://arxiv.org/abs/2412.16919v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16919"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2412.17048",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17048v1",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "published": "2024-12-22T14:59:19Z"
    },
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.17640",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17640v2",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "published": "2024-12-23T15:18:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.19248",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.19248v1",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "published": "2024-12-26T15:08:36Z"
    },
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2501.00018",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00018v1",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "published": "2024-12-16T03:33:05Z"
    },
    "metadata": {
      "arxiv_id": "2501.00018",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "authors": [
        "Linqin Wang",
        "Yaping Liu",
        "Zhengtao Yu",
        "Shengxiang Gao",
        "Cunli Mao",
        "Yuxin Huang",
        "Wenjun Wang",
        "Ling Dong"
      ],
      "published": "2024-12-16T03:33:05Z",
      "updated": "2024-12-16T03:33:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00018v1",
      "landing_url": "https://arxiv.org/abs/2501.00018v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00018"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2501.01046",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01046v3",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "published": "2025-01-02T04:11:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.02293",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.02293v2",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "published": "2025-01-04T14:03:56Z"
    },
    "metadata": {
      "arxiv_id": "2501.02293",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "authors": [
        "Ellison Murray",
        "Morriel Kasher",
        "Predrag Spasojevic"
      ],
      "published": "2025-01-04T14:03:56Z",
      "updated": "2025-01-09T20:11:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02293v2",
      "landing_url": "https://arxiv.org/abs/2501.02293v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.02293"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2501.02350",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.02350v1",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "published": "2025-01-04T18:12:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.04644",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.04644v2",
      "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
      "summary": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
      "published": "2025-01-08T17:52:35Z"
    },
    "metadata": {
      "arxiv_id": "2501.04644",
      "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
      "summary": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
      "authors": [
        "Hanzhao Li",
        "Yuke Li",
        "Xinsheng Wang",
        "Jingbin Hu",
        "Qicong Xie",
        "Shan Yang",
        "Lei Xie"
      ],
      "published": "2025-01-08T17:52:35Z",
      "updated": "2025-04-30T09:30:49Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04644v2",
      "landing_url": "https://arxiv.org/abs/2501.04644v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.04644"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2501.05586",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05586v1",
      "title": "FreeSVC: Towards Zero-shot Multilingual Singing Voice Conversion",
      "summary": "This work presents FreeSVC, a promising multilingual singing voice conversion approach that leverages an enhanced VITS model with Speaker-invariant Clustering (SPIN) for better content representation and the State-of-the-Art (SOTA) speaker encoder ECAPA2. FreeSVC incorporates trainable language embeddings to handle multiple languages and employs an advanced speaker encoder to disentangle speaker characteristics from linguistic content. Designed for zero-shot learning, FreeSVC enables cross-lingual singing voice conversion without extensive language-specific training. We demonstrate that a multilingual content extractor is crucial for optimal cross-language conversion. Our source code and models are publicly available.",
      "published": "2025-01-09T21:39:09Z"
    },
    "metadata": {
      "arxiv_id": "2501.05586",
      "title": "FreeSVC: Towards Zero-shot Multilingual Singing Voice Conversion",
      "summary": "This work presents FreeSVC, a promising multilingual singing voice conversion approach that leverages an enhanced VITS model with Speaker-invariant Clustering (SPIN) for better content representation and the State-of-the-Art (SOTA) speaker encoder ECAPA2. FreeSVC incorporates trainable language embeddings to handle multiple languages and employs an advanced speaker encoder to disentangle speaker characteristics from linguistic content. Designed for zero-shot learning, FreeSVC enables cross-lingual singing voice conversion without extensive language-specific training. We demonstrate that a multilingual content extractor is crucial for optimal cross-language conversion. Our source code and models are publicly available.",
      "authors": [
        "Alef Iury Siqueira Ferreira",
        "Lucas Rafael Gris",
        "Augusto Seben da Rosa",
        "Frederico Santos de Oliveira",
        "Edresson Casanova",
        "Rafael Teixeira Sousa",
        "Arnaldo Candido Junior",
        "Anderson da Silva Soares",
        "Arlindo Galvão Filho"
      ],
      "published": "2025-01-09T21:39:09Z",
      "updated": "2025-01-09T21:39:09Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05586v1",
      "landing_url": "https://arxiv.org/abs/2501.05586v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890068"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2501.05787",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05787v1",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "published": "2025-01-10T08:41:42Z"
    },
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2501.13831",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.13831v1",
      "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing",
      "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model.",
      "published": "2025-01-23T16:54:27Z"
    },
    "metadata": {
      "arxiv_id": "2501.13831",
      "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing",
      "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model.",
      "authors": [
        "Hao Zhang",
        "Felix Stahlberg",
        "Shankar Kumar"
      ],
      "published": "2025-01-23T16:54:27Z",
      "updated": "2025-01-23T16:54:27Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13831v1",
      "landing_url": "https://arxiv.org/abs/2501.13831v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13831"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2501.15613",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.15613v1",
      "title": "Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task Learning",
      "summary": "Voice conversion (VC) modifies voice characteristics while preserving linguistic content. This paper presents the Stepback network, a novel model for converting speaker identity using non-parallel data. Unlike traditional VC methods that rely on parallel data, our approach leverages deep learning techniques to enhance disentanglement completion and linguistic content preservation. The Stepback network incorporates a dual flow of different domain data inputs and uses constraints with self-destructive amendments to optimize the content encoder. Extensive experiments show that our model significantly improves VC performance, reducing training costs while achieving high-quality voice conversion. The Stepback network's design offers a promising solution for advanced voice conversion tasks.",
      "published": "2025-01-26T17:43:32Z"
    },
    "metadata": {
      "arxiv_id": "2501.15613",
      "title": "Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task Learning",
      "summary": "Voice conversion (VC) modifies voice characteristics while preserving linguistic content. This paper presents the Stepback network, a novel model for converting speaker identity using non-parallel data. Unlike traditional VC methods that rely on parallel data, our approach leverages deep learning techniques to enhance disentanglement completion and linguistic content preservation. The Stepback network incorporates a dual flow of different domain data inputs and uses constraints with self-destructive amendments to optimize the content encoder. Extensive experiments show that our model significantly improves VC performance, reducing training costs while achieving high-quality voice conversion. The Stepback network's design offers a promising solution for advanced voice conversion tasks.",
      "authors": [
        "Qian Yang",
        "Calbert Graham"
      ],
      "published": "2025-01-26T17:43:32Z",
      "updated": "2025-01-26T17:43:32Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15613v1",
      "landing_url": "https://arxiv.org/abs/2501.15613v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.15613"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2501.15907",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.15907v2",
      "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
      "summary": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
      "published": "2025-01-27T09:59:20Z"
    },
    "metadata": {
      "arxiv_id": "2501.15907",
      "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
      "summary": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2025-01-27T09:59:20Z",
      "updated": "2025-10-08T06:46:48Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15907v2",
      "landing_url": "https://arxiv.org/abs/2501.15907v2",
      "doi": "https://doi.org/10.1109/TASLPRO.2025.3612835"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2501.16113",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.16113v1",
      "title": "Fixed-sized clusters $k$-Means",
      "summary": "We present a $k$-means-based clustering algorithm, which optimizes the mean square error, for given cluster sizes. A straightforward application is balanced clustering, where the sizes of each cluster are equal. In the $k$-means assignment phase, the algorithm solves an assignment problem using the Hungarian algorithm. This makes the assignment phase time complexity $O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
      "published": "2025-01-27T15:04:35Z"
    },
    "metadata": {
      "arxiv_id": "2501.16113",
      "title": "Fixed-sized clusters $k$-Means",
      "summary": "We present a $k$-means-based clustering algorithm, which optimizes the mean square error, for given cluster sizes. A straightforward application is balanced clustering, where the sizes of each cluster are equal. In the $k$-means assignment phase, the algorithm solves an assignment problem using the Hungarian algorithm. This makes the assignment phase time complexity $O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
      "authors": [
        "Mikko I. Malinen",
        "Pasi Fränti"
      ],
      "published": "2025-01-27T15:04:35Z",
      "updated": "2025-01-27T15:04:35Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.16113v1",
      "landing_url": "https://arxiv.org/abs/2501.16113v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.16113"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2501.16131",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.16131v1",
      "title": "Optimized Self-supervised Training with BEST-RQ for Speech Recognition",
      "summary": "Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.",
      "published": "2025-01-27T15:20:50Z"
    },
    "metadata": {
      "arxiv_id": "2501.16131",
      "title": "Optimized Self-supervised Training with BEST-RQ for Speech Recognition",
      "summary": "Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.",
      "authors": [
        "Ilja Baumann",
        "Dominik Wagner",
        "Korbinian Riedhammer",
        "Tobias Bocklet"
      ],
      "published": "2025-01-27T15:20:50Z",
      "updated": "2025-01-27T15:20:50Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.16131v1",
      "landing_url": "https://arxiv.org/abs/2501.16131v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.16131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.00250",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.00250v1",
      "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
      "summary": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
      "published": "2025-02-01T01:16:27Z"
    },
    "metadata": {
      "arxiv_id": "2502.00250",
      "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
      "summary": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
      "authors": [
        "Takumu Fujioka",
        "Gouhei Tanaka"
      ],
      "published": "2025-02-01T01:16:27Z",
      "updated": "2025-02-01T01:16:27Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00250v1",
      "landing_url": "https://arxiv.org/abs/2502.00250v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00250"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2502.02942",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02942v1",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "published": "2025-02-05T07:14:39Z"
    },
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2502.03128",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03128v1",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "published": "2025-02-05T12:36:21Z"
    },
    "metadata": {
      "arxiv_id": "2502.03128",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "authors": [
        "Yuancheng Wang",
        "Jiachen Zheng",
        "Junan Zhang",
        "Xueyao Zhang",
        "Huan Liao",
        "Zhizheng Wu"
      ],
      "published": "2025-02-05T12:36:21Z",
      "updated": "2025-02-05T12:36:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03128v1",
      "landing_url": "https://arxiv.org/abs/2502.03128v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03128"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2502.03605",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03605v1",
      "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
      "summary": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
      "published": "2025-02-05T20:48:27Z"
    },
    "metadata": {
      "arxiv_id": "2502.03605",
      "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
      "summary": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
      "authors": [
        "Subhadip Ghosh",
        "Endalk Y. Gebru",
        "Chandramouli V. Kashyap",
        "Ramesh Harjani",
        "Sachin S. Sapatnekar"
      ],
      "published": "2025-02-05T20:48:27Z",
      "updated": "2025-02-05T20:48:27Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03605v1",
      "landing_url": "https://arxiv.org/abs/2502.03605v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03605"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2502.03930",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03930v4",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "published": "2025-02-06T10:09:49Z"
    },
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2502.04519",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.04519v2",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "published": "2025-02-06T21:40:09Z"
    },
    "metadata": {
      "arxiv_id": "2502.04519",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "authors": [
        "Zexin Cai",
        "Henry Li Xinyuan",
        "Ashi Garg",
        "Leibny Paola García-Perera",
        "Kevin Duh",
        "Sanjeev Khudanpur",
        "Matthew Wiesner",
        "Nicholas Andrews"
      ],
      "published": "2025-02-06T21:40:09Z",
      "updated": "2025-08-20T17:34:21Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04519v2",
      "landing_url": "https://arxiv.org/abs/2502.04519v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.04519"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2502.05236",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05236v2",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "published": "2025-02-07T06:47:11Z"
    },
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2502.05512",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05512v1",
      "title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
      "summary": "Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.",
      "published": "2025-02-08T10:23:20Z"
    },
    "metadata": {
      "arxiv_id": "2502.05512",
      "title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
      "summary": "Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.",
      "authors": [
        "Wei Deng",
        "Siyi Zhou",
        "Jingchen Shu",
        "Jinchao Wang",
        "Lu Wang"
      ],
      "published": "2025-02-08T10:23:20Z",
      "updated": "2025-02-08T10:23:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05512v1",
      "landing_url": "https://arxiv.org/abs/2502.05512v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05512"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.05713",
    "anchor": "discrete speech tokens",
    "search_term": "vq gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05713v1",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "published": "2025-02-08T22:25:53Z"
    },
    "metadata": {
      "arxiv_id": "2502.05713",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "authors": [
        "An Zhao",
        "Moucheng Xu",
        "Ahmed H. Shahin",
        "Wim Wuyts",
        "Mark G. Jones",
        "Joseph Jacob",
        "Daniel C. Alexander"
      ],
      "published": "2025-02-08T22:25:53Z",
      "updated": "2025-02-08T22:25:53Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05713v1",
      "landing_url": "https://arxiv.org/abs/2502.05713v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05713"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      }
    ]
  },
  {
    "arxiv_id": "2502.05837",
    "anchor": "acoustic tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05837v1",
      "title": "Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models",
      "summary": "Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impact of combining KD loss with alternative pruning techniques, including Low-Rank Factorization (LRF) and l0 regularization, on a conformer-based pre-trained network under the paradigm of Self-Supervised Learning (SSL). We also propose a strategy to jointly prune and train an RNN-T-based ASR model, demonstrating that this approach yields superior performance compared to pruning a pre-trained network first and then using it for ASR training. This approach led to a significant reduction in word error rate: l0 and KD combination achieves the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER) improvement over the baseline, while LRF and KD combination yields the best results for streaming ASR, improving RWER by 13.4%.",
      "published": "2025-02-09T10:17:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.05837",
      "title": "Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models",
      "summary": "Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impact of combining KD loss with alternative pruning techniques, including Low-Rank Factorization (LRF) and l0 regularization, on a conformer-based pre-trained network under the paradigm of Self-Supervised Learning (SSL). We also propose a strategy to jointly prune and train an RNN-T-based ASR model, demonstrating that this approach yields superior performance compared to pruning a pre-trained network first and then using it for ASR training. This approach led to a significant reduction in word error rate: l0 and KD combination achieves the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER) improvement over the baseline, while LRF and KD combination yields the best results for streaming ASR, improving RWER by 13.4%.",
      "authors": [
        "Shiva Kumar C",
        "Jitendra Kumar Dhiman",
        "Nagaraj Adiga",
        "Shatrughan Singh"
      ],
      "published": "2025-02-09T10:17:25Z",
      "updated": "2025-02-09T10:17:25Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05837v1",
      "landing_url": "https://arxiv.org/abs/2502.05837v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05837"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.06490",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.06490v4",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "published": "2025-02-10T14:08:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2502.09520",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.09520v2",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "published": "2025-02-13T17:35:57Z"
    },
    "metadata": {
      "arxiv_id": "2502.09520",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "authors": [
        "Francesco Pezone",
        "Sergio Barbarossa",
        "Giuseppe Caire"
      ],
      "published": "2025-02-13T17:35:57Z",
      "updated": "2025-10-10T10:21:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09520v2",
      "landing_url": "https://arxiv.org/abs/2502.09520v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09520"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2502.10728",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.10728v1",
      "title": "Construction A Lattice Design Based on the Truncated Union Bound",
      "summary": "This paper considers $n= 128$ dimensional construction A lattice design, using binary codes with known minimum Hamming distance and codeword multiplicity, the number of minimum weight codeword. A truncated theta series of the lattice is explicitly given to obtain the truncated union bound to estimate the word error rate under maximum likelihood decoding. The best component code is selected by minimizing the required volume-to-noise ratio (VNR) for a target word error rate $P_e$. The estimate becomes accurate for $P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH codes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is achieved compared to that by the classic balanced distance rule and the equal error probability rule. The $(128, 106, 8)$ EBCH code gives the best-known $n=128$ construction A lattice at $P_e= 10^{-5}$.",
      "published": "2025-02-15T08:39:42Z"
    },
    "metadata": {
      "arxiv_id": "2502.10728",
      "title": "Construction A Lattice Design Based on the Truncated Union Bound",
      "summary": "This paper considers $n= 128$ dimensional construction A lattice design, using binary codes with known minimum Hamming distance and codeword multiplicity, the number of minimum weight codeword. A truncated theta series of the lattice is explicitly given to obtain the truncated union bound to estimate the word error rate under maximum likelihood decoding. The best component code is selected by minimizing the required volume-to-noise ratio (VNR) for a target word error rate $P_e$. The estimate becomes accurate for $P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH codes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is achieved compared to that by the classic balanced distance rule and the equal error probability rule. The $(128, 106, 8)$ EBCH code gives the best-known $n=128$ construction A lattice at $P_e= 10^{-5}$.",
      "authors": [
        "Jiajie Xue",
        "Brian M. Kurkoski",
        "Emanuele Viterbo"
      ],
      "published": "2025-02-15T08:39:42Z",
      "updated": "2025-02-15T08:39:42Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.10728v1",
      "landing_url": "https://arxiv.org/abs/2502.10728v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.10728"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.11094",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11094v1",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "published": "2025-02-16T12:14:17Z"
    },
    "metadata": {
      "arxiv_id": "2502.11094",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "authors": [
        "Zhengyan Sheng",
        "Zhihao Du",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Yexin Yang",
        "Zhenhua Ling"
      ],
      "published": "2025-02-16T12:14:17Z",
      "updated": "2025-02-16T12:14:17Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11094v1",
      "landing_url": "https://arxiv.org/abs/2502.11094v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11094"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.11318",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11318v1",
      "title": "Alias4SBML: A Python Package for Generating Alias Nodes in SBML Models",
      "summary": "Interpreting biological networks becomes challenging when molecular components, such as genes or proteins, participate in numerous interactions, resulting in densely connected regions and overlapping interactions that obscure functional relationships and biological insights. To address this, we introduce Alias4SBML, a Python package that enhances SBML model visualizations by generating alias nodes-duplicate representations of highly connected molecular components-to redistribute interactions and reduce visual congestion. Applying Alias4SBML to the SBML models, including one with 59 species and 41 reactions and another with 701 species and 505 reactions, demonstrated significant improvements in readability, with edge length reductions of up to 50.88 %. Our approach preserves the structural integrity of the network while facilitating clearer interpretation of complex biological systems, offering a flexible and scalable solution for visualizing biological models more efficiently.",
      "published": "2025-02-16T23:32:58Z"
    },
    "metadata": {
      "arxiv_id": "2502.11318",
      "title": "Alias4SBML: A Python Package for Generating Alias Nodes in SBML Models",
      "summary": "Interpreting biological networks becomes challenging when molecular components, such as genes or proteins, participate in numerous interactions, resulting in densely connected regions and overlapping interactions that obscure functional relationships and biological insights. To address this, we introduce Alias4SBML, a Python package that enhances SBML model visualizations by generating alias nodes-duplicate representations of highly connected molecular components-to redistribute interactions and reduce visual congestion. Applying Alias4SBML to the SBML models, including one with 59 species and 41 reactions and another with 701 species and 505 reactions, demonstrated significant improvements in readability, with edge length reductions of up to 50.88 %. Our approach preserves the structural integrity of the network while facilitating clearer interpretation of complex biological systems, offering a flexible and scalable solution for visualizing biological models more efficiently.",
      "authors": [
        "Adel Heydarabadipour",
        "Herbert M Sauro"
      ],
      "published": "2025-02-16T23:32:58Z",
      "updated": "2025-02-16T23:32:58Z",
      "categories": [
        "q-bio.MN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11318v1",
      "landing_url": "https://arxiv.org/abs/2502.11318v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11318"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2502.12408",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12408v2",
      "title": "On the Robust Approximation of ASR Metrics",
      "summary": "Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\\%.",
      "published": "2025-02-18T01:10:17Z"
    },
    "metadata": {
      "arxiv_id": "2502.12408",
      "title": "On the Robust Approximation of ASR Metrics",
      "summary": "Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\\%.",
      "authors": [
        "Abdul Waheed",
        "Hanin Atwany",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "published": "2025-02-18T01:10:17Z",
      "updated": "2025-06-04T22:29:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12408v2",
      "landing_url": "https://arxiv.org/abs/2502.12408v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.12408"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.12448",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12448v1",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "published": "2025-02-18T02:29:51Z"
    },
    "metadata": {
      "arxiv_id": "2502.12448",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-02-18T02:29:51Z",
      "updated": "2025-02-18T02:29:51Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12448v1",
      "landing_url": "https://arxiv.org/abs/2502.12448v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12448"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2502.12672",
    "anchor": "speech representation",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12672v3",
      "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization",
      "summary": "Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.",
      "published": "2025-02-18T09:23:42Z"
    },
    "metadata": {
      "arxiv_id": "2502.12672",
      "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization",
      "summary": "Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.",
      "authors": [
        "Tzu-Quan Lin",
        "Wei-Ping Huang",
        "Hao Tang",
        "Hung-yi Lee"
      ],
      "published": "2025-02-18T09:23:42Z",
      "updated": "2025-12-18T04:50:21Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12672v3",
      "landing_url": "https://arxiv.org/abs/2502.12672v3",
      "doi": "https://doi.org/10.1109/TASLPRO.2025.3635827"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.16142",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16142v1",
      "title": "Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration",
      "summary": "In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.",
      "published": "2025-02-22T08:30:38Z"
    },
    "metadata": {
      "arxiv_id": "2502.16142",
      "title": "Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration",
      "summary": "In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.",
      "authors": [
        "Haoxuan Wang"
      ],
      "published": "2025-02-22T08:30:38Z",
      "updated": "2025-02-22T08:30:38Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16142v1",
      "landing_url": "https://arxiv.org/abs/2502.16142v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16142"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.16240",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16240v1",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "published": "2025-02-22T14:25:55Z"
    },
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.16474",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16474v1",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "published": "2025-02-23T07:17:28Z"
    },
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.16897",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16897v2",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "published": "2025-02-24T06:50:40Z"
    },
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2502.17239",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.17239v1",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "published": "2025-02-24T15:16:34Z"
    },
    "metadata": {
      "arxiv_id": "2502.17239",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "authors": [
        "Tianpeng Li",
        "Jun Liu",
        "Tao Zhang",
        "Yuanbo Fang",
        "Da Pan",
        "Mingrui Wang",
        "Zheng Liang",
        "Zehuan Li",
        "Mingan Lin",
        "Guosheng Dong",
        "Jianhua Xu",
        "Haoze Sun",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-02-24T15:16:34Z",
      "updated": "2025-02-24T15:16:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17239v1",
      "landing_url": "https://arxiv.org/abs/2502.17239v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17239"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.18200",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.18200v2",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "published": "2025-02-25T13:41:06Z"
    },
    "metadata": {
      "arxiv_id": "2502.18200",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "authors": [
        "Jiangjing Hu",
        "Haotian Wu",
        "Wenjing Zhang",
        "Fengyu Wang",
        "Wenjun Xu",
        "Hui Gao",
        "Deniz Gündüz"
      ],
      "published": "2025-02-25T13:41:06Z",
      "updated": "2025-05-29T05:24:20Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18200v2",
      "landing_url": "https://arxiv.org/abs/2502.18200v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18200"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2503.00493",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.00493v4",
      "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
      "summary": "Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",
      "published": "2025-03-01T13:44:50Z"
    },
    "metadata": {
      "arxiv_id": "2503.00493",
      "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
      "summary": "Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",
      "authors": [
        "Boyi Kang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "Zhen Ye",
        "Mingshuai Liu",
        "Ziqian Wang",
        "Yike Zhu",
        "Guobin Ma",
        "Jun Chen",
        "Longshuai Xiao",
        "Chao Weng",
        "Wei Xue",
        "Lei Xie"
      ],
      "published": "2025-03-01T13:44:50Z",
      "updated": "2025-06-10T06:55:05Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00493v4",
      "landing_url": "https://arxiv.org/abs/2503.00493v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.00493"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.00733",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.00733v1",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "published": "2025-03-02T05:15:40Z"
    },
    "metadata": {
      "arxiv_id": "2503.00733",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "authors": [
        "Alexander H. Liu",
        "Sang-gil Lee",
        "Chao-Han Huck Yang",
        "Yuan Gong",
        "Yu-Chiang Frank Wang",
        "James R. Glass",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "published": "2025-03-02T05:15:40Z",
      "updated": "2025-03-02T05:15:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00733v1",
      "landing_url": "https://arxiv.org/abs/2503.00733v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00733"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2503.01650",
    "anchor": "acoustic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01650v1",
      "title": "CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving",
      "summary": "In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel method designed to enhance data efficiency in learning-based autonomous driving systems. CAPS addresses the challenge of imbalanced training datasets in imitation learning by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data representation, which helps reveal meaningful patterns in the data. These patterns are used to group the data into clusters, with each sample being assigned a cluster ID. The cluster IDs are then used to re-balance the dataset, ensuring that rare yet valuable samples receive higher priority during training. By ensuring a more diverse and informative training set, CAPS improves the generalization of the trained planner across a wide range of driving scenarios. We evaluate our method through closed-loop simulations in the CARLA environment. The results on Bench2Drive scenarios demonstrate that our framework outperforms state-of-the-art methods, leading to notable improvements in model performance.",
      "published": "2025-03-03T15:27:11Z"
    },
    "metadata": {
      "arxiv_id": "2503.01650",
      "title": "CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving",
      "summary": "In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel method designed to enhance data efficiency in learning-based autonomous driving systems. CAPS addresses the challenge of imbalanced training datasets in imitation learning by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data representation, which helps reveal meaningful patterns in the data. These patterns are used to group the data into clusters, with each sample being assigned a cluster ID. The cluster IDs are then used to re-balance the dataset, ensuring that rare yet valuable samples receive higher priority during training. By ensuring a more diverse and informative training set, CAPS improves the generalization of the trained planner across a wide range of driving scenarios. We evaluate our method through closed-loop simulations in the CARLA environment. The results on Bench2Drive scenarios demonstrate that our framework outperforms state-of-the-art methods, leading to notable improvements in model performance.",
      "authors": [
        "Hamidreza Mirkhani",
        "Behzad Khamidehi",
        "Ehsan Ahmadi",
        "Fazel Arasteh",
        "Mohammed Elmahgiubi",
        "Weize Zhang",
        "Umar Rajguru",
        "Kasra Rezaee"
      ],
      "published": "2025-03-03T15:27:11Z",
      "updated": "2025-03-03T15:27:11Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01650v1",
      "landing_url": "https://arxiv.org/abs/2503.01650v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01650"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2503.01710",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01710v1",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "published": "2025-03-03T16:23:10Z"
    },
    "metadata": {
      "arxiv_id": "2503.01710",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "authors": [
        "Xinsheng Wang",
        "Mingqi Jiang",
        "Ziyang Ma",
        "Ziyu Zhang",
        "Songxiang Liu",
        "Linqin Li",
        "Zheng Liang",
        "Qixi Zheng",
        "Rui Wang",
        "Xiaoqin Feng",
        "Weizhen Bian",
        "Zhen Ye",
        "Sitong Cheng",
        "Ruibin Yuan",
        "Zhixian Zhao",
        "Xinfa Zhu",
        "Jiahao Pan",
        "Liumeng Xue",
        "Pengcheng Zhu",
        "Yunlin Chen",
        "Zhifei Li",
        "Xie Chen",
        "Lei Xie",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-03-03T16:23:10Z",
      "updated": "2025-03-03T16:23:10Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01710v1",
      "landing_url": "https://arxiv.org/abs/2503.01710v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01710"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2503.02862",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.02862v1",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "published": "2025-03-04T18:40:38Z"
    },
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2503.04606",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04606v3",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "published": "2025-03-06T16:53:14Z"
    },
    "metadata": {
      "arxiv_id": "2503.04606",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "published": "2025-03-06T16:53:14Z",
      "updated": "2025-04-29T10:34:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04606v3",
      "landing_url": "https://arxiv.org/abs/2503.04606v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04606"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2503.04713",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04713v2",
      "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
      "summary": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
      "published": "2025-03-06T18:57:40Z"
    },
    "metadata": {
      "arxiv_id": "2503.04713",
      "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
      "summary": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
      "authors": [
        "Anuj Diwan",
        "Zhisheng Zheng",
        "David Harwath",
        "Eunsol Choi"
      ],
      "published": "2025-03-06T18:57:40Z",
      "updated": "2025-09-24T19:42:38Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04713v2",
      "landing_url": "https://arxiv.org/abs/2503.04713v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.04713"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2503.06921",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.06921v2",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "published": "2025-03-10T05:00:24Z"
    },
    "metadata": {
      "arxiv_id": "2503.06921",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "authors": [
        "Youngeun Kim",
        "Seunghwan Lee",
        "Aecheon Jung",
        "Bogon Ryu",
        "Sungeun Hong"
      ],
      "published": "2025-03-10T05:00:24Z",
      "updated": "2025-08-07T10:57:05Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06921v2",
      "landing_url": "https://arxiv.org/abs/2503.06921v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06921"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2503.09509",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.09509v2",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "published": "2025-03-12T16:18:45Z"
    },
    "metadata": {
      "arxiv_id": "2503.09509",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Kedong Xu",
        "Hong Gu",
        "Kejie Huang"
      ],
      "published": "2025-03-12T16:18:45Z",
      "updated": "2025-07-30T16:58:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09509v2",
      "landing_url": "https://arxiv.org/abs/2503.09509v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.09509"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2503.11315",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.11315v2",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "published": "2025-03-14T11:31:30Z"
    },
    "metadata": {
      "arxiv_id": "2503.11315",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "published": "2025-03-14T11:31:30Z",
      "updated": "2025-06-05T05:58:37Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11315v2",
      "landing_url": "https://arxiv.org/abs/2503.11315v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.11315"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.11513",
    "anchor": "semantic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.11513v1",
      "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
      "summary": "Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
      "published": "2025-03-14T15:36:39Z"
    },
    "metadata": {
      "arxiv_id": "2503.11513",
      "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
      "summary": "Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
      "authors": [
        "Ziqin Zhou",
        "Yifan Yang",
        "Yuqing Yang",
        "Tianyu He",
        "Houwen Peng",
        "Kai Qiu",
        "Qi Dai",
        "Lili Qiu",
        "Chong Luo",
        "Lingqiao Liu"
      ],
      "published": "2025-03-14T15:36:39Z",
      "updated": "2025-03-14T15:36:39Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11513v1",
      "landing_url": "https://arxiv.org/abs/2503.11513v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.11513"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2503.12115",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12115v2",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "published": "2025-03-15T12:50:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2503.14928",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.14928v1",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "published": "2025-03-19T06:28:17Z"
    },
    "metadata": {
      "arxiv_id": "2503.14928",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "published": "2025-03-19T06:28:17Z",
      "updated": "2025-03-19T06:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14928v1",
      "landing_url": "https://arxiv.org/abs/2503.14928v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14928"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2503.18769",
    "anchor": "semantic tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.18769v2",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "published": "2025-03-24T15:16:51Z"
    },
    "metadata": {
      "arxiv_id": "2503.18769",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Bui Quang Huy"
      ],
      "published": "2025-03-24T15:16:51Z",
      "updated": "2025-03-27T06:39:47Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18769v2",
      "landing_url": "https://arxiv.org/abs/2503.18769v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.18769"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2503.20499",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.20499v3",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "published": "2025-03-26T12:39:06Z"
    },
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2503.21686",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.21686v2",
      "title": "Molecular Quantum Transformer",
      "summary": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
      "published": "2025-03-27T16:54:15Z"
    },
    "metadata": {
      "arxiv_id": "2503.21686",
      "title": "Molecular Quantum Transformer",
      "summary": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
      "authors": [
        "Yuichi Kamata",
        "Quoc Hoan Tran",
        "Yasuhiro Endo",
        "Hirotaka Oshima"
      ],
      "published": "2025-03-27T16:54:15Z",
      "updated": "2025-05-16T02:38:13Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.21686v2",
      "landing_url": "https://arxiv.org/abs/2503.21686v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.21686"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2503.22692",
    "anchor": "acoustic tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.22692v1",
      "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA",
      "summary": "Transcription of aviation communications has several applications, from assisting air traffic controllers in identifying the accuracy of read-back errors to search and rescue operations. Recent advances in artificial intelligence have provided unprecedented opportunities for improving aviation communication transcription tasks. OpenAI's Whisper is one of the leading automatic speech recognition models. However, fine-tuning Whisper for aviation communication transcription is not computationally efficient. Thus, this paper aims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation to fine-tune a more computationally efficient version of Whisper, distil-Whisper. To perform the fine-tuning, we used the Air Traffic Control Corpus dataset from the Linguistic Data Consortium, which contains approximately 70 hours of controller and pilot transmissions near three major airports in the US. The objective was to reduce the word error rate to enhance accuracy in the transcription of aviation communication. First, starting with an initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we performed a grid search. We applied a 5-fold cross-validation to find the best combination of distil-Whisper hyperparameters. Then, we fine-tuned the model for LoRA hyperparameters, achieving an impressive average word error rate of 3.86% across five folds. This result highlights the model's potential for use in the cockpit.",
      "published": "2025-03-13T22:12:45Z"
    },
    "metadata": {
      "arxiv_id": "2503.22692",
      "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA",
      "summary": "Transcription of aviation communications has several applications, from assisting air traffic controllers in identifying the accuracy of read-back errors to search and rescue operations. Recent advances in artificial intelligence have provided unprecedented opportunities for improving aviation communication transcription tasks. OpenAI's Whisper is one of the leading automatic speech recognition models. However, fine-tuning Whisper for aviation communication transcription is not computationally efficient. Thus, this paper aims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation to fine-tune a more computationally efficient version of Whisper, distil-Whisper. To perform the fine-tuning, we used the Air Traffic Control Corpus dataset from the Linguistic Data Consortium, which contains approximately 70 hours of controller and pilot transmissions near three major airports in the US. The objective was to reduce the word error rate to enhance accuracy in the transcription of aviation communication. First, starting with an initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we performed a grid search. We applied a 5-fold cross-validation to find the best combination of distil-Whisper hyperparameters. Then, we fine-tuned the model for LoRA hyperparameters, achieving an impressive average word error rate of 3.86% across five folds. This result highlights the model's potential for use in the cockpit.",
      "authors": [
        "Shokoufeh Mirzaei",
        "Jesse Arzate",
        "Yukti Vijay"
      ],
      "published": "2025-03-13T22:12:45Z",
      "updated": "2025-03-13T22:12:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.22692v1",
      "landing_url": "https://arxiv.org/abs/2503.22692v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.22692"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2503.24164",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.24164v2",
      "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
      "summary": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
      "published": "2025-03-31T14:46:34Z"
    },
    "metadata": {
      "arxiv_id": "2503.24164",
      "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
      "summary": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
      "authors": [
        "Ngoc Dung Huynh",
        "Mohamed Reda Bouadjenek",
        "Imran Razzak",
        "Hakim Hacid",
        "Sunil Aryal"
      ],
      "published": "2025-03-31T14:46:34Z",
      "updated": "2025-07-07T14:41:48Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.24164v2",
      "landing_url": "https://arxiv.org/abs/2503.24164v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.24164"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2504.03669",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.03669v1",
      "title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment",
      "summary": "Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.",
      "published": "2025-03-20T13:45:13Z"
    },
    "metadata": {
      "arxiv_id": "2504.03669",
      "title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment",
      "summary": "Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.",
      "authors": [
        "Caicheng Wang",
        "Zili Wang",
        "Shuyou Zhang",
        "Yongzhe Xiang",
        "Zheyi Li",
        "Jianrong Tan"
      ],
      "published": "2025-03-20T13:45:13Z",
      "updated": "2025-03-20T13:45:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03669v1",
      "landing_url": "https://arxiv.org/abs/2504.03669v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03669"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2504.05197",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.05197v2",
      "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
      "summary": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
      "published": "2025-04-07T15:47:09Z"
    },
    "metadata": {
      "arxiv_id": "2504.05197",
      "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
      "summary": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Tao Wang",
        "Jianhua Tao",
        "Zheng Lian",
        "Zhengqi Wen",
        "Chenxing Li",
        "Ruibo Fu",
        "Ye Bai",
        "Xiaohui Zhang"
      ],
      "published": "2025-04-07T15:47:09Z",
      "updated": "2025-05-05T16:34:37Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05197v2",
      "landing_url": "https://arxiv.org/abs/2504.05197v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.05197"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2504.07053",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.07053v2",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "published": "2025-04-09T17:14:33Z"
    },
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2504.08274",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.08274v1",
      "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
      "summary": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
      "published": "2025-04-11T06:12:57Z"
    },
    "metadata": {
      "arxiv_id": "2504.08274",
      "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
      "summary": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
      "authors": [
        "Haowei Lou",
        "Hye-young Paik",
        "Sheng Li",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2025-04-11T06:12:57Z",
      "updated": "2025-04-11T06:12:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08274v1",
      "landing_url": "https://arxiv.org/abs/2504.08274v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08274"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2504.09101",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.09101v1",
      "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
      "summary": "In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.",
      "published": "2025-04-12T06:46:51Z"
    },
    "metadata": {
      "arxiv_id": "2504.09101",
      "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
      "summary": "In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.",
      "authors": [
        "Abdulmajid Murad",
        "Massimiliano Ruocco"
      ],
      "published": "2025-04-12T06:46:51Z",
      "updated": "2025-04-12T06:46:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09101v1",
      "landing_url": "https://arxiv.org/abs/2504.09101v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.09101"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2504.09862",
    "anchor": "semantic tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.09862v2",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "published": "2025-04-14T04:18:25Z"
    },
    "metadata": {
      "arxiv_id": "2504.09862",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "authors": [
        "Zengyuan Lai",
        "Jiarui Yang",
        "Songpengcheng Xia",
        "Lizhou Lin",
        "Lan Sun",
        "Renwen Wang",
        "Jianran Liu",
        "Qi Wu",
        "Ling Pei"
      ],
      "published": "2025-04-14T04:18:25Z",
      "updated": "2025-11-17T04:20:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09862v2",
      "landing_url": "https://arxiv.org/abs/2504.09862v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.09862"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2504.10352",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.10352v3",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "published": "2025-04-14T16:03:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.10352",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Yuxuan Hu",
        "Haibin Wu",
        "Hui Wang",
        "Jianwei Yu",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yanqing Liu",
        "Yan Lu",
        "Kai Yu",
        "Xie Chen"
      ],
      "published": "2025-04-14T16:03:21Z",
      "updated": "2025-08-05T15:33:39Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10352v3",
      "landing_url": "https://arxiv.org/abs/2504.10352v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.10352"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2504.12005",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12005v1",
      "title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
      "summary": "Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.",
      "published": "2025-04-16T11:59:56Z"
    },
    "metadata": {
      "arxiv_id": "2504.12005",
      "title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
      "summary": "Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.",
      "authors": [
        "Soobin Suh",
        "Dabi Ahn",
        "Heewoong Park",
        "Jonghun Park"
      ],
      "published": "2025-04-16T11:59:56Z",
      "updated": "2025-04-16T11:59:56Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12005v1",
      "landing_url": "https://arxiv.org/abs/2504.12005v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12005"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2504.12339",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12339v2",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "published": "2025-04-15T01:44:56Z"
    },
    "metadata": {
      "arxiv_id": "2504.12339",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "authors": [
        "Yaodong Song",
        "Hongjie Chen",
        "Jie Lian",
        "Yuxin Zhang",
        "Guangmin Xia",
        "Zehan Li",
        "Genliang Zhao",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "published": "2025-04-15T01:44:56Z",
      "updated": "2025-05-28T14:24:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12339v2",
      "landing_url": "https://arxiv.org/abs/2504.12339v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.12339"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2504.12715",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12715v1",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "published": "2025-04-17T07:43:52Z"
    },
    "metadata": {
      "arxiv_id": "2504.12715",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "authors": [
        "Long Zeng",
        "Jianxiang Yu",
        "Jiapeng Zhu",
        "Qingsong Zhong",
        "Xiang Li"
      ],
      "published": "2025-04-17T07:43:52Z",
      "updated": "2025-04-17T07:43:52Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12715v1",
      "landing_url": "https://arxiv.org/abs/2504.12715v1",
      "doi": "https://doi.org/10.1145/3696410.3714656"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2504.14075",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.14075v1",
      "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
      "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
      "published": "2025-04-18T20:57:16Z"
    },
    "metadata": {
      "arxiv_id": "2504.14075",
      "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
      "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
      "authors": [
        "Wei Dong",
        "Yan Min",
        "Han Zhou",
        "Jun Chen"
      ],
      "published": "2025-04-18T20:57:16Z",
      "updated": "2025-04-18T20:57:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14075v1",
      "landing_url": "https://arxiv.org/abs/2504.14075v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14075"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2504.14092",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.14092v1",
      "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
      "summary": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
      "published": "2025-04-18T22:19:40Z"
    },
    "metadata": {
      "arxiv_id": "2504.14092",
      "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
      "summary": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
      "authors": [
        "Wei Dong",
        "Han Zhou",
        "Seyed Amirreza Mousavi",
        "Jun Chen"
      ],
      "published": "2025-04-18T22:19:40Z",
      "updated": "2025-04-18T22:19:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14092v1",
      "landing_url": "https://arxiv.org/abs/2504.14092v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14092"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2504.14113",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.14113v1",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "published": "2025-04-19T00:13:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2504.15509",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.15509v1",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "published": "2025-04-22T01:05:32Z"
    },
    "metadata": {
      "arxiv_id": "2504.15509",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "authors": [
        "Keqi Deng",
        "Wenxi Chen",
        "Xie Chen",
        "Philip C. Woodland"
      ],
      "published": "2025-04-22T01:05:32Z",
      "updated": "2025-04-22T01:05:32Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15509v1",
      "landing_url": "https://arxiv.org/abs/2504.15509v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.15509"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2504.15822",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.15822v1",
      "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion",
      "summary": "Using a multi-accented corpus of parallel utterances for use with commercial speech devices, we present a case study to show that it is possible to quantify a degree of confidence about a source speaker's identity in the case of one-to-one voice conversion. Following voice conversion using a HiFi-GAN vocoder, we compare information leakage for a range speaker characteristics; assuming a \"worst-case\" white-box scenario, we quantify our confidence to perform inference and narrow the pool of likely source speakers, reinforcing the regulatory obligation and moral duty that providers of synthetic voices have to ensure the privacy of their speakers' data.",
      "published": "2025-04-22T12:09:03Z"
    },
    "metadata": {
      "arxiv_id": "2504.15822",
      "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion",
      "summary": "Using a multi-accented corpus of parallel utterances for use with commercial speech devices, we present a case study to show that it is possible to quantify a degree of confidence about a source speaker's identity in the case of one-to-one voice conversion. Following voice conversion using a HiFi-GAN vocoder, we compare information leakage for a range speaker characteristics; assuming a \"worst-case\" white-box scenario, we quantify our confidence to perform inference and narrow the pool of likely source speakers, reinforcing the regulatory obligation and moral duty that providers of synthetic voices have to ensure the privacy of their speakers' data.",
      "authors": [
        "Scott Wellington",
        "Xuechen Liu",
        "Junichi Yamagishi"
      ],
      "published": "2025-04-22T12:09:03Z",
      "updated": "2025-04-22T12:09:03Z",
      "categories": [
        "cs.SD",
        "cs.CR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15822v1",
      "landing_url": "https://arxiv.org/abs/2504.15822v1",
      "doi": "https://doi.org/10.1109/BIOSIG61931.2024.10786731"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2504.18539",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.18539v2",
      "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
      "summary": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.",
      "published": "2025-01-23T05:11:19Z"
    },
    "metadata": {
      "arxiv_id": "2504.18539",
      "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
      "summary": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.",
      "authors": [
        "Sungnyun Kim",
        "Sungwoo Cho",
        "Sangmin Bae",
        "Kangwook Jang",
        "Se-Young Yun"
      ],
      "published": "2025-01-23T05:11:19Z",
      "updated": "2025-04-30T05:16:51Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.18539v2",
      "landing_url": "https://arxiv.org/abs/2504.18539v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.18539"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2504.19046",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.19046v1",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "published": "2025-04-26T22:49:08Z"
    },
    "metadata": {
      "arxiv_id": "2504.19046",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "authors": [
        "Billel Essaid",
        "Hamza Kheddar",
        "Noureddine Batel"
      ],
      "published": "2025-04-26T22:49:08Z",
      "updated": "2025-04-26T22:49:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.19046v1",
      "landing_url": "https://arxiv.org/abs/2504.19046v1",
      "doi": "https://doi.org/10.1109/ICTIS62692.2024.10894163"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2504.20629",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.20629v2",
      "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
      "summary": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
      "published": "2025-04-29T10:56:24Z"
    },
    "metadata": {
      "arxiv_id": "2504.20629",
      "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
      "summary": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
      "authors": [
        "Jeongsoo Choi",
        "Ji-Hoon Kim",
        "Kim Sung-Bin",
        "Tae-Hyun Oh",
        "Joon Son Chung"
      ],
      "published": "2025-04-29T10:56:24Z",
      "updated": "2025-10-03T10:54:34Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.20629v2",
      "landing_url": "https://arxiv.org/abs/2504.20629v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.20629"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2505.00127",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.00127v1",
      "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs",
      "summary": "Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.",
      "published": "2025-04-30T18:48:06Z"
    },
    "metadata": {
      "arxiv_id": "2505.00127",
      "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs",
      "summary": "Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.",
      "authors": [
        "Jinyan Su",
        "Jennifer Healey",
        "Preslav Nakov",
        "Claire Cardie"
      ],
      "published": "2025-04-30T18:48:06Z",
      "updated": "2025-04-30T18:48:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.00127v1",
      "landing_url": "https://arxiv.org/abs/2505.00127v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.00127"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2505.04382",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.04382v3",
      "title": "Discrete Optimal Transport and Voice Conversion",
      "summary": "In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.",
      "published": "2025-05-07T13:04:29Z"
    },
    "metadata": {
      "arxiv_id": "2505.04382",
      "title": "Discrete Optimal Transport and Voice Conversion",
      "summary": "In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.",
      "authors": [
        "Anton Selitskiy",
        "Maitreya Kocharekar"
      ],
      "published": "2025-05-07T13:04:29Z",
      "updated": "2025-11-30T10:52:57Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.04382v3",
      "landing_url": "https://arxiv.org/abs/2505.04382v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.04382"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2505.05159",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.05159v3",
      "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
      "summary": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
      "published": "2025-05-08T11:55:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.05159",
      "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
      "summary": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
      "authors": [
        "Linhan Ma",
        "Dake Guo",
        "He Wang",
        "Jin Xu",
        "Lei Xie"
      ],
      "published": "2025-05-08T11:55:19Z",
      "updated": "2025-05-15T08:28:37Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05159v3",
      "landing_url": "https://arxiv.org/abs/2505.05159v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.05159"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2505.05738",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.05738v2",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "published": "2025-05-09T02:34:06Z"
    },
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.06252",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.06252v3",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "published": "2025-04-30T04:16:32Z"
    },
    "metadata": {
      "arxiv_id": "2505.06252",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "authors": [
        "Zirui Wang",
        "Tingfeng Lan",
        "Zhaoyuan Su",
        "Juncheng Yang",
        "Yue Cheng"
      ],
      "published": "2025-04-30T04:16:32Z",
      "updated": "2025-11-08T18:45:50Z",
      "categories": [
        "cs.DB",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06252v3",
      "landing_url": "https://arxiv.org/abs/2505.06252v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.06252"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2505.08278",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.08278v1",
      "title": "Investigating self-supervised features for expressive, multilingual voice conversion",
      "summary": "Voice conversion (VC) systems are widely used for several applications, from speaker anonymisation to personalised speech synthesis. Supervised approaches learn a mapping between different speakers using parallel data, which is expensive to produce. Unsupervised approaches are typically trained to reconstruct the input signal, which is composed of the content and the speaker information. Disentangling these components is a challenge and often leads to speaker leakage or prosodic information removal. In this paper, we explore voice conversion by leveraging the potential of self-supervised learning (SSL). A combination of the latent representations of SSL models, concatenated with speaker embeddings, is fed to a vocoder which is trained to reconstruct the input. Zero-shot voice conversion results show that this approach allows to keep the prosody and content of the source speaker while matching the speaker similarity of a VC system based on phonetic posteriorgrams (PPGs).",
      "published": "2025-05-13T06:44:03Z"
    },
    "metadata": {
      "arxiv_id": "2505.08278",
      "title": "Investigating self-supervised features for expressive, multilingual voice conversion",
      "summary": "Voice conversion (VC) systems are widely used for several applications, from speaker anonymisation to personalised speech synthesis. Supervised approaches learn a mapping between different speakers using parallel data, which is expensive to produce. Unsupervised approaches are typically trained to reconstruct the input signal, which is composed of the content and the speaker information. Disentangling these components is a challenge and often leads to speaker leakage or prosodic information removal. In this paper, we explore voice conversion by leveraging the potential of self-supervised learning (SSL). A combination of the latent representations of SSL models, concatenated with speaker embeddings, is fed to a vocoder which is trained to reconstruct the input. Zero-shot voice conversion results show that this approach allows to keep the prosody and content of the source speaker while matching the speaker similarity of a VC system based on phonetic posteriorgrams (PPGs).",
      "authors": [
        "Álvaro Martín-Cortinas",
        "Daniel Sáez-Trigueros",
        "Grzegorz Beringer",
        "Iván Vallés-Pérez",
        "Roberto Barra-Chicote",
        "Biel Tura-Vecino",
        "Adam Gabryś",
        "Piotr Bilinski",
        "Thomas Merritt",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2025-05-13T06:44:03Z",
      "updated": "2025-05-13T06:44:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.08278v1",
      "landing_url": "https://arxiv.org/abs/2505.08278v1",
      "doi": "https://doi.org/10.1109/ICASSPW62465.2024.10627128"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2505.11225",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.11225v2",
      "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
      "summary": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
      "published": "2025-05-16T13:21:28Z"
    },
    "metadata": {
      "arxiv_id": "2505.11225",
      "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
      "summary": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
      "authors": [
        "Chengyu Huang",
        "Zhengxin Zhang",
        "Claire Cardie"
      ],
      "published": "2025-05-16T13:21:28Z",
      "updated": "2025-11-11T05:07:45Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11225v2",
      "landing_url": "https://arxiv.org/abs/2505.11225v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.11225"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2505.11391",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.11391v3",
      "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
      "summary": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
      "published": "2025-05-16T15:56:07Z"
    },
    "metadata": {
      "arxiv_id": "2505.11391",
      "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
      "summary": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
      "authors": [
        "Julius Richter",
        "Danilo de Oliveira",
        "Tal Peer",
        "Timo Gerkmann"
      ],
      "published": "2025-05-16T15:56:07Z",
      "updated": "2025-10-24T13:26:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11391v3",
      "landing_url": "https://arxiv.org/abs/2505.11391v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.11391"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2505.11918",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.11918v1",
      "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
      "summary": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
      "published": "2025-05-17T09:02:18Z"
    },
    "metadata": {
      "arxiv_id": "2505.11918",
      "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
      "summary": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
      "authors": [
        "Zhiheng Chen",
        "Ruofan Wu",
        "Guanhua Fang"
      ],
      "published": "2025-05-17T09:02:18Z",
      "updated": "2025-05-17T09:02:18Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11918v1",
      "landing_url": "https://arxiv.org/abs/2505.11918v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.11918"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2505.13000",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13000v2",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "published": "2025-05-19T11:41:08Z"
    },
    "metadata": {
      "arxiv_id": "2505.13000",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "authors": [
        "Jiaqi Li",
        "Xiaolong Lin",
        "Zhekai Li",
        "Shixi Huang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Zhenpeng Zhan",
        "Zhizheng Wu"
      ],
      "published": "2025-05-19T11:41:08Z",
      "updated": "2025-10-01T15:01:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13000v2",
      "landing_url": "https://arxiv.org/abs/2505.13000v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13000"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2505.13830",
    "anchor": "acoustic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13830v2",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "published": "2025-05-20T02:18:45Z"
    },
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k means"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.14470",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14470v2",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "published": "2025-05-20T15:05:14Z"
    },
    "metadata": {
      "arxiv_id": "2505.14470",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "authors": [
        "Nadav Har-Tuv",
        "Or Tal",
        "Yossi Adi"
      ],
      "published": "2025-05-20T15:05:14Z",
      "updated": "2025-06-04T08:23:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14470v2",
      "landing_url": "https://arxiv.org/abs/2505.14470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14470"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2505.14989",
    "anchor": "acoustic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14989v1",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "published": "2025-05-21T00:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2505.16616",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16616v1",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "published": "2025-05-22T12:50:32Z"
    },
    "metadata": {
      "arxiv_id": "2505.16616",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "authors": [
        "Javier Perez",
        "Dimme de Groot",
        "Jorge Martinez"
      ],
      "published": "2025-05-22T12:50:32Z",
      "updated": "2025-05-22T12:50:32Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16616v1",
      "landing_url": "https://arxiv.org/abs/2505.16616v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16616"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.16691",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16691v2",
      "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion",
      "summary": "Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/",
      "published": "2025-05-22T13:57:02Z"
    },
    "metadata": {
      "arxiv_id": "2505.16691",
      "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion",
      "summary": "Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/",
      "authors": [
        "Advait Joglekar",
        "Divyanshu Singh",
        "Rooshil Rohit Bhatia",
        "S. Umesh"
      ],
      "published": "2025-05-22T13:57:02Z",
      "updated": "2025-05-23T05:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16691v2",
      "landing_url": "https://arxiv.org/abs/2505.16691v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16691"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2505.17076",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17076v3",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "published": "2025-05-20T06:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2505.17446",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17446v2",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "published": "2025-05-23T04:03:27Z"
    },
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2505.17477",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17477v1",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "published": "2025-05-23T04:59:27Z"
    },
    "metadata": {
      "arxiv_id": "2505.17477",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "authors": [
        "Victor OK Li",
        "Yang Han",
        "Jacqueline CK Lam",
        "Lawrence YL Cheung"
      ],
      "published": "2025-05-23T04:59:27Z",
      "updated": "2025-05-23T04:59:27Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17477v1",
      "landing_url": "https://arxiv.org/abs/2505.17477v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17477"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2505.17589",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenizer",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17589v2",
      "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
      "summary": "In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at https://funaudiollm.github.io/cosyvoice3.",
      "published": "2025-05-23T07:55:21Z"
    },
    "metadata": {
      "arxiv_id": "2505.17589",
      "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
      "summary": "In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at https://funaudiollm.github.io/cosyvoice3.",
      "authors": [
        "Zhihao Du",
        "Changfeng Gao",
        "Yuxuan Wang",
        "Fan Yu",
        "Tianyu Zhao",
        "Hao Wang",
        "Xiang Lv",
        "Hui Wang",
        "Chongjia Ni",
        "Xian Shi",
        "Keyu An",
        "Guanrou Yang",
        "Yabin Li",
        "Yanni Chen",
        "Zhifu Gao",
        "Qian Chen",
        "Yue Gu",
        "Mengzhe Chen",
        "Yafeng Chen",
        "Shiliang Zhang",
        "Wen Wang",
        "Jieping Ye"
      ],
      "published": "2025-05-23T07:55:21Z",
      "updated": "2025-05-27T07:48:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17589v2",
      "landing_url": "https://arxiv.org/abs/2505.17589v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17589"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      }
    ]
  },
  {
    "arxiv_id": "2505.17604",
    "anchor": "semantic tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17604v1",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "published": "2025-05-23T08:15:05Z"
    },
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.18231",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.18231v2",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "published": "2025-05-23T12:40:07Z"
    },
    "metadata": {
      "arxiv_id": "2505.18231",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "published": "2025-05-23T12:40:07Z",
      "updated": "2025-12-14T08:17:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "landing_url": "https://arxiv.org/abs/2505.18231v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.18231"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2505.18298",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.18298v1",
      "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards",
      "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in mathematical tasks, often enhanced through reinforcement learning (RL). However, RL-trained models frequently produce unnecessarily long reasoning traces -- even for simple queries -- leading to increased inference costs and latency. While recent approaches attempt to control verbosity by adding length penalties to the reward function, these methods rely on fixed penalty terms that are hard to tune and cannot adapt as the model's reasoning capability evolves, limiting their effectiveness. In this work, we propose an adaptive reward-shaping method that enables LLMs to \"think fast and right\" -- producing concise outputs without sacrificing correctness. Our method dynamically adjusts the reward trade-off between accuracy and response length based on model performance: when accuracy is high, the length penalty increases to encourage faster length reduction; when accuracy drops, the penalty is relaxed to preserve correctness. This adaptive reward accelerates early-stage length reduction while avoiding over-compression in later stages. Experiments across multiple datasets show that our approach consistently and dramatically reduces reasoning length while largely maintaining accuracy, offering a new direction for cost-efficient adaptive reasoning in large-scale language models.",
      "published": "2025-05-23T18:44:46Z"
    },
    "metadata": {
      "arxiv_id": "2505.18298",
      "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards",
      "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in mathematical tasks, often enhanced through reinforcement learning (RL). However, RL-trained models frequently produce unnecessarily long reasoning traces -- even for simple queries -- leading to increased inference costs and latency. While recent approaches attempt to control verbosity by adding length penalties to the reward function, these methods rely on fixed penalty terms that are hard to tune and cannot adapt as the model's reasoning capability evolves, limiting their effectiveness. In this work, we propose an adaptive reward-shaping method that enables LLMs to \"think fast and right\" -- producing concise outputs without sacrificing correctness. Our method dynamically adjusts the reward trade-off between accuracy and response length based on model performance: when accuracy is high, the length penalty increases to encourage faster length reduction; when accuracy drops, the penalty is relaxed to preserve correctness. This adaptive reward accelerates early-stage length reduction while avoiding over-compression in later stages. Experiments across multiple datasets show that our approach consistently and dramatically reduces reasoning length while largely maintaining accuracy, offering a new direction for cost-efficient adaptive reasoning in large-scale language models.",
      "authors": [
        "Jinyan Su",
        "Claire Cardie"
      ],
      "published": "2025-05-23T18:44:46Z",
      "updated": "2025-05-23T18:44:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18298v1",
      "landing_url": "https://arxiv.org/abs/2505.18298v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18298"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2505.18864",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.18864v1",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "published": "2025-05-24T20:46:36Z"
    },
    "metadata": {
      "arxiv_id": "2505.18864",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "authors": [
        "Binhao Ma",
        "Hanqing Guo",
        "Zhengping Jay Luo",
        "Rui Duan"
      ],
      "published": "2025-05-24T20:46:36Z",
      "updated": "2025-05-24T20:46:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18864v1",
      "landing_url": "https://arxiv.org/abs/2505.18864v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18864"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.19043",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19043v2",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "published": "2025-05-25T08:43:40Z"
    },
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.19462",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19462v2",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "published": "2025-05-26T03:35:44Z"
    },
    "metadata": {
      "arxiv_id": "2505.19462",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "authors": [
        "Puyuan Peng",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2025-05-26T03:35:44Z",
      "updated": "2025-05-31T22:36:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19462v2",
      "landing_url": "https://arxiv.org/abs/2505.19462v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19462"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2505.19470",
    "anchor": "discrete speech tokens",
    "search_term": "vq vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19470v2",
      "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
      "summary": "Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.",
      "published": "2025-05-26T03:51:44Z"
    },
    "metadata": {
      "arxiv_id": "2505.19470",
      "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
      "summary": "Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.",
      "authors": [
        "Futoshi Futami",
        "Masahiro Fujisawa"
      ],
      "published": "2025-05-26T03:51:44Z",
      "updated": "2025-11-06T07:57:00Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19470v2",
      "landing_url": "https://arxiv.org/abs/2505.19470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19470"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq vae"
      }
    ]
  },
  {
    "arxiv_id": "2505.19595",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19595v2",
      "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
      "summary": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
      "published": "2025-05-26T07:07:16Z"
    },
    "metadata": {
      "arxiv_id": "2505.19595",
      "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
      "summary": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
      "authors": [
        "Jeongsoo Choi",
        "Zhikang Niu",
        "Ji-Hoon Kim",
        "Chunhui Wang",
        "Joon Son Chung",
        "Xie Chen"
      ],
      "published": "2025-05-26T07:07:16Z",
      "updated": "2025-05-30T16:52:09Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19595v2",
      "landing_url": "https://arxiv.org/abs/2505.19595v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19595"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2505.19669",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19669v2",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "published": "2025-05-26T08:25:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.19669",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "authors": [
        "Haiyang Sun",
        "Shujie Hu",
        "Shujie Liu",
        "Lingwei Meng",
        "Hui Wang",
        "Bing Han",
        "Yifan Yang",
        "Yanqing Liu",
        "Sheng Zhao",
        "Yan Lu",
        "Yanmin Qian"
      ],
      "published": "2025-05-26T08:25:01Z",
      "updated": "2025-06-02T10:03:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19669v2",
      "landing_url": "https://arxiv.org/abs/2505.19669v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19669"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2505.19760",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19760v2",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "published": "2025-05-26T09:43:09Z"
    },
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.19774",
    "anchor": "speech representation",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19774v1",
      "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
      "summary": "Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most research has focused on either causal or full-context speech encoders, there's limited exploration to effectively handle both streaming and non-streaming applications, while achieving state-of-the-art performance. We introduce DuRep, a Dual-mode Speech Representation learning setup, which enables a single speech encoder to function efficiently in both offline and online modes without additional parameters or mode-specific adjustments, across downstream tasks. DuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6% improvements in streaming and non-streaming modes, over baseline encoders on Multilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new performance benchmarks across ASR and non-ASR tasks. Our analysis reveals interesting trade-offs between acoustic and semantic information across encoder layers.",
      "published": "2025-05-26T09:57:59Z"
    },
    "metadata": {
      "arxiv_id": "2505.19774",
      "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
      "summary": "Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most research has focused on either causal or full-context speech encoders, there's limited exploration to effectively handle both streaming and non-streaming applications, while achieving state-of-the-art performance. We introduce DuRep, a Dual-mode Speech Representation learning setup, which enables a single speech encoder to function efficiently in both offline and online modes without additional parameters or mode-specific adjustments, across downstream tasks. DuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6% improvements in streaming and non-streaming modes, over baseline encoders on Multilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new performance benchmarks across ASR and non-ASR tasks. Our analysis reveals interesting trade-offs between acoustic and semantic information across encoder layers.",
      "authors": [
        "Prabash Reddy Male",
        "Swayambhu Nath Ray",
        "Harish Arsikere",
        "Akshat Jaiswal",
        "Prakhar Swarup",
        "Prantik Sen",
        "Debmalya Chakrabarty",
        "K V Vijay Girish",
        "Nikhil Bhave",
        "Frederick Weber",
        "Sambuddha Bhattacharya",
        "Sri Garimella"
      ],
      "published": "2025-05-26T09:57:59Z",
      "updated": "2025-05-26T09:57:59Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19774v1",
      "landing_url": "https://arxiv.org/abs/2505.19774v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.19774"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k means"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.20741",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.20741v1",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "published": "2025-05-27T05:31:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.20741",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "authors": [
        "Jiatong Shi",
        "Hye-Jin Shim",
        "Shinji Watanabe"
      ],
      "published": "2025-05-27T05:31:19Z",
      "updated": "2025-05-27T05:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.20741v1",
      "landing_url": "https://arxiv.org/abs/2505.20741v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.20741"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.21194",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.21194v1",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "published": "2025-05-27T13:42:33Z"
    },
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2505.24291",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24291v1",
      "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
      "summary": "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.",
      "published": "2025-05-30T07:04:23Z"
    },
    "metadata": {
      "arxiv_id": "2505.24291",
      "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
      "summary": "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.",
      "authors": [
        "Kaidi Wang",
        "Wenhao Guan",
        "Ziyue Jiang",
        "Hukai Huang",
        "Peijie Chen",
        "Weijie Wu",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:04:23Z",
      "updated": "2025-05-30T07:04:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24291v1",
      "landing_url": "https://arxiv.org/abs/2505.24291v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24291"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2505.24314",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24314v1",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "published": "2025-05-30T07:53:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.24314",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "authors": [
        "Peijie Chen",
        "Wenhao Guan",
        "Kaidi Wang",
        "Weijie Wu",
        "Hukai Huang",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:53:01Z",
      "updated": "2025-05-30T07:53:01Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24314v1",
      "landing_url": "https://arxiv.org/abs/2505.24314v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24314"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.24496",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24496v1",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "published": "2025-05-30T11:47:29Z"
    },
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2505.24717",
    "anchor": "discrete speech tokens",
    "search_term": "transformer",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24717v1",
      "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
      "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
      "published": "2025-05-30T15:39:54Z"
    },
    "metadata": {
      "arxiv_id": "2505.24717",
      "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
      "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
      "authors": [
        "Benjamin Holzschuh",
        "Qiang Liu",
        "Georg Kohl",
        "Nils Thuerey"
      ],
      "published": "2025-05-30T15:39:54Z",
      "updated": "2025-05-30T15:39:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24717v1",
      "landing_url": "https://arxiv.org/abs/2505.24717v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24717"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "speech representation",
        "search_term": "transformer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "transformer"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "transformer"
      }
    ]
  },
  {
    "arxiv_id": "2506.00809",
    "anchor": "acoustic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.00809v1",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "published": "2025-06-01T03:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2506.00809",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2025-06-01T03:23:27Z",
      "updated": "2025-06-01T03:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00809v1",
      "landing_url": "https://arxiv.org/abs/2506.00809v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00809"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.00843",
    "anchor": "acoustic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.00843v1",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "published": "2025-06-01T05:38:39Z"
    },
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.01322",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.01322v1",
      "title": "Zero-Shot Text-to-Speech for Vietnamese",
      "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
      "published": "2025-06-02T05:07:06Z"
    },
    "metadata": {
      "arxiv_id": "2506.01322",
      "title": "Zero-Shot Text-to-Speech for Vietnamese",
      "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
      "authors": [
        "Thi Vu",
        "Linh The Nguyen",
        "Dat Quoc Nguyen"
      ],
      "published": "2025-06-02T05:07:06Z",
      "updated": "2025-06-02T05:07:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01322v1",
      "landing_url": "https://arxiv.org/abs/2506.01322v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01322"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2506.01510",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.01510v1",
      "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion",
      "summary": "We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations. Samples and code: https://www.kamperh.com/linearvc/.",
      "published": "2025-06-02T10:18:02Z"
    },
    "metadata": {
      "arxiv_id": "2506.01510",
      "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion",
      "summary": "We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations. Samples and code: https://www.kamperh.com/linearvc/.",
      "authors": [
        "Herman Kamper",
        "Benjamin van Niekerk",
        "Julian Zaïdi",
        "Marc-André Carbonneau"
      ],
      "published": "2025-06-02T10:18:02Z",
      "updated": "2025-06-02T10:18:02Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01510v1",
      "landing_url": "https://arxiv.org/abs/2506.01510v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01510"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2506.01731",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.01731v1",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "published": "2025-06-02T14:42:50Z"
    },
    "metadata": {
      "arxiv_id": "2506.01731",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "authors": [
        "Anna Leschanowsky",
        "Kishor Kayyar Lakshminarayana",
        "Anjana Rajasekhar",
        "Lyonel Behringer",
        "Ibrahim Kilinc",
        "Guillaume Fuchs",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-06-02T14:42:50Z",
      "updated": "2025-06-02T14:42:50Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01731v1",
      "landing_url": "https://arxiv.org/abs/2506.01731v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01731"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.02863",
    "anchor": "discrete speech tokens",
    "search_term": "text to speech",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.02863v2",
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "summary": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
      "published": "2025-06-03T13:28:55Z"
    },
    "metadata": {
      "arxiv_id": "2506.02863",
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "summary": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dading Chong",
        "Karan Thakkar",
        "Tiantian Feng",
        "Dongchao Yang",
        "Junhyeok Lee",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Zengyi Qin",
        "Shrikanth Narayanan",
        "Mounya Elhiali",
        "Najim Dehak"
      ],
      "published": "2025-06-03T13:28:55Z",
      "updated": "2025-09-26T13:07:51Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02863v2",
      "landing_url": "https://arxiv.org/abs/2506.02863v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.02863"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "speech representation",
        "search_term": "text to speech"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "text to speech"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "text to speech"
      }
    ]
  },
  {
    "arxiv_id": "2506.04134",
    "anchor": "discrete speech tokens",
    "search_term": "speech generation",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.04134v4",
      "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
      "summary": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
      "published": "2025-06-04T16:26:49Z"
    },
    "metadata": {
      "arxiv_id": "2506.04134",
      "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
      "summary": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
      "authors": [
        "Jinting Wang",
        "Shan Yang",
        "Chenxing Li",
        "Dong Yu",
        "Li Liu"
      ],
      "published": "2025-06-04T16:26:49Z",
      "updated": "2025-11-11T03:49:45Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04134v4",
      "landing_url": "https://arxiv.org/abs/2506.04134v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.04134"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech generation"
      }
    ]
  },
  {
    "arxiv_id": "2506.04786",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.04786v2",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "published": "2025-06-05T09:14:25Z"
    },
    "metadata": {
      "arxiv_id": "2506.04786",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "authors": [
        "Thore Gerlach",
        "Sascha Mücke",
        "Christian Bauckhage"
      ],
      "published": "2025-06-05T09:14:25Z",
      "updated": "2025-09-04T20:11:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04786v2",
      "landing_url": "https://arxiv.org/abs/2506.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04786"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2506.05432",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.05432v2",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "published": "2025-06-05T08:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2506.05432",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "authors": [
        "Yuxuan Yue",
        "Zukang Xu",
        "Zhihang Yuan",
        "Dawei Yang",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "published": "2025-06-05T08:58:58Z",
      "updated": "2025-06-26T06:17:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
      "landing_url": "https://arxiv.org/abs/2506.05432v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05432"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2506.06990",
    "anchor": "discrete speech tokens",
    "search_term": "k means",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.06990v2",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "published": "2025-06-08T04:37:28Z"
    },
    "metadata": {
      "arxiv_id": "2506.06990",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "authors": [
        "Mingyi Li",
        "Michael R. Metel",
        "Akiko Takeda"
      ],
      "published": "2025-06-08T04:37:28Z",
      "updated": "2025-06-11T06:52:53Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06990v2",
      "landing_url": "https://arxiv.org/abs/2506.06990v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.06990"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k means"
      }
    ]
  },
  {
    "arxiv_id": "2506.08686",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.08686v2",
      "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
      "summary": "A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses.",
      "published": "2025-06-10T10:52:04Z"
    },
    "metadata": {
      "arxiv_id": "2506.08686",
      "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
      "summary": "A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses.",
      "authors": [
        "Soham Poddar",
        "Paramita Koley",
        "Janardan Misra",
        "Sanjay Podder",
        "Navveen Balani",
        "Niloy Ganguly",
        "Saptarshi Ghosh"
      ],
      "published": "2025-06-10T10:52:04Z",
      "updated": "2025-06-29T08:48:15Z",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.08686v2",
      "landing_url": "https://arxiv.org/abs/2506.08686v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.08686"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2506.09349",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09349v4",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "published": "2025-06-11T02:57:22Z"
    },
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenizer"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech generation"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2506.09549",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09549v1",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "published": "2025-06-11T09:32:12Z"
    },
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2506.09709",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09709v1",
      "title": "Training-Free Voice Conversion with Factorized Optimal Transport",
      "summary": "This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline. In contrast with original pipeline, our algorithm performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio. MKL-VC replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution. Factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets show MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. MKL-VC achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain.",
      "published": "2025-06-11T13:23:03Z"
    },
    "metadata": {
      "arxiv_id": "2506.09709",
      "title": "Training-Free Voice Conversion with Factorized Optimal Transport",
      "summary": "This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline. In contrast with original pipeline, our algorithm performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio. MKL-VC replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution. Factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets show MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. MKL-VC achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain.",
      "authors": [
        "Alexander Lobashev",
        "Assel Yermekova",
        "Maria Larchenko"
      ],
      "published": "2025-06-11T13:23:03Z",
      "updated": "2025-06-11T13:23:03Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09709v1",
      "landing_url": "https://arxiv.org/abs/2506.09709v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09709"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  }
]