[
  {
    "arxiv_id": "2206.06192",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.06192v2",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "published": "2022-06-13T14:26:40Z"
    },
    "metadata": {
      "arxiv_id": "2206.06192",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "authors": [
        "Arlo Faria",
        "Adam Janin",
        "Korbinian Riedhammer",
        "Sidhi Adkoli"
      ],
      "published": "2022-06-13T14:26:40Z",
      "updated": "2022-06-27T14:44:58Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06192v2",
      "landing_url": "https://arxiv.org/abs/2206.06192v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.06192"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2206.07288",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.07288v1",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "published": "2022-06-15T04:04:14Z"
    },
    "metadata": {
      "arxiv_id": "2206.07288",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "authors": [
        "Ziyi Chen",
        "Haoran Miao",
        "Pengyuan Zhang"
      ],
      "published": "2022-06-15T04:04:14Z",
      "updated": "2022-06-15T04:04:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07288v1",
      "landing_url": "https://arxiv.org/abs/2206.07288v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07288"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2206.11706",
    "anchor": "discrete speech tokens",
    "search_term": "unit discovery",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.11706v2",
      "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
      "summary": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
      "published": "2022-06-23T13:53:59Z"
    },
    "metadata": {
      "arxiv_id": "2206.11706",
      "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
      "summary": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
      "authors": [
        "Werner van der Merwe",
        "Herman Kamper",
        "Johan du Preez"
      ],
      "published": "2022-06-23T13:53:59Z",
      "updated": "2022-06-29T07:47:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11706v2",
      "landing_url": "https://arxiv.org/abs/2206.11706v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.11706"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      }
    ]
  },
  {
    "arxiv_id": "2206.12351",
    "anchor": "acoustic tokens",
    "search_term": "vq-gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.12351v1",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "published": "2022-06-24T15:47:42Z"
    },
    "metadata": {
      "arxiv_id": "2206.12351",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "authors": [
        "Alex F. McKinney",
        "Chris G. Willcocks"
      ],
      "published": "2022-06-24T15:47:42Z",
      "updated": "2022-06-24T15:47:42Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12351v1",
      "landing_url": "https://arxiv.org/abs/2206.12351v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12351"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      }
    ]
  },
  {
    "arxiv_id": "2206.13680",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.13680v1",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "published": "2022-06-28T01:14:09Z"
    },
    "metadata": {
      "arxiv_id": "2206.13680",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "authors": [
        "Amber Afshan",
        "Abeer Alwan"
      ],
      "published": "2022-06-28T01:14:09Z",
      "updated": "2022-06-28T01:14:09Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13680v1",
      "landing_url": "https://arxiv.org/abs/2206.13680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.13680"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2207.00344",
    "anchor": "discrete speech tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.00344v1",
      "title": "Automatic Evaluation of Speaker Similarity",
      "summary": "We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level.",
      "published": "2022-07-01T11:23:16Z"
    },
    "metadata": {
      "arxiv_id": "2207.00344",
      "title": "Automatic Evaluation of Speaker Similarity",
      "summary": "We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level.",
      "authors": [
        "Deja Kamil",
        "Sanchez Ariadna",
        "Roth Julian",
        "Cotescu Marius"
      ],
      "published": "2022-07-01T11:23:16Z",
      "updated": "2022-07-01T11:23:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00344v1",
      "landing_url": "https://arxiv.org/abs/2207.00344v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00344"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2207.00756",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.00756v1",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "published": "2022-07-02T06:51:12Z"
    },
    "metadata": {
      "arxiv_id": "2207.00756",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "authors": [
        "Liumeng Xue",
        "Shan Yang",
        "Na Hu",
        "Dan Su",
        "Lei Xie"
      ],
      "published": "2022-07-02T06:51:12Z",
      "updated": "2022-07-02T06:51:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00756v1",
      "landing_url": "https://arxiv.org/abs/2207.00756v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00756"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2207.03067",
    "anchor": "acoustic tokens",
    "search_term": "residual vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.03067v1",
      "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
      "summary": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
      "published": "2022-07-07T03:23:25Z"
    },
    "metadata": {
      "arxiv_id": "2207.03067",
      "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
      "summary": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Huaying Xue",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2022-07-07T03:23:25Z",
      "updated": "2022-07-07T03:23:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03067v1",
      "landing_url": "https://arxiv.org/abs/2207.03067v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03067"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      }
    ]
  },
  {
    "arxiv_id": "2207.04356",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.04356v1",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "published": "2022-07-10T01:02:22Z"
    },
    "metadata": {
      "arxiv_id": "2207.04356",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "authors": [
        "Wen-Chin Huang",
        "Shu-Wen Yang",
        "Tomoki Hayashi",
        "Tomoki Toda"
      ],
      "published": "2022-07-10T01:02:22Z",
      "updated": "2022-07-10T01:02:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04356v1",
      "landing_url": "https://arxiv.org/abs/2207.04356v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3193761"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2207.08187",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.08187v1",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "published": "2022-07-17T14:15:45Z"
    },
    "metadata": {
      "arxiv_id": "2207.08187",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "authors": [
        "Sannara Ek",
        "Romain Rombourg",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-07-17T14:15:45Z",
      "updated": "2022-07-17T14:15:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08187v1",
      "landing_url": "https://arxiv.org/abs/2207.08187v1",
      "doi": "https://doi.org/10.1109/PerComWorkshops53856.2022.9767369"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2207.10317",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.10317v2",
      "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
      "summary": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
      "published": "2022-07-21T06:19:37Z"
    },
    "metadata": {
      "arxiv_id": "2207.10317",
      "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
      "summary": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
      "authors": [
        "Fatemeh Nasiri",
        "Wassim Hamidouche",
        "Luce Morin",
        "Nicolas Dholland",
        "Jean-Yves Aubié"
      ],
      "published": "2022-07-21T06:19:37Z",
      "updated": "2022-07-23T08:46:39Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10317v2",
      "landing_url": "https://arxiv.org/abs/2207.10317v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.10317"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2207.11226",
    "anchor": "acoustic tokens",
    "search_term": "vq-gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.11226v1",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "published": "2022-07-18T07:11:28Z"
    },
    "metadata": {
      "arxiv_id": "2207.11226",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "authors": [
        "Lior Ben-Moshe",
        "Sagie Benaim",
        "Lior Wolf"
      ],
      "published": "2022-07-18T07:11:28Z",
      "updated": "2022-07-18T07:11:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.11226v1",
      "landing_url": "https://arxiv.org/abs/2207.11226v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.11226"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      }
    ]
  },
  {
    "arxiv_id": "2208.03161",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.03161v1",
      "title": "Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations",
      "summary": "Deep Learning (DL) methods have shown promising results for solving ill-posed inverse problems such as MR image reconstruction from undersampled $k$-space data. However, these approaches currently have no guarantees for reconstruction quality and the reliability of such algorithms is only poorly understood. Adversarial attacks offer a valuable tool to understand possible failure modes and worst case performance of DL-based reconstruction algorithms. In this paper we describe adversarial attacks on multi-coil $k$-space measurements and evaluate them on the recently proposed E2E-VarNet and a simpler UNet-based model. In contrast to prior work, the attacks are targeted to specifically alter diagnostically relevant regions. Using two realistic attack models (adversarial $k$-space noise and adversarial rotations) we are able to show that current state-of-the-art DL-based reconstruction algorithms are indeed sensitive to such perturbations to a degree where relevant diagnostic information may be lost. Surprisingly, in our experiments the UNet and the more sophisticated E2E-VarNet were similarly sensitive to such attacks. Our findings add further to the evidence that caution must be exercised as DL-based methods move closer to clinical practice.",
      "published": "2022-08-05T13:39:40Z"
    },
    "metadata": {
      "arxiv_id": "2208.03161",
      "title": "Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations",
      "summary": "Deep Learning (DL) methods have shown promising results for solving ill-posed inverse problems such as MR image reconstruction from undersampled $k$-space data. However, these approaches currently have no guarantees for reconstruction quality and the reliability of such algorithms is only poorly understood. Adversarial attacks offer a valuable tool to understand possible failure modes and worst case performance of DL-based reconstruction algorithms. In this paper we describe adversarial attacks on multi-coil $k$-space measurements and evaluate them on the recently proposed E2E-VarNet and a simpler UNet-based model. In contrast to prior work, the attacks are targeted to specifically alter diagnostically relevant regions. Using two realistic attack models (adversarial $k$-space noise and adversarial rotations) we are able to show that current state-of-the-art DL-based reconstruction algorithms are indeed sensitive to such perturbations to a degree where relevant diagnostic information may be lost. Surprisingly, in our experiments the UNet and the more sophisticated E2E-VarNet were similarly sensitive to such attacks. Our findings add further to the evidence that caution must be exercised as DL-based methods move closer to clinical practice.",
      "authors": [
        "Jan Nikolas Morshuis",
        "Sergios Gatidis",
        "Matthias Hein",
        "Christian F. Baumgartner"
      ],
      "published": "2022-08-05T13:39:40Z",
      "updated": "2022-08-05T13:39:40Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.03161v1",
      "landing_url": "https://arxiv.org/abs/2208.03161v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.03161"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      }
    ]
  },
  {
    "arxiv_id": "2208.05445",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.05445v1",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "published": "2022-08-10T16:56:39Z"
    },
    "metadata": {
      "arxiv_id": "2208.05445",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "authors": [
        "Jaejin Cho",
        "Jes'us Villalba",
        "Laureano Moro-Velazquez",
        "Najim Dehak"
      ],
      "published": "2022-08-10T16:56:39Z",
      "updated": "2022-08-10T16:56:39Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05445v1",
      "landing_url": "https://arxiv.org/abs/2208.05445v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3197315"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2208.08757",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.08757v1",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "published": "2022-08-18T10:36:27Z"
    },
    "metadata": {
      "arxiv_id": "2208.08757",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "authors": [
        "SiCheng Yang",
        "Methawee Tantrawenith",
        "Haolin Zhuang",
        "Zhiyong Wu",
        "Aolan Sun",
        "Jianzong Wang",
        "Ning Cheng",
        "Huaizhen Tang",
        "Xintao Zhao",
        "Jie Wang",
        "Helen Meng"
      ],
      "published": "2022-08-18T10:36:27Z",
      "updated": "2022-08-18T10:36:27Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.08757v1",
      "landing_url": "https://arxiv.org/abs/2208.08757v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2208.09030",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.09030v3",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "published": "2022-08-18T19:02:30Z"
    },
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2209.04167",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.04167v1",
      "title": "Overlapped speech and gender detection with WavLM pre-trained features",
      "summary": "This article focuses on overlapped speech and gender detection in order to study interactions between women and men in French audiovisual media (Gender Equality Monitoring project). In this application context, we need to automatically segment the speech signal according to speakers gender, and to identify when at least two speakers speak at the same time. We propose to use WavLM model which has the advantage of being pre-trained on a huge amount of speech data, to build an overlapped speech detection (OSD) and a gender detection (GD) systems. In this study, we use two different corpora. The DIHARD III corpus which is well adapted for the OSD task but lack gender information. The ALLIES corpus fits with the project application context. Our best OSD system is a Temporal Convolutional Network (TCN) with WavLM pre-trained features as input, which reaches a new state-of-the-art F1-score performance on DIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of the French broadcast news ALLIES data, and obtains an accuracy of 97.9%. This work opens new perspectives for human science researchers regarding the differences of representation between women and men in French media.",
      "published": "2022-09-09T08:00:47Z"
    },
    "metadata": {
      "arxiv_id": "2209.04167",
      "title": "Overlapped speech and gender detection with WavLM pre-trained features",
      "summary": "This article focuses on overlapped speech and gender detection in order to study interactions between women and men in French audiovisual media (Gender Equality Monitoring project). In this application context, we need to automatically segment the speech signal according to speakers gender, and to identify when at least two speakers speak at the same time. We propose to use WavLM model which has the advantage of being pre-trained on a huge amount of speech data, to build an overlapped speech detection (OSD) and a gender detection (GD) systems. In this study, we use two different corpora. The DIHARD III corpus which is well adapted for the OSD task but lack gender information. The ALLIES corpus fits with the project application context. Our best OSD system is a Temporal Convolutional Network (TCN) with WavLM pre-trained features as input, which reaches a new state-of-the-art F1-score performance on DIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of the French broadcast news ALLIES data, and obtains an accuracy of 97.9%. This work opens new perspectives for human science researchers regarding the differences of representation between women and men in French media.",
      "authors": [
        "Martin Lebourdais",
        "Marie Tahon",
        "Antoine Laurent",
        "Sylvain Meignier"
      ],
      "published": "2022-09-09T08:00:47Z",
      "updated": "2022-09-09T08:00:47Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04167v1",
      "landing_url": "https://arxiv.org/abs/2209.04167v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.04167"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2209.09897",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.09897v1",
      "title": "Improving GANs with A Dynamic Discriminator",
      "summary": "Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change to the bi-classification task for the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined for learning GANs.",
      "published": "2022-09-20T17:57:33Z"
    },
    "metadata": {
      "arxiv_id": "2209.09897",
      "title": "Improving GANs with A Dynamic Discriminator",
      "summary": "Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change to the bi-classification task for the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined for learning GANs.",
      "authors": [
        "Ceyuan Yang",
        "Yujun Shen",
        "Yinghao Xu",
        "Deli Zhao",
        "Bo Dai",
        "Bolei Zhou"
      ],
      "published": "2022-09-20T17:57:33Z",
      "updated": "2022-09-20T17:57:33Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09897v1",
      "landing_url": "https://arxiv.org/abs/2209.09897v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.09897"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2209.11404",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.11404v3",
      "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
      "summary": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
      "published": "2022-09-23T04:25:19Z"
    },
    "metadata": {
      "arxiv_id": "2209.11404",
      "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
      "summary": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
      "authors": [
        "Weitao Feng",
        "Lei Bai",
        "Yongqiang Yao",
        "Fengwei Yu",
        "Wanli Ouyang"
      ],
      "published": "2022-09-23T04:25:19Z",
      "updated": "2023-04-18T02:15:17Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11404v3",
      "landing_url": "https://arxiv.org/abs/2209.11404v3",
      "doi": "https://doi.org/10.1007/s11263-023-01943-2"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2209.15472",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.15472v1",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "published": "2022-09-30T13:56:25Z"
    },
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2210.02747",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.02747v2",
      "title": "Flow Matching for Generative Modeling",
      "summary": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
      "published": "2022-10-06T08:32:20Z"
    },
    "metadata": {
      "arxiv_id": "2210.02747",
      "title": "Flow Matching for Generative Modeling",
      "summary": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
      "authors": [
        "Yaron Lipman",
        "Ricky T. Q. Chen",
        "Heli Ben-Hamu",
        "Maximilian Nickel",
        "Matt Le"
      ],
      "published": "2022-10-06T08:32:20Z",
      "updated": "2023-02-08T15:46:05Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02747v2",
      "landing_url": "https://arxiv.org/abs/2210.02747v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.02747"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2210.03273",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.03273v3",
      "title": "A Unified Encoder-Decoder Framework with Entity Memory",
      "summary": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
      "published": "2022-10-07T01:15:30Z"
    },
    "metadata": {
      "arxiv_id": "2210.03273",
      "title": "A Unified Encoder-Decoder Framework with Entity Memory",
      "summary": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
      "authors": [
        "Zhihan Zhang",
        "Wenhao Yu",
        "Chenguang Zhu",
        "Meng Jiang"
      ],
      "published": "2022-10-07T01:15:30Z",
      "updated": "2023-04-24T01:54:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.03273v3",
      "landing_url": "https://arxiv.org/abs/2210.03273v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.03273"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2210.06007",
    "anchor": "speech representation",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.06007v2",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "published": "2022-10-12T08:23:20Z"
    },
    "metadata": {
      "arxiv_id": "2210.06007",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "authors": [
        "Yueh-Kao Wu",
        "Ching-Yu Chiu",
        "Yi-Hsuan Yang"
      ],
      "published": "2022-10-12T08:23:20Z",
      "updated": "2022-10-31T08:54:08Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06007v2",
      "landing_url": "https://arxiv.org/abs/2210.06007v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06007"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2210.07323",
    "anchor": "speech representation",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.07323v3",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "published": "2022-10-13T19:46:39Z"
    },
    "metadata": {
      "arxiv_id": "2210.07323",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "authors": [
        "Ali Safaya",
        "Engin Erzin"
      ],
      "published": "2022-10-13T19:46:39Z",
      "updated": "2022-12-23T11:11:03Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.07323v3",
      "landing_url": "https://arxiv.org/abs/2210.07323v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.07323"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2210.13771",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.13771v1",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "published": "2022-10-25T05:12:47Z"
    },
    "metadata": {
      "arxiv_id": "2210.13771",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "authors": [
        "Hui Lu",
        "Disong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "published": "2022-10-25T05:12:47Z",
      "updated": "2022-10-25T05:12:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13771v1",
      "landing_url": "https://arxiv.org/abs/2210.13771v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2210.13805",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.13805v1",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "published": "2022-10-25T07:26:47Z"
    },
    "metadata": {
      "arxiv_id": "2210.13805",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Kexin Zhu",
        "Jing Xiao"
      ],
      "published": "2022-10-25T07:26:47Z",
      "updated": "2022-10-25T07:26:47Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13805v1",
      "landing_url": "https://arxiv.org/abs/2210.13805v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13805"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2210.16611",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16611v2",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "published": "2022-10-29T14:22:43Z"
    },
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2210.16755",
    "anchor": "discrete speech tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.16755v1",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "published": "2022-10-30T06:38:19Z"
    },
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2210.17052",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.17052v1",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "published": "2022-10-31T04:04:48Z"
    },
    "metadata": {
      "arxiv_id": "2210.17052",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "authors": [
        "Won-Seok Choi",
        "Dong-Sig Han",
        "Hyundo Lee",
        "Junseok Park",
        "Byoung-Tak Zhang"
      ],
      "published": "2022-10-31T04:04:48Z",
      "updated": "2022-10-31T04:04:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17052v1",
      "landing_url": "https://arxiv.org/abs/2210.17052v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.17052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.05304",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.05304v1",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "published": "2022-11-10T02:45:36Z"
    },
    "metadata": {
      "arxiv_id": "2211.05304",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "authors": [
        "Nico Lingg",
        "Miguel Sarabia",
        "Luca Zappella",
        "Barry-John Theobald"
      ],
      "published": "2022-11-10T02:45:36Z",
      "updated": "2022-11-10T02:45:36Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05304v1",
      "landing_url": "https://arxiv.org/abs/2211.05304v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05304"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.08282",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.08282v1",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "published": "2022-11-15T16:32:36Z"
    },
    "metadata": {
      "arxiv_id": "2211.08282",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "authors": [
        "T. Anderson Keller",
        "Xavier Suau",
        "Luca Zappella"
      ],
      "published": "2022-11-15T16:32:36Z",
      "updated": "2022-11-15T16:32:36Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08282v1",
      "landing_url": "https://arxiv.org/abs/2211.08282v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08282"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2211.09944",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09944v3",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "published": "2022-11-17T23:38:29Z"
    },
    "metadata": {
      "arxiv_id": "2211.09944",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "authors": [
        "Tzu-Quan Lin",
        "Hung-yi Lee",
        "Hao Tang"
      ],
      "published": "2022-11-17T23:38:29Z",
      "updated": "2024-08-29T19:25:59Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09944v3",
      "landing_url": "https://arxiv.org/abs/2211.09944v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09944"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2211.09988",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.09988v1",
      "title": "Exploring WavLM on Speech Enhancement",
      "summary": "There is a surge in interest in self-supervised learning approaches for end-to-end speech encoding in recent years as they have achieved great success. Especially, WavLM showed state-of-the-art performance on various speech processing tasks. To better understand the efficacy of self-supervised learning models for speech enhancement, in this work, we design and conduct a series of experiments with three resource conditions by combining WavLM and two high-quality speech enhancement systems. Also, we propose a regression-based WavLM training objective and a noise-mixing data configuration to further boost the downstream enhancement performance. The experiments on the DNS challenge dataset and a simulation dataset show that the WavLM benefits the speech enhancement task in terms of both speech quality and speech recognition accuracy, especially for low fine-tuning resources. For the high fine-tuning resource condition, only the word error rate is substantially improved.",
      "published": "2022-11-18T02:23:16Z"
    },
    "metadata": {
      "arxiv_id": "2211.09988",
      "title": "Exploring WavLM on Speech Enhancement",
      "summary": "There is a surge in interest in self-supervised learning approaches for end-to-end speech encoding in recent years as they have achieved great success. Especially, WavLM showed state-of-the-art performance on various speech processing tasks. To better understand the efficacy of self-supervised learning models for speech enhancement, in this work, we design and conduct a series of experiments with three resource conditions by combining WavLM and two high-quality speech enhancement systems. Also, we propose a regression-based WavLM training objective and a noise-mixing data configuration to further boost the downstream enhancement performance. The experiments on the DNS challenge dataset and a simulation dataset show that the WavLM benefits the speech enhancement task in terms of both speech quality and speech recognition accuracy, especially for low fine-tuning resources. For the high fine-tuning resource condition, only the word error rate is substantially improved.",
      "authors": [
        "Hyungchan Song",
        "Sanyuan Chen",
        "Zhuo Chen",
        "Yu Wu",
        "Takuya Yoshioka",
        "Min Tang",
        "Jong Won Shin",
        "Shujie Liu"
      ],
      "published": "2022-11-18T02:23:16Z",
      "updated": "2022-11-18T02:23:16Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09988v1",
      "landing_url": "https://arxiv.org/abs/2211.09988v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.09988"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2211.14363",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.14363v1",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "published": "2022-11-25T20:09:22Z"
    },
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2211.16112",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.16112v2",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "published": "2022-11-29T11:35:13Z"
    },
    "metadata": {
      "arxiv_id": "2211.16112",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Keisuke Kinoshita",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-11-29T11:35:13Z",
      "updated": "2023-07-21T07:28:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16112v2",
      "landing_url": "https://arxiv.org/abs/2211.16112v2",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10094784"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2211.17091",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.17091v4",
      "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
      "summary": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
      "published": "2022-11-28T20:04:12Z"
    },
    "metadata": {
      "arxiv_id": "2211.17091",
      "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
      "summary": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
      "authors": [
        "Dongjun Kim",
        "Yeongmin Kim",
        "Se Jung Kwon",
        "Wanmo Kang",
        "Il-Chul Moon"
      ],
      "published": "2022-11-28T20:04:12Z",
      "updated": "2023-06-04T22:19:27Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.17091v4",
      "landing_url": "https://arxiv.org/abs/2211.17091v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.17091"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2212.07847",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.07847v1",
      "title": "Hierarchical Codebook Design for Near-Field MmWave MIMO Communications Systems",
      "summary": "Communications system with analog or hybrid analog/digital architectures usually relies on a pre-defined codebook to perform beamforming. With the increase in the size of the antenna array, the characteristics of the spherical wavefront in the near-field situation are not negligible. Therefore, it is necessary to design a codebook that is adaptive to near-field scenarios. In this letter, we investigate the hierarchical codebook design method in the near-field situation. We develop a steering beam gain calculation method and design the lower-layer codebook to satisfy the coverage of the Fresnel region. For the upper-layer codebook, we propose beam rotation and beam relocation methods to place an arbitrary beam pattern at target locations. The simulation results show the superiority of the proposed near-field hierarchical codebook design.",
      "published": "2022-12-15T14:16:13Z"
    },
    "metadata": {
      "arxiv_id": "2212.07847",
      "title": "Hierarchical Codebook Design for Near-Field MmWave MIMO Communications Systems",
      "summary": "Communications system with analog or hybrid analog/digital architectures usually relies on a pre-defined codebook to perform beamforming. With the increase in the size of the antenna array, the characteristics of the spherical wavefront in the near-field situation are not negligible. Therefore, it is necessary to design a codebook that is adaptive to near-field scenarios. In this letter, we investigate the hierarchical codebook design method in the near-field situation. We develop a steering beam gain calculation method and design the lower-layer codebook to satisfy the coverage of the Fresnel region. For the upper-layer codebook, we propose beam rotation and beam relocation methods to place an arbitrary beam pattern at target locations. The simulation results show the superiority of the proposed near-field hierarchical codebook design.",
      "authors": [
        "Jiawei Chen",
        "Feifei Gao",
        "Mengnan Jian",
        "Wanmai Yuan"
      ],
      "published": "2022-12-15T14:16:13Z",
      "updated": "2022-12-15T14:16:13Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.07847v1",
      "landing_url": "https://arxiv.org/abs/2212.07847v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.07847"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2212.09058",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.09058v1",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "published": "2022-12-18T10:41:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2212.11187",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.11187v1",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "published": "2022-12-21T16:56:55Z"
    },
    "metadata": {
      "arxiv_id": "2212.11187",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "authors": [
        "Julien Denize",
        "Jaonary Rabarisoa",
        "Astrid Orcesi",
        "Romain Hérault"
      ],
      "published": "2022-12-21T16:56:55Z",
      "updated": "2022-12-21T16:56:55Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11187v1",
      "landing_url": "https://arxiv.org/abs/2212.11187v1",
      "doi": "https://doi.org/10.1007/s00138-023-01444-9"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2212.11444",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.11444v1",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "published": "2022-12-22T01:26:38Z"
    },
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2301.00652",
    "anchor": "speech representation",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.00652v1",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "published": "2022-12-14T06:09:08Z"
    },
    "metadata": {
      "arxiv_id": "2301.00652",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "authors": [
        "Ching-Feng Yeh",
        "Wei-Ning Hsu",
        "Paden Tomasello",
        "Abdelrahman Mohamed"
      ],
      "published": "2022-12-14T06:09:08Z",
      "updated": "2022-12-14T06:09:08Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00652v1",
      "landing_url": "https://arxiv.org/abs/2301.00652v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00652"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2301.02033",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.02033v1",
      "title": "Physics-informed self-supervised deep learning reconstruction for accelerated first-pass perfusion cardiac MRI",
      "summary": "First-pass perfusion cardiac magnetic resonance (FPP-CMR) is becoming an essential non-invasive imaging method for detecting deficits of myocardial blood flow, allowing the assessment of coronary heart disease. Nevertheless, acquisitions suffer from relatively low spatial resolution and limited heart coverage. Compressed sensing (CS) methods have been proposed to accelerate FPP-CMR and achieve higher spatial resolution. However, the long reconstruction times have limited the widespread clinical use of CS in FPP-CMR. Deep learning techniques based on supervised learning have emerged as alternatives for speeding up reconstructions. However, these approaches require fully sampled data for training, which is not possible to obtain, particularly high-resolution FPP-CMR images. Here, we propose a physics-informed self-supervised deep learning FPP-CMR reconstruction approach for accelerating FPP-CMR scans and hence facilitate high spatial resolution imaging. The proposed method provides high-quality FPP-CMR images from 10x undersampled data without using fully sampled reference data.",
      "published": "2023-01-05T12:11:17Z"
    },
    "metadata": {
      "arxiv_id": "2301.02033",
      "title": "Physics-informed self-supervised deep learning reconstruction for accelerated first-pass perfusion cardiac MRI",
      "summary": "First-pass perfusion cardiac magnetic resonance (FPP-CMR) is becoming an essential non-invasive imaging method for detecting deficits of myocardial blood flow, allowing the assessment of coronary heart disease. Nevertheless, acquisitions suffer from relatively low spatial resolution and limited heart coverage. Compressed sensing (CS) methods have been proposed to accelerate FPP-CMR and achieve higher spatial resolution. However, the long reconstruction times have limited the widespread clinical use of CS in FPP-CMR. Deep learning techniques based on supervised learning have emerged as alternatives for speeding up reconstructions. However, these approaches require fully sampled data for training, which is not possible to obtain, particularly high-resolution FPP-CMR images. Here, we propose a physics-informed self-supervised deep learning FPP-CMR reconstruction approach for accelerating FPP-CMR scans and hence facilitate high spatial resolution imaging. The proposed method provides high-quality FPP-CMR images from 10x undersampled data without using fully sampled reference data.",
      "authors": [
        "Elena Martín-González",
        "Ebraham Alskaf",
        "Amedeo Chiribiri",
        "Pablo Casaseca-de-la-Higuera",
        "Carlos Alberola-López",
        "Rita G Nunes",
        "Teresa M Correia"
      ],
      "published": "2023-01-05T12:11:17Z",
      "updated": "2023-01-05T12:11:17Z",
      "categories": [
        "eess.IV",
        "cs.LG",
        "physics.med-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02033v1",
      "landing_url": "https://arxiv.org/abs/2301.02033v1",
      "doi": "https://doi.org/10.1007/978-3-030-88552-6_9"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      }
    ]
  },
  {
    "arxiv_id": "2301.04388",
    "anchor": "speech representation",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.04388v3",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "published": "2023-01-11T10:20:56Z"
    },
    "metadata": {
      "arxiv_id": "2301.04388",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "authors": [
        "George Close",
        "William Ravenscroft",
        "Thomas Hain",
        "Stefan Goetze"
      ],
      "published": "2023-01-11T10:20:56Z",
      "updated": "2023-06-26T09:31:53Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04388v3",
      "landing_url": "https://arxiv.org/abs/2301.04388v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095666"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2302.04304",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.04304v3",
      "title": "Q-Diffusion: Quantizing Diffusion Models",
      "summary": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.",
      "published": "2023-02-08T19:38:59Z"
    },
    "metadata": {
      "arxiv_id": "2302.04304",
      "title": "Q-Diffusion: Quantizing Diffusion Models",
      "summary": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.",
      "authors": [
        "Xiuyu Li",
        "Yijiang Liu",
        "Long Lian",
        "Huanrui Yang",
        "Zhen Dong",
        "Daniel Kang",
        "Shanghang Zhang",
        "Kurt Keutzer"
      ],
      "published": "2023-02-08T19:38:59Z",
      "updated": "2023-06-08T09:21:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04304v3",
      "landing_url": "https://arxiv.org/abs/2302.04304v3",
      "doi": "https://doi.org/10.48550/arXiv.2302.04304"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2302.05756",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.05756v1",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "published": "2023-02-11T18:33:42Z"
    },
    "metadata": {
      "arxiv_id": "2302.05756",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "authors": [
        "Cong Han",
        "Vishal Choudhari",
        "Yinghao Aaron Li",
        "Nima Mesgarani"
      ],
      "published": "2023-02-11T18:33:42Z",
      "updated": "2023-02-11T18:33:42Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05756v1",
      "landing_url": "https://arxiv.org/abs/2302.05756v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05756"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2302.08137",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.08137v1",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "published": "2023-02-16T08:10:41Z"
    },
    "metadata": {
      "arxiv_id": "2302.08137",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "published": "2023-02-16T08:10:41Z",
      "updated": "2023-02-16T08:10:41Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08137v1",
      "landing_url": "https://arxiv.org/abs/2302.08137v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08137"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2302.12434",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.12434v1",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "published": "2023-02-24T03:33:13Z"
    },
    "metadata": {
      "arxiv_id": "2302.12434",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "authors": [
        "Jiangyi Deng",
        "Yanjiao Chen",
        "Yinan Zhong",
        "Qianhao Miao",
        "Xueluan Gong",
        "Wenyuan Xu"
      ],
      "published": "2023-02-24T03:33:13Z",
      "updated": "2023-02-24T03:33:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.12434v1",
      "landing_url": "https://arxiv.org/abs/2302.12434v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.12434"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2303.02939",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.02939v3",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "published": "2023-03-06T07:17:15Z"
    },
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2303.05938",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.05938v1",
      "title": "ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction",
      "summary": "Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction.",
      "published": "2023-03-10T14:19:02Z"
    },
    "metadata": {
      "arxiv_id": "2303.05938",
      "title": "ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction",
      "summary": "Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction.",
      "authors": [
        "Zhengdi Yu",
        "Shaoli Huang",
        "Chen Fang",
        "Toby P. Breckon",
        "Jue Wang"
      ],
      "published": "2023-03-10T14:19:02Z",
      "updated": "2023-03-10T14:19:02Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.05938v1",
      "landing_url": "https://arxiv.org/abs/2303.05938v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.05938"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      }
    ]
  },
  {
    "arxiv_id": "2303.06982",
    "anchor": "speech representation",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.06982v3",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "published": "2023-03-13T10:32:44Z"
    },
    "metadata": {
      "arxiv_id": "2303.06982",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2023-03-13T10:32:44Z",
      "updated": "2024-01-11T11:15:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06982v3",
      "landing_url": "https://arxiv.org/abs/2303.06982v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06982"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.08685",
    "anchor": "semantic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.08685v2",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "published": "2023-03-15T15:12:36Z"
    },
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2303.09295",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.09295v1",
      "title": "DIRE for Diffusion-Generated Image Detection",
      "summary": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by eight diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code and dataset are available at https://github.com/ZhendongWang6/DIRE.",
      "published": "2023-03-16T13:15:03Z"
    },
    "metadata": {
      "arxiv_id": "2303.09295",
      "title": "DIRE for Diffusion-Generated Image Detection",
      "summary": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by eight diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code and dataset are available at https://github.com/ZhendongWang6/DIRE.",
      "authors": [
        "Zhendong Wang",
        "Jianmin Bao",
        "Wengang Zhou",
        "Weilun Wang",
        "Hezhen Hu",
        "Hong Chen",
        "Houqiang Li"
      ],
      "published": "2023-03-16T13:15:03Z",
      "updated": "2023-03-16T13:15:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09295v1",
      "landing_url": "https://arxiv.org/abs/2303.09295v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09295"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2303.11131",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.11131v1",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "published": "2023-03-20T14:07:13Z"
    },
    "metadata": {
      "arxiv_id": "2303.11131",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "authors": [
        "Maryam Fazel-Zarandi",
        "Wei-Ning Hsu"
      ],
      "published": "2023-03-20T14:07:13Z",
      "updated": "2023-03-20T14:07:13Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.11131v1",
      "landing_url": "https://arxiv.org/abs/2303.11131v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.11131"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.12187",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.12187v1",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "published": "2023-02-28T02:10:13Z"
    },
    "metadata": {
      "arxiv_id": "2303.12187",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "authors": [
        "Xiaoming Ren",
        "Chao Li",
        "Shenjian Wang",
        "Biao Li"
      ],
      "published": "2023-02-28T02:10:13Z",
      "updated": "2023-02-28T02:10:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12187v1",
      "landing_url": "https://arxiv.org/abs/2303.12187v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12187"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2303.12197",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.12197v1",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "published": "2023-03-21T21:04:03Z"
    },
    "metadata": {
      "arxiv_id": "2303.12197",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "authors": [
        "Tejas Jayashankar",
        "Jilong Wu",
        "Leda Sari",
        "David Kant",
        "Vimal Manohar",
        "Qing He"
      ],
      "published": "2023-03-21T21:04:03Z",
      "updated": "2023-03-21T21:04:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12197v1",
      "landing_url": "https://arxiv.org/abs/2303.12197v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12197"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2303.13909",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.13909v1",
      "title": "Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis",
      "summary": "In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficiently rich information for the synthesized speech to be closely matched to the real speech. During the experiments, the proposed ideas were applied to a representative neural vocoder (HiFi-GAN) and an end-to-end TTS system (VITS). The results demonstrate that the proposed models can achieve comparable speech quality with a 2.31 times faster and 14.5 times more lightweight discriminator when used in HiFi-GAN and a 1.90 times faster and 9.62 times more lightweight discriminator when used in VITS. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/.",
      "published": "2023-03-24T10:46:40Z"
    },
    "metadata": {
      "arxiv_id": "2303.13909",
      "title": "Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis",
      "summary": "In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficiently rich information for the synthesized speech to be closely matched to the real speech. During the experiments, the proposed ideas were applied to a representative neural vocoder (HiFi-GAN) and an end-to-end TTS system (VITS). The results demonstrate that the proposed models can achieve comparable speech quality with a 2.31 times faster and 14.5 times more lightweight discriminator when used in HiFi-GAN and a 1.90 times faster and 9.62 times more lightweight discriminator when used in VITS. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/.",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Shogo Seki"
      ],
      "published": "2023-03-24T10:46:40Z",
      "updated": "2023-03-24T10:46:40Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.13909v1",
      "landing_url": "https://arxiv.org/abs/2303.13909v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2303.16203",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.16203v3",
      "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
      "summary": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/",
      "published": "2023-03-28T17:59:56Z"
    },
    "metadata": {
      "arxiv_id": "2303.16203",
      "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
      "summary": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/",
      "authors": [
        "Alexander C. Li",
        "Mihir Prabhudesai",
        "Shivam Duggal",
        "Ellis Brown",
        "Deepak Pathak"
      ],
      "published": "2023-03-28T17:59:56Z",
      "updated": "2023-09-13T01:16:45Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.16203v3",
      "landing_url": "https://arxiv.org/abs/2303.16203v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.16203"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2304.00649",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.00649v1",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "published": "2023-04-02T23:08:11Z"
    },
    "metadata": {
      "arxiv_id": "2304.00649",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "authors": [
        "Shammur Absar Chowdhury",
        "Ahmed Ali"
      ],
      "published": "2023-04-02T23:08:11Z",
      "updated": "2023-04-02T23:08:11Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00649v1",
      "landing_url": "https://arxiv.org/abs/2304.00649v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.00649"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2304.02160",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.02160v1",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "published": "2023-04-04T23:19:53Z"
    },
    "metadata": {
      "arxiv_id": "2304.02160",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "authors": [
        "Ke Chen",
        "Gordon Wichern",
        "François G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2023-04-04T23:19:53Z",
      "updated": "2023-04-04T23:19:53Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02160v1",
      "landing_url": "https://arxiv.org/abs/2304.02160v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02160"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2304.03940",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.03940v1",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "published": "2023-04-08T07:03:01Z"
    },
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2304.04052",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.04052v1",
      "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
      "summary": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
      "published": "2023-04-08T15:44:29Z"
    },
    "metadata": {
      "arxiv_id": "2304.04052",
      "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
      "summary": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
      "authors": [
        "Zihao Fu",
        "Wai Lam",
        "Qian Yu",
        "Anthony Man-Cho So",
        "Shengding Hu",
        "Zhiyuan Liu",
        "Nigel Collier"
      ],
      "published": "2023-04-08T15:44:29Z",
      "updated": "2023-04-08T15:44:29Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04052v1",
      "landing_url": "https://arxiv.org/abs/2304.04052v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.04052"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2304.04974",
    "anchor": "speech representation",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.04974v3",
      "title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR",
      "summary": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
      "published": "2023-04-11T04:46:12Z"
    },
    "metadata": {
      "arxiv_id": "2304.04974",
      "title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR",
      "summary": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
      "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Qiushi Zhu",
        "Eng Siong Chng"
      ],
      "published": "2023-04-11T04:46:12Z",
      "updated": "2024-04-18T06:39:57Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04974v3",
      "landing_url": "https://arxiv.org/abs/2304.04974v3",
      "doi": "https://doi.org/10.48550/arXiv.2304.04974"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2304.09226",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.09226v1",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "published": "2023-04-18T18:26:56Z"
    },
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2305.00225",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.00225v1",
      "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
      "summary": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
      "published": "2023-04-29T10:32:44Z"
    },
    "metadata": {
      "arxiv_id": "2305.00225",
      "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
      "summary": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
      "authors": [
        "Vignesh V Menon",
        "Jingwen Zhu",
        "Prajit T Rajendran",
        "Hadi Amirpour",
        "Patrick Le Callet",
        "Christian Timmerer"
      ],
      "published": "2023-04-29T10:32:44Z",
      "updated": "2023-04-29T10:32:44Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.00225v1",
      "landing_url": "https://arxiv.org/abs/2305.00225v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.00225"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2305.13651",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.13651v2",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "published": "2023-05-23T03:49:41Z"
    },
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2305.15688",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15688v1",
      "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
      "summary": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
      "published": "2023-05-25T03:34:24Z"
    },
    "metadata": {
      "arxiv_id": "2305.15688",
      "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
      "summary": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
      "authors": [
        "Jiqing Zhang",
        "Yuanchen Wang",
        "Wenxi Liu",
        "Meng Li",
        "Jinpeng Bai",
        "Baocai Yin",
        "Xin Yang"
      ],
      "published": "2023-05-25T03:34:24Z",
      "updated": "2023-05-25T03:34:24Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15688v1",
      "landing_url": "https://arxiv.org/abs/2305.15688v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15688"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2305.16608",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.16608v1",
      "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
      "summary": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
      "published": "2023-05-26T04:01:16Z"
    },
    "metadata": {
      "arxiv_id": "2305.16608",
      "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
      "summary": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
      "authors": [
        "Yi-Chiao Wu",
        "Israel D. Gebru",
        "Dejan Marković",
        "Alexander Richard"
      ],
      "published": "2023-05-26T04:01:16Z",
      "updated": "2023-05-26T04:01:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16608v1",
      "landing_url": "https://arxiv.org/abs/2305.16608v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10096509"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2305.17161",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17161v2",
      "title": "Flow Matching for Scalable Simulation-Based Inference",
      "summary": "Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.",
      "published": "2023-05-26T18:00:01Z"
    },
    "metadata": {
      "arxiv_id": "2305.17161",
      "title": "Flow Matching for Scalable Simulation-Based Inference",
      "summary": "Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.",
      "authors": [
        "Maximilian Dax",
        "Jonas Wildberger",
        "Simon Buchholz",
        "Stephen R. Green",
        "Jakob H. Macke",
        "Bernhard Schölkopf"
      ],
      "published": "2023-05-26T18:00:01Z",
      "updated": "2023-10-27T12:37:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17161v2",
      "landing_url": "https://arxiv.org/abs/2305.17161v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.17161"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2305.17209",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17209v2",
      "title": "Functional Flow Matching",
      "summary": "We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.",
      "published": "2023-05-26T19:07:47Z"
    },
    "metadata": {
      "arxiv_id": "2305.17209",
      "title": "Functional Flow Matching",
      "summary": "We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.",
      "authors": [
        "Gavin Kerrigan",
        "Giosue Migliorini",
        "Padhraic Smyth"
      ],
      "published": "2023-05-26T19:07:47Z",
      "updated": "2023-12-05T19:53:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17209v2",
      "landing_url": "https://arxiv.org/abs/2305.17209v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.17209"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2305.18739",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18739v1",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "published": "2023-05-30T04:26:48Z"
    },
    "metadata": {
      "arxiv_id": "2305.18739",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "authors": [
        "Jaeuk Byun",
        "Youna Ji",
        "Soo Whan Chung",
        "Soyeon Choe",
        "Min Seok Choi"
      ],
      "published": "2023-05-30T04:26:48Z",
      "updated": "2023-05-30T04:26:48Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18739v1",
      "landing_url": "https://arxiv.org/abs/2305.18739v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095881"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.01070",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01070v1",
      "title": "Hierarchical Attention Encoder Decoder",
      "summary": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
      "published": "2023-06-01T18:17:23Z"
    },
    "metadata": {
      "arxiv_id": "2306.01070",
      "title": "Hierarchical Attention Encoder Decoder",
      "summary": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
      "authors": [
        "Asier Mujika"
      ],
      "published": "2023-06-01T18:17:23Z",
      "updated": "2023-06-01T18:17:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01070v1",
      "landing_url": "https://arxiv.org/abs/2306.01070v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01070"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2306.01084",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01084v2",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "published": "2023-06-01T18:51:34Z"
    },
    "metadata": {
      "arxiv_id": "2306.01084",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "authors": [
        "Jiatong Shi",
        "Yun Tang",
        "Hirofumi Inaguma",
        "Hongyu GOng",
        "Juan Pino",
        "Shinji Watanabe"
      ],
      "published": "2023-06-01T18:51:34Z",
      "updated": "2023-06-22T18:34:22Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01084v2",
      "landing_url": "https://arxiv.org/abs/2306.01084v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.01084"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2306.01303",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01303v1",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "published": "2023-06-02T07:03:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.01303",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "authors": [
        "Haoyu Wang",
        "Siyuan Wang",
        "Wei-Qiang Zhang",
        "Jinfeng Bai"
      ],
      "published": "2023-06-02T07:03:06Z",
      "updated": "2023-06-02T07:03:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01303v1",
      "landing_url": "https://arxiv.org/abs/2306.01303v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01303"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.02972",
    "anchor": "speech representation",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.02972v1",
      "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
      "summary": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "published": "2023-06-05T15:35:19Z"
    },
    "metadata": {
      "arxiv_id": "2306.02972",
      "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
      "summary": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "authors": [
        "Khazar Khorrami",
        "María Andrea Cruz Blandón",
        "Tuomas Virtanen",
        "Okko Räsänen"
      ],
      "published": "2023-06-05T15:35:19Z",
      "updated": "2023-06-05T15:35:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.02972v1",
      "landing_url": "https://arxiv.org/abs/2306.02972v1",
      "doi": "https://doi.org/10.23919/EUSIPCO58844.2023.10290051"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2306.04374",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.04374v1",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "published": "2023-06-07T12:14:16Z"
    },
    "metadata": {
      "arxiv_id": "2306.04374",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "authors": [
        "Shikhar Vashishth",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Ankur Bapna",
        "Min Ma",
        "Wei Han",
        "Vera Axelrod",
        "Partha Talukdar"
      ],
      "published": "2023-06-07T12:14:16Z",
      "updated": "2023-06-07T12:14:16Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04374v1",
      "landing_url": "https://arxiv.org/abs/2306.04374v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.04374"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.06082",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06082v4",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "published": "2023-05-31T12:24:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.06082",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "authors": [
        "Marcin Przewięźlikowski",
        "Mateusz Pyla",
        "Bartosz Zieliński",
        "Bartłomiej Twardowski",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "published": "2023-05-31T12:24:06Z",
      "updated": "2024-10-19T08:00:13Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06082v4",
      "landing_url": "https://arxiv.org/abs/2306.06082v4",
      "doi": "https://doi.org/10.1016/j.knosys.2024.112572"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2306.06672",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06672v1",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "published": "2023-06-11T12:53:46Z"
    },
    "metadata": {
      "arxiv_id": "2306.06672",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "authors": [
        "William Chen",
        "Xuankai Chang",
        "Yifan Peng",
        "Zhaoheng Ni",
        "Soumi Maiti",
        "Shinji Watanabe"
      ],
      "published": "2023-06-11T12:53:46Z",
      "updated": "2023-06-11T12:53:46Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06672v1",
      "landing_url": "https://arxiv.org/abs/2306.06672v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06672"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2306.06814",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.06814v1",
      "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
      "summary": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "published": "2023-06-12T01:21:41Z"
    },
    "metadata": {
      "arxiv_id": "2306.06814",
      "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
      "summary": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "authors": [
        "Ji-Sang Hwang",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "published": "2023-06-12T01:21:41Z",
      "updated": "2023-06-12T01:21:41Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06814v1",
      "landing_url": "https://arxiv.org/abs/2306.06814v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06814"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2306.07547",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.07547v6",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "published": "2023-06-13T05:38:34Z"
    },
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.07716",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.07716v3",
      "title": "Dynamically Masked Discriminator for Generative Adversarial Networks",
      "summary": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.",
      "published": "2023-06-13T12:07:01Z"
    },
    "metadata": {
      "arxiv_id": "2306.07716",
      "title": "Dynamically Masked Discriminator for Generative Adversarial Networks",
      "summary": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.",
      "authors": [
        "Wentian Zhang",
        "Haozhe Liu",
        "Bing Li",
        "Jinheng Xie",
        "Yawen Huang",
        "Yuexiang Li",
        "Yefeng Zheng",
        "Bernard Ghanem"
      ],
      "published": "2023-06-13T12:07:01Z",
      "updated": "2024-01-04T13:58:50Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07716v3",
      "landing_url": "https://arxiv.org/abs/2306.07716v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.07716"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2306.08920",
    "anchor": "discrete speech tokens",
    "search_term": "unit discovery",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.08920v1",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "published": "2023-06-15T07:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      }
    ]
  },
  {
    "arxiv_id": "2306.10125",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.10125v4",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "published": "2023-06-16T18:23:10Z"
    },
    "metadata": {
      "arxiv_id": "2306.10125",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "authors": [
        "Kexin Zhang",
        "Qingsong Wen",
        "Chaoli Zhang",
        "Rongyao Cai",
        "Ming Jin",
        "Yong Liu",
        "James Zhang",
        "Yuxuan Liang",
        "Guansong Pang",
        "Dongjin Song",
        "Shirui Pan"
      ],
      "published": "2023-06-16T18:23:10Z",
      "updated": "2024-04-08T15:38:59Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10125v4",
      "landing_url": "https://arxiv.org/abs/2306.10125v4",
      "doi": "https://doi.org/10.48550/arXiv.2306.10125"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2306.10521",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.10521v2",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "published": "2023-06-18T10:59:06Z"
    },
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.12785",
    "anchor": "acoustic tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.12785v1",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "published": "2023-06-22T10:29:24Z"
    },
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2306.14422",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.14422v2",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "published": "2023-06-26T05:04:58Z"
    },
    "metadata": {
      "arxiv_id": "2306.14422",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "authors": [
        "Wen-Chin Huang",
        "Lester Phillip Violeta",
        "Songxiang Liu",
        "Jiatong Shi",
        "Tomoki Toda"
      ],
      "published": "2023-06-26T05:04:58Z",
      "updated": "2023-07-06T08:17:31Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14422v2",
      "landing_url": "https://arxiv.org/abs/2306.14422v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14422"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2306.14891",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.14891v2",
      "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction",
      "summary": "Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.",
      "published": "2023-06-26T17:58:00Z"
    },
    "metadata": {
      "arxiv_id": "2306.14891",
      "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction",
      "summary": "Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.",
      "authors": [
        "Majed El Helou"
      ],
      "published": "2023-06-26T17:58:00Z",
      "updated": "2023-07-01T22:22:14Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14891v2",
      "landing_url": "https://arxiv.org/abs/2306.14891v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14891"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2306.15030",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.15030v2",
      "title": "Equivariant flow matching",
      "summary": "Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivariant flow matching exploits the physical symmetries of the target energy for efficient, simulation-free training of equivariant CNFs. We demonstrate the effectiveness of flow matching on rotation and permutation invariant many-particle systems and a small molecule, alanine dipeptide, where for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization. Our results show that the equivariant flow matching objective yields flows with shorter integration paths, improved sampling efficiency, and higher scalability compared to existing methods.",
      "published": "2023-06-26T19:40:10Z"
    },
    "metadata": {
      "arxiv_id": "2306.15030",
      "title": "Equivariant flow matching",
      "summary": "Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivariant flow matching exploits the physical symmetries of the target energy for efficient, simulation-free training of equivariant CNFs. We demonstrate the effectiveness of flow matching on rotation and permutation invariant many-particle systems and a small molecule, alanine dipeptide, where for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization. Our results show that the equivariant flow matching objective yields flows with shorter integration paths, improved sampling efficiency, and higher scalability compared to existing methods.",
      "authors": [
        "Leon Klein",
        "Andreas Krämer",
        "Frank Noé"
      ],
      "published": "2023-06-26T19:40:10Z",
      "updated": "2023-11-23T21:53:19Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15030v2",
      "landing_url": "https://arxiv.org/abs/2306.15030v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15030"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2306.15354",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.15354v3",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "published": "2023-06-27T10:09:43Z"
    },
    "metadata": {
      "arxiv_id": "2306.15354",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "published": "2023-06-27T10:09:43Z",
      "updated": "2023-09-25T02:36:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15354v3",
      "landing_url": "https://arxiv.org/abs/2306.15354v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.15354"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2306.16317",
    "anchor": "discrete speech tokens",
    "search_term": "length reduction",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.16317v2",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "published": "2023-06-28T15:49:20Z"
    },
    "metadata": {
      "arxiv_id": "2306.16317",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "authors": [
        "Joshua A. Grochow",
        "Youming Qiao"
      ],
      "published": "2023-06-28T15:49:20Z",
      "updated": "2024-04-12T13:10:08Z",
      "categories": [
        "cs.CC",
        "cs.DS",
        "math.AG",
        "math.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.16317v2",
      "landing_url": "https://arxiv.org/abs/2306.16317v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.16317"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2307.00393",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.00393v1",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "published": "2023-07-01T17:44:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.00393",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "authors": [
        "Houjian Guo",
        "Chaoran Liu",
        "Carlos Toshinori Ishi",
        "Hiroshi Ishiguro"
      ],
      "published": "2023-07-01T17:44:18Z",
      "updated": "2023-07-01T17:44:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00393v1",
      "landing_url": "https://arxiv.org/abs/2307.00393v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00393"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2307.02720",
    "anchor": "speech representation",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.02720v1",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "published": "2023-07-06T02:03:31Z"
    },
    "metadata": {
      "arxiv_id": "2307.02720",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "authors": [
        "Gene-Ping Yang",
        "Yue Gu",
        "Qingming Tang",
        "Dongsu Du",
        "Yuzong Liu"
      ],
      "published": "2023-07-06T02:03:31Z",
      "updated": "2023-07-06T02:03:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02720v1",
      "landing_url": "https://arxiv.org/abs/2307.02720v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.02720"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2307.03436",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.03436v1",
      "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
      "summary": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
      "published": "2023-07-07T07:47:45Z"
    },
    "metadata": {
      "arxiv_id": "2307.03436",
      "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
      "summary": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
      "authors": [
        "Zicheng Zhang",
        "Hao Chen",
        "Xun Cao",
        "Zhan Ma"
      ],
      "published": "2023-07-07T07:47:45Z",
      "updated": "2023-07-07T07:47:45Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03436v1",
      "landing_url": "https://arxiv.org/abs/2307.03436v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.03436"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2307.03672",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.03672v3",
      "title": "Simulation-free Schrödinger bridges via score and flow matching",
      "summary": "We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.",
      "published": "2023-07-07T15:42:35Z"
    },
    "metadata": {
      "arxiv_id": "2307.03672",
      "title": "Simulation-free Schrödinger bridges via score and flow matching",
      "summary": "We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.",
      "authors": [
        "Alexander Tong",
        "Nikolay Malkin",
        "Kilian Fatras",
        "Lazar Atanackovic",
        "Yanlei Zhang",
        "Guillaume Huguet",
        "Guy Wolf",
        "Yoshua Bengio"
      ],
      "published": "2023-07-07T15:42:35Z",
      "updated": "2024-03-11T14:42:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03672v3",
      "landing_url": "https://arxiv.org/abs/2307.03672v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.03672"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2307.04686",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.04686v2",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "published": "2023-07-10T16:42:03Z"
    },
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2307.08698",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.08698v1",
      "title": "Flow Matching in Latent Space",
      "summary": "Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.",
      "published": "2023-07-17T17:57:56Z"
    },
    "metadata": {
      "arxiv_id": "2307.08698",
      "title": "Flow Matching in Latent Space",
      "summary": "Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.",
      "authors": [
        "Quan Dao",
        "Hao Phung",
        "Binh Nguyen",
        "Anh Tran"
      ],
      "published": "2023-07-17T17:57:56Z",
      "updated": "2023-07-17T17:57:56Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08698v1",
      "landing_url": "https://arxiv.org/abs/2307.08698v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.08698"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2307.10982",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.10982v2",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "published": "2023-07-20T16:09:57Z"
    },
    "metadata": {
      "arxiv_id": "2307.10982",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "authors": [
        "Anjali Raj",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Min Ma",
        "Shikhar Vashishth"
      ],
      "published": "2023-07-20T16:09:57Z",
      "updated": "2023-09-25T12:49:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10982v2",
      "landing_url": "https://arxiv.org/abs/2307.10982v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.10982"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2307.11394",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.11394v3",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "published": "2023-07-21T07:22:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.11394",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-07-21T07:22:18Z",
      "updated": "2024-01-25T19:48:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11394v3",
      "landing_url": "https://arxiv.org/abs/2307.11394v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.11394"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2307.15139",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.15139v1",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "published": "2023-07-27T18:31:04Z"
    },
    "metadata": {
      "arxiv_id": "2307.15139",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "authors": [
        "Chuanxia Zheng",
        "Andrea Vedaldi"
      ],
      "published": "2023-07-27T18:31:04Z",
      "updated": "2023-07-27T18:31:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15139v1",
      "landing_url": "https://arxiv.org/abs/2307.15139v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2308.00531",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00531v1",
      "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
      "summary": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
      "published": "2023-08-01T13:25:10Z"
    },
    "metadata": {
      "arxiv_id": "2308.00531",
      "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
      "summary": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
      "authors": [
        "Wentao Gong",
        "Haonan Tong",
        "Sihua Wang",
        "Zhaohui Yang",
        "Xinxin He",
        "Changchuan Yin"
      ],
      "published": "2023-08-01T13:25:10Z",
      "updated": "2023-08-01T13:25:10Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00531v1",
      "landing_url": "https://arxiv.org/abs/2308.00531v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00531"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2308.00721",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00721v4",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "published": "2023-07-31T03:56:46Z"
    },
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2308.06382",
    "anchor": "discrete speech tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.06382v2",
      "title": "Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion",
      "summary": "Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \\textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \\textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.",
      "published": "2023-08-11T20:44:19Z"
    },
    "metadata": {
      "arxiv_id": "2308.06382",
      "title": "Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion",
      "summary": "Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \\textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \\textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.",
      "authors": [
        "Siyuan Shan",
        "Yang Li",
        "Amartya Banerjee",
        "Junier B. Oliva"
      ],
      "published": "2023-08-11T20:44:19Z",
      "updated": "2023-12-30T22:48:13Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06382v2",
      "landing_url": "https://arxiv.org/abs/2308.06382v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06382"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2308.16692",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.16692v2",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "published": "2023-08-31T12:53:09Z"
    },
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2309.00126",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00126v1",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "published": "2023-08-31T20:25:44Z"
    },
    "metadata": {
      "arxiv_id": "2309.00126",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Jiawen Kang",
        "Yujia Xiao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-08-31T20:25:44Z",
      "updated": "2023-08-31T20:25:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00126v1",
      "landing_url": "https://arxiv.org/abs/2309.00126v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.00126"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      }
    ]
  },
  {
    "arxiv_id": "2309.00169",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00169v3",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "published": "2023-08-31T23:26:10Z"
    },
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.02432",
    "anchor": "acoustic tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.02432v1",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "published": "2023-09-05T17:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2309.03619",
    "anchor": "speech representation",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.03619v2",
      "title": "Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction",
      "summary": "Self-supervised learning (SSL) has emerged as a promising paradigm for learning flexible speech representations from unlabeled data. By designing pretext tasks that exploit statistical regularities, SSL models can capture useful representations that are transferable to downstream tasks. This study provides an empirical analysis of Barlow Twins (BT), an SSL technique inspired by theories of redundancy reduction in human perception. On downstream tasks, BT representations accelerated learning and transferred across domains. However, limitations exist in disentangling key explanatory factors, with redundancy reduction and invariance alone insufficient for factorization of learned latents into modular, compact, and informative codes. Our ablations study isolated gains from invariance constraints, but the gains were context-dependent. Overall, this work substantiates the potential of Barlow Twins for sample-efficient speech encoding. However, challenges remain in achieving fully hierarchical representations. The analysis methodology and insights pave a path for extensions incorporating further inductive priors and perceptual principles to further enhance the BT self-supervision framework.",
      "published": "2023-09-07T10:23:59Z"
    },
    "metadata": {
      "arxiv_id": "2309.03619",
      "title": "Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction",
      "summary": "Self-supervised learning (SSL) has emerged as a promising paradigm for learning flexible speech representations from unlabeled data. By designing pretext tasks that exploit statistical regularities, SSL models can capture useful representations that are transferable to downstream tasks. This study provides an empirical analysis of Barlow Twins (BT), an SSL technique inspired by theories of redundancy reduction in human perception. On downstream tasks, BT representations accelerated learning and transferred across domains. However, limitations exist in disentangling key explanatory factors, with redundancy reduction and invariance alone insufficient for factorization of learned latents into modular, compact, and informative codes. Our ablations study isolated gains from invariance constraints, but the gains were context-dependent. Overall, this work substantiates the potential of Barlow Twins for sample-efficient speech encoding. However, challenges remain in achieving fully hierarchical representations. The analysis methodology and insights pave a path for extensions incorporating further inductive priors and perceptual principles to further enhance the BT self-supervision framework.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-09-07T10:23:59Z",
      "updated": "2024-01-24T13:37:11Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.03619v2",
      "landing_url": "https://arxiv.org/abs/2309.03619v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.03619"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2309.07937",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07937v3",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "published": "2023-09-14T03:13:18Z"
    },
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.09920",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09920v1",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "published": "2023-09-18T16:34:40Z"
    },
    "metadata": {
      "arxiv_id": "2309.09920",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "authors": [
        "Danilo de Oliveira",
        "Timo Gerkmann"
      ],
      "published": "2023-09-18T16:34:40Z",
      "updated": "2023-09-18T16:34:40Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09920v1",
      "landing_url": "https://arxiv.org/abs/2309.09920v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09920"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2309.11641",
    "anchor": "discrete speech tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11641v2",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "published": "2023-09-20T21:11:36Z"
    },
    "metadata": {
      "arxiv_id": "2309.11641",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "authors": [
        "Angello Hoyos",
        "Mariano Rivera"
      ],
      "published": "2023-09-20T21:11:36Z",
      "updated": "2024-02-08T20:52:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11641v2",
      "landing_url": "https://arxiv.org/abs/2309.11641v2",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2309.11977",
    "anchor": "acoustic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11977v3",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "published": "2023-09-21T11:22:22Z"
    },
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2309.13860",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.13860v2",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "published": "2023-09-25T04:07:34Z"
    },
    "metadata": {
      "arxiv_id": "2309.13860",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Yakun Song",
        "Zhikang Niu",
        "Xie Chen"
      ],
      "published": "2023-09-25T04:07:34Z",
      "updated": "2023-09-29T06:48:11Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13860v2",
      "landing_url": "https://arxiv.org/abs/2309.13860v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.13860"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2309.14129",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14129v3",
      "title": "Speaker anonymization using neural audio codec language models",
      "summary": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
      "published": "2023-09-25T13:32:09Z"
    },
    "metadata": {
      "arxiv_id": "2309.14129",
      "title": "Speaker anonymization using neural audio codec language models",
      "summary": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
      "authors": [
        "Michele Panariello",
        "Francesco Nespoli",
        "Massimiliano Todisco",
        "Nicholas Evans"
      ],
      "published": "2023-09-25T13:32:09Z",
      "updated": "2024-01-12T15:05:34Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14129v3",
      "landing_url": "https://arxiv.org/abs/2309.14129v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.14129"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.15505",
    "anchor": "discrete speech tokens",
    "search_term": "finite scalar quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.15505v2",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "published": "2023-09-27T09:13:40Z"
    },
    "metadata": {
      "arxiv_id": "2309.15505",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "authors": [
        "Fabian Mentzer",
        "David Minnen",
        "Eirikur Agustsson",
        "Michael Tschannen"
      ],
      "published": "2023-09-27T09:13:40Z",
      "updated": "2023-10-12T07:55:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15505v2",
      "landing_url": "https://arxiv.org/abs/2309.15505v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.15505"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2310.01195",
    "anchor": "discrete speech tokens",
    "search_term": "k-means clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.01195v2",
      "title": "Federated K-means Clustering",
      "summary": "Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.",
      "published": "2023-10-02T13:32:00Z"
    },
    "metadata": {
      "arxiv_id": "2310.01195",
      "title": "Federated K-means Clustering",
      "summary": "Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.",
      "authors": [
        "Swier Garst",
        "Marcel Reinders"
      ],
      "published": "2023-10-02T13:32:00Z",
      "updated": "2024-02-16T14:02:02Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01195v2",
      "landing_url": "https://arxiv.org/abs/2310.01195v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.01195"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means clustering"
      }
    ]
  },
  {
    "arxiv_id": "2310.02720",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.02720v2",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "published": "2023-10-04T10:52:13Z"
    },
    "metadata": {
      "arxiv_id": "2310.02720",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "authors": [
        "Jiatong Shi",
        "Hirofumi Inaguma",
        "Xutai Ma",
        "Ilia Kulikov",
        "Anna Sun"
      ],
      "published": "2023-10-04T10:52:13Z",
      "updated": "2024-01-30T08:52:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02720v2",
      "landing_url": "https://arxiv.org/abs/2310.02720v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.02720"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.03975",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.03975v1",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "published": "2023-10-06T02:19:09Z"
    },
    "metadata": {
      "arxiv_id": "2310.03975",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "authors": [
        "Takashi Maekaku",
        "Jiatong Shi",
        "Xuankai Chang",
        "Yuya Fujita",
        "Shinji Watanabe"
      ],
      "published": "2023-10-06T02:19:09Z",
      "updated": "2023-10-06T02:19:09Z",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03975v1",
      "landing_url": "https://arxiv.org/abs/2310.03975v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.03975"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.05718",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.05718v3",
      "title": "EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders",
      "summary": "Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE .",
      "published": "2023-10-09T13:39:26Z"
    },
    "metadata": {
      "arxiv_id": "2310.05718",
      "title": "EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders",
      "summary": "Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE .",
      "authors": [
        "Gulcin Baykal",
        "Melih Kandemir",
        "Gozde Unal"
      ],
      "published": "2023-10-09T13:39:26Z",
      "updated": "2024-07-15T09:57:48Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05718v3",
      "landing_url": "https://arxiv.org/abs/2310.05718v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.05718"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2310.08104",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08104v1",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "published": "2023-10-12T08:00:25Z"
    },
    "metadata": {
      "arxiv_id": "2310.08104",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "authors": [
        "Matthew Baas",
        "Herman Kamper"
      ],
      "published": "2023-10-12T08:00:25Z",
      "updated": "2023-10-12T08:00:25Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08104v1",
      "landing_url": "https://arxiv.org/abs/2310.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08104"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2310.08225",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08225v2",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "published": "2023-10-12T11:17:40Z"
    },
    "metadata": {
      "arxiv_id": "2310.08225",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "authors": [
        "Chanho Park",
        "Chengsong Lu",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2023-10-12T11:17:40Z",
      "updated": "2025-01-29T11:28:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08225v2",
      "landing_url": "https://arxiv.org/abs/2310.08225v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.08225"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2310.09382",
    "anchor": "discrete speech tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09382v1",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "published": "2023-10-13T20:03:18Z"
    },
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2310.09570",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09570v1",
      "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
      "summary": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
      "published": "2023-10-14T12:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2310.09570",
      "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
      "summary": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
      "authors": [
        "Vignesh V Menon",
        "Reza Farahani",
        "Prajit T Rajendran",
        "Samira Afzal",
        "Klaus Schoeffmann",
        "Christian Timmerer"
      ],
      "published": "2023-10-14T12:05:18Z",
      "updated": "2023-10-14T12:05:18Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09570v1",
      "landing_url": "https://arxiv.org/abs/2310.09570v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09570"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2310.09653",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.09653v2",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "published": "2023-10-14T19:51:17Z"
    },
    "metadata": {
      "arxiv_id": "2310.09653",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Rafael Valle",
        "Boris Ginsburg",
        "Rishabh Ranjan",
        "Shlomo Dubnov",
        "Farinaz Koushanfar",
        "Julian McAuley"
      ],
      "published": "2023-10-14T19:51:17Z",
      "updated": "2024-05-03T16:45:39Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09653v2",
      "landing_url": "https://arxiv.org/abs/2310.09653v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.09653"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2310.10325",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10325v2",
      "title": "Towards image compression with perfect realism at ultra-low bitrates",
      "summary": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
      "published": "2023-10-16T12:08:35Z"
    },
    "metadata": {
      "arxiv_id": "2310.10325",
      "title": "Towards image compression with perfect realism at ultra-low bitrates",
      "summary": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
      "authors": [
        "Marlène Careil",
        "Matthew J. Muckley",
        "Jakob Verbeek",
        "Stéphane Lathuilière"
      ],
      "published": "2023-10-16T12:08:35Z",
      "updated": "2024-03-19T09:54:41Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10325v2",
      "landing_url": "https://arxiv.org/abs/2310.10325v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.10325"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2310.10803",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10803v3",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "published": "2023-10-16T20:05:36Z"
    },
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.10922",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10922v1",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "published": "2023-10-17T01:31:59Z"
    },
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2310.11541",
    "anchor": "speech representation",
    "search_term": "unit discovery",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.11541v1",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "published": "2023-10-17T19:27:23Z"
    },
    "metadata": {
      "arxiv_id": "2310.11541",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "authors": [
        "Noé Tits"
      ],
      "published": "2023-10-17T19:27:23Z",
      "updated": "2023-10-17T19:27:23Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11541v1",
      "landing_url": "https://arxiv.org/abs/2310.11541v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      }
    ]
  },
  {
    "arxiv_id": "2310.14580",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14580v4",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "published": "2023-10-23T05:38:41Z"
    },
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2310.15163",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.15163v4",
      "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
      "summary": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
      "published": "2023-10-23T17:58:24Z"
    },
    "metadata": {
      "arxiv_id": "2310.15163",
      "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
      "summary": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
      "authors": [
        "Ahmed Telili",
        "Wassim Hamidouche",
        "Hadi Amirpour",
        "Sid Ahmed Fezza",
        "Luce Morin",
        "Christian Timmerer"
      ],
      "published": "2023-10-23T17:58:24Z",
      "updated": "2025-03-11T06:48:06Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15163v4",
      "landing_url": "https://arxiv.org/abs/2310.15163v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.15163"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2310.15817",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.15817v2",
      "title": "Discriminator Guidance for Autoregressive Diffusion Models",
      "summary": "We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.",
      "published": "2023-10-24T13:14:22Z"
    },
    "metadata": {
      "arxiv_id": "2310.15817",
      "title": "Discriminator Guidance for Autoregressive Diffusion Models",
      "summary": "We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.",
      "authors": [
        "Filip Ekström Kelvinius",
        "Fredrik Lindsten"
      ],
      "published": "2023-10-24T13:14:22Z",
      "updated": "2024-09-21T12:34:38Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15817v2",
      "landing_url": "https://arxiv.org/abs/2310.15817v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.15817"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2310.16550",
    "anchor": "acoustic tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.16550v1",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "published": "2023-10-25T11:04:32Z"
    },
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  },
  {
    "arxiv_id": "2311.02898",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.02898v2",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "published": "2023-11-06T06:13:39Z"
    },
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2311.03389",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03389v4",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "published": "2023-11-04T04:54:17Z"
    },
    "metadata": {
      "arxiv_id": "2311.03389",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-11-04T04:54:17Z",
      "updated": "2025-01-11T06:05:41Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03389v4",
      "landing_url": "https://arxiv.org/abs/2311.03389v4",
      "doi": "https://doi.org/10.48550/arXiv.2311.03389"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2311.03792",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03792v1",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "published": "2023-11-07T08:20:06Z"
    },
    "metadata": {
      "arxiv_id": "2311.03792",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "authors": [
        "Jakir Hasan",
        "Shrestha Datta",
        "Ameya Debnath"
      ],
      "published": "2023-11-07T08:20:06Z",
      "updated": "2023-11-07T08:20:06Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03792v1",
      "landing_url": "https://arxiv.org/abs/2311.03792v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03792"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2311.08104",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.08104v1",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "published": "2023-11-14T12:03:46Z"
    },
    "metadata": {
      "arxiv_id": "2311.08104",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "authors": [
        "Anders R. Bargum",
        "Stefania Serafin",
        "Cumhur Erkut"
      ],
      "published": "2023-11-14T12:03:46Z",
      "updated": "2023-11-14T12:03:46Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08104v1",
      "landing_url": "https://arxiv.org/abs/2311.08104v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2311.10319",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10319v6",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "published": "2023-11-17T04:04:29Z"
    },
    "metadata": {
      "arxiv_id": "2311.10319",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "authors": [
        "Pranav Singh",
        "Raviteja Chukkapalli",
        "Shravan Chaudhari",
        "Luoyao Chen",
        "Mei Chen",
        "Jinqian Pan",
        "Craig Smuda",
        "Jacopo Cirrone"
      ],
      "published": "2023-11-17T04:04:29Z",
      "updated": "2024-05-17T17:42:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10319v6",
      "landing_url": "https://arxiv.org/abs/2311.10319v6",
      "doi": "https://doi.org/10.1038/s41598-024-61822-9"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2311.10525",
    "anchor": "discrete speech tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10525v1",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "published": "2023-11-17T13:45:31Z"
    },
    "metadata": {
      "arxiv_id": "2311.10525",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "authors": [
        "Junliang Wang",
        "Qinghua Zhang",
        "Guanhua Zhu",
        "Guoxi Sun"
      ],
      "published": "2023-11-17T13:45:31Z",
      "updated": "2023-11-17T13:45:31Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10525v1",
      "landing_url": "https://arxiv.org/abs/2311.10525v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.10525"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2311.10959",
    "anchor": "discrete speech tokens",
    "search_term": "reconstruction",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10959v3",
      "title": "Structure-Aware Sparse-View X-ray 3D Reconstruction",
      "summary": "X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data are released at https://github.com/caiyuanhao1998/SAX-NeRF",
      "published": "2023-11-18T03:39:02Z"
    },
    "metadata": {
      "arxiv_id": "2311.10959",
      "title": "Structure-Aware Sparse-View X-ray 3D Reconstruction",
      "summary": "X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data are released at https://github.com/caiyuanhao1998/SAX-NeRF",
      "authors": [
        "Yuanhao Cai",
        "Jiahao Wang",
        "Alan Yuille",
        "Zongwei Zhou",
        "Angtian Wang"
      ],
      "published": "2023-11-18T03:39:02Z",
      "updated": "2024-03-23T17:36:19Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10959v3",
      "landing_url": "https://arxiv.org/abs/2311.10959v3",
      "doi": "https://doi.org/10.48550/arXiv.2311.10959"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      }
    ]
  },
  {
    "arxiv_id": "2311.15435",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.15435v1",
      "title": "Functional Diffusion",
      "summary": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",
      "published": "2023-11-26T21:35:34Z"
    },
    "metadata": {
      "arxiv_id": "2311.15435",
      "title": "Functional Diffusion",
      "summary": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",
      "authors": [
        "Biao Zhang",
        "Peter Wonka"
      ],
      "published": "2023-11-26T21:35:34Z",
      "updated": "2023-11-26T21:35:34Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15435v1",
      "landing_url": "https://arxiv.org/abs/2311.15435v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.15435"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2311.16361",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.16361v2",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "published": "2023-11-27T22:52:45Z"
    },
    "metadata": {
      "arxiv_id": "2311.16361",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "authors": [
        "Weicheng Zhu",
        "Sheng Liu",
        "Carlos Fernandez-Granda",
        "Narges Razavian"
      ],
      "published": "2023-11-27T22:52:45Z",
      "updated": "2023-11-29T23:19:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16361v2",
      "landing_url": "https://arxiv.org/abs/2311.16361v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.16361"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2312.02116",
    "anchor": "discrete speech tokens",
    "search_term": "vq-gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.02116v4",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "published": "2023-12-04T18:48:02Z"
    },
    "metadata": {
      "arxiv_id": "2312.02116",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "authors": [
        "Michael Tschannen",
        "Cian Eastwood",
        "Fabian Mentzer"
      ],
      "published": "2023-12-04T18:48:02Z",
      "updated": "2024-07-17T16:32:09Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02116v4",
      "landing_url": "https://arxiv.org/abs/2312.02116v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.02116"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      }
    ]
  },
  {
    "arxiv_id": "2312.04410",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.04410v1",
      "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
      "summary": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
      "published": "2023-12-07T16:26:23Z"
    },
    "metadata": {
      "arxiv_id": "2312.04410",
      "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
      "summary": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
      "authors": [
        "Jiayi Guo",
        "Xingqian Xu",
        "Yifan Pu",
        "Zanlin Ni",
        "Chaofei Wang",
        "Manushree Vasu",
        "Shiji Song",
        "Gao Huang",
        "Humphrey Shi"
      ],
      "published": "2023-12-07T16:26:23Z",
      "updated": "2023-12-07T16:26:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04410v1",
      "landing_url": "https://arxiv.org/abs/2312.04410v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.04410"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2312.05415",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.05415v1",
      "title": "An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis",
      "summary": "We propose a new model architecture specifically suited for text-to-speech (TTS) models. We combine WavLM, a pre-trained self-supervised learning (SSL) speech model, and the BEST-RQ vector quantization framework. We assess the extent to which the more task-agnostic WavLM, coupled with the superior suitability of the simplistic BEST-RQ framework for a wider array of downstream tasks, yields favorable outcomes. Experiments on the LibriSpeech dataset with SUPERB benchmarking assert that the proposed model significantly underperforms. We speculate the underlying reason for this performance is related to the difference between featurizing raw audio waveforms and spectrograms with a quantizer. We discuss the limitations of this approach to better guide future advancements in TTS.",
      "published": "2023-12-08T23:59:25Z"
    },
    "metadata": {
      "arxiv_id": "2312.05415",
      "title": "An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis",
      "summary": "We propose a new model architecture specifically suited for text-to-speech (TTS) models. We combine WavLM, a pre-trained self-supervised learning (SSL) speech model, and the BEST-RQ vector quantization framework. We assess the extent to which the more task-agnostic WavLM, coupled with the superior suitability of the simplistic BEST-RQ framework for a wider array of downstream tasks, yields favorable outcomes. Experiments on the LibriSpeech dataset with SUPERB benchmarking assert that the proposed model significantly underperforms. We speculate the underlying reason for this performance is related to the difference between featurizing raw audio waveforms and spectrograms with a quantizer. We discuss the limitations of this approach to better guide future advancements in TTS.",
      "authors": [
        "Via Nielson",
        "Steven Hillis"
      ],
      "published": "2023-12-08T23:59:25Z",
      "updated": "2023-12-08T23:59:25Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.05415v1",
      "landing_url": "https://arxiv.org/abs/2312.05415v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.05415"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2312.06381",
    "anchor": "acoustic tokens",
    "search_term": "external quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.06381v1",
      "title": "Instability and quantization in quantum hydrodynamics",
      "summary": "In this short paper, we show how a quantum nonlocal effect of far-apart wavepackets in the Schrodinger picture of wavefunctions is replaced by a local instability problem when considering the hydrodynamical formulation of quantum mechanics, known as the Madelung picture. As a second result, we show how the Madelung equations describe quantized energies without any external quantization conditions.",
      "published": "2023-12-11T13:44:48Z"
    },
    "metadata": {
      "arxiv_id": "2312.06381",
      "title": "Instability and quantization in quantum hydrodynamics",
      "summary": "In this short paper, we show how a quantum nonlocal effect of far-apart wavepackets in the Schrodinger picture of wavefunctions is replaced by a local instability problem when considering the hydrodynamical formulation of quantum mechanics, known as the Madelung picture. As a second result, we show how the Madelung equations describe quantized energies without any external quantization conditions.",
      "authors": [
        "Yakir Aharonov",
        "Tomer Shushi"
      ],
      "published": "2023-12-11T13:44:48Z",
      "updated": "2023-12-11T13:44:48Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.06381v1",
      "landing_url": "https://arxiv.org/abs/2312.06381v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.06381"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      }
    ]
  },
  {
    "arxiv_id": "2312.07780",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.07780v2",
      "title": "Bitrate Ladder Construction using Visual Information Fidelity",
      "summary": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
      "published": "2023-12-12T22:48:50Z"
    },
    "metadata": {
      "arxiv_id": "2312.07780",
      "title": "Bitrate Ladder Construction using Visual Information Fidelity",
      "summary": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
      "authors": [
        "Krishna Srikar Durbha",
        "Hassene Tmar",
        "Cosmin Stejerean",
        "Ioannis Katsavounidis",
        "Alan C. Bovik"
      ],
      "published": "2023-12-12T22:48:50Z",
      "updated": "2024-02-29T03:44:59Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07780v2",
      "landing_url": "https://arxiv.org/abs/2312.07780v2",
      "doi": "https://doi.org/10.1109/PCS60826.2024.10566405"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2312.08089",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08089v2",
      "title": "Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier",
      "summary": "With the rapid development of speech synthesis and voice conversion technologies, Audio Deepfake has become a serious threat to the Automatic Speaker Verification (ASV) system. Numerous countermeasures are proposed to detect this type of attack. In this paper, we report our efforts to combine the self-supervised WavLM model and Multi-Fusion Attentive classifier for audio deepfake detection. Our method exploits the WavLM model to extract features that are more conducive to spoofing detection for the first time. Then, we propose a novel Multi-Fusion Attentive (MFA) classifier based on the Attentive Statistics Pooling (ASP) layer. The MFA captures the complementary information of audio features at both time and layer levels. Experiments demonstrate that our methods achieve state-of-the-art results on the ASVspoof 2021 DF set and provide competitive results on the ASVspoof 2019 and 2021 LA set.",
      "published": "2023-12-13T12:09:15Z"
    },
    "metadata": {
      "arxiv_id": "2312.08089",
      "title": "Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier",
      "summary": "With the rapid development of speech synthesis and voice conversion technologies, Audio Deepfake has become a serious threat to the Automatic Speaker Verification (ASV) system. Numerous countermeasures are proposed to detect this type of attack. In this paper, we report our efforts to combine the self-supervised WavLM model and Multi-Fusion Attentive classifier for audio deepfake detection. Our method exploits the WavLM model to extract features that are more conducive to spoofing detection for the first time. Then, we propose a novel Multi-Fusion Attentive (MFA) classifier based on the Attentive Statistics Pooling (ASP) layer. The MFA captures the complementary information of audio features at both time and layer levels. Experiments demonstrate that our methods achieve state-of-the-art results on the ASVspoof 2021 DF set and provide competitive results on the ASVspoof 2019 and 2021 LA set.",
      "authors": [
        "Yinlin Guo",
        "Haofan Huang",
        "Xi Chen",
        "He Zhao",
        "Yuehai Wang"
      ],
      "published": "2023-12-13T12:09:15Z",
      "updated": "2024-01-10T03:01:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08089v2",
      "landing_url": "https://arxiv.org/abs/2312.08089v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08089"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2312.08676",
    "anchor": "semantic tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.08676v2",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "published": "2023-12-14T06:26:55Z"
    },
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2312.14134",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.14134v3",
      "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
      "summary": "Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.",
      "published": "2023-12-21T18:55:05Z"
    },
    "metadata": {
      "arxiv_id": "2312.14134",
      "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
      "summary": "Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.",
      "authors": [
        "Tao Huang",
        "Guangqi Jiang",
        "Yanjie Ze",
        "Huazhe Xu"
      ],
      "published": "2023-12-21T18:55:05Z",
      "updated": "2024-08-09T03:06:42Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14134v3",
      "landing_url": "https://arxiv.org/abs/2312.14134v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.14134"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2401.01498",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01498v1",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "published": "2024-01-03T02:03:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2401.03195",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03195v2",
      "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
      "summary": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
      "published": "2024-01-06T11:37:20Z"
    },
    "metadata": {
      "arxiv_id": "2401.03195",
      "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
      "summary": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
      "authors": [
        "Ali Falahati",
        "Mohammad Karim Safavi",
        "Ardavan Elahi",
        "Farhad Pakdaman",
        "Moncef Gabbouj"
      ],
      "published": "2024-01-06T11:37:20Z",
      "updated": "2024-03-14T03:59:19Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03195v2",
      "landing_url": "https://arxiv.org/abs/2401.03195v2",
      "doi": "https://doi.org/10.1109/MVIP62238.2024.10491154"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2401.04405",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04405v1",
      "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
      "summary": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
      "published": "2024-01-09T08:01:47Z"
    },
    "metadata": {
      "arxiv_id": "2401.04405",
      "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
      "summary": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
      "authors": [
        "Jinhai Yang",
        "Mengxi Guo",
        "Shijie Zhao",
        "Junlin Li",
        "Li Zhang"
      ],
      "published": "2024-01-09T08:01:47Z",
      "updated": "2024-01-09T08:01:47Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04405v1",
      "landing_url": "https://arxiv.org/abs/2401.04405v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04405"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2401.04964",
    "anchor": "speech representation",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04964v2",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "published": "2024-01-10T07:11:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.04964",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "authors": [
        "Bo Wang",
        "Xiran Xu",
        "Zechen Zhang",
        "Haolin Zhu",
        "YuJie Yan",
        "Xihong Wu",
        "Jing Chen"
      ],
      "published": "2024-01-10T07:11:36Z",
      "updated": "2024-02-01T04:51:22Z",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04964v2",
      "landing_url": "https://arxiv.org/abs/2401.04964v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04964"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2401.05779",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.05779v4",
      "title": "Erasing Undesirable Influence in Diffusion Models",
      "summary": "Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.",
      "published": "2024-01-11T09:30:36Z"
    },
    "metadata": {
      "arxiv_id": "2401.05779",
      "title": "Erasing Undesirable Influence in Diffusion Models",
      "summary": "Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.",
      "authors": [
        "Jing Wu",
        "Trung Le",
        "Munawar Hayat",
        "Mehrtash Harandi"
      ],
      "published": "2024-01-11T09:30:36Z",
      "updated": "2024-11-20T09:31:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05779v4",
      "landing_url": "https://arxiv.org/abs/2401.05779v4",
      "doi": "https://doi.org/10.48550/arXiv.2401.05779"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2401.08833",
    "anchor": "speech representation",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.08833v1",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "published": "2024-01-16T21:13:22Z"
    },
    "metadata": {
      "arxiv_id": "2401.08833",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "authors": [
        "Alexander H. Liu",
        "Sung-Lin Yeh",
        "James Glass"
      ],
      "published": "2024-01-16T21:13:22Z",
      "updated": "2024-01-16T21:13:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08833v1",
      "landing_url": "https://arxiv.org/abs/2401.08833v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.08833"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      }
    ]
  },
  {
    "arxiv_id": "2402.01061",
    "anchor": "discrete speech tokens",
    "search_term": "k-means clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01061v2",
      "title": "On the power of linear programming for K-means clustering",
      "summary": "In [SIAM J. Optim., 2022], the authors introduced a new linear programming (LP) relaxation for K-means clustering. In this paper, we further investigate both theoretical and computational properties of this relaxation. As evident from our numerical experiments with both synthetic real-world data sets, the proposed LP relaxation is almost always tight; i.e. its optimal solution is feasible for the original nonconvex problem. To better understand this unexpected behaviour, on the theoretical side, we focus on K-means clustering with two clusters, and we obtain sufficient conditions under which the LP relaxation is tight. We further analyze the sufficient conditions when the input is generated according to a popular stochastic model and obtain recovery guarantees for the LP relaxation. We conclude our theoretical study by constructing a family of inputs for which the LP relaxation is never tight. Denoting by $n$ the number of data points to be clustered, the LP relaxation contains $Ω(n^3)$ inequalities making it impractical for large data sets. To address the scalability issue, by building upon a cutting-plane algorithm together with the GPU implementation of PDLP, a first-order method LP solver, we develop an efficient algorithm that solves the proposed LP and hence the K-means clustering problem, for up to $n \\leq 4000$ data points.",
      "published": "2024-02-01T23:19:48Z"
    },
    "metadata": {
      "arxiv_id": "2402.01061",
      "title": "On the power of linear programming for K-means clustering",
      "summary": "In [SIAM J. Optim., 2022], the authors introduced a new linear programming (LP) relaxation for K-means clustering. In this paper, we further investigate both theoretical and computational properties of this relaxation. As evident from our numerical experiments with both synthetic real-world data sets, the proposed LP relaxation is almost always tight; i.e. its optimal solution is feasible for the original nonconvex problem. To better understand this unexpected behaviour, on the theoretical side, we focus on K-means clustering with two clusters, and we obtain sufficient conditions under which the LP relaxation is tight. We further analyze the sufficient conditions when the input is generated according to a popular stochastic model and obtain recovery guarantees for the LP relaxation. We conclude our theoretical study by constructing a family of inputs for which the LP relaxation is never tight. Denoting by $n$ the number of data points to be clustered, the LP relaxation contains $Ω(n^3)$ inequalities making it impractical for large data sets. To address the scalability issue, by building upon a cutting-plane algorithm together with the GPU implementation of PDLP, a first-order method LP solver, we develop an efficient algorithm that solves the proposed LP and hence the K-means clustering problem, for up to $n \\leq 4000$ data points.",
      "authors": [
        "Antonio De Rosa",
        "Aida Khajavirad",
        "Yakun Wang"
      ],
      "published": "2024-02-01T23:19:48Z",
      "updated": "2024-08-16T01:31:44Z",
      "categories": [
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01061v2",
      "landing_url": "https://arxiv.org/abs/2402.01061v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.01061"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means clustering"
      }
    ]
  },
  {
    "arxiv_id": "2402.01204",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01204v4",
      "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
      "summary": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
      "published": "2024-02-02T08:17:41Z"
    },
    "metadata": {
      "arxiv_id": "2402.01204",
      "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
      "summary": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
      "authors": [
        "Wei-Yao Wang",
        "Wei-Wei Du",
        "Derek Xu",
        "Wei Wang",
        "Wen-Chih Peng"
      ],
      "published": "2024-02-02T08:17:41Z",
      "updated": "2024-09-10T07:02:47Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01204v4",
      "landing_url": "https://arxiv.org/abs/2402.01204v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.01204"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2402.01399",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01399v3",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "published": "2024-02-02T13:31:17Z"
    },
    "metadata": {
      "arxiv_id": "2402.01399",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "authors": [
        "Alice Bizeul",
        "Bernhard Schölkopf",
        "Carl Allen"
      ],
      "published": "2024-02-02T13:31:17Z",
      "updated": "2024-10-15T13:16:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01399v3",
      "landing_url": "https://arxiv.org/abs/2402.01399v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.01399"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2402.03513",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03513v1",
      "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
      "summary": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
      "published": "2024-02-05T21:01:01Z"
    },
    "metadata": {
      "arxiv_id": "2402.03513",
      "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
      "summary": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
      "authors": [
        "Vignesh V Menon",
        "Prajit T Rajendran",
        "Amritha Premkumar",
        "Benjamin Bross",
        "Detlev Marpe"
      ],
      "published": "2024-02-05T21:01:01Z",
      "updated": "2024-02-05T21:01:01Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03513v1",
      "landing_url": "https://arxiv.org/abs/2402.03513v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03513"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2402.03687",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03687v3",
      "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
      "summary": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard.",
      "published": "2024-02-06T04:17:44Z"
    },
    "metadata": {
      "arxiv_id": "2402.03687",
      "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
      "summary": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard.",
      "authors": [
        "Lingxiao Zhao",
        "Xueying Ding",
        "Leman Akoglu"
      ],
      "published": "2024-02-06T04:17:44Z",
      "updated": "2024-12-02T18:57:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03687v3",
      "landing_url": "https://arxiv.org/abs/2402.03687v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.03687"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2402.03701",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03701v2",
      "title": "Unified Discrete Diffusion for Categorical Data",
      "summary": "Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.",
      "published": "2024-02-06T04:42:36Z"
    },
    "metadata": {
      "arxiv_id": "2402.03701",
      "title": "Unified Discrete Diffusion for Categorical Data",
      "summary": "Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.",
      "authors": [
        "Lingxiao Zhao",
        "Xueying Ding",
        "Lijun Yu",
        "Leman Akoglu"
      ],
      "published": "2024-02-06T04:42:36Z",
      "updated": "2024-08-12T16:22:38Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03701v2",
      "landing_url": "https://arxiv.org/abs/2402.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03701"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2402.05774",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.05774v1",
      "title": "Stable Autonomous Flow Matching",
      "summary": "In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.",
      "published": "2024-02-08T16:01:24Z"
    },
    "metadata": {
      "arxiv_id": "2402.05774",
      "title": "Stable Autonomous Flow Matching",
      "summary": "In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.",
      "authors": [
        "Christopher Iliffe Sprague",
        "Arne Elofsson",
        "Hossein Azizpour"
      ],
      "published": "2024-02-08T16:01:24Z",
      "updated": "2024-02-08T16:01:24Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05774v1",
      "landing_url": "https://arxiv.org/abs/2402.05774v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.05774"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2402.13071",
    "anchor": "discrete speech tokens",
    "search_term": "codec-superb",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.13071v3",
      "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
      "summary": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.",
      "published": "2024-02-20T15:13:38Z"
    },
    "metadata": {
      "arxiv_id": "2402.13071",
      "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
      "summary": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.",
      "authors": [
        "Haibin Wu",
        "Ho-Lam Chung",
        "Yi-Cheng Lin",
        "Yuan-Kuei Wu",
        "Xuanjun Chen",
        "Yu-Chi Pai",
        "Hsiu-Hsuan Wang",
        "Kai-Wei Chang",
        "Alexander H. Liu",
        "Hung-yi Lee"
      ],
      "published": "2024-02-20T15:13:38Z",
      "updated": "2024-09-18T12:02:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13071v3",
      "landing_url": "https://arxiv.org/abs/2402.13071v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.13071"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      }
    ]
  },
  {
    "arxiv_id": "2403.08206",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.08206v2",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "published": "2024-03-13T03:03:15Z"
    },
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2403.10071",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.10071v1",
      "title": "Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling",
      "summary": "Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",
      "published": "2024-03-15T07:24:13Z"
    },
    "metadata": {
      "arxiv_id": "2403.10071",
      "title": "Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling",
      "summary": "Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",
      "authors": [
        "Baoquan Zhang",
        "Huaibin Wang",
        "Luo Chuyao",
        "Xutao Li",
        "Liang Guotao",
        "Yunming Ye",
        "Xiaochen Qi",
        "Yao He"
      ],
      "published": "2024-03-15T07:24:13Z",
      "updated": "2024-03-15T07:24:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.10071v1",
      "landing_url": "https://arxiv.org/abs/2403.10071v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.10071"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2403.11130",
    "anchor": "discrete speech tokens",
    "search_term": "vocabulary size",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.11130v2",
      "title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models",
      "summary": "This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language.",
      "published": "2024-03-17T07:44:44Z"
    },
    "metadata": {
      "arxiv_id": "2403.11130",
      "title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models",
      "summary": "This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language.",
      "authors": [
        "Mohamed Taher Alrefaie",
        "Nour Eldin Morsy",
        "Nada Samir"
      ],
      "published": "2024-03-17T07:44:44Z",
      "updated": "2024-09-20T16:48:58Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11130v2",
      "landing_url": "https://arxiv.org/abs/2403.11130v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.11130"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      }
    ]
  },
  {
    "arxiv_id": "2403.11984",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.11984v1",
      "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
      "summary": "Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.",
      "published": "2024-03-18T17:21:35Z"
    },
    "metadata": {
      "arxiv_id": "2403.11984",
      "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
      "summary": "Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.",
      "authors": [
        "Andrew Katz",
        "Mitchell Gerhardt",
        "Michelle Soledad"
      ],
      "published": "2024-03-18T17:21:35Z",
      "updated": "2024-03-18T17:21:35Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11984v1",
      "landing_url": "https://arxiv.org/abs/2403.11984v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2403.14562",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.14562v2",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "published": "2024-03-21T17:06:17Z"
    },
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2403.19461",
    "anchor": "acoustic tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.19461v2",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "published": "2024-03-28T14:32:57Z"
    },
    "metadata": {
      "arxiv_id": "2403.19461",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "authors": [
        "Simon Idoko",
        "Basant Sharma",
        "Arun Kumar Singh"
      ],
      "published": "2024-03-28T14:32:57Z",
      "updated": "2024-04-25T14:29:08Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19461v2",
      "landing_url": "https://arxiv.org/abs/2403.19461v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.19461"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2404.04940",
    "anchor": "discrete speech tokens",
    "search_term": "k-means clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04940v2",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "published": "2024-04-07T12:25:03Z"
    },
    "metadata": {
      "arxiv_id": "2404.04940",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "authors": [
        "Yichen Bao",
        "Han Lu",
        "Quanxue Gao"
      ],
      "published": "2024-04-07T12:25:03Z",
      "updated": "2024-11-07T08:59:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04940v2",
      "landing_url": "https://arxiv.org/abs/2404.04940v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means clustering"
      }
    ]
  },
  {
    "arxiv_id": "2404.06079",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.06079v2",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "published": "2024-04-09T07:37:41Z"
    },
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2404.14774",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.14774v2",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "published": "2024-04-23T06:29:48Z"
    },
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2404.16619",
    "anchor": "discrete speech tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16619v1",
      "title": "The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge",
      "summary": "This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.",
      "published": "2024-04-25T14:02:25Z"
    },
    "metadata": {
      "arxiv_id": "2404.16619",
      "title": "The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge",
      "summary": "This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.",
      "authors": [
        "Yixuan Zhou",
        "Shuoyi Zhou",
        "Shun Lei",
        "Zhiyong Wu",
        "Menglin Wu"
      ],
      "published": "2024-04-25T14:02:25Z",
      "updated": "2024-04-25T14:02:25Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16619v1",
      "landing_url": "https://arxiv.org/abs/2404.16619v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16619"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2404.16743",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.16743v2",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "published": "2024-04-25T16:57:05Z"
    },
    "metadata": {
      "arxiv_id": "2404.16743",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "authors": [
        "Chanho Park",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2024-04-25T16:57:05Z",
      "updated": "2024-04-26T11:11:02Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16743v2",
      "landing_url": "https://arxiv.org/abs/2404.16743v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.16743"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2405.00930",
    "anchor": "speech representation",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.00930v2",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "published": "2024-05-02T01:11:15Z"
    },
    "metadata": {
      "arxiv_id": "2405.00930",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "authors": [
        "Pengcheng Li",
        "Jianzong Wang",
        "Xulong Zhang",
        "Yong Zhang",
        "Jing Xiao",
        "Ning Cheng"
      ],
      "published": "2024-05-02T01:11:15Z",
      "updated": "2024-11-24T09:30:29Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00930v2",
      "landing_url": "https://arxiv.org/abs/2405.00930v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.00930"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2405.02171",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02171v1",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "published": "2024-05-03T15:20:30Z"
    },
    "metadata": {
      "arxiv_id": "2405.02171",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "authors": [
        "Zhilu Zhang",
        "Ruohao Wang",
        "Hongzhi Zhang",
        "Wangmeng Zuo"
      ],
      "published": "2024-05-03T15:20:30Z",
      "updated": "2024-05-03T15:20:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02171v1",
      "landing_url": "https://arxiv.org/abs/2405.02171v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02171"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "speech representation",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2405.02330",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.02330v1",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "published": "2024-04-25T13:49:50Z"
    },
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2405.03083",
    "anchor": "discrete speech tokens",
    "search_term": "k-means clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03083v3",
      "title": "Causal K-Means Clustering",
      "summary": "Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.",
      "published": "2024-05-05T23:59:51Z"
    },
    "metadata": {
      "arxiv_id": "2405.03083",
      "title": "Causal K-Means Clustering",
      "summary": "Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.",
      "authors": [
        "Kwangho Kim",
        "Jisu Kim",
        "Edward H. Kennedy"
      ],
      "published": "2024-05-05T23:59:51Z",
      "updated": "2025-02-10T05:14:12Z",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03083v3",
      "landing_url": "https://arxiv.org/abs/2405.03083v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.03083"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means clustering"
      }
    ]
  },
  {
    "arxiv_id": "2405.03110",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03110v1",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "published": "2024-05-06T02:06:26Z"
    },
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2405.04485",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.04485v1",
      "title": "Adapting WavLM for Speech Emotion Recognition",
      "summary": "Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.",
      "published": "2024-05-07T16:53:42Z"
    },
    "metadata": {
      "arxiv_id": "2405.04485",
      "title": "Adapting WavLM for Speech Emotion Recognition",
      "summary": "Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.",
      "authors": [
        "Daria Diatlova",
        "Anton Udalov",
        "Vitalii Shutov",
        "Egor Spirin"
      ],
      "published": "2024-05-07T16:53:42Z",
      "updated": "2024-05-07T16:53:42Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04485v1",
      "landing_url": "https://arxiv.org/abs/2405.04485v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.04485"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2405.04752",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.04752v2",
      "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
      "summary": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
      "published": "2024-05-08T01:40:13Z"
    },
    "metadata": {
      "arxiv_id": "2405.04752",
      "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
      "summary": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
      "authors": [
        "Sunghwan Ahn",
        "Beom Jun Woo",
        "Min Hyun Han",
        "Chanyeong Moon",
        "Nam Soo Kim"
      ],
      "published": "2024-05-08T01:40:13Z",
      "updated": "2024-09-24T07:40:54Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04752v2",
      "landing_url": "https://arxiv.org/abs/2405.04752v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.04752"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.08429",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.08429v1",
      "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
      "summary": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
      "published": "2024-05-14T08:45:34Z"
    },
    "metadata": {
      "arxiv_id": "2405.08429",
      "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
      "summary": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
      "authors": [
        "Martín Bayón-Gutiérrez",
        "María Teresa García-Ordás",
        "Héctor Alaiz Moretón",
        "Jose Aveleira-Mata",
        "Sergio Rubio Martín",
        "José Alberto Benítez-Andrades"
      ],
      "published": "2024-05-14T08:45:34Z",
      "updated": "2024-05-14T08:45:34Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08429v1",
      "landing_url": "https://arxiv.org/abs/2405.08429v1",
      "doi": "https://doi.org/10.1093/jigpal/jzae048"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2405.09768",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09768v1",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "published": "2024-05-16T02:18:41Z"
    },
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2405.14206",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.14206v2",
      "title": "LG-VQ: Language-Guided Codebook Learning",
      "summary": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
      "published": "2024-05-23T06:04:40Z"
    },
    "metadata": {
      "arxiv_id": "2405.14206",
      "title": "LG-VQ: Language-Guided Codebook Learning",
      "summary": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
      "authors": [
        "Guotao Liang",
        "Baoquan Zhang",
        "Yaowei Wang",
        "Xutao Li",
        "Yunming Ye",
        "Huaibin Wang",
        "Chuyao Luo",
        "Kola Ye",
        "linfeng Luo"
      ],
      "published": "2024-05-23T06:04:40Z",
      "updated": "2024-10-09T04:30:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.14206v2",
      "landing_url": "https://arxiv.org/abs/2405.14206v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.14206"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2405.16136",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16136v1",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "published": "2024-05-25T09:10:12Z"
    },
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2405.16577",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16577v1",
      "title": "Reflected Flow Matching",
      "summary": "Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.",
      "published": "2024-05-26T14:09:43Z"
    },
    "metadata": {
      "arxiv_id": "2405.16577",
      "title": "Reflected Flow Matching",
      "summary": "Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.",
      "authors": [
        "Tianyu Xie",
        "Yu Zhu",
        "Longlin Yu",
        "Tong Yang",
        "Ziheng Cheng",
        "Shiyue Zhang",
        "Xiangyu Zhang",
        "Cheng Zhang"
      ],
      "published": "2024-05-26T14:09:43Z",
      "updated": "2024-05-26T14:09:43Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16577v1",
      "landing_url": "https://arxiv.org/abs/2405.16577v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16577"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2405.20452",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.20452v1",
      "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
      "summary": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
      "published": "2024-05-30T19:58:01Z"
    },
    "metadata": {
      "arxiv_id": "2405.20452",
      "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
      "summary": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
      "authors": [
        "Jorge F. Silva",
        "Victor Faraggi",
        "Camilo Ramirez",
        "Alvaro Egana",
        "Eduardo Pavez"
      ],
      "published": "2024-05-30T19:58:01Z",
      "updated": "2024-05-30T19:58:01Z",
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20452v1",
      "landing_url": "https://arxiv.org/abs/2405.20452v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20452"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.00976",
    "anchor": "acoustic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.00976v2",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "published": "2024-06-03T04:16:30Z"
    },
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2406.02563",
    "anchor": "discrete speech tokens",
    "search_term": "vocabulary size",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02563v1",
      "title": "A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system",
      "summary": "Unlike hybrid speech recognition systems where the use of tokens was restricted to phones, biphones or triphones the choice of tokens in the end-to-end ASR systems is derived from the text corpus of the training data. The use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece is popular in identifying the tokens that are used in the overall training process of the speech recognition system. Popular toolkits, like ESPNet use a pre-defined vocabulary size (number of tokens) for these tokenization algorithms, but there is no discussion on how vocabulary size was derived. In this paper, we build a cost function, assuming the tokenization process to be a black-box to enable choosing the number of tokens which might most benefit building an end-to-end ASR. We show through experiments on LibriSpeech 100 hour set that the performance of an end-to-end ASR system improves when the number of tokens are chosen carefully.",
      "published": "2024-04-29T12:16:21Z"
    },
    "metadata": {
      "arxiv_id": "2406.02563",
      "title": "A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system",
      "summary": "Unlike hybrid speech recognition systems where the use of tokens was restricted to phones, biphones or triphones the choice of tokens in the end-to-end ASR systems is derived from the text corpus of the training data. The use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece is popular in identifying the tokens that are used in the overall training process of the speech recognition system. Popular toolkits, like ESPNet use a pre-defined vocabulary size (number of tokens) for these tokenization algorithms, but there is no discussion on how vocabulary size was derived. In this paper, we build a cost function, assuming the tokenization process to be a black-box to enable choosing the number of tokens which might most benefit building an end-to-end ASR. We show through experiments on LibriSpeech 100 hour set that the performance of an end-to-end ASR system improves when the number of tokens are chosen carefully.",
      "authors": [
        "Sunil Kumar Kopparapu",
        "Ashish Panda"
      ],
      "published": "2024-04-29T12:16:21Z",
      "updated": "2024-04-29T12:16:21Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02563v1",
      "landing_url": "https://arxiv.org/abs/2406.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02563"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      }
    ]
  },
  {
    "arxiv_id": "2406.02770",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02770v1",
      "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
      "summary": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
      "published": "2024-06-04T20:37:30Z"
    },
    "metadata": {
      "arxiv_id": "2406.02770",
      "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
      "summary": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
      "authors": [
        "Kathrin Donandt",
        "Karim Böttger",
        "Dirk Söffker"
      ],
      "published": "2024-06-04T20:37:30Z",
      "updated": "2024-06-04T20:37:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02770v1",
      "landing_url": "https://arxiv.org/abs/2406.02770v1",
      "doi": "https://doi.org/10.1109/ITSC55140.2022.9922148"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2406.03460",
    "anchor": "acoustic tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03460v1",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "published": "2024-06-05T17:07:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2406.03706",
    "anchor": "acoustic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.03706v1",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "published": "2024-06-06T03:06:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2406.04262",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04262v2",
      "title": "Near-field Beam Training with Sparse DFT Codebook",
      "summary": "Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime.",
      "published": "2024-06-06T17:08:28Z"
    },
    "metadata": {
      "arxiv_id": "2406.04262",
      "title": "Near-field Beam Training with Sparse DFT Codebook",
      "summary": "Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime.",
      "authors": [
        "Cong Zhou",
        "Chenyu Wu",
        "Changsheng You",
        "Shuo Shi"
      ],
      "published": "2024-06-06T17:08:28Z",
      "updated": "2024-06-18T12:26:09Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04262v2",
      "landing_url": "https://arxiv.org/abs/2406.04262v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04262"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2406.04633",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04633v1",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "published": "2024-06-07T04:34:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.04843",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04843v2",
      "title": "Variational Flow Matching for Graph Generation",
      "summary": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.",
      "published": "2024-06-07T11:16:17Z"
    },
    "metadata": {
      "arxiv_id": "2406.04843",
      "title": "Variational Flow Matching for Graph Generation",
      "summary": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.",
      "authors": [
        "Floor Eijkelboom",
        "Grigory Bartosh",
        "Christian Andersson Naesseth",
        "Max Welling",
        "Jan-Willem van de Meent"
      ],
      "published": "2024-06-07T11:16:17Z",
      "updated": "2025-08-15T21:12:02Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04843v2",
      "landing_url": "https://arxiv.org/abs/2406.04843v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04843"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2406.05661",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05661v4",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "published": "2024-06-09T06:30:28Z"
    },
    "metadata": {
      "arxiv_id": "2406.05661",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2024-06-09T06:30:28Z",
      "updated": "2025-02-18T10:07:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05661v4",
      "landing_url": "https://arxiv.org/abs/2406.05661v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.05661"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2406.06371",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.06371v5",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "published": "2024-06-10T15:32:42Z"
    },
    "metadata": {
      "arxiv_id": "2406.06371",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "authors": [
        "Marcely Zanon Boito",
        "Vivek Iyer",
        "Nikolaos Lagos",
        "Laurent Besacier",
        "Ioan Calapodescu"
      ],
      "published": "2024-06-10T15:32:42Z",
      "updated": "2024-11-21T10:45:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06371v5",
      "landing_url": "https://arxiv.org/abs/2406.06371v5",
      "doi": "https://doi.org/10.48550/arXiv.2406.06371"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2406.08266",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08266v2",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "published": "2024-06-12T14:34:41Z"
    },
    "metadata": {
      "arxiv_id": "2406.08266",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "authors": [
        "Hengyu Li",
        "Kangdi Mei",
        "Zhaoci Liu",
        "Yang Ai",
        "Liping Chen",
        "Jie Zhang",
        "Zhenhua Ling"
      ],
      "published": "2024-06-12T14:34:41Z",
      "updated": "2024-06-13T06:26:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08266v2",
      "landing_url": "https://arxiv.org/abs/2406.08266v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08266"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2406.08353",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08353v3",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "published": "2024-06-12T15:59:25Z"
    },
    "metadata": {
      "arxiv_id": "2406.08353",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "published": "2024-06-12T15:59:25Z",
      "updated": "2025-03-23T16:45:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08353v3",
      "landing_url": "https://arxiv.org/abs/2406.08353v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.08353"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2406.08445",
    "anchor": "acoustic tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08445v1",
      "title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models",
      "summary": "Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability.",
      "published": "2024-06-12T17:37:09Z"
    },
    "metadata": {
      "arxiv_id": "2406.08445",
      "title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models",
      "summary": "Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability.",
      "authors": [
        "Chun Yin",
        "Tai-Shih Chi",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "published": "2024-06-12T17:37:09Z",
      "updated": "2024-06-12T17:37:09Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08445v1",
      "landing_url": "https://arxiv.org/abs/2406.08445v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08445"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2406.10735",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10735v1",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "published": "2024-06-15T20:43:07Z"
    },
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2406.11037",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11037v1",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "published": "2024-06-16T18:20:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2406.11837",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11837v1",
      "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
      "summary": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.",
      "published": "2024-06-17T17:59:57Z"
    },
    "metadata": {
      "arxiv_id": "2406.11837",
      "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
      "summary": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.",
      "authors": [
        "Lei Zhu",
        "Fangyun Wei",
        "Yanye Lu",
        "Dong Chen"
      ],
      "published": "2024-06-17T17:59:57Z",
      "updated": "2024-06-17T17:59:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11837v1",
      "landing_url": "https://arxiv.org/abs/2406.11837v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11837"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2406.13275",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13275v2",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "published": "2024-06-19T07:09:46Z"
    },
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2406.13431",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13431v2",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "published": "2024-06-19T10:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2406.14294",
    "anchor": "discrete speech tokens",
    "search_term": "dasb",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14294v2",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "published": "2024-06-20T13:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2406.16508",
    "anchor": "discrete speech tokens",
    "search_term": "vocabulary size",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.16508v2",
      "title": "Large Vocabulary Size Improves Large Language Models",
      "summary": "This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.",
      "published": "2024-06-24T10:27:07Z"
    },
    "metadata": {
      "arxiv_id": "2406.16508",
      "title": "Large Vocabulary Size Improves Large Language Models",
      "summary": "This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.",
      "authors": [
        "Sho Takase",
        "Ryokan Ri",
        "Shun Kiyono",
        "Takuya Kato"
      ],
      "published": "2024-06-24T10:27:07Z",
      "updated": "2025-05-28T02:07:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16508v2",
      "landing_url": "https://arxiv.org/abs/2406.16508v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.16508"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      }
    ]
  },
  {
    "arxiv_id": "2406.17310",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17310v1",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "published": "2024-06-25T06:46:47Z"
    },
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2406.17722",
    "anchor": "discrete speech tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17722v1",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "published": "2024-06-25T17:10:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.17722",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "authors": [
        "Kentaro Seki",
        "Shinnosuke Takamichi",
        "Norihiro Takamune",
        "Yuki Saito",
        "Kanami Imamura",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-06-25T17:10:39Z",
      "updated": "2024-06-25T17:10:39Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17722v1",
      "landing_url": "https://arxiv.org/abs/2406.17722v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17722"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2407.01392",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.01392v4",
      "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
      "summary": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
      "published": "2024-07-01T15:43:25Z"
    },
    "metadata": {
      "arxiv_id": "2407.01392",
      "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
      "summary": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
      "authors": [
        "Boyuan Chen",
        "Diego Marti Monso",
        "Yilun Du",
        "Max Simchowitz",
        "Russ Tedrake",
        "Vincent Sitzmann"
      ],
      "published": "2024-07-01T15:43:25Z",
      "updated": "2024-12-10T01:32:23Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01392v4",
      "landing_url": "https://arxiv.org/abs/2407.01392v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.01392"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2407.02826",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.02826v1",
      "title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech",
      "summary": "It was shown that pre-trained models with self-supervised learning (SSL) techniques are effective in various downstream speech tasks. However, most such models are trained on single-speaker speech data, limiting their effectiveness in mixture speech. This motivates us to explore pre-training on mixture speech. This work presents SA-WavLM, a novel pre-trained model for mixture speech. Specifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the representations of each speaker in the input mixture are first extracted individually and then merged before the final prediction. In this pipeline, SA-WavLM performs speaker-informed extractions with the consideration of the interactions between different speakers. Furthermore, a speaker shuffling strategy is proposed to enhance the robustness towards the speaker absence. Experiments show that SA-WavLM either matches or improves upon the state-of-the-art pre-trained models.",
      "published": "2024-07-03T06:07:42Z"
    },
    "metadata": {
      "arxiv_id": "2407.02826",
      "title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech",
      "summary": "It was shown that pre-trained models with self-supervised learning (SSL) techniques are effective in various downstream speech tasks. However, most such models are trained on single-speaker speech data, limiting their effectiveness in mixture speech. This motivates us to explore pre-training on mixture speech. This work presents SA-WavLM, a novel pre-trained model for mixture speech. Specifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the representations of each speaker in the input mixture are first extracted individually and then merged before the final prediction. In this pipeline, SA-WavLM performs speaker-informed extractions with the consideration of the interactions between different speakers. Furthermore, a speaker shuffling strategy is proposed to enhance the robustness towards the speaker absence. Experiments show that SA-WavLM either matches or improves upon the state-of-the-art pre-trained models.",
      "authors": [
        "Jingru Lin",
        "Meng Ge",
        "Junyi Ao",
        "Liqun Deng",
        "Haizhou Li"
      ],
      "published": "2024-07-03T06:07:42Z",
      "updated": "2024-07-03T06:07:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02826v1",
      "landing_url": "https://arxiv.org/abs/2407.02826v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.02826"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2407.03892",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic bpe",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03892v1",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "published": "2024-07-04T12:35:32Z"
    },
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      }
    ]
  },
  {
    "arxiv_id": "2407.05407",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.05407v2",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "published": "2024-07-07T15:16:19Z"
    },
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2407.08152",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08152v2",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "published": "2024-07-11T03:10:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2407.13623",
    "anchor": "acoustic tokens",
    "search_term": "vocabulary size",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.13623v3",
      "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
      "summary": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.",
      "published": "2024-07-18T15:58:54Z"
    },
    "metadata": {
      "arxiv_id": "2407.13623",
      "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
      "summary": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.",
      "authors": [
        "Chaofan Tao",
        "Qian Liu",
        "Longxu Dou",
        "Niklas Muennighoff",
        "Zhongwei Wan",
        "Ping Luo",
        "Min Lin",
        "Ngai Wong"
      ],
      "published": "2024-07-18T15:58:54Z",
      "updated": "2024-11-01T02:41:36Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13623v3",
      "landing_url": "https://arxiv.org/abs/2407.13623v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.13623"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      }
    ]
  },
  {
    "arxiv_id": "2407.15595",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15595v2",
      "title": "Discrete Flow Matching",
      "summary": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions:(i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($ε$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",
      "published": "2024-07-22T12:33:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.15595",
      "title": "Discrete Flow Matching",
      "summary": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions:(i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($ε$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",
      "authors": [
        "Itai Gat",
        "Tal Remez",
        "Neta Shaul",
        "Felix Kreuk",
        "Ricky T. Q. Chen",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Yaron Lipman"
      ],
      "published": "2024-07-22T12:33:27Z",
      "updated": "2024-11-05T10:02:42Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15595v2",
      "landing_url": "https://arxiv.org/abs/2407.15595v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.15595"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2407.15614",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15614v3",
      "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
      "summary": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
      "published": "2024-07-22T13:20:47Z"
    },
    "metadata": {
      "arxiv_id": "2407.15614",
      "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
      "summary": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
      "authors": [
        "Ferran Maura",
        "Miguel Casasnovas",
        "Boris Bellalta"
      ],
      "published": "2024-07-22T13:20:47Z",
      "updated": "2024-09-30T10:37:16Z",
      "categories": [
        "cs.NI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15614v3",
      "landing_url": "https://arxiv.org/abs/2407.15614v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15614"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2407.15641",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15641v1",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "summary": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "published": "2024-07-22T13:59:58Z"
    },
    "metadata": {
      "arxiv_id": "2407.15641",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "summary": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "authors": [
        "Shahan Nercessian",
        "Johannes Imort",
        "Ninon Devis",
        "Frederik Blang"
      ],
      "published": "2024-07-22T13:59:58Z",
      "updated": "2024-07-22T13:59:58Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15641v1",
      "landing_url": "https://arxiv.org/abs/2407.15641v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15641"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2407.15835",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15835v3",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "published": "2024-07-22T17:51:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2407.18505",
    "anchor": "discrete speech tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.18505v1",
      "title": "VoxSim: A perceptual voice similarity dataset",
      "summary": "This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset generalises to the out-of-domain VCC2018 dataset.",
      "published": "2024-07-26T04:27:13Z"
    },
    "metadata": {
      "arxiv_id": "2407.18505",
      "title": "VoxSim: A perceptual voice similarity dataset",
      "summary": "This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset generalises to the out-of-domain VCC2018 dataset.",
      "authors": [
        "Junseok Ahn",
        "Youkyum Kim",
        "Yeunju Choi",
        "Doyeop Kwak",
        "Ji-Hoon Kim",
        "Seongkyu Mun",
        "Joon Son Chung"
      ],
      "published": "2024-07-26T04:27:13Z",
      "updated": "2024-07-26T04:27:13Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18505v1",
      "landing_url": "https://arxiv.org/abs/2407.18505v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.18505"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2408.01932",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.01932v2",
      "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
      "summary": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
      "published": "2024-08-04T05:12:21Z"
    },
    "metadata": {
      "arxiv_id": "2408.01932",
      "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
      "summary": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
      "authors": [
        "Krishna Srikar Durbha",
        "Alan C. Bovik"
      ],
      "published": "2024-08-04T05:12:21Z",
      "updated": "2025-11-15T01:13:58Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01932v2",
      "landing_url": "https://arxiv.org/abs/2408.01932v2",
      "doi": "https://doi.org/10.1109/TIP.2025.3625750"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2408.04505",
    "anchor": "discrete speech tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.04505v1",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "published": "2024-08-08T15:03:45Z"
    },
    "metadata": {
      "arxiv_id": "2408.04505",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "authors": [
        "Nurettin Turan",
        "Michael Baur",
        "Jianqing Li",
        "Wolfgang Utschick"
      ],
      "published": "2024-08-08T15:03:45Z",
      "updated": "2024-08-08T15:03:45Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04505v1",
      "landing_url": "https://arxiv.org/abs/2408.04505v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04505"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2408.07414",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.07414v1",
      "title": "WavLM model ensemble for audio deepfake detection",
      "summary": "Audio deepfake detection has become a pivotal task over the last couple of years, as many recent speech synthesis and voice cloning systems generate highly realistic speech samples, thus enabling their use in malicious activities. In this paper we address the issue of audio deepfake detection as it was set in the ASVspoof5 challenge. First, we benchmark ten types of pretrained representations and show that the self-supervised representations stemming from the wav2vec2 and wavLM families perform best. Of the two, wavLM is better when restricting the pretraining data to LibriSpeech, as required by the challenge rules. To further improve performance, we finetune the wavLM model for the deepfake detection task. We extend the ASVspoof5 dataset with samples from other deepfake detection datasets and apply data augmentation. Our final challenge submission consists of a late fusion combination of four models and achieves an equal error rate of 6.56% and 17.08% on the two evaluation sets.",
      "published": "2024-08-14T09:43:35Z"
    },
    "metadata": {
      "arxiv_id": "2408.07414",
      "title": "WavLM model ensemble for audio deepfake detection",
      "summary": "Audio deepfake detection has become a pivotal task over the last couple of years, as many recent speech synthesis and voice cloning systems generate highly realistic speech samples, thus enabling their use in malicious activities. In this paper we address the issue of audio deepfake detection as it was set in the ASVspoof5 challenge. First, we benchmark ten types of pretrained representations and show that the self-supervised representations stemming from the wav2vec2 and wavLM families perform best. Of the two, wavLM is better when restricting the pretraining data to LibriSpeech, as required by the challenge rules. To further improve performance, we finetune the wavLM model for the deepfake detection task. We extend the ASVspoof5 dataset with samples from other deepfake detection datasets and apply data augmentation. Our final challenge submission consists of a late fusion combination of four models and achieves an equal error rate of 6.56% and 17.08% on the two evaluation sets.",
      "authors": [
        "David Combei",
        "Adriana Stan",
        "Dan Oneata",
        "Horia Cucu"
      ],
      "published": "2024-08-14T09:43:35Z",
      "updated": "2024-08-14T09:43:35Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07414v1",
      "landing_url": "https://arxiv.org/abs/2408.07414v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.07414"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2408.11512",
    "anchor": "acoustic tokens",
    "search_term": "vocabulary size",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11512v2",
      "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation",
      "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed for the general machine translation task in WMT24. IKUN and IKUN-C represent an open system and a constrained system, respectively, built on Llama-3-8b and Mistral-7B-v0.3. Both systems are designed to handle all 11 language directions using a single model. According to automatic evaluation metrics, IKUN-C achieved 6 first-place and 3 second-place finishes among all constrained systems, while IKUN secured 1 first-place and 2 second-place finishes across both open and constrained systems. These encouraging results suggest that large language models (LLMs) are nearing the level of proficiency required for effective multilingual machine translation. The systems are based on a two-stage approach: first, continuous pre-training on monolingual data in 10 languages, followed by fine-tuning on high-quality parallel data for 11 language directions. The primary difference between IKUN and IKUN-C lies in their monolingual pre-training strategy. IKUN-C is pre-trained using constrained monolingual data, whereas IKUN leverages monolingual data from the OSCAR dataset. In the second phase, both systems are fine-tuned on parallel data sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
      "published": "2024-08-21T10:44:10Z"
    },
    "metadata": {
      "arxiv_id": "2408.11512",
      "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation",
      "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed for the general machine translation task in WMT24. IKUN and IKUN-C represent an open system and a constrained system, respectively, built on Llama-3-8b and Mistral-7B-v0.3. Both systems are designed to handle all 11 language directions using a single model. According to automatic evaluation metrics, IKUN-C achieved 6 first-place and 3 second-place finishes among all constrained systems, while IKUN secured 1 first-place and 2 second-place finishes across both open and constrained systems. These encouraging results suggest that large language models (LLMs) are nearing the level of proficiency required for effective multilingual machine translation. The systems are based on a two-stage approach: first, continuous pre-training on monolingual data in 10 languages, followed by fine-tuning on high-quality parallel data for 11 language directions. The primary difference between IKUN and IKUN-C lies in their monolingual pre-training strategy. IKUN-C is pre-trained using constrained monolingual data, whereas IKUN leverages monolingual data from the OSCAR dataset. In the second phase, both systems are fine-tuned on parallel data sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
      "authors": [
        "Baohao Liao",
        "Christian Herold",
        "Shahram Khadivi",
        "Christof Monz"
      ],
      "published": "2024-08-21T10:44:10Z",
      "updated": "2024-08-29T12:25:14Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11512v2",
      "landing_url": "https://arxiv.org/abs/2408.11512v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11512"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      }
    ]
  },
  {
    "arxiv_id": "2408.15616",
    "anchor": "discrete speech tokens",
    "search_term": "word error rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.15616v1",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "published": "2024-08-28T08:14:51Z"
    },
    "metadata": {
      "arxiv_id": "2408.15616",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "authors": [
        "Korbinian Kuhn",
        "Verena Kersken",
        "Gottfried Zimmermann"
      ],
      "published": "2024-08-28T08:14:51Z",
      "updated": "2024-08-28T08:14:51Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15616v1",
      "landing_url": "https://arxiv.org/abs/2408.15616v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-32"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.00588",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00588v3",
      "title": "Diffusion Policy Policy Optimization",
      "summary": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
      "published": "2024-09-01T02:47:50Z"
    },
    "metadata": {
      "arxiv_id": "2409.00588",
      "title": "Diffusion Policy Policy Optimization",
      "summary": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
      "authors": [
        "Allen Z. Ren",
        "Justin Lidard",
        "Lars L. Ankile",
        "Anthony Simeonov",
        "Pulkit Agrawal",
        "Anirudha Majumdar",
        "Benjamin Burchfiel",
        "Hongkai Dai",
        "Max Simchowitz"
      ],
      "published": "2024-09-01T02:47:50Z",
      "updated": "2024-12-09T21:30:07Z",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00588v3",
      "landing_url": "https://arxiv.org/abs/2409.00588v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00588"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2409.00750",
    "anchor": "acoustic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.00750v3",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "published": "2024-09-01T15:26:30Z"
    },
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2409.01995",
    "anchor": "discrete speech tokens",
    "search_term": "speech token vocoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.01995v4",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "published": "2024-09-03T15:41:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      }
    ]
  },
  {
    "arxiv_id": "2409.02384",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.02384v1",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "published": "2024-09-04T02:20:59Z"
    },
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2409.03701",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03701v2",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "published": "2024-09-05T16:57:39Z"
    },
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2409.05004",
    "anchor": "semantic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.05004v2",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "published": "2024-09-08T07:24:03Z"
    },
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2409.05032",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.05032v1",
      "title": "Exploring WavLM Back-ends for Speech Spoofing and Deepfake Detection",
      "summary": "This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: Speech Deepfake Detection - Open Condition, which consists of a stand-alone speech deepfake (bonafide vs spoof) detection task. Recently, large-scale self-supervised models become a standard in Automatic Speech Recognition (ASR) and other speech processing tasks. Thus, we leverage a pre-trained WavLM as a front-end model and pool its representations with different back-end techniques. The complete framework is fine-tuned using only the trained dataset of the challenge, similar to the close condition. Besides, we adopt data-augmentation by adding noise and reverberation using MUSAN noise and RIR datasets. We also experiment with codec augmentations to increase the performance of our method. Ultimately, we use the Bosaris toolkit for score calibration and system fusion to get better Cllr scores. Our fused system achieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.",
      "published": "2024-09-08T08:54:36Z"
    },
    "metadata": {
      "arxiv_id": "2409.05032",
      "title": "Exploring WavLM Back-ends for Speech Spoofing and Deepfake Detection",
      "summary": "This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: Speech Deepfake Detection - Open Condition, which consists of a stand-alone speech deepfake (bonafide vs spoof) detection task. Recently, large-scale self-supervised models become a standard in Automatic Speech Recognition (ASR) and other speech processing tasks. Thus, we leverage a pre-trained WavLM as a front-end model and pool its representations with different back-end techniques. The complete framework is fine-tuned using only the trained dataset of the challenge, similar to the close condition. Besides, we adopt data-augmentation by adding noise and reverberation using MUSAN noise and RIR datasets. We also experiment with codec augmentations to increase the performance of our method. Ultimately, we use the Bosaris toolkit for score calibration and system fusion to get better Cllr scores. Our fused system achieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.",
      "authors": [
        "Theophile Stourbe",
        "Victor Miara",
        "Theo Lepage",
        "Reda Dehak"
      ],
      "published": "2024-09-08T08:54:36Z",
      "updated": "2024-09-08T08:54:36Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05032v1",
      "landing_url": "https://arxiv.org/abs/2409.05032v1",
      "doi": "https://doi.org/10.21437/ASVspoof.2024-11"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2409.07276",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07276v3",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "published": "2024-09-11T13:49:48Z"
    },
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2409.08277",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.08277v1",
      "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
      "summary": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
      "published": "2024-09-12T17:59:46Z"
    },
    "metadata": {
      "arxiv_id": "2409.08277",
      "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
      "summary": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
      "authors": [
        "Andrea Conti",
        "Matteo Poggi",
        "Valerio Cambareri",
        "Stefano Mattoccia"
      ],
      "published": "2024-09-12T17:59:46Z",
      "updated": "2024-09-12T17:59:46Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08277v1",
      "landing_url": "https://arxiv.org/abs/2409.08277v1",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2409.09253",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09253v1",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "published": "2024-09-14T01:45:04Z"
    },
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2409.10103",
    "anchor": "discrete speech tokens",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10103v1",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "published": "2024-09-16T09:07:08Z"
    },
    "metadata": {
      "arxiv_id": "2409.10103",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "authors": [
        "Ryota Komatsu",
        "Takahiro Shinozaki"
      ],
      "published": "2024-09-16T09:07:08Z",
      "updated": "2024-09-16T09:07:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10103v1",
      "landing_url": "https://arxiv.org/abs/2409.10103v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10103"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "hubert"
      }
    ]
  },
  {
    "arxiv_id": "2409.10870",
    "anchor": "acoustic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10870v1",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "published": "2024-09-17T03:46:01Z"
    },
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      }
    ]
  },
  {
    "arxiv_id": "2409.11003",
    "anchor": "semantic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11003v1",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "published": "2024-09-17T09:08:43Z"
    },
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2409.11228",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11228v2",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "published": "2024-09-17T14:21:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.14085",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.14085v1",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "published": "2024-09-21T09:39:36Z"
    },
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      }
    ]
  },
  {
    "arxiv_id": "2409.16302",
    "anchor": "speech representation",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.16302v2",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "published": "2024-09-10T11:00:24Z"
    },
    "metadata": {
      "arxiv_id": "2409.16302",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "authors": [
        "Teresa Dorszewski",
        "Albert Kjøller Jacobsen",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-09-10T11:00:24Z",
      "updated": "2025-01-17T12:27:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16302v2",
      "landing_url": "https://arxiv.org/abs/2409.16302v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.16302"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "residual vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "grouped vq"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "discriminator"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "speech representation",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "internal quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "external quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "bitrate"
      },
      {
        "anchor": "speech representation",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "speech representation",
        "search_term": "length reduction"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "unit discovery"
      },
      {
        "anchor": "speech representation",
        "search_term": "reconstruction"
      },
      {
        "anchor": "speech representation",
        "search_term": "word error rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "stoi"
      },
      {
        "anchor": "speech representation",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "speech representation",
        "search_term": "codec-superb"
      },
      {
        "anchor": "speech representation",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2409.18558",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.18558v1",
      "title": "XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier detection system for SVDD 2024 Challenge",
      "summary": "This paper introduces the model structure used in the SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal speech intonations and varying speech rates. In this paper, we propose the XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB stands for XLS-R, WavLM, and SLS Blend, representing the integration of these technologies for the purpose of SVDD. Specifically, we used the best performing model structure XLS-R&SLS from the ASVspoof DF dataset, and applied SLS to WavLM to form the WavLM&SLS structure. Finally, we integrated two models to form the XWSB system. Experimental results show that our system demonstrates advanced recognition capabilities in the SVDD challenge, specifically achieving an EER of 2.32% in the CtrSVDD track. The code and data can be found at https://github.com/QiShanZhang/XWSB_for_ SVDD2024.",
      "published": "2024-09-27T08:55:51Z"
    },
    "metadata": {
      "arxiv_id": "2409.18558",
      "title": "XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier detection system for SVDD 2024 Challenge",
      "summary": "This paper introduces the model structure used in the SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal speech intonations and varying speech rates. In this paper, we propose the XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB stands for XLS-R, WavLM, and SLS Blend, representing the integration of these technologies for the purpose of SVDD. Specifically, we used the best performing model structure XLS-R&SLS from the ASVspoof DF dataset, and applied SLS to WavLM to form the WavLM&SLS structure. Finally, we integrated two models to form the XWSB system. Experimental results show that our system demonstrates advanced recognition capabilities in the SVDD challenge, specifically achieving an EER of 2.32% in the CtrSVDD track. The code and data can be found at https://github.com/QiShanZhang/XWSB_for_ SVDD2024.",
      "authors": [
        "Qishan Zhang",
        "Shuangbing Wen",
        "Fangke Yan",
        "Tao Hu",
        "Jun Li"
      ],
      "published": "2024-09-27T08:55:51Z",
      "updated": "2024-09-27T08:55:51Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18558v1",
      "landing_url": "https://arxiv.org/abs/2409.18558v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.18558"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2410.03298",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.03298v1",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "published": "2024-10-04T10:21:15Z"
    },
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.06424",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06424v2",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "published": "2024-10-08T23:39:34Z"
    },
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2410.08325",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08325v1",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "published": "2024-10-10T19:29:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2410.08469",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08469v2",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "published": "2024-10-11T02:42:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2410.14411",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.14411v1",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "published": "2024-10-18T12:24:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.14411",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "authors": [
        "Hubert Siuzdak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer"
      ],
      "published": "2024-10-18T12:24:05Z",
      "updated": "2024-10-18T12:24:05Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14411v1",
      "landing_url": "https://arxiv.org/abs/2410.14411v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14411"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.15017",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15017v2",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "published": "2024-10-19T07:14:14Z"
    },
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.15764",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.15764v3",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "published": "2024-10-21T08:23:31Z"
    },
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.17081",
    "anchor": "discrete speech tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17081v2",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "published": "2024-10-22T15:02:37Z"
    },
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2410.22807",
    "anchor": "acoustic tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.22807v1",
      "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
      "summary": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
      "published": "2024-10-30T08:36:17Z"
    },
    "metadata": {
      "arxiv_id": "2410.22807",
      "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
      "summary": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
      "authors": [
        "Hui-Peng Du",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-30T08:36:17Z",
      "updated": "2024-10-30T08:36:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22807v1",
      "landing_url": "https://arxiv.org/abs/2410.22807v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22807"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.24177",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.24177v1",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "published": "2024-10-31T17:43:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2411.01407",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.01407v1",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "published": "2024-11-03T02:14:03Z"
    },
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.04257",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04257v3",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "published": "2024-11-06T21:00:45Z"
    },
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2411.07625",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.07625v3",
      "title": "Flow Matching Posterior Sampling: A Training-free Conditional Generation for Flow Matching",
      "summary": "Training-free conditional generation based on flow matching aims to leverage pre-trained unconditional flow matching models to perform conditional generation without retraining. Recently, a successful training-free conditional generation approach incorporates conditions via posterior sampling, which relies on the availability of a score function in the unconditional diffusion model. However, flow matching models do not possess an explicit score function, rendering such a strategy inapplicable. Approximate posterior sampling for flow matching has been explored, but it is limited to linear inverse problems. In this paper, we propose Flow Matching-based Posterior Sampling (FMPS) to expand its application scope. We introduce a correction term by steering the velocity field. This correction term can be reformulated to incorporate a surrogate score function, thereby bridging the gap between flow matching models and score-based posterior sampling. Hence, FMPS enables the posterior sampling to be adjusted within the flow matching framework. Further, we propose two practical implementations of the correction mechanism: one aimed at improving generation quality, and the other focused on computational efficiency. Experimental results on diverse conditional generation tasks demonstrate that our method achieves superior generation quality compared to existing state-of-the-art approaches, validating the effectiveness and generality of FMPS.",
      "published": "2024-11-12T08:14:39Z"
    },
    "metadata": {
      "arxiv_id": "2411.07625",
      "title": "Flow Matching Posterior Sampling: A Training-free Conditional Generation for Flow Matching",
      "summary": "Training-free conditional generation based on flow matching aims to leverage pre-trained unconditional flow matching models to perform conditional generation without retraining. Recently, a successful training-free conditional generation approach incorporates conditions via posterior sampling, which relies on the availability of a score function in the unconditional diffusion model. However, flow matching models do not possess an explicit score function, rendering such a strategy inapplicable. Approximate posterior sampling for flow matching has been explored, but it is limited to linear inverse problems. In this paper, we propose Flow Matching-based Posterior Sampling (FMPS) to expand its application scope. We introduce a correction term by steering the velocity field. This correction term can be reformulated to incorporate a surrogate score function, thereby bridging the gap between flow matching models and score-based posterior sampling. Hence, FMPS enables the posterior sampling to be adjusted within the flow matching framework. Further, we propose two practical implementations of the correction mechanism: one aimed at improving generation quality, and the other focused on computational efficiency. Experimental results on diverse conditional generation tasks demonstrate that our method achieves superior generation quality compared to existing state-of-the-art approaches, validating the effectiveness and generality of FMPS.",
      "authors": [
        "Kaiyu Song",
        "Hanjiang Lai",
        "Yan Pan",
        "Kun Yue",
        "Jian yin"
      ],
      "published": "2024-11-12T08:14:39Z",
      "updated": "2025-08-09T08:22:53Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07625v3",
      "landing_url": "https://arxiv.org/abs/2411.07625v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.07625"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2411.08742",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.08742v1",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "published": "2024-11-13T16:20:20Z"
    },
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.09943",
    "anchor": "acoustic tokens",
    "search_term": "voice conversion",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.09943v1",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "published": "2024-11-15T04:43:44Z"
    },
    "metadata": {
      "arxiv_id": "2411.09943",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "authors": [
        "Songting Liu"
      ],
      "published": "2024-11-15T04:43:44Z",
      "updated": "2024-11-15T04:43:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09943v1",
      "landing_url": "https://arxiv.org/abs/2411.09943v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09943"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "voice conversion"
      }
    ]
  },
  {
    "arxiv_id": "2411.14100",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.14100v2",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "published": "2024-11-21T13:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2411.14642",
    "anchor": "discrete speech tokens",
    "search_term": "vq-vae",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.14642v1",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "published": "2024-11-22T00:21:39Z"
    },
    "metadata": {
      "arxiv_id": "2411.14642",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "authors": [
        "Armani Rodriguez",
        "Silvija Kokalj-Filipovic"
      ],
      "published": "2024-11-22T00:21:39Z",
      "updated": "2024-11-22T00:21:39Z",
      "categories": [
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14642v1",
      "landing_url": "https://arxiv.org/abs/2411.14642v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.14642"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-vae"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-vae"
      }
    ]
  },
  {
    "arxiv_id": "2411.16550",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.16550v1",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "published": "2024-11-25T16:32:29Z"
    },
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "speech representation",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2411.17100",
    "anchor": "speech representation",
    "search_term": "hubert",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.17100v2",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "published": "2024-11-26T04:37:11Z"
    },
    "metadata": {
      "arxiv_id": "2411.17100",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "authors": [
        "Yifan Yang",
        "Jianheng Zhuo",
        "Zengrui Jin",
        "Ziyang Ma",
        "Xiaoyu Yang",
        "Zengwei Yao",
        "Liyong Guo",
        "Wei Kang",
        "Fangjun Kuang",
        "Long Lin",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2024-11-26T04:37:11Z",
      "updated": "2025-03-22T04:46:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17100v2",
      "landing_url": "https://arxiv.org/abs/2411.17100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17100"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "hubert"
      },
      {
        "anchor": "speech representation",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2412.02563",
    "anchor": "semantic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.02563v1",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "published": "2024-12-03T16:52:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2412.03685",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.03685v2",
      "title": "Sprite Sheet Diffusion: Generate Game Character for Animation",
      "summary": "In the game development process, creating character animations is a vital step that involves several stages. Typically for 2D games, illustrators begin by designing the main character image, which serves as the foundation for all subsequent animations. To create a smooth motion sequence, these subsequent animations involve drawing the character in different poses and actions, such as running, jumping, or attacking. This process requires significant manual effort from illustrators, as they must meticulously ensure consistency in design, proportions, and style across multiple motion frames. Each frame is drawn individually, making this a time-consuming and labor-intensive task. Generative models, such as diffusion models, have the potential to revolutionize this process by automating the creation of sprite sheets. Diffusion models, known for their ability to generate diverse images, can be adapted to create character animations. By leveraging the capabilities of diffusion models, we can significantly reduce the manual workload for illustrators, accelerate the animation creation process, and open up new creative possibilities in game development.",
      "published": "2024-12-04T19:40:05Z"
    },
    "metadata": {
      "arxiv_id": "2412.03685",
      "title": "Sprite Sheet Diffusion: Generate Game Character for Animation",
      "summary": "In the game development process, creating character animations is a vital step that involves several stages. Typically for 2D games, illustrators begin by designing the main character image, which serves as the foundation for all subsequent animations. To create a smooth motion sequence, these subsequent animations involve drawing the character in different poses and actions, such as running, jumping, or attacking. This process requires significant manual effort from illustrators, as they must meticulously ensure consistency in design, proportions, and style across multiple motion frames. Each frame is drawn individually, making this a time-consuming and labor-intensive task. Generative models, such as diffusion models, have the potential to revolutionize this process by automating the creation of sprite sheets. Diffusion models, known for their ability to generate diverse images, can be adapted to create character animations. By leveraging the capabilities of diffusion models, we can significantly reduce the manual workload for illustrators, accelerate the animation creation process, and open up new creative possibilities in game development.",
      "authors": [
        "Cheng-An Hsieh",
        "Jing Zhang",
        "Ava Yan"
      ],
      "published": "2024-12-04T19:40:05Z",
      "updated": "2025-03-16T21:42:15Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03685v2",
      "landing_url": "https://arxiv.org/abs/2412.03685v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.03685"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2412.04917",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.04917v1",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "published": "2024-12-06T10:16:04Z"
    },
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2412.10117",
    "anchor": "discrete speech tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10117v3",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "published": "2024-12-13T12:59:39Z"
    },
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2412.11024",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11024v2",
      "title": "Exploring Diffusion and Flow Matching Under Generator Matching",
      "summary": "In this paper, we present a comprehensive theoretical comparison of diffusion and flow matching under the Generator Matching framework. Despite their apparent differences, both diffusion and flow matching can be viewed under the unified framework of Generator Matching. By recasting both diffusion and flow matching under the same generative Markov framework, we provide theoretical insights into why flow matching models can be more robust empirically and how novel model classes can be constructed by mixing deterministic and stochastic components. Our analysis offers a fresh perspective on the relationships between state-of-the-art generative modeling paradigms.",
      "published": "2024-12-15T02:35:31Z"
    },
    "metadata": {
      "arxiv_id": "2412.11024",
      "title": "Exploring Diffusion and Flow Matching Under Generator Matching",
      "summary": "In this paper, we present a comprehensive theoretical comparison of diffusion and flow matching under the Generator Matching framework. Despite their apparent differences, both diffusion and flow matching can be viewed under the unified framework of Generator Matching. By recasting both diffusion and flow matching under the same generative Markov framework, we provide theoretical insights into why flow matching models can be more robust empirically and how novel model classes can be constructed by mixing deterministic and stochastic components. Our analysis offers a fresh perspective on the relationships between state-of-the-art generative modeling paradigms.",
      "authors": [
        "Zeeshan Patel",
        "James DeLoye",
        "Lance Mathias"
      ],
      "published": "2024-12-15T02:35:31Z",
      "updated": "2024-12-17T07:45:29Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11024v2",
      "landing_url": "https://arxiv.org/abs/2412.11024v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.11024"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2412.12276",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.12276v3",
      "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
      "summary": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
      "published": "2024-12-16T19:00:18Z"
    },
    "metadata": {
      "arxiv_id": "2412.12276",
      "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
      "summary": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
      "authors": [
        "Seungwook Han",
        "Jinyeop Song",
        "Jeff Gore",
        "Pulkit Agrawal"
      ],
      "published": "2024-12-16T19:00:18Z",
      "updated": "2025-06-02T12:55:12Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12276v3",
      "landing_url": "https://arxiv.org/abs/2412.12276v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.12276"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2412.16846",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16846v2",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "published": "2024-12-22T04:03:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.17640",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17640v2",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "published": "2024-12-23T15:18:24Z"
    },
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2412.19248",
    "anchor": "semantic tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.19248v1",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "published": "2024-12-26T15:08:36Z"
    },
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2501.00018",
    "anchor": "speech representation",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00018v1",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "published": "2024-12-16T03:33:05Z"
    },
    "metadata": {
      "arxiv_id": "2501.00018",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "authors": [
        "Linqin Wang",
        "Yaping Liu",
        "Zhengtao Yu",
        "Shengxiang Gao",
        "Cunli Mao",
        "Yuxin Huang",
        "Wenjun Wang",
        "Ling Dong"
      ],
      "published": "2024-12-16T03:33:05Z",
      "updated": "2024-12-16T03:33:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00018v1",
      "landing_url": "https://arxiv.org/abs/2501.00018v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00018"
    },
    "queries": [
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2501.01046",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01046v3",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "published": "2025-01-02T04:11:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.02350",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.02350v1",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "published": "2025-01-04T18:12:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2501.09983",
    "anchor": "discrete speech tokens",
    "search_term": "k-means clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.09983v2",
      "title": "Strong Consistency of Sparse K-means Clustering",
      "summary": "In this paper, we study the strong consistency of the sparse K-means clustering for high dimensional data. We prove the consistency in both risk and clustering for the Euclidean distance. We discuss the characterization of the limit of the clustering under some special cases. For the general (non-Euclidean) distance, we prove the consistency in risk. Our result naturally extends to other models with the same objective function but different constraints such as l0 or l1 penalty in recent literature.",
      "published": "2025-01-17T06:50:24Z"
    },
    "metadata": {
      "arxiv_id": "2501.09983",
      "title": "Strong Consistency of Sparse K-means Clustering",
      "summary": "In this paper, we study the strong consistency of the sparse K-means clustering for high dimensional data. We prove the consistency in both risk and clustering for the Euclidean distance. We discuss the characterization of the limit of the clustering under some special cases. For the general (non-Euclidean) distance, we prove the consistency in risk. Our result naturally extends to other models with the same objective function but different constraints such as l0 or l1 penalty in recent literature.",
      "authors": [
        "Jeungju Kim",
        "Johan Lim"
      ],
      "published": "2025-01-17T06:50:24Z",
      "updated": "2025-04-14T04:36:14Z",
      "categories": [
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.09983v2",
      "landing_url": "https://arxiv.org/abs/2501.09983v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.09983"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "k-means clustering"
      }
    ]
  },
  {
    "arxiv_id": "2501.18875",
    "anchor": "discrete speech tokens",
    "search_term": "self-supervised learning",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.18875v2",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "published": "2025-01-31T04:11:34Z"
    },
    "metadata": {
      "arxiv_id": "2501.18875",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "authors": [
        "M. Hadi Sepanj",
        "Benyamin Ghojogh",
        "Paul Fieguth"
      ],
      "published": "2025-01-31T04:11:34Z",
      "updated": "2025-11-16T22:59:47Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18875v2",
      "landing_url": "https://arxiv.org/abs/2501.18875v2",
      "doi": "https://doi.org/10.1109/ACCESS.2025.3628158"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "self-supervised learning"
      }
    ]
  },
  {
    "arxiv_id": "2502.02019",
    "anchor": "acoustic tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02019v1",
      "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
      "summary": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
      "published": "2025-02-04T05:16:15Z"
    },
    "metadata": {
      "arxiv_id": "2502.02019",
      "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
      "summary": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
      "authors": [
        "Yi-Chiao Wu",
        "Dejan Marković",
        "Steven Krenn",
        "Israel D. Gebru",
        "Alexander Richard"
      ],
      "published": "2025-02-04T05:16:15Z",
      "updated": "2025-02-04T05:16:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02019v1",
      "landing_url": "https://arxiv.org/abs/2502.02019v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02019"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.02150",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02150v3",
      "title": "On the Guidance of Flow Matching",
      "summary": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.",
      "published": "2025-02-04T09:27:57Z"
    },
    "metadata": {
      "arxiv_id": "2502.02150",
      "title": "On the Guidance of Flow Matching",
      "summary": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.",
      "authors": [
        "Ruiqi Feng",
        "Chenglei Yu",
        "Wenhao Deng",
        "Peiyan Hu",
        "Tailin Wu"
      ],
      "published": "2025-02-04T09:27:57Z",
      "updated": "2025-05-26T17:27:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02150v3",
      "landing_url": "https://arxiv.org/abs/2502.02150v3",
      "doi": "https://doi.org/10.48550/arXiv.2502.02150"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2502.02942",
    "anchor": "acoustic tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02942v1",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "published": "2025-02-05T07:14:39Z"
    },
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2502.03930",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03930v4",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "published": "2025-02-06T10:09:49Z"
    },
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.05236",
    "anchor": "acoustic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05236v2",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "published": "2025-02-07T06:47:11Z"
    },
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2502.05713",
    "anchor": "discrete speech tokens",
    "search_term": "vq-gan",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05713v1",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "published": "2025-02-08T22:25:53Z"
    },
    "metadata": {
      "arxiv_id": "2502.05713",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "authors": [
        "An Zhao",
        "Moucheng Xu",
        "Ahmed H. Shahin",
        "Wim Wuyts",
        "Mark G. Jones",
        "Joseph Jacob",
        "Daniel C. Alexander"
      ],
      "published": "2025-02-08T22:25:53Z",
      "updated": "2025-02-08T22:25:53Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05713v1",
      "landing_url": "https://arxiv.org/abs/2502.05713v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05713"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "speech representation",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      }
    ]
  },
  {
    "arxiv_id": "2502.06490",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.06490v4",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "published": "2025-02-10T14:08:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "k-means clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-vae"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "self-supervised learning"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "hubert"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "reconstruction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "voice conversion"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "word error rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.09616",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.09616v1",
      "title": "Variational Rectified Flow Matching",
      "summary": "We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.",
      "published": "2025-02-13T18:59:15Z"
    },
    "metadata": {
      "arxiv_id": "2502.09616",
      "title": "Variational Rectified Flow Matching",
      "summary": "We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.",
      "authors": [
        "Pengsheng Guo",
        "Alexander G. Schwing"
      ],
      "published": "2025-02-13T18:59:15Z",
      "updated": "2025-02-13T18:59:15Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09616v1",
      "landing_url": "https://arxiv.org/abs/2502.09616v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.09616"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2502.11897",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11897v2",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "published": "2025-02-17T15:22:31Z"
    },
    "metadata": {
      "arxiv_id": "2502.11897",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-02-17T15:22:31Z",
      "updated": "2025-04-02T13:25:35Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11897v2",
      "landing_url": "https://arxiv.org/abs/2502.11897v2",
      "doi": null
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2502.12007",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12007v1",
      "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
      "summary": "This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.",
      "published": "2025-02-17T16:43:47Z"
    },
    "metadata": {
      "arxiv_id": "2502.12007",
      "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
      "summary": "This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.",
      "authors": [
        "Yuchen Yang",
        "Thomas Thebaud",
        "Najim Dehak"
      ],
      "published": "2025-02-17T16:43:47Z",
      "updated": "2025-02-17T16:43:47Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12007v1",
      "landing_url": "https://arxiv.org/abs/2502.12007v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12007"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2502.12146",
    "anchor": "discrete speech tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12146v1",
      "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
      "summary": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
      "published": "2025-02-17T18:57:26Z"
    },
    "metadata": {
      "arxiv_id": "2502.12146",
      "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
      "summary": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
      "authors": [
        "Ye Tian",
        "Ling Yang",
        "Xinchen Zhang",
        "Yunhai Tong",
        "Mengdi Wang",
        "Bin Cui"
      ],
      "published": "2025-02-17T18:57:26Z",
      "updated": "2025-02-17T18:57:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12146v1",
      "landing_url": "https://arxiv.org/abs/2502.12146v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12146"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "speech representation",
        "search_term": "diffusion"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "diffusion"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2502.16240",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16240v1",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "published": "2025-02-22T14:25:55Z"
    },
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.16474",
    "anchor": "semantic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16474v1",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "published": "2025-02-23T07:17:28Z"
    },
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2503.01261",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01261v2",
      "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
      "summary": "Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
      "published": "2025-03-03T07:38:18Z"
    },
    "metadata": {
      "arxiv_id": "2503.01261",
      "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
      "summary": "Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
      "authors": [
        "Guotao Liang",
        "Baoquan Zhang",
        "Zhiyuan Wen",
        "Junteng Zhao",
        "Yunming Ye",
        "Kola Ye",
        "Yao He"
      ],
      "published": "2025-03-03T07:38:18Z",
      "updated": "2025-03-11T06:09:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01261v2",
      "landing_url": "https://arxiv.org/abs/2503.01261v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.01261"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2503.02862",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.02862v1",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "published": "2025-03-04T18:40:38Z"
    },
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2503.08434",
    "anchor": "semantic tokens",
    "search_term": "diffusion",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.08434v4",
      "title": "Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models",
      "summary": "Recent advances in large-scale text-to-image models have revolutionized creative fields by generating visually captivating outputs from textual prompts; however, while traditional photography offers precise control over camera settings to shape visual aesthetics - such as depth-of-field via aperture - current diffusion models typically rely on prompt engineering to mimic such effects. This approach often results in crude approximations and inadvertently alters the scene content. In this work, we propose Bokeh Diffusion, a scene-consistent bokeh control framework that explicitly conditions a diffusion model on a physical defocus blur parameter. To overcome the scarcity of paired real-world images captured under different camera settings, we introduce a hybrid training pipeline that aligns in-the-wild images with synthetic blur augmentations, providing diverse scenes and subjects as well as supervision to learn the separation of image content from lens blur. Central to our framework is our grounded self-attention mechanism, trained on image pairs with different bokeh levels of the same scene, which enables blur strength to be adjusted in both directions while preserving the underlying scene. Extensive experiments demonstrate that our approach enables flexible, lens-like blur control, supports downstream applications such as real image editing via inversion, and generalizes effectively across both Stable Diffusion and FLUX architectures.",
      "published": "2025-03-11T13:49:12Z"
    },
    "metadata": {
      "arxiv_id": "2503.08434",
      "title": "Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models",
      "summary": "Recent advances in large-scale text-to-image models have revolutionized creative fields by generating visually captivating outputs from textual prompts; however, while traditional photography offers precise control over camera settings to shape visual aesthetics - such as depth-of-field via aperture - current diffusion models typically rely on prompt engineering to mimic such effects. This approach often results in crude approximations and inadvertently alters the scene content. In this work, we propose Bokeh Diffusion, a scene-consistent bokeh control framework that explicitly conditions a diffusion model on a physical defocus blur parameter. To overcome the scarcity of paired real-world images captured under different camera settings, we introduce a hybrid training pipeline that aligns in-the-wild images with synthetic blur augmentations, providing diverse scenes and subjects as well as supervision to learn the separation of image content from lens blur. Central to our framework is our grounded self-attention mechanism, trained on image pairs with different bokeh levels of the same scene, which enables blur strength to be adjusted in both directions while preserving the underlying scene. Extensive experiments demonstrate that our approach enables flexible, lens-like blur control, supports downstream applications such as real image editing via inversion, and generalizes effectively across both Stable Diffusion and FLUX architectures.",
      "authors": [
        "Armando Fortes",
        "Tianyi Wei",
        "Shangchen Zhou",
        "Xingang Pan"
      ],
      "published": "2025-03-11T13:49:12Z",
      "updated": "2025-06-16T13:29:51Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08434v4",
      "landing_url": "https://arxiv.org/abs/2503.08434v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.08434"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "diffusion"
      }
    ]
  },
  {
    "arxiv_id": "2503.09787",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.09787v1",
      "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
      "summary": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
      "published": "2025-03-12T19:39:09Z"
    },
    "metadata": {
      "arxiv_id": "2503.09787",
      "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
      "summary": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
      "authors": [
        "Riku Takahashi",
        "Ryugo Morita",
        "Fuma Kimishima",
        "Kosuke Iwama",
        "Jinjia Zhou"
      ],
      "published": "2025-03-12T19:39:09Z",
      "updated": "2025-03-12T19:39:09Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09787v1",
      "landing_url": "https://arxiv.org/abs/2503.09787v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.09787"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2503.10832",
    "anchor": "discrete speech tokens",
    "search_term": "codebook",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.10832v1",
      "title": "Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size",
      "summary": "Vector Quantization (VQ) techniques face significant challenges in codebook utilization, limiting reconstruction fidelity in image modeling. We introduce a Dual Codebook mechanism that effectively addresses this limitation by partitioning the representation into complementary global and local components. The global codebook employs a lightweight transformer for concurrent updates of all code vectors, while the local codebook maintains precise feature representation through deterministic selection. This complementary approach is trained from scratch without requiring pre-trained knowledge. Experimental evaluation across multiple standard benchmark datasets demonstrates state-of-the-art reconstruction quality while using a compact codebook of size 512 - half the size of previous methods that require pre-training. Our approach achieves significant FID improvements across diverse image domains, particularly excelling in scene and face reconstruction tasks. These results establish Dual Codebook VQ as an efficient paradigm for high-fidelity image reconstruction with significantly reduced computational requirements.",
      "published": "2025-03-13T19:31:18Z"
    },
    "metadata": {
      "arxiv_id": "2503.10832",
      "title": "Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size",
      "summary": "Vector Quantization (VQ) techniques face significant challenges in codebook utilization, limiting reconstruction fidelity in image modeling. We introduce a Dual Codebook mechanism that effectively addresses this limitation by partitioning the representation into complementary global and local components. The global codebook employs a lightweight transformer for concurrent updates of all code vectors, while the local codebook maintains precise feature representation through deterministic selection. This complementary approach is trained from scratch without requiring pre-trained knowledge. Experimental evaluation across multiple standard benchmark datasets demonstrates state-of-the-art reconstruction quality while using a compact codebook of size 512 - half the size of previous methods that require pre-training. Our approach achieves significant FID improvements across diverse image domains, particularly excelling in scene and face reconstruction tasks. These results establish Dual Codebook VQ as an efficient paradigm for high-fidelity image reconstruction with significantly reduced computational requirements.",
      "authors": [
        "Parisa Boodaghi Malidarreh",
        "Jillur Rahman Saurav",
        "Thuong Le Hoai Pham",
        "Amir Hajighasemi",
        "Anahita Samadi",
        "Saurabh Shrinivas Maydeo",
        "Mohammad Sadegh Nasr",
        "Jacob M. Luber"
      ],
      "published": "2025-03-13T19:31:18Z",
      "updated": "2025-03-13T19:31:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.10832v1",
      "landing_url": "https://arxiv.org/abs/2503.10832v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.10832"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "speech representation",
        "search_term": "codebook"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codebook"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codebook"
      }
    ]
  },
  {
    "arxiv_id": "2503.12115",
    "anchor": "semantic tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12115v2",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "published": "2025-03-15T12:50:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2503.16117",
    "anchor": "discrete speech tokens",
    "search_term": "discriminator",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.16117v2",
      "title": "Improving Discriminator Guidance in Diffusion Models",
      "summary": "Discriminator Guidance has become a popular method for efficiently refining pre-trained Score-Matching Diffusion models. However, in this paper, we demonstrate that the standard implementation of this technique does not necessarily lead to a distribution closer to the real data distribution. Specifically, we show that training the discriminator using Cross-Entropy loss, as commonly done, can in fact increase the Kullback-Leibler divergence between the model and target distributions, particularly when the discriminator overfits. To address this, we propose a theoretically sound training objective for discriminator guidance that properly minimizes the KL divergence. We analyze its properties and demonstrate empirically across multiple datasets that our proposed method consistently improves over the conventional method by producing samples of higher quality.",
      "published": "2025-03-20T13:04:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.16117",
      "title": "Improving Discriminator Guidance in Diffusion Models",
      "summary": "Discriminator Guidance has become a popular method for efficiently refining pre-trained Score-Matching Diffusion models. However, in this paper, we demonstrate that the standard implementation of this technique does not necessarily lead to a distribution closer to the real data distribution. Specifically, we show that training the discriminator using Cross-Entropy loss, as commonly done, can in fact increase the Kullback-Leibler divergence between the model and target distributions, particularly when the discriminator overfits. To address this, we propose a theoretically sound training objective for discriminator guidance that properly minimizes the KL divergence. We analyze its properties and demonstrate empirically across multiple datasets that our proposed method consistently improves over the conventional method by producing samples of higher quality.",
      "authors": [
        "Alexandre Verine",
        "Ahmed Mehdi Inane",
        "Florian Le Bronnec",
        "Benjamin Negrevergne",
        "Yann Chevaleyre"
      ],
      "published": "2025-03-20T13:04:43Z",
      "updated": "2025-06-11T12:24:13Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16117v2",
      "landing_url": "https://arxiv.org/abs/2503.16117v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.16117"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discriminator"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discriminator"
      }
    ]
  },
  {
    "arxiv_id": "2503.18429",
    "anchor": "acoustic tokens",
    "search_term": "residual vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.18429v1",
      "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
      "summary": "In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion. Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements. Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism.",
      "published": "2025-03-24T08:16:47Z"
    },
    "metadata": {
      "arxiv_id": "2503.18429",
      "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
      "summary": "In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion. Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements. Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism.",
      "authors": [
        "Dingcheng Zhen",
        "Shunshun Yin",
        "Shiyang Qin",
        "Hou Yi",
        "Ziwei Zhang",
        "Siyuan Liu",
        "Gan Qi",
        "Ming Tao"
      ],
      "published": "2025-03-24T08:16:47Z",
      "updated": "2025-03-24T08:16:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18429v1",
      "landing_url": "https://arxiv.org/abs/2503.18429v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.18429"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      }
    ]
  },
  {
    "arxiv_id": "2503.20499",
    "anchor": "semantic tokens",
    "search_term": "speaker similarity",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.20499v3",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "published": "2025-03-26T12:39:06Z"
    },
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "queries": [
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      }
    ]
  },
  {
    "arxiv_id": "2504.05304",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.05304v3",
      "title": "Gaussian Mixture Flow Matching Models",
      "summary": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.",
      "published": "2025-04-07T17:59:42Z"
    },
    "metadata": {
      "arxiv_id": "2504.05304",
      "title": "Gaussian Mixture Flow Matching Models",
      "summary": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.",
      "authors": [
        "Hansheng Chen",
        "Kai Zhang",
        "Hao Tan",
        "Zexiang Xu",
        "Fujun Luan",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Sai Bi"
      ],
      "published": "2025-04-07T17:59:42Z",
      "updated": "2025-08-30T06:33:43Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05304v3",
      "landing_url": "https://arxiv.org/abs/2504.05304v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.05304"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2504.06561",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.06561v1",
      "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
      "summary": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
      "published": "2025-04-09T03:49:00Z"
    },
    "metadata": {
      "arxiv_id": "2504.06561",
      "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
      "summary": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
      "authors": [
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2025-04-09T03:49:00Z",
      "updated": "2025-04-09T03:49:00Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06561v1",
      "landing_url": "https://arxiv.org/abs/2504.06561v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.06561"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2504.07053",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.07053v2",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "published": "2025-04-09T17:14:33Z"
    },
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2504.14113",
    "anchor": "discrete speech tokens",
    "search_term": "vector quantization",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.14113v1",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "published": "2025-04-19T00:13:21Z"
    },
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vector quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vector quantization"
      }
    ]
  },
  {
    "arxiv_id": "2504.17872",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.17872v1",
      "title": "Flow Matching Ergodic Coverage",
      "summary": "Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.",
      "published": "2025-04-24T18:18:35Z"
    },
    "metadata": {
      "arxiv_id": "2504.17872",
      "title": "Flow Matching Ergodic Coverage",
      "summary": "Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.",
      "authors": [
        "Max Muchen Sun",
        "Allison Pinosky",
        "Todd Murphey"
      ],
      "published": "2025-04-24T18:18:35Z",
      "updated": "2025-04-24T18:18:35Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.17872v1",
      "landing_url": "https://arxiv.org/abs/2504.17872v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.17872"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2505.03682",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.03682v1",
      "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
      "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
      "published": "2025-05-06T16:27:47Z"
    },
    "metadata": {
      "arxiv_id": "2505.03682",
      "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
      "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
      "authors": [
        "Christopher Flöter",
        "Sergej Geringer",
        "Guido Reina",
        "Daniel Weiskopf",
        "Timo Ropinski"
      ],
      "published": "2025-05-06T16:27:47Z",
      "updated": "2025-05-06T16:27:47Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.03682v1",
      "landing_url": "https://arxiv.org/abs/2505.03682v1",
      "doi": "https://doi.org/10.1145/3715669.3725870"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.05738",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.05738v2",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "published": "2025-05-09T02:34:06Z"
    },
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "speech representation",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.09407",
    "anchor": "discrete speech tokens",
    "search_term": "encoder decoder",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.09407v1",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "summary": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
      "published": "2025-05-14T14:04:44Z"
    },
    "metadata": {
      "arxiv_id": "2505.09407",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "summary": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "published": "2025-05-14T14:04:44Z",
      "updated": "2025-05-14T14:04:44Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.09407v1",
      "landing_url": "https://arxiv.org/abs/2505.09407v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.09407"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "speech representation",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "encoder decoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "encoder decoder"
      }
    ]
  },
  {
    "arxiv_id": "2505.12053",
    "anchor": "discrete speech tokens",
    "search_term": "variable frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.12053v2",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "published": "2025-05-17T15:32:54Z"
    },
    "metadata": {
      "arxiv_id": "2505.12053",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "published": "2025-05-17T15:32:54Z",
      "updated": "2025-09-28T09:51:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "landing_url": "https://arxiv.org/abs/2505.12053v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12053"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.12994",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.12994v3",
      "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
      "summary": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
      "published": "2025-05-19T11:31:32Z"
    },
    "metadata": {
      "arxiv_id": "2505.12994",
      "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
      "summary": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
      "authors": [
        "Xuanjun Chen",
        "I-Ming Lin",
        "Lin Zhang",
        "Jiawei Du",
        "Haibin Wu",
        "Hung-yi Lee",
        "Jyh-Shing Roger Jang"
      ],
      "published": "2025-05-19T11:31:32Z",
      "updated": "2025-08-03T07:58:47Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12994v3",
      "landing_url": "https://arxiv.org/abs/2505.12994v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.12994"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2505.13000",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13000v2",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "published": "2025-05-19T11:41:08Z"
    },
    "metadata": {
      "arxiv_id": "2505.13000",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "authors": [
        "Jiaqi Li",
        "Xiaolong Lin",
        "Zhekai Li",
        "Shixi Huang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Zhenpeng Zhan",
        "Zhizheng Wu"
      ],
      "published": "2025-05-19T11:41:08Z",
      "updated": "2025-10-01T15:01:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13000v2",
      "landing_url": "https://arxiv.org/abs/2505.13000v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13000"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.13830",
    "anchor": "discrete speech tokens",
    "search_term": "acoustic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.13830v2",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "published": "2025-05-20T02:18:45Z"
    },
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic tokens"
      }
    ]
  },
  {
    "arxiv_id": "2505.14989",
    "anchor": "acoustic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.14989v1",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "published": "2025-05-21T00:27:38Z"
    },
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      }
    ]
  },
  {
    "arxiv_id": "2505.16119",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16119v2",
      "title": "Source Separation by Flow Matching",
      "summary": "We consider the problem of single-channel audio source separation with the goal of reconstructing $K$ sources from their mixture. We address this ill-posed problem with FLOSS (FLOw matching for Source Separation), a constrained generation method based on flow matching, ensuring strict mixture consistency. Flow matching is a general methodology that, when given samples from two probability distributions defined on the same space, learns an ordinary differential equation to output a sample from one of the distributions when provided with a sample from the other. In our context, we have access to samples from the joint distribution of $K$ sources and so the corresponding samples from the lower-dimensional distribution of their mixture. To apply flow matching, we augment these mixture samples with artificial noise components to match the dimensionality of the $K$ source distribution. Additionally, as any permutation of the sources yields the same mixture, we adopt an equivariant formulation of flow matching which relies on a neural network architecture that is equivariant by design. We demonstrate the performance of the method for the separation of overlapping speech.",
      "published": "2025-05-22T01:52:06Z"
    },
    "metadata": {
      "arxiv_id": "2505.16119",
      "title": "Source Separation by Flow Matching",
      "summary": "We consider the problem of single-channel audio source separation with the goal of reconstructing $K$ sources from their mixture. We address this ill-posed problem with FLOSS (FLOw matching for Source Separation), a constrained generation method based on flow matching, ensuring strict mixture consistency. Flow matching is a general methodology that, when given samples from two probability distributions defined on the same space, learns an ordinary differential equation to output a sample from one of the distributions when provided with a sample from the other. In our context, we have access to samples from the joint distribution of $K$ sources and so the corresponding samples from the lower-dimensional distribution of their mixture. To apply flow matching, we augment these mixture samples with artificial noise components to match the dimensionality of the $K$ source distribution. Additionally, as any permutation of the sources yields the same mixture, we adopt an equivariant formulation of flow matching which relies on a neural network architecture that is equivariant by design. We demonstrate the performance of the method for the separation of overlapping speech.",
      "authors": [
        "Robin Scheibler",
        "John R. Hershey",
        "Arnaud Doucet",
        "Henry Li"
      ],
      "published": "2025-05-22T01:52:06Z",
      "updated": "2025-07-18T00:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16119v2",
      "landing_url": "https://arxiv.org/abs/2505.16119v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16119"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2505.16177",
    "anchor": "discrete speech tokens",
    "search_term": "bitrate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16177v1",
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
      "summary": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
      "published": "2025-05-22T03:31:33Z"
    },
    "metadata": {
      "arxiv_id": "2505.16177",
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
      "summary": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
      "authors": [
        "Linfeng Qi",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "published": "2025-05-22T03:31:33Z",
      "updated": "2025-05-22T03:31:33Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16177v1",
      "landing_url": "https://arxiv.org/abs/2505.16177v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16177"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "bitrate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "bitrate"
      }
    ]
  },
  {
    "arxiv_id": "2505.16845",
    "anchor": "discrete speech tokens",
    "search_term": "frame rate",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.16845v1",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "published": "2025-05-22T16:10:01Z"
    },
    "metadata": {
      "arxiv_id": "2505.16845",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2025-05-22T16:10:01Z",
      "updated": "2025-05-22T16:10:01Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16845v1",
      "landing_url": "https://arxiv.org/abs/2505.16845v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16845"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "speech representation",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.17076",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17076v3",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "published": "2025-05-20T06:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "frame rate"
      }
    ]
  },
  {
    "arxiv_id": "2505.17446",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17446v2",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "published": "2025-05-23T04:03:27Z"
    },
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "speech representation",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech tokenization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2505.17604",
    "anchor": "discrete speech tokens",
    "search_term": "semantic tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.17604v1",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "published": "2025-05-23T08:15:05Z"
    },
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "semantic tokens"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "unit discovery"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "speaker similarity"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2505.19043",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19043v2",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "published": "2025-05-25T08:43:40Z"
    },
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "offline clustering"
      }
    ]
  },
  {
    "arxiv_id": "2505.19760",
    "anchor": "discrete speech tokens",
    "search_term": "pesq",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.19760v2",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "published": "2025-05-26T09:43:09Z"
    },
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "speech representation",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "pesq"
      }
    ]
  },
  {
    "arxiv_id": "2505.21194",
    "anchor": "discrete speech tokens",
    "search_term": "deduplication",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.21194v1",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "published": "2025-05-27T13:42:33Z"
    },
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "speech representation",
        "search_term": "deduplication"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "deduplication"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "deduplication"
      }
    ]
  },
  {
    "arxiv_id": "2505.23207",
    "anchor": "discrete speech tokens",
    "search_term": "wavlm",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.23207v1",
      "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
      "summary": "Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.",
      "published": "2025-05-29T07:47:48Z"
    },
    "metadata": {
      "arxiv_id": "2505.23207",
      "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
      "summary": "Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.",
      "authors": [
        "Zhaokai Sun",
        "Li Zhang",
        "Qing Wang",
        "Pan Zhou",
        "Lei Xie"
      ],
      "published": "2025-05-29T07:47:48Z",
      "updated": "2025-05-29T07:47:48Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.23207v1",
      "landing_url": "https://arxiv.org/abs/2505.23207v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.23207"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wavlm"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "wavlm"
      }
    ]
  },
  {
    "arxiv_id": "2505.24496",
    "anchor": "discrete speech tokens",
    "search_term": "speech tokenization",
    "search_record": {
      "id": "http://arxiv.org/abs/2505.24496v1",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "published": "2025-05-30T11:47:29Z"
    },
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech tokenization"
      }
    ]
  },
  {
    "arxiv_id": "2506.00843",
    "anchor": "acoustic tokens",
    "search_term": "gumbel vq",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.00843v1",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "published": "2025-06-01T05:38:39Z"
    },
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "queries": [
      {
        "anchor": "acoustic tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2506.05350",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.05350v1",
      "title": "Contrastive Flow Matching",
      "summary": "Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.",
      "published": "2025-06-05T17:59:58Z"
    },
    "metadata": {
      "arxiv_id": "2506.05350",
      "title": "Contrastive Flow Matching",
      "summary": "Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.",
      "authors": [
        "George Stoica",
        "Vivek Ramanujan",
        "Xiang Fan",
        "Ali Farhadi",
        "Ranjay Krishna",
        "Judy Hoffman"
      ],
      "published": "2025-06-05T17:59:58Z",
      "updated": "2025-06-05T17:59:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05350v1",
      "landing_url": "https://arxiv.org/abs/2506.05350v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.05350"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "speech representation",
        "search_term": "flow matching"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2506.05940",
    "anchor": "discrete speech tokens",
    "search_term": "flow matching",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.05940v4",
      "title": "Exponential Family Variational Flow Matching for Tabular Data Generation",
      "summary": "While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop TabbyFlow, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce Exponential Family Variational Flow Matching (EF-VFM), which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines.",
      "published": "2025-06-06T10:07:48Z"
    },
    "metadata": {
      "arxiv_id": "2506.05940",
      "title": "Exponential Family Variational Flow Matching for Tabular Data Generation",
      "summary": "While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop TabbyFlow, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce Exponential Family Variational Flow Matching (EF-VFM), which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines.",
      "authors": [
        "Andrés Guzmán-Cordero",
        "Floor Eijkelboom",
        "Jan-Willem van de Meent"
      ],
      "published": "2025-06-06T10:07:48Z",
      "updated": "2025-10-03T17:32:46Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05940v4",
      "landing_url": "https://arxiv.org/abs/2506.05940v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.05940"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "flow matching"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "flow matching"
      }
    ]
  },
  {
    "arxiv_id": "2506.07081",
    "anchor": "discrete speech tokens",
    "search_term": "neural audio codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.07081v2",
      "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
      "summary": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
      "published": "2025-06-08T10:54:23Z"
    },
    "metadata": {
      "arxiv_id": "2506.07081",
      "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
      "summary": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
      "authors": [
        "Sathvik Udupa",
        "Shinji Watanabe",
        "Petr Schwarz",
        "Jan Cernocky"
      ],
      "published": "2025-06-08T10:54:23Z",
      "updated": "2025-06-19T09:40:25Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07081v2",
      "landing_url": "https://arxiv.org/abs/2506.07081v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07081"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "speech representation",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "neural audio codec"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "neural audio codec"
      }
    ]
  },
  {
    "arxiv_id": "2506.09349",
    "anchor": "discrete speech tokens",
    "search_term": "offline clustering",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09349v4",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "published": "2025-06-11T02:57:22Z"
    },
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "offline clustering"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "gumbel vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "finite scalar quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "residual vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "grouped vq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-gan"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "speech token vocoder"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "wav2vec 2 0"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vq-wav2vec"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "supervised tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "internal quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "external quantization"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "discrete speech tokens"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "vocabulary size"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "length reduction"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "acoustic bpe"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "variable frame rate"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "codec-superb"
      },
      {
        "anchor": "discrete speech tokens",
        "search_term": "dasb"
      }
    ]
  },
  {
    "arxiv_id": "2506.09549",
    "anchor": "discrete speech tokens",
    "search_term": "stoi",
    "search_record": {
      "id": "http://arxiv.org/abs/2506.09549v1",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "published": "2025-06-11T09:32:12Z"
    },
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "queries": [
      {
        "anchor": "discrete speech tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "pesq"
      },
      {
        "anchor": "acoustic tokens",
        "search_term": "stoi"
      },
      {
        "anchor": "semantic tokens",
        "search_term": "stoi"
      }
    ]
  }
]