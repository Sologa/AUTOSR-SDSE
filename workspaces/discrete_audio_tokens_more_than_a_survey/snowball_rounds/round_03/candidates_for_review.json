[
  {
    "metadata": {
      "title": "Extracting the meaning of a word: an artificial intelligence approach",
      "summary": "Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.",
      "abstract": "Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.",
      "doi": "https://doi.org/10.1590/2318-0889202537e2514829",
      "openalex_id": "https://openalex.org/W4415283286",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Early phonetic learning without phonetic categories -- Insights from large-scale simulations on realistic input",
      "summary": "Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.",
      "abstract": "Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.",
      "doi": "https://doi.org/10.31234/osf.io/fc4wh",
      "openalex_id": "https://openalex.org/W4230289889",
      "arxiv_id": "",
      "publication_date": "2019-05-01",
      "published": "2019-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Visually Grounded Models of Spoken Language: A Survey of Datasets, Architectures and Evaluation Techniques",
      "summary": "This survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. Such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. Several fields have made important contributions to this approach to modeling or mimicking the process of learning language: Machine Learning, Natural Language and Speech Processing, Computer Vision and Cognitive Science. The current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. We discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. We then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.",
      "abstract": "This survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. Such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. Several fields have made important contributions to this approach to modeling or mimicking the process of learning language: Machine Learning, Natural Language and Speech Processing, Computer Vision and Cognitive Science. The current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. We discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. We then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.",
      "doi": "https://doi.org/10.1613/jair.1.12967",
      "openalex_id": "https://openalex.org/W3157861865",
      "arxiv_id": "",
      "publication_date": "2022-02-18",
      "published": "2022-02-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
      "summary": "We present an unsupervised end-to-end training scheme where we discover\\ndiscrete subword units from speech without using any labels. The discrete\\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\\ngiven a variety of speakers, and a TTS-Decoder trained to project the\\ndiscovered units back to the designated speech. We propose a discrete encoding\\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\\ndifferentiable. We found that the proposed encoding method offers automatic\\nextraction of speech content from speaker style, and is sufficient to cover\\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\\nsynthesize speech with the same content as the input of ASR-Encoder but with\\ndifferent speaker characteristics, which achieves voice conversion (VC). We\\nfurther improve the quality of VC using adversarial training, where we train a\\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\\nevaluations show that the proposed approach offers strong VC results as it\\neliminates speaker identity while preserving content within speech. In the\\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\\nbitrate.\\n",
      "abstract": "We present an unsupervised end-to-end training scheme where we discover\\ndiscrete subword units from speech without using any labels. The discrete\\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\\ngiven a variety of speakers, and a TTS-Decoder trained to project the\\ndiscovered units back to the designated speech. We propose a discrete encoding\\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\\ndifferentiable. We found that the proposed encoding method offers automatic\\nextraction of speech content from speaker style, and is sufficient to cover\\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\\nsynthesize speech with the same content as the input of ASR-Encoder but with\\ndifferent speaker characteristics, which achieves voice conversion (VC). We\\nfurther improve the quality of VC using adversarial training, where we train a\\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\\nevaluations show that the proposed approach offers strong VC results as it\\neliminates speaker identity while preserving content within speech. In the\\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\\nbitrate.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-2048",
      "openalex_id": "https://openalex.org/W2947445680",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Word Segmentation on Discovered Phone Units With Dynamic Programming and Self-Supervised Scoring",
      "summary": "Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.",
      "abstract": "Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.",
      "doi": "https://doi.org/10.1109/taslp.2022.3229264",
      "openalex_id": "https://openalex.org/W4313182775",
      "arxiv_id": "",
      "publication_date": "2022-12-14",
      "published": "2022-12-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Augmenting Contrastive Learning of Speech Representations in the Time Domain",
      "summary": "Contrastive Predictive Coding (CPC), based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC (relative improvement of 18-22%), beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15% relative.",
      "abstract": "Contrastive Predictive Coding (CPC), based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC (relative improvement of 18-22%), beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15% relative.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383605",
      "openalex_id": "https://openalex.org/W3039910566",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization",
      "summary": "In a recent paper, we have presented a generative adversarial network\\n(GAN)-based model for unconditional generation of the mel-spectrograms of\\nsinging voices. As the generator of the model is designed to take a\\nvariable-length sequence of noise vectors as input, it can generate\\nmel-spectrograms of variable length. However, our previous listening test shows\\nthat the quality of the generated audio leaves room for improvement. The\\npresent paper extends and expands that previous work in the following aspects.\\nFirst, we employ a hierarchical architecture in the generator to induce some\\nstructure in the temporal dimension. Second, we introduce a cycle\\nregularization mechanism to the generator to avoid mode collapse. Third, we\\nevaluate the performance of the new model not only for generating singing\\nvoices, but also for generating speech voices. Evaluation result shows that new\\nmodel outperforms the prior one both objectively and subjectively. We also\\nemploy the model to unconditionally generate sequences of piano and violin\\nmusic and find the result promising. Audio examples, as well as the code for\\nimplementing our model, will be publicly available online upon paper\\npublication.\\n",
      "abstract": "In a recent paper, we have presented a generative adversarial network\\n(GAN)-based model for unconditional generation of the mel-spectrograms of\\nsinging voices. As the generator of the model is designed to take a\\nvariable-length sequence of noise vectors as input, it can generate\\nmel-spectrograms of variable length. However, our previous listening test shows\\nthat the quality of the generated audio leaves room for improvement. The\\npresent paper extends and expands that previous work in the following aspects.\\nFirst, we employ a hierarchical architecture in the generator to induce some\\nstructure in the temporal dimension. Second, we introduce a cycle\\nregularization mechanism to the generator to avoid mode collapse. Third, we\\nevaluate the performance of the new model not only for generating singing\\nvoices, but also for generating speech voices. Evaluation result shows that new\\nmodel outperforms the prior one both objectively and subjectively. We also\\nemploy the model to unconditionally generate sequences of piano and violin\\nmusic and find the result promising. Audio examples, as well as the code for\\nimplementing our model, will be publicly available online upon paper\\npublication.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-1137",
      "openalex_id": "https://openalex.org/W3024973272",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
      "summary": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.",
      "abstract": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1337",
      "openalex_id": "https://openalex.org/W2950414763",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-Supervised Speaker Adaptation for End-to-End Speech Synthesis with Pretrained Models",
      "summary": "Recently, end-to-end text-to-speech (TTS) models have achieved a remarkable performance, however, requiring a large amount of paired text and speech data for training. On the other hand, we can easily collect unpaired dozen minutes of speech recordings for a target speaker without corresponding text data. To make use of such accessible data, the proposed method leverages the recent great success of state-of-the-art end-to-end automatic speech recognition (ASR) systems and obtains corresponding transcriptions from pretrained ASR models. Although these models could only provide text output instead of intermediate linguistic features like phonemes, end-to-end TTS can be well trained with such raw text data directly. Thus, the proposed method can greatly simplify a speaker adaptation pipeline by consistently employing end-to-end ASR/TTS ecosystems. The experimental results show that our proposed method achieved comparable performance to a paired data adaptation method in terms of subjective speaker similarity and objective cepstral distance measures.",
      "abstract": "Recently, end-to-end text-to-speech (TTS) models have achieved a remarkable performance, however, requiring a large amount of paired text and speech data for training. On the other hand, we can easily collect unpaired dozen minutes of speech recordings for a target speaker without corresponding text data. To make use of such accessible data, the proposed method leverages the recent great success of state-of-the-art end-to-end automatic speech recognition (ASR) systems and obtains corresponding transcriptions from pretrained ASR models. Although these models could only provide text output instead of intermediate linguistic features like phonemes, end-to-end TTS can be well trained with such raw text data directly. Thus, the proposed method can greatly simplify a speaker adaptation pipeline by consistently employing end-to-end ASR/TTS ecosystems. The experimental results show that our proposed method achieved comparable performance to a paired data adaptation method in terms of subjective speaker similarity and objective cepstral distance measures.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053371",
      "openalex_id": "https://openalex.org/W3015853838",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Feature Learning for Speech Using Correspondence and Siamese Networks",
      "summary": "In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.",
      "abstract": "In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.",
      "doi": "https://doi.org/10.1109/lsp.2020.2973798",
      "openalex_id": "https://openalex.org/W3006358483",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Clinicopathological Characteristics of Pure and Mixed Invasive Micropapillary Breast Carcinomas: A Single Center Experience",
      "summary": "Compared to the mixed IMPC, pure IMPC appears to have a more aggressive behavior with lower locoregional recurrence-free survival and more locoregional recurrences. This may be due to the low progesterone receptor positivity rate.",
      "abstract": "Compared to the mixed IMPC, pure IMPC appears to have a more aggressive behavior with lower locoregional recurrence-free survival and more locoregional recurrences. This may be due to the low progesterone receptor positivity rate.",
      "doi": "https://doi.org/10.4274/balkanmedj.galenos.2022.2022-4-7",
      "openalex_id": "https://openalex.org/W4286716540",
      "arxiv_id": "",
      "publication_date": "2022-07-22",
      "published": "2022-07-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Hierarchical Subspace Model for Language-Attuned Acoustic Unit Discovery",
      "summary": "In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.",
      "abstract": "In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414899",
      "openalex_id": "https://openalex.org/W3100202343",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adversarial Feature Learning and Unsupervised Clustering Based Speech Synthesis for Found Data With Acoustic and Textual Noise",
      "summary": "Attention-based sequence-to-sequence (seq2seq) speech synthesis has achieved\\nextraordinary performance. But a studio-quality corpus with manual\\ntranscription is necessary to train such seq2seq systems. In this paper, we\\npropose an approach to build high-quality and stable seq2seq based speech\\nsynthesis system using challenging found data, where training speech contains\\nnoisy interferences (acoustic noise) and texts are imperfect speech recognition\\ntranscripts (textual noise). To deal with text-side noise, we propose a VQVAE\\nbased heuristic method to compensate erroneous linguistic feature with phonetic\\ninformation learned directly from speech. As for the speech-side noise, we\\npropose to learn a noise-independent feature in the auto-regressive decoder\\nthrough adversarial training and data augmentation, which does not need an\\nextra speech enhancement model. Experiments show the effectiveness of the\\nproposed approach in dealing with text-side and speech-side noise. Surpassing\\nthe denoising approach based on a state-of-the-art speech enhancement model,\\nour system built on noisy found data can synthesize clean and high-quality\\nspeech with MOS close to the system built on the clean counterpart.\\n",
      "abstract": "Attention-based sequence-to-sequence (seq2seq) speech synthesis has achieved\\nextraordinary performance. But a studio-quality corpus with manual\\ntranscription is necessary to train such seq2seq systems. In this paper, we\\npropose an approach to build high-quality and stable seq2seq based speech\\nsynthesis system using challenging found data, where training speech contains\\nnoisy interferences (acoustic noise) and texts are imperfect speech recognition\\ntranscripts (textual noise). To deal with text-side noise, we propose a VQVAE\\nbased heuristic method to compensate erroneous linguistic feature with phonetic\\ninformation learned directly from speech. As for the speech-side noise, we\\npropose to learn a noise-independent feature in the auto-regressive decoder\\nthrough adversarial training and data augmentation, which does not need an\\nextra speech enhancement model. Experiments show the effectiveness of the\\nproposed approach in dealing with text-side and speech-side noise. Surpassing\\nthe denoising approach based on a state-of-the-art speech enhancement model,\\nour system built on noisy found data can synthesize clean and high-quality\\nspeech with MOS close to the system built on the clean counterpart.\\n",
      "doi": "https://doi.org/10.1109/lsp.2020.3025410",
      "openalex_id": "https://openalex.org/W3022057435",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introducing meta-analysis in the evaluation of computational models of infant language development",
      "summary": "Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modellers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large-scale cumulative empirical data from infants, as quantified by meta-analyses conducted across a large number of individual behavioural studies. We formalise the connection between measurable model and human behaviour, and then present a conceptual framework for meta-analytic evaluation of computational models. We exemplify the meta-analytic model evaluation approach with two modelling experiments on infant-directed speech preference and native/non-native vowel discrimination.",
      "abstract": "Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modellers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large-scale cumulative empirical data from infants, as quantified by meta-analyses conducted across a large number of individual behavioural studies. We formalise the connection between measurable model and human behaviour, and then present a conceptual framework for meta-analytic evaluation of computational models. We exemplify the meta-analytic model evaluation approach with two modelling experiments on infant-directed speech preference and native/non-native vowel discrimination.",
      "doi": "https://doi.org/10.31234/osf.io/yjz5a",
      "openalex_id": "https://openalex.org/W4200598898",
      "arxiv_id": "",
      "publication_date": "2021-12-14",
      "published": "2021-12-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introducing Meta‐analysis in the Evaluation of Computational Models of Infant Language Development",
      "summary": "Abstract Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large‐scale cumulative empirical data from infants, as quantified by meta‐analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta‐analytic evaluation of computational models. We exemplify the meta‐analytic model evaluation approach with two modeling experiments on infant‐directed speech preference and native/non‐native vowel discrimination.",
      "abstract": "Abstract Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large‐scale cumulative empirical data from infants, as quantified by meta‐analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta‐analytic evaluation of computational models. We exemplify the meta‐analytic model evaluation approach with two modeling experiments on infant‐directed speech preference and native/non‐native vowel discrimination.",
      "doi": "https://doi.org/10.1111/cogs.13307",
      "openalex_id": "https://openalex.org/W4382918397",
      "arxiv_id": "",
      "publication_date": "2023-07-01",
      "published": "2023-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
      "summary": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are not available for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady-state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs.",
      "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are not available for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady-state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2336",
      "openalex_id": "https://openalex.org/W2972964185",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
      "summary": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "abstract": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1671",
      "openalex_id": "https://openalex.org/W3093121832",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Camp: A Two-Stage Approach to Modelling Prosody in Context",
      "summary": "Prosody is an integral part of communication, but remains an open problem in state-of-the-art speech synthesis. There are two major issues faced when modelling prosody: (1) prosody varies at a slower rate compared with other content in the acoustic signal (e.g. segmental information and background noise); (2) determining appropriate prosody without sufficient context is an ill-posed problem. In this paper, we propose solutions to both these issues. To mitigate the challenge of modelling a slow-varying signal, we learn to disentangle prosodic information using a word level representation. To alleviate the ill-posed nature of prosody modelling, we use syntactic and semantic information derived from text to learn a context-dependent prior over our prosodic space. Our Context-Aware Model of Prosody (CAMP) outperforms the state-of-the-art technique, closing the gap with natural speech by 26%. We also find that replacing attention with a jointly-trained duration model improves prosody significantly.",
      "abstract": "Prosody is an integral part of communication, but remains an open problem in state-of-the-art speech synthesis. There are two major issues faced when modelling prosody: (1) prosody varies at a slower rate compared with other content in the acoustic signal (e.g. segmental information and background noise); (2) determining appropriate prosody without sufficient context is an ill-posed problem. In this paper, we propose solutions to both these issues. To mitigate the challenge of modelling a slow-varying signal, we learn to disentangle prosodic information using a word level representation. To alleviate the ill-posed nature of prosody modelling, we use syntactic and semantic information derived from text to learn a context-dependent prior over our prosodic space. Our Context-Aware Model of Prosody (CAMP) outperforms the state-of-the-art technique, closing the gap with natural speech by 26%. We also find that replacing attention with a jointly-trained duration model improves prosody significantly.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414413",
      "openalex_id": "https://openalex.org/W3097320728",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spatial position constraint for unsupervised learning of speech representations",
      "summary": "The success of supervised learning techniques for automatic speech processing does not always extend to problems with limited annotated speech. Unsupervised representation learning aims at utilizing unlabelled data to learn a transformation that makes speech easily distinguishable for classification tasks, whereby deep auto-encoder variants have been most successful in finding such representations. This paper proposes a novel mechanism to incorporate geometric position of speech samples within the global structure of an unlabelled feature set. Regression to the geometric position is also added as an additional constraint for the representation learning auto-encoder. The representation learnt by the proposed model has been evaluated over a supervised classification task for limited vocabulary keyword spotting, with the proposed representation outperforming the commonly used cepstral features by about 9% in terms of classification accuracy, despite using a limited amount of labels during supervision. Furthermore, a small keyword dataset has been collected for Kadazan, an indigenous, low-resourced Southeast Asian language. Analysis for the Kadazan dataset also confirms the superiority of the proposed representation for limited annotation. The results are significant as they confirm that the proposed method can learn unsupervised speech representations effectively for classification tasks with scarce labelled data.",
      "abstract": "The success of supervised learning techniques for automatic speech processing does not always extend to problems with limited annotated speech. Unsupervised representation learning aims at utilizing unlabelled data to learn a transformation that makes speech easily distinguishable for classification tasks, whereby deep auto-encoder variants have been most successful in finding such representations. This paper proposes a novel mechanism to incorporate geometric position of speech samples within the global structure of an unlabelled feature set. Regression to the geometric position is also added as an additional constraint for the representation learning auto-encoder. The representation learnt by the proposed model has been evaluated over a supervised classification task for limited vocabulary keyword spotting, with the proposed representation outperforming the commonly used cepstral features by about 9% in terms of classification accuracy, despite using a limited amount of labels during supervision. Furthermore, a small keyword dataset has been collected for Kadazan, an indigenous, low-resourced Southeast Asian language. Analysis for the Kadazan dataset also confirms the superiority of the proposed representation for limited annotation. The results are significant as they confirm that the proposed method can learn unsupervised speech representations effectively for classification tasks with scarce labelled data.",
      "doi": "https://doi.org/10.7717/peerj-cs.650",
      "openalex_id": "https://openalex.org/W3185249749",
      "arxiv_id": "",
      "publication_date": "2021-07-21",
      "published": "2021-07-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Virtual Phone Discovery for Speech Synthesis Without Text",
      "summary": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "abstract": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "doi": "https://doi.org/10.1109/globalsip45357.2019.8969412",
      "openalex_id": "https://openalex.org/W3003750857",
      "arxiv_id": "",
      "publication_date": "2019-11-01",
      "published": "2019-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Unsupervised Acoustic Word Embeddings using Speaker and Gender Information",
      "summary": "For many languages, there is little or no labelled speech data available for training speech processing models. In zero-resource settings where unlabelled speech audio is the only available resource, speech applications for search, discovery and indexing often need to compare speech segments of different durations. Acoustic word embeddings are fixed dimensional representations of variable length speech sequences, allowing for efficient comparisons. Unsupervised acoustic word embedding models often still retain nuisance factors such as a speaker's identity and gender. Here we investigate how to improve the invariance of unsupervised acoustic embeddings to speaker and gender characteristics. We assume that speaker and gender labels are available for the untranscribed training data. We then consider two different methods for normalising out these factors: speaker and gender conditioning, and adversarial training. We apply both methods to two unsupervised embedding models: a recurrent neural network (RNN) autoencoder and a RNN correspondence autoencoder. In a word discrimination task, we find little benefit by explicitly normalising the embeddings to speaker and gender on English data. But on Xitsonga, substantial improvements are achieved. We speculate that this is due to the higher number of speakers present in the unlabelled Xitsonga training data.",
      "abstract": "For many languages, there is little or no labelled speech data available for training speech processing models. In zero-resource settings where unlabelled speech audio is the only available resource, speech applications for search, discovery and indexing often need to compare speech segments of different durations. Acoustic word embeddings are fixed dimensional representations of variable length speech sequences, allowing for efficient comparisons. Unsupervised acoustic word embedding models often still retain nuisance factors such as a speaker's identity and gender. Here we investigate how to improve the invariance of unsupervised acoustic embeddings to speaker and gender characteristics. We assume that speaker and gender labels are available for the untranscribed training data. We then consider two different methods for normalising out these factors: speaker and gender conditioning, and adversarial training. We apply both methods to two unsupervised embedding models: a recurrent neural network (RNN) autoencoder and a RNN correspondence autoencoder. In a word discrimination task, we find little benefit by explicitly normalising the embeddings to speaker and gender on English data. But on Xitsonga, substantial improvements are achieved. We speculate that this is due to the higher number of speakers present in the unlabelled Xitsonga training data.",
      "doi": "https://doi.org/10.1109/saupec/robmech/prasa48453.2020.9040986",
      "openalex_id": "https://openalex.org/W3011209123",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Slowness Regularized Contrastive Predictive Coding for Acoustic Unit Discovery",
      "summary": "Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.",
      "abstract": "Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.",
      "doi": "https://doi.org/10.1109/taslp.2024.3350888",
      "openalex_id": "https://openalex.org/W4390887450",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery",
      "summary": "Unsupervised spoken term discovery consists of two tasks: finding the acoustic segment boundaries and labeling acoustically similar segments with the same labels. We perform segmentation based on the assumption that the frame feature vectors are more similar within a segment than across the segments. Therefore, for strong segmentation performance, it is crucial that the features represent the phonetic properties of a frame more than other factors of variability. We achieve this via a self-expressing autoencoder framework. It consists of a single encoder and two decoders with shared weights. The encoder projects the input features into a latent representation. One of the decoders tries to reconstruct the input from these latent representations and the other from the self-expressed version of them. We use the obtained features to segment and cluster the speech data. We evaluate the performance of the proposed method in the Zero Resource 2020 challenge unit discovery task. The proposed system consistently outperforms the baseline, demonstrating the usefulness of the method in learning representations.",
      "abstract": "Unsupervised spoken term discovery consists of two tasks: finding the acoustic segment boundaries and labeling acoustically similar segments with the same labels. We perform segmentation based on the assumption that the frame feature vectors are more similar within a segment than across the segments. Therefore, for strong segmentation performance, it is crucial that the features represent the phonetic properties of a frame more than other factors of variability. We achieve this via a self-expressing autoencoder framework. It consists of a single encoder and two decoders with shared weights. The encoder projects the input features into a latent representation. One of the decoders tries to reconstruct the input from these latent representations and the other from the self-expressed version of them. We use the obtained features to segment and cluster the speech data. We evaluate the performance of the proposed method in the Zero Resource 2020 challenge unit discovery task. The proposed system consistently outperforms the baseline, demonstrating the usefulness of the method in learning representations.",
      "doi": "https://doi.org/10.21437/interspeech.2020-3000",
      "openalex_id": "https://openalex.org/W3044386551",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Operation LiLi: Using Crowd-Sourced Data and Automatic Alignment to Investigate the Phonetics and Phonology of Less-Resourced Languages",
      "summary": "Less-resourced languages are usually left out of phonetic studies based on large corpora. We contribute to the recent efforts to fill this gap by assessing how to use open-access, crowd-sourced audio data from Lingua Libre for phonetic research. Lingua Libre is a participative linguistic library developed by Wikimedia France in 2015. It contains more than 670k recordings in approximately 150 languages across nearly 740 speakers. As a proof of concept, we consider the Inventory Size Hypothesis, which predicts that, in a given system, variation in the realization of each vowel will be inversely related to the number of vowel categories. We investigate data from 10 languages with various numbers of vowel categories, i.e., German, Afrikaans, French, Catalan, Italian, Romanian, Polish, Russian, Spanish, and Basque. Audio files are extracted from Lingua Libre to be aligned and segmented using the Munich Automatic Segmentation System. Information on the formants of the vowel segments is then extracted to measure how vowels expand in the acoustic space and whether this is correlated with the number of vowel categories in the language. The results provide valuable insight into the question of vowel dispersion and demonstrate the wealth of information that crowd-sourced data has to offer.",
      "abstract": "Less-resourced languages are usually left out of phonetic studies based on large corpora. We contribute to the recent efforts to fill this gap by assessing how to use open-access, crowd-sourced audio data from Lingua Libre for phonetic research. Lingua Libre is a participative linguistic library developed by Wikimedia France in 2015. It contains more than 670k recordings in approximately 150 languages across nearly 740 speakers. As a proof of concept, we consider the Inventory Size Hypothesis, which predicts that, in a given system, variation in the realization of each vowel will be inversely related to the number of vowel categories. We investigate data from 10 languages with various numbers of vowel categories, i.e., German, Afrikaans, French, Catalan, Italian, Romanian, Polish, Russian, Spanish, and Basque. Audio files are extracted from Lingua Libre to be aligned and segmented using the Munich Automatic Segmentation System. Information on the formants of the vowel segments is then extracted to measure how vowels expand in the acoustic space and whether this is correlated with the number of vowel categories in the language. The results provide valuable insight into the question of vowel dispersion and demonstrate the wealth of information that crowd-sourced data has to offer.",
      "doi": "https://doi.org/10.3390/languages7030234",
      "openalex_id": "https://openalex.org/W4295977671",
      "arxiv_id": "",
      "publication_date": "2022-09-08",
      "published": "2022-09-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning",
      "summary": "More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.",
      "abstract": "More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.",
      "doi": "https://doi.org/10.48550/arxiv.2006.02814",
      "openalex_id": "https://openalex.org/W3032892481",
      "arxiv_id": "",
      "publication_date": "2020-06-04",
      "published": "2020-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Acoustic Word Embeddings for Zero-Resource Languages Using Self-Supervised Contrastive Learning and Multilingual Adaptation",
      "summary": "Acoustic word embeddings (AWEs) are fixed-dimensional representations of variable-length speech segments. For zero-resource languages where labelled data is not available, one AWE approach is to use unsupervised autoencoder-based recurrent models. Another recent approach is to use multilingual transfer: a supervised AWE model is trained on several well-resourced languages and then applied to an unseen zero-resource language. We consider how a recent contrastive learning loss can be used in both the purely unsupervised and multilingual transfer settings. Firstly, we show that terms from an unsupervised term discovery system can be used for contrastive self-supervision, resulting in improvements over previous unsupervised monolingual AWE models. Secondly, we consider how multilingual AWE models can be adapted to a specific zero-resource language using discovered terms. We find that self-supervised contrastive adaptation outperforms adapted multilingual correspondence autoencoder and Siamese AWE models, giving the best overall results in a word discrimination task on six zero-resource languages.",
      "abstract": "Acoustic word embeddings (AWEs) are fixed-dimensional representations of variable-length speech segments. For zero-resource languages where labelled data is not available, one AWE approach is to use unsupervised autoencoder-based recurrent models. Another recent approach is to use multilingual transfer: a supervised AWE model is trained on several well-resourced languages and then applied to an unseen zero-resource language. We consider how a recent contrastive learning loss can be used in both the purely unsupervised and multilingual transfer settings. Firstly, we show that terms from an unsupervised term discovery system can be used for contrastive self-supervision, resulting in improvements over previous unsupervised monolingual AWE models. Secondly, we consider how multilingual AWE models can be adapted to a specific zero-resource language using discovered terms. We find that self-supervised contrastive adaptation outperforms adapted multilingual correspondence autoencoder and Siamese AWE models, giving the best overall results in a word discrimination task on six zero-resource languages.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383594",
      "openalex_id": "https://openalex.org/W3139534224",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery",
      "summary": "Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.",
      "abstract": "Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.",
      "doi": "https://doi.org/10.48550/arxiv.2105.01786",
      "openalex_id": "https://openalex.org/W3158457675",
      "arxiv_id": "",
      "publication_date": "2021-05-04",
      "published": "2021-05-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Isotropy Analysis for Self-Supervised Acoustic Unit Embeddings on the Zero Resource Speech Challenge 2021 Framework",
      "summary": "In recent years, self-supervised representation learning has gained much attention for its proven advantages in many downstream tasks. Consequently, various self-supervised representation learning methods have been developed. However, few studies have investigated the resulting embedding space or analyzed why any particular approach performs better than any other. Here, we are interested in investigating the geometry in terms of the isotropy of embedding spaces learned by self-supervised speech representation, which can influence the performance in discriminating acoustic units on the Zero Resource Speech Challenge 2021 (ZR2021) framework. Most top systems from the published ZR2021 results are based on the contrastive predictive coding (CPC) technique. Here, we propose using hidden-unit BERT (HuBERT) self-supervised representation learning, and we provide detailed analyses and comparisons of their isotropies of embedding space, which might influence performance. Furthermore, we use simple yet effective feature fusion techniques to combine both models' strengths, leading to the ability to reduce the ABX error rate and outperform top models in the ZR2021 dev-other dataset.",
      "abstract": "In recent years, self-supervised representation learning has gained much attention for its proven advantages in many downstream tasks. Consequently, various self-supervised representation learning methods have been developed. However, few studies have investigated the resulting embedding space or analyzed why any particular approach performs better than any other. Here, we are interested in investigating the geometry in terms of the isotropy of embedding spaces learned by self-supervised speech representation, which can influence the performance in discriminating acoustic units on the Zero Resource Speech Challenge 2021 (ZR2021) framework. Most top systems from the published ZR2021 results are based on the contrastive predictive coding (CPC) technique. Here, we propose using hidden-unit BERT (HuBERT) self-supervised representation learning, and we provide detailed analyses and comparisons of their isotropies of embedding space, which might influence performance. Furthermore, we use simple yet effective feature fusion techniques to combine both models' strengths, leading to the ability to reduce the ABX error rate and outperform top models in the ZR2021 dev-other dataset.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095119",
      "openalex_id": "https://openalex.org/W4372260421",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Speech with Prosodic Prominence based on SSL-Visually Grounded Models",
      "summary": "Despite many existing works that address expressive speech synthesis with a desired prosody, few have focused on generating speech with prosody prominence. Most previous studies addressing this issue generate speech from given text labels with a contrastive focus emphasizing a specific word. In contrast, this paper investigates whether we can control prosody based on the contrastive focus that appears in images. Given an image and its caption, our system first discovers spoken terms associated with objects or situations in natural images based on a self-supervised visually grounded model. Then it generates speech with prosody prominence based on the contrastive focus of these spoken terms in a way that best describes the images. The framework can perform the task with/without text annotation, making it applicable for untranscribed, unsegmented speech utterances in unknown languages.",
      "abstract": "Despite many existing works that address expressive speech synthesis with a desired prosody, few have focused on generating speech with prosody prominence. Most previous studies addressing this issue generate speech from given text labels with a contrastive focus emphasizing a specific word. In contrast, this paper investigates whether we can control prosody based on the contrastive focus that appears in images. Given an image and its caption, our system first discovers spoken terms associated with objects or situations in natural images based on a self-supervised visually grounded model. Then it generates speech with prosody prominence based on the contrastive focus of these spoken terms in a way that best describes the images. The framework can perform the task with/without text annotation, making it applicable for untranscribed, unsegmented speech utterances in unknown languages.",
      "doi": "https://doi.org/10.1109/o-cocosda60357.2023.10482965",
      "openalex_id": "https://openalex.org/W4393656336",
      "arxiv_id": "",
      "publication_date": "2023-12-04",
      "published": "2023-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "nnAudio: An on-the-Fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolutional Neural Networks",
      "summary": "In this paper, we present nnAudio, a new neural network-based audio processing framework with graphics processing unit (GPU) support that leverages 1D convolutional neural networks to perform time domain to frequency domain conversion. It allows on-the-fly spectrogram extraction due to its fast speed, without the need to store any spectrograms on the disk. Moreover, this approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, and hence, the transformation process can be made trainable, further optimizing the waveform-to-spectrogram transformation for the specific task that the neural network is trained on. All spectrogram implementations scale as Big-O of linear time with respect to the input length. nnAudio, however, leverages the compute unified device architecture (CUDA) of 1D convolutional neural network from PyTorch, its short-time Fourier transform (STFT), Mel spectrogram, and constant-Q transform (CQT) implementations are an order of magnitude faster than other implementations using only the central processing unit (CPU). We tested our framework on three different machines with NVIDIA GPUs, and our framework significantly reduces the spectrogram extraction time from the order of seconds (using a popular python library librosa) to the order of milliseconds, given that the audio recordings are of the same length. When applying nnAudio to variable input audio lengths, an average of 11.5 hours are required to extract 34 spectrogram types with different parameters from the MusicNet dataset using librosa. An average of 2.8 hours is required for nnAudio, which is still four times faster than librosa. Our proposed framework also outperforms existing GPU processing libraries such as Kapre and torchaudio in terms of processing speed.",
      "abstract": "In this paper, we present nnAudio, a new neural network-based audio processing framework with graphics processing unit (GPU) support that leverages 1D convolutional neural networks to perform time domain to frequency domain conversion. It allows on-the-fly spectrogram extraction due to its fast speed, without the need to store any spectrograms on the disk. Moreover, this approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, and hence, the transformation process can be made trainable, further optimizing the waveform-to-spectrogram transformation for the specific task that the neural network is trained on. All spectrogram implementations scale as Big-O of linear time with respect to the input length. nnAudio, however, leverages the compute unified device architecture (CUDA) of 1D convolutional neural network from PyTorch, its short-time Fourier transform (STFT), Mel spectrogram, and constant-Q transform (CQT) implementations are an order of magnitude faster than other implementations using only the central processing unit (CPU). We tested our framework on three different machines with NVIDIA GPUs, and our framework significantly reduces the spectrogram extraction time from the order of seconds (using a popular python library librosa) to the order of milliseconds, given that the audio recordings are of the same length. When applying nnAudio to variable input audio lengths, an average of 11.5 hours are required to extract 34 spectrogram types with different parameters from the MusicNet dataset using librosa. An average of 2.8 hours is required for nnAudio, which is still four times faster than librosa. Our proposed framework also outperforms existing GPU processing libraries such as Kapre and torchaudio in terms of processing speed.",
      "doi": "https://doi.org/10.1109/access.2020.3019084",
      "openalex_id": "https://openalex.org/W3081424945",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We Are",
      "summary": "Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.",
      "abstract": "Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.661",
      "openalex_id": "https://openalex.org/W3017258074",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analysis of Layer-Wise Training in Direct Speech to Speech Translation Using BI-LSTM",
      "summary": "Speech-to-speech translation (S2ST) is the process of translation of speech from one language to another. Traditional S2ST systems follow a cascaded approach, where three modules automatics speech recognition (ASR), machine translation (MT), and text-to-speech translation (TTS) are concatenated to obtain the final translated speech utterance. The cascaded nature of the system results in the propagation of errors from one module to another. This, in turn, leads to the degradation in the overall performance of the S2ST task. With the evolution of the deep learning approaches to speech processing, many attempts have been made to perform end-to-end and direct speech-to-speech translation (DS2ST). But most of these approaches rely on language transcripts in one way or the other. In this work, we aim to perform the DS2ST task without using language transcripts. In this direction we have performed three experiments: First, we have investigated the direct learning of mapping function from source to target language with the increase in the number of utterances. Second, we have analyzed how the learning function improves with an increase in the number of Bi-LSTM layers. Third, we have observed how the system behaves with the unknown speakers (not used during training) during inference. From the experiments, it has been observed that with the increase in the number of utterances and layers, the quality of translation improves. And also, with a speaker and text-dependent training of approximately 4.4 hrs of speech, the model can generate the target language utterance even for unknown speakers. Though the generated utterance quality is not that good, but intelligent to some extent to be perceived.",
      "abstract": "Speech-to-speech translation (S2ST) is the process of translation of speech from one language to another. Traditional S2ST systems follow a cascaded approach, where three modules automatics speech recognition (ASR), machine translation (MT), and text-to-speech translation (TTS) are concatenated to obtain the final translated speech utterance. The cascaded nature of the system results in the propagation of errors from one module to another. This, in turn, leads to the degradation in the overall performance of the S2ST task. With the evolution of the deep learning approaches to speech processing, many attempts have been made to perform end-to-end and direct speech-to-speech translation (DS2ST). But most of these approaches rely on language transcripts in one way or the other. In this work, we aim to perform the DS2ST task without using language transcripts. In this direction we have performed three experiments: First, we have investigated the direct learning of mapping function from source to target language with the increase in the number of utterances. Second, we have analyzed how the learning function improves with an increase in the number of Bi-LSTM layers. Third, we have observed how the system behaves with the unknown speakers (not used during training) during inference. From the experiments, it has been observed that with the increase in the number of utterances and layers, the quality of translation improves. And also, with a speaker and text-dependent training of approximately 4.4 hrs of speech, the model can generate the target language utterance even for unknown speakers. Though the generated utterance quality is not that good, but intelligent to some extent to be perceived.",
      "doi": "https://doi.org/10.1109/o-cocosda202257103.2022.9997945",
      "openalex_id": "https://openalex.org/W4313203218",
      "arxiv_id": "",
      "publication_date": "2022-11-01",
      "published": "2022-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Faster Approach For Direct Speech to Speech Translation",
      "summary": "As the world is pacing towards globalization, the demand for automatic language translators is increasing rapidly. Traditional translation systems consist of multiple steps like speech recognition, text to text machine translation, and speech generation. Issue with these systems are, latency due to multiple steps and error propagation from first steps toward last steps. Another challenge is that many spoken languages do not have text representation, so traditional system involving speech to text and text to text translation do not work. In this paper, we are presenting a recurrent neural network (RNN) based translation system that can generate a direct waveform of target language audio. We have used the sparse coding technique for the extraction and inversion of audio features. An attention-based multi-layered sequence to sequence model is trained using a novel technique on a dataset of Spanish to English audio and no intermediate text representation is used while training or inference. We have done performance comparison of proposed approaches using latency, bilingual evaluation understudy (BLEU) score and Perceptual Evaluation of Speech Quality PESQ score analysis. The resulting system provides a very fast translation with good translation accuracy and audio quality.",
      "abstract": "As the world is pacing towards globalization, the demand for automatic language translators is increasing rapidly. Traditional translation systems consist of multiple steps like speech recognition, text to text machine translation, and speech generation. Issue with these systems are, latency due to multiple steps and error propagation from first steps toward last steps. Another challenge is that many spoken languages do not have text representation, so traditional system involving speech to text and text to text translation do not work. In this paper, we are presenting a recurrent neural network (RNN) based translation system that can generate a direct waveform of target language audio. We have used the sparse coding technique for the extraction and inversion of audio features. An attention-based multi-layered sequence to sequence model is trained using a novel technique on a dataset of Spanish to English audio and no intermediate text representation is used while training or inference. We have done performance comparison of proposed approaches using latency, bilingual evaluation understudy (BLEU) score and Perceptual Evaluation of Speech Quality PESQ score analysis. The resulting system provides a very fast translation with good translation accuracy and audio quality.",
      "doi": "https://doi.org/10.1109/wintechcon55229.2022.9832314",
      "openalex_id": "https://openalex.org/W4287848452",
      "arxiv_id": "",
      "publication_date": "2022-06-02",
      "published": "2022-06-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Investigation Of Data Augmentation Techniques For Bi-LSTM Based Direct Speech To Speech Translation",
      "summary": "Direct speech-to-speech translation (DS2ST) system translates the speech in the source language directly to the speech in the target language. It has been shown in the literature that deep learning systems trained using parallel datasets have given a good translation of the speech. However, getting large datasets to train deep learning networks for DS2ST tasks extensively is not easy. Also, the parallel data might not capture the variabilities like session, gender, speaker, and domain variation that might be present in the real-world dataset. In this work, we explore the data-augmentation techniques such that the pool of the training data can be increased and the DS2ST task can be generalized for all the variations. This work uses noise injection, speed perturbation, pitch perturbation, and vocal tract modification-based data-augmentation approaches as an initial attempt. From the experimental results, it has been found that these augmentation approaches improve the performance of the DS2ST system when compared with the clean/original data. Mel-cepstral distortion (MCD) and intelligibility score (IS) are used as metrics to compare the translated speech with the target language speech. Among the augmentation approaches explored, speed perturbation provides the best improvement of 6.125 in terms of MCD. Vocal tract modification improves the performance of the speaker variability in the dataset. This study shows the robustness of the DS2ST system trained on augmented data.",
      "abstract": "Direct speech-to-speech translation (DS2ST) system translates the speech in the source language directly to the speech in the target language. It has been shown in the literature that deep learning systems trained using parallel datasets have given a good translation of the speech. However, getting large datasets to train deep learning networks for DS2ST tasks extensively is not easy. Also, the parallel data might not capture the variabilities like session, gender, speaker, and domain variation that might be present in the real-world dataset. In this work, we explore the data-augmentation techniques such that the pool of the training data can be increased and the DS2ST task can be generalized for all the variations. This work uses noise injection, speed perturbation, pitch perturbation, and vocal tract modification-based data-augmentation approaches as an initial attempt. From the experimental results, it has been found that these augmentation approaches improve the performance of the DS2ST system when compared with the clean/original data. Mel-cepstral distortion (MCD) and intelligibility score (IS) are used as metrics to compare the translated speech with the target language speech. Among the augmentation approaches explored, speed perturbation provides the best improvement of 6.125 in terms of MCD. Vocal tract modification improves the performance of the speaker variability in the dataset. This study shows the robustness of the DS2ST system trained on augmented data.",
      "doi": "https://doi.org/10.1109/ncc56989.2023.10067896",
      "openalex_id": "https://openalex.org/W4328054436",
      "arxiv_id": "",
      "publication_date": "2023-02-23",
      "published": "2023-02-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using Machine Learning for Speech Extraction and Translation: HiTEK Languages",
      "summary": "Speech processing deals with retrieving and vocalizing conversational words/sentences i.e. articulatory phonetics, manner of articulation, place of articulation, articulatory gestures, articulatory phonology, articulatory speech recognition, and articulatory synthesis from Multilingual Source to Target language. The research focuses on multilingual speech recording in a single utterance and translation to a target language. Greedy method is used for fetching speech from the user. It consists of the grammatical structures of the speech in the dictionary using cohesion based method for term similarity. It translates speech to text then maps text to a set of phones resulting in target language speech. Multilingual Supervised Speech Dictionary is built for speech to speech translation, currently consisting of four languages Hindi, Telugu, English and Kannada with 100 phones for each language.",
      "abstract": "Speech processing deals with retrieving and vocalizing conversational words/sentences i.e. articulatory phonetics, manner of articulation, place of articulation, articulatory gestures, articulatory phonology, articulatory speech recognition, and articulatory synthesis from Multilingual Source to Target language. The research focuses on multilingual speech recording in a single utterance and translation to a target language. Greedy method is used for fetching speech from the user. It consists of the grammatical structures of the speech in the dictionary using cohesion based method for term similarity. It translates speech to text then maps text to a set of phones resulting in target language speech. Multilingual Supervised Speech Dictionary is built for speech to speech translation, currently consisting of four languages Hindi, Telugu, English and Kannada with 100 phones for each language.",
      "doi": "https://doi.org/10.23919/indiacom54597.2022.9763300",
      "openalex_id": "https://openalex.org/W4225319245",
      "arxiv_id": "",
      "publication_date": "2022-03-23",
      "published": "2022-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Translatotron 2: Robust direct speech-to-speech translation.",
      "summary": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.",
      "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3186843219",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Direct Speech-to-Speech Neural Network Methodology for Spanish-English Translation",
      "summary": "In this work, a novel direct speech-to-speech methodology for translation is proposed; it is based on an LSTMneural network structure which has proven useful for translation in the classical way, i.e., the one consistingof three stages: speech-to-text conversion, text-to-text translation, and text-to-speech synthesis. In contrastwith traditional approaches, the one in this work belongs to the recently appeared idea of direct translationwithout text representation, as this sort of training better corresponds to the way oral language learning takesplace in humans. As a proof of concept digits are translated from an audio source in Spanish and pronouncedas an audio signal in English. Advantages and disadvantages of the proposal when compared with traditionalmethodologies are discussed.",
      "abstract": "In this work, a novel direct speech-to-speech methodology for translation is proposed; it is based on an LSTMneural network structure which has proven useful for translation in the classical way, i.e., the one consistingof three stages: speech-to-text conversion, text-to-text translation, and text-to-speech synthesis. In contrastwith traditional approaches, the one in this work belongs to the recently appeared idea of direct translationwithout text representation, as this sort of training better corresponds to the way oral language learning takesplace in humans. As a proof of concept digits are translated from an audio source in Spanish and pronouncedas an audio signal in English. Advantages and disadvantages of the proposal when compared with traditionalmethodologies are discussed.",
      "doi": "https://doi.org/10.4108/eai.13-7-2018.164109",
      "openalex_id": "https://openalex.org/W3019991683",
      "arxiv_id": "",
      "publication_date": "2018-07-13",
      "published": "2018-07-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Induction of Structured Phoneme Inventories",
      "summary": "This extended abstract surveying the work on phonological typology was prepared for \"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology\" to be held at EMNLP 2020.",
      "abstract": "This extended abstract surveying the work on phonological typology was prepared for \"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology\" to be held at EMNLP 2020.",
      "doi": "https://doi.org/10.48550/arxiv.2010.05959",
      "openalex_id": "https://openalex.org/W3092674239",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings",
      "summary": "Documenting languages helps to prevent the extinction of endangered dialects, many of which are otherwise expected to disappear by the end of the century. When documenting oral languages, unsupervised word segmentation (UWS) from speech is a useful, yet challenging, task. It consists in producing time-stamps for slicing utterances into smaller segments corresponding to words, being performed from phonetic transcriptions, or in the absence of these, from the output of unsupervised speech discretization models. These discretization models are trained using raw speech only, producing discrete speech units that can be applied for downstream (text-based) tasks. In this paper we compare five of these models: three Bayesian and two neural approaches, with regards to the exploitability of the produced units for UWS. For the UWS task, we experiment with two models, using as our target language the Mboshi (Bantu C25), an unwritten language from Congo-Brazzaville. Additionally, we report results for Finnish, Hungarian, Romanian and Russian in equally low-resource settings, using only 4 hours of speech. Our results suggest that neural models for speech discretization are difficult to exploit in our setting, and that it might be necessary to adapt them to limit sequence length. We obtain our best UWS results by using Bayesian models that produce high quality, yet compressed, discrete representations of the input speech signal.",
      "abstract": "Documenting languages helps to prevent the extinction of endangered dialects, many of which are otherwise expected to disappear by the end of the century. When documenting oral languages, unsupervised word segmentation (UWS) from speech is a useful, yet challenging, task. It consists in producing time-stamps for slicing utterances into smaller segments corresponding to words, being performed from phonetic transcriptions, or in the absence of these, from the output of unsupervised speech discretization models. These discretization models are trained using raw speech only, producing discrete speech units that can be applied for downstream (text-based) tasks. In this paper we compare five of these models: three Bayesian and two neural approaches, with regards to the exploitability of the produced units for UWS. For the UWS task, we experiment with two models, using as our target language the Mboshi (Bantu C25), an unwritten language from Congo-Brazzaville. Additionally, we report results for Finnish, Hungarian, Romanian and Russian in equally low-resource settings, using only 4 hours of speech. Our results suggest that neural models for speech discretization are difficult to exploit in our setting, and that it might be necessary to adapt them to limit sequence length. We obtain our best UWS results by using Bayesian models that produce high quality, yet compressed, discrete representations of the input speech signal.",
      "doi": "https://doi.org/10.48550/arxiv.2106.04298",
      "openalex_id": "https://openalex.org/W3170928308",
      "arxiv_id": "",
      "publication_date": "2021-06-08",
      "published": "2021-06-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparative Analysis of Direct Speech-to-Speech Translation and Voice Conversion Using Bi-LSTM",
      "summary": "This study aims to conduct a comparative analysis between Direct Speech to Speech Translation (DS2ST) and Voice Conversion (VC) utilizing a Bi-Directional Long Short-Term Memory (Bi-LSTM) network. The process in VC entails mapping speaker-specific characteristics between the target and source speakers. On the other hand, DS2ST aims to map linguistic information of speech of two different spoken languages, which may be for the same speaker. Since both involve similar strategies and there is a rich literature on VC, it may be appropriate to do a comparative study between the VC and DS2ST frameworks. Accordingly, this work develops the VC and DS2ST model using the Bi-LSTM and analyzes the research. The findings reveal that VC can be performed with less number of layers compared to DS2ST using the Bi-LSTM model. This may be attributed to the differences among the speaker specific information and sound unit information. We have evaluated the system up to four layers and found that the MCD value improves from 7.484 to 6.061 in the case of DS2ST, while in the case of VC, it is from 5.549 to 5.370.",
      "abstract": "This study aims to conduct a comparative analysis between Direct Speech to Speech Translation (DS2ST) and Voice Conversion (VC) utilizing a Bi-Directional Long Short-Term Memory (Bi-LSTM) network. The process in VC entails mapping speaker-specific characteristics between the target and source speakers. On the other hand, DS2ST aims to map linguistic information of speech of two different spoken languages, which may be for the same speaker. Since both involve similar strategies and there is a rich literature on VC, it may be appropriate to do a comparative study between the VC and DS2ST frameworks. Accordingly, this work develops the VC and DS2ST model using the Bi-LSTM and analyzes the research. The findings reveal that VC can be performed with less number of layers compared to DS2ST using the Bi-LSTM model. This may be attributed to the differences among the speaker specific information and sound unit information. We have evaluated the system up to four layers and found that the MCD value improves from 7.484 to 6.061 in the case of DS2ST, while in the case of VC, it is from 5.549 to 5.370.",
      "doi": "https://doi.org/10.1109/o-cocosda60357.2023.10482964",
      "openalex_id": "https://openalex.org/W4393657127",
      "arxiv_id": "",
      "publication_date": "2023-12-04",
      "published": "2023-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Low-Resource Neural Machine Translation",
      "summary": "Neural approaches have achieved state-of-the-art accuracy on machine translation but suffer from the high cost of collecting large scale parallel data. Thus, a lot of research has been conducted for neural machine translation (NMT) with very limited parallel data, i.e., the low-resource setting. In this paper, we provide a survey for low-resource NMT and classify related works into three categories according to the auxiliary data they used: (1) exploiting monolingual data of source and/or target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.",
      "abstract": "Neural approaches have achieved state-of-the-art accuracy on machine translation but suffer from the high cost of collecting large scale parallel data. Thus, a lot of research has been conducted for neural machine translation (NMT) with very limited parallel data, i.e., the low-resource setting. In this paper, we provide a survey for low-resource NMT and classify related works into three categories according to the auxiliary data they used: (1) exploiting monolingual data of source and/or target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.",
      "doi": "https://doi.org/10.24963/ijcai.2021/629",
      "openalex_id": "https://openalex.org/W3193077216",
      "arxiv_id": "",
      "publication_date": "2021-08-01",
      "published": "2021-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Assessing Evaluation Metrics for Speech-to-Speech Translation",
      "summary": "Speech-to-speech translation combines machine translation with speech synthesis, introducing evaluation challenges not present in either task alone. How to automatically evaluate speech-to-speech translation is an open question which has not previously been explored. Translating to speech rather than to text is often motivated by unwritten languages or languages without standardized orthographies. However, we show that the previously used automatic metric for this task is best equipped for standardized high-resource languages only. In this work, we first evaluate current metrics for speech-to-speech translation, and second assess how translation to dialectal variants rather than to standardized languages impacts various evaluation methods.",
      "abstract": "Speech-to-speech translation combines machine translation with speech synthesis, introducing evaluation challenges not present in either task alone. How to automatically evaluate speech-to-speech translation is an open question which has not previously been explored. Translating to speech rather than to text is often motivated by unwritten languages or languages without standardized orthographies. However, we show that the previously used automatic metric for this task is best equipped for standardized high-resource languages only. In this work, we first evaluate current metrics for speech-to-speech translation, and second assess how translation to dialectal variants rather than to standardized languages impacts various evaluation methods.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688073",
      "openalex_id": "https://openalex.org/W3208643357",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Denoispeech: Denoising Text to Speech with Frame-Level Noise Modeling",
      "summary": "While neural-based text to speech (TTS) models can synthesize natural and intelligible voice, they usually require high-quality speech data, which is costly to collect. In many scenarios, only noisy speech of a target speaker is available, which presents challenges for TTS model training for this speaker. Previous works usually address the challenge using two methods: 1) training the TTS model using the speech denoised with an enhancement model; 2) taking a single noise embedding as input when training with noisy speech. However, they usually cannot handle speech with real-world complicated noise such as those with high variations along time. In this paper, we develop DenoiSpeech, a TTS system that can synthesize clean speech for a speaker with noisy speech data. In DenoiSpeech, we handle real-world noisy speech by modeling the fine-grained frame-level noise with a noise condition module, which is jointly trained with the TTS model. Experimental results on real-world data show that DenoiSpeech outperforms the previous two methods by 0.31 and 0.66 MOS respectively. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "While neural-based text to speech (TTS) models can synthesize natural and intelligible voice, they usually require high-quality speech data, which is costly to collect. In many scenarios, only noisy speech of a target speaker is available, which presents challenges for TTS model training for this speaker. Previous works usually address the challenge using two methods: 1) training the TTS model using the speech denoised with an enhancement model; 2) taking a single noise embedding as input when training with noisy speech. However, they usually cannot handle speech with real-world complicated noise such as those with high variations along time. In this paper, we develop DenoiSpeech, a TTS system that can synthesize clean speech for a speaker with noisy speech data. In DenoiSpeech, we handle real-world noisy speech by modeling the fine-grained frame-level noise with a noise condition module, which is jointly trained with the TTS model. Experimental results on real-world data show that DenoiSpeech outperforms the previous two methods by 0.31 and 0.66 MOS respectively. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9413934",
      "openalex_id": "https://openalex.org/W3163906773",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-granularity Knowledge Sharing in Low-resource Neural Machine Translation",
      "summary": "As the rapid development of deep learning methods, neural machine translation (NMT) has attracted more and more attention in recent years. However, lack of bilingual resources decreases the performance of the low-resource NMT model seriously. To overcome this problem, several studies put their efforts on knowledge transfer from high-resource language pairs to low-resource language pairs. However, these methods usually focus on one single granularity of language and the parameter sharing among different granularities in NMT is not well studied. In this article, we propose to improve the parameter sharing in low-resource NMT by introducing multi-granularity knowledge such as word, phrase and sentence. This knowledge can be monolingual and bilingual. We build the knowledge sharing model for low-resource NMT based on a multi-task learning framework, three auxiliary tasks such as syntax parsing, cross-lingual named entity recognition, and natural language generation are selected for the low-resource NMT. Experimental results show that the proposed method consistently outperforms six strong baseline systems on several low-resource language pairs.",
      "abstract": "As the rapid development of deep learning methods, neural machine translation (NMT) has attracted more and more attention in recent years. However, lack of bilingual resources decreases the performance of the low-resource NMT model seriously. To overcome this problem, several studies put their efforts on knowledge transfer from high-resource language pairs to low-resource language pairs. However, these methods usually focus on one single granularity of language and the parameter sharing among different granularities in NMT is not well studied. In this article, we propose to improve the parameter sharing in low-resource NMT by introducing multi-granularity knowledge such as word, phrase and sentence. This knowledge can be monolingual and bilingual. We build the knowledge sharing model for low-resource NMT based on a multi-task learning framework, three auxiliary tasks such as syntax parsing, cross-lingual named entity recognition, and natural language generation are selected for the low-resource NMT. Experimental results show that the proposed method consistently outperforms six strong baseline systems on several low-resource language pairs.",
      "doi": "https://doi.org/10.1145/3639930",
      "openalex_id": "https://openalex.org/W4390725317",
      "arxiv_id": "",
      "publication_date": "2024-01-09",
      "published": "2024-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription",
      "summary": "Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, so as to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.",
      "abstract": "Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, so as to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.",
      "doi": "https://doi.org/10.48550/arxiv.2109.07940",
      "openalex_id": "https://openalex.org/W3200345197",
      "arxiv_id": "",
      "publication_date": "2021-09-16",
      "published": "2021-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language",
      "summary": "Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.",
      "abstract": "Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01759",
      "openalex_id": "https://openalex.org/W4386057807",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes",
      "summary": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
      "abstract": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
      "doi": "https://doi.org/10.1109/ijcb57857.2023.10449102",
      "openalex_id": "https://openalex.org/W4392411961",
      "arxiv_id": "",
      "publication_date": "2023-09-25",
      "published": "2023-09-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration",
      "summary": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",
      "abstract": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",
      "doi": "https://doi.org/10.1109/wacv57701.2024.00508",
      "openalex_id": "https://openalex.org/W4394625601",
      "arxiv_id": "",
      "publication_date": "2024-01-03",
      "published": "2024-01-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Deep Vector Quantization Clustering Method for Polarimetric SAR Images",
      "summary": "Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.",
      "abstract": "Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.",
      "doi": "https://doi.org/10.3390/rs13112127",
      "openalex_id": "https://openalex.org/W3168380356",
      "arxiv_id": "",
      "publication_date": "2021-05-28",
      "published": "2021-05-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attributable and Scalable Opinion Summarization",
      "summary": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.",
      "abstract": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.473",
      "openalex_id": "https://openalex.org/W4385570501",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PCVAE: Generating Prior Context for Dialogue Response Generation",
      "summary": "Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.",
      "abstract": "Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.",
      "doi": "https://doi.org/10.24963/ijcai.2022/564",
      "openalex_id": "https://openalex.org/W4285599837",
      "arxiv_id": "",
      "publication_date": "2022-07-01",
      "published": "2022-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phase-Aware Speech Enhancement With Complex Wiener Filter",
      "summary": "In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.",
      "abstract": "In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.",
      "doi": "https://doi.org/10.1109/access.2023.3341919",
      "openalex_id": "https://openalex.org/W4389633906",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
      "summary": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.",
      "abstract": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.893",
      "openalex_id": "https://openalex.org/W4389524339",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AbNatiV: VQ-VAE-based assessment of antibody and nanobody nativeness for hit selection, humanisation, and engineering",
      "summary": "Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.",
      "abstract": "Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.",
      "doi": "https://doi.org/10.1101/2023.04.28.538712",
      "openalex_id": "https://openalex.org/W4367366153",
      "arxiv_id": "",
      "publication_date": "2023-04-29",
      "published": "2023-04-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tokenized and Continuous Embedding Compressions of Protein Sequence and Structure",
      "summary": "Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.",
      "abstract": "Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.",
      "doi": "https://doi.org/10.1101/2024.08.06.606920",
      "openalex_id": "https://openalex.org/W4401409675",
      "arxiv_id": "",
      "publication_date": "2024-08-08",
      "published": "2024-08-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "TAG2G: A Diffusion-Based Approach to Interlocutor-Aware Co-Speech Gesture Generation",
      "summary": "Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.",
      "abstract": "Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.",
      "doi": "https://doi.org/10.3390/electronics13173364",
      "openalex_id": "https://openalex.org/W4401892801",
      "arxiv_id": "",
      "publication_date": "2024-08-24",
      "published": "2024-08-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parallel Learning: Overview and Perspective for Computational Learning Across Syn2Real and Sim2Real",
      "summary": "The virtual-to-real paradigm, i.e., training models on virtual data and then applying them to solve real-world problems, has attracted more and more attention from various domains by successfully alleviating the data shortage problem in machine learning. To summarize the advances in recent years, this survey comprehensively reviews the literature, from the viewport of parallel intelligence. First, an extended parallel learning framework is proposed to cover main domains including computer vision, natural language processing, robotics, and autonomous driving. Second, a multi-dimensional taxonomy is designed to organize the literature in a hierarchical structure. Third, the related virtual-to-real works are analyzed and compared according to the three principles of parallel learning known as description, prediction, and prescription, which cover the methods for constructing virtual worlds, generating labeled data, domain transferring, model training and testing, as well as optimizing the strategies to guide the task-oriented data generator for better learning performance. Key issues remained in virtual-to-real are discussed. Furthermore, the future research directions from the viewpoint of parallel learning are suggested.",
      "abstract": "The virtual-to-real paradigm, i.e., training models on virtual data and then applying them to solve real-world problems, has attracted more and more attention from various domains by successfully alleviating the data shortage problem in machine learning. To summarize the advances in recent years, this survey comprehensively reviews the literature, from the viewport of parallel intelligence. First, an extended parallel learning framework is proposed to cover main domains including computer vision, natural language processing, robotics, and autonomous driving. Second, a multi-dimensional taxonomy is designed to organize the literature in a hierarchical structure. Third, the related virtual-to-real works are analyzed and compared according to the three principles of parallel learning known as description, prediction, and prescription, which cover the methods for constructing virtual worlds, generating labeled data, domain transferring, model training and testing, as well as optimizing the strategies to guide the task-oriented data generator for better learning performance. Key issues remained in virtual-to-real are discussed. Furthermore, the future research directions from the viewpoint of parallel learning are suggested.",
      "doi": "https://doi.org/10.1109/jas.2023.123375",
      "openalex_id": "https://openalex.org/W4322730856",
      "arxiv_id": "",
      "publication_date": "2023-03-01",
      "published": "2023-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Artificial Intelligence Approaches for Energetic Materials by Design: State of the Art, Challenges, and Future Directions",
      "summary": "Abstract Artificial intelligence (AI) is rapidly emerging as a enabling tool for solving complex materials design problems. This paper aims to review recent advances in AI‐driven materials‐by‐design and their applications to energetic materials (EM). Trained with data from numerical simulations and/or physical experiments, AI models can assimilate trends and patterns within the design parameter space, identify optimal material designs (micro‐morphologies, combinations of materials in composites, etc.), and point to designs with superior/targeted property and performance metrics. We review approaches focusing on such capabilities with respect to the three main stages of materials‐by‐design, namely representation learning of microstructure morphology (i. e., shape descriptors), structure‐property‐performance (S−P−P) linkage estimation, and optimization/design exploration. We leave out “process” as much work remains to be done to establish the connectivity between process and structure. We provide a perspective view of these methods in terms of their potential, practicality, and efficacy towards the realization of materials‐by‐design. Specifically, methods in the literature are evaluated in terms of their capacity to learn from a small/limited number of data, computational complexity, generalizability/scalability to other material species and operating conditions, interpretability of the model predictions, and the burden of supervision/data annotation. Finally, we suggest a few promising future research directions for EM materials‐by‐design, such as meta‐learning, active learning, Bayesian learning, and semi‐/weakly‐supervised learning, to bridge the gap between machine learning research and EM research.",
      "abstract": "Abstract Artificial intelligence (AI) is rapidly emerging as a enabling tool for solving complex materials design problems. This paper aims to review recent advances in AI‐driven materials‐by‐design and their applications to energetic materials (EM). Trained with data from numerical simulations and/or physical experiments, AI models can assimilate trends and patterns within the design parameter space, identify optimal material designs (micro‐morphologies, combinations of materials in composites, etc.), and point to designs with superior/targeted property and performance metrics. We review approaches focusing on such capabilities with respect to the three main stages of materials‐by‐design, namely representation learning of microstructure morphology (i. e., shape descriptors), structure‐property‐performance (S−P−P) linkage estimation, and optimization/design exploration. We leave out “process” as much work remains to be done to establish the connectivity between process and structure. We provide a perspective view of these methods in terms of their potential, practicality, and efficacy towards the realization of materials‐by‐design. Specifically, methods in the literature are evaluated in terms of their capacity to learn from a small/limited number of data, computational complexity, generalizability/scalability to other material species and operating conditions, interpretability of the model predictions, and the burden of supervision/data annotation. Finally, we suggest a few promising future research directions for EM materials‐by‐design, such as meta‐learning, active learning, Bayesian learning, and semi‐/weakly‐supervised learning, to bridge the gap between machine learning research and EM research.",
      "doi": "https://doi.org/10.1002/prep.202200276",
      "openalex_id": "https://openalex.org/W4321328070",
      "arxiv_id": "",
      "publication_date": "2023-02-18",
      "published": "2023-02-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration",
      "summary": "Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.",
      "abstract": "Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01802",
      "openalex_id": "https://openalex.org/W4386076005",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
      "summary": "We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.",
      "abstract": "We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1936",
      "openalex_id": "https://openalex.org/W3094247854",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Non-Intrusive Binaural Speech Intelligibility Prediction From Discrete Latent Representations",
      "summary": "Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.",
      "abstract": "Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.",
      "doi": "https://doi.org/10.1109/lsp.2022.3161115",
      "openalex_id": "https://openalex.org/W3217035088",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Whole brain Probabilistic Generative Model toward Realizing Cognitive Architecture for Developmental Robots",
      "summary": "Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.",
      "abstract": "Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3138466558",
      "arxiv_id": "",
      "publication_date": "2021-03-15",
      "published": "2021-03-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder",
      "summary": "In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.",
      "abstract": "In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.",
      "doi": "https://doi.org/10.48550/arxiv.2103.02858",
      "openalex_id": "https://openalex.org/W3134286091",
      "arxiv_id": "",
      "publication_date": "2021-03-04",
      "published": "2021-03-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Double Articulation Analyzer with Prosody for Unsupervised Word and Phoneme Discovery",
      "summary": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "abstract": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "doi": "https://doi.org/10.48550/arxiv.2103.08199",
      "openalex_id": "https://openalex.org/W3136576491",
      "arxiv_id": "",
      "publication_date": "2021-03-15",
      "published": "2021-03-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recent Advances in Synthesis and Interaction of Speech, Text, and Vision",
      "summary": "In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.",
      "abstract": "In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.",
      "doi": "https://doi.org/10.3390/electronics13091726",
      "openalex_id": "https://openalex.org/W4396510248",
      "arxiv_id": "",
      "publication_date": "2024-04-30",
      "published": "2024-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Visualtts: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over",
      "summary": "In this paper, we formulate a novel task to synthesize speech in sync with a silent pre-recorded video, denoted as automatic voice over (AVO). Unlike traditional speech synthesis, AVO seeks to generate not only human-sounding speech, but also perfect lip-speech synchronization. A natural solution to AVO is to condition the speech rendering on the temporal progression of lip sequence in the video. We propose a novel text-to-speech model that is conditioned on visual input, named VisualTTS, for accurate lip-speech synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1) textual-visual attention, and 2) visual fusion strategy during acoustic decoding, which both contribute to forming accurate alignment between the input text content and lip motion in input lip sequence. Experimental results show that VisualTTS achieves accurate lip-speech synchronization and outperforms all baseline systems.",
      "abstract": "In this paper, we formulate a novel task to synthesize speech in sync with a silent pre-recorded video, denoted as automatic voice over (AVO). Unlike traditional speech synthesis, AVO seeks to generate not only human-sounding speech, but also perfect lip-speech synchronization. A natural solution to AVO is to condition the speech rendering on the temporal progression of lip sequence in the video. We propose a novel text-to-speech model that is conditioned on visual input, named VisualTTS, for accurate lip-speech synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1) textual-visual attention, and 2) visual fusion strategy during acoustic decoding, which both contribute to forming accurate alignment between the input text content and lip motion in input lip sequence. Experimental results show that VisualTTS achieves accurate lip-speech synchronization and outperforms all baseline systems.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746421",
      "openalex_id": "https://openalex.org/W3204420730",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Common brain activity features discretization for predicting perceived speech quality",
      "summary": "The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.",
      "abstract": "The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.",
      "doi": "https://doi.org/10.1016/j.procs.2022.12.195",
      "openalex_id": "https://openalex.org/W4319997228",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Penerapan Optical Character Recognition (OCR) Dengan Text-To-Speech (TTS) dalam Konversi Gambar ke Suara",
      "summary": "Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning",
      "abstract": "Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning",
      "doi": "https://doi.org/10.24036/voteteknika.v11i4.125218",
      "openalex_id": "https://openalex.org/W4393389449",
      "arxiv_id": "",
      "publication_date": "2023-12-02",
      "published": "2023-12-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A high-performance neuroprosthesis for speech decoding and avatar control",
      "summary": "Speech neuroprostheses have the potential to restore communication to people living with paralysis, but naturalistic speed and expressivity are elusive<sup>1</sup>. Here we use high-density surface recordings of the speech cortex in a clinical-trial participant with severe limb and vocal paralysis to achieve high-performance real-time decoding across three complementary speech-related output modalities: text, speech audio and facial-avatar animation. We trained and evaluated deep-learning models using neural data collected as the participant attempted to silently speak sentences. For text, we demonstrate accurate and rapid large-vocabulary decoding with a median rate of 78 words per minute and median word error rate of 25%. For speech audio, we demonstrate intelligible and rapid speech synthesis and personalization to the participant's pre-injury voice. For facial-avatar animation, we demonstrate the control of virtual orofacial movements for speech and non-speech communicative gestures. The decoders reached high performance with less than two weeks of training. Our findings introduce a multimodal speech-neuroprosthetic approach that has substantial promise to restore full, embodied communication to people living with severe paralysis.",
      "abstract": "Speech neuroprostheses have the potential to restore communication to people living with paralysis, but naturalistic speed and expressivity are elusive<sup>1</sup>. Here we use high-density surface recordings of the speech cortex in a clinical-trial participant with severe limb and vocal paralysis to achieve high-performance real-time decoding across three complementary speech-related output modalities: text, speech audio and facial-avatar animation. We trained and evaluated deep-learning models using neural data collected as the participant attempted to silently speak sentences. For text, we demonstrate accurate and rapid large-vocabulary decoding with a median rate of 78 words per minute and median word error rate of 25%. For speech audio, we demonstrate intelligible and rapid speech synthesis and personalization to the participant's pre-injury voice. For facial-avatar animation, we demonstrate the control of virtual orofacial movements for speech and non-speech communicative gestures. The decoders reached high performance with less than two weeks of training. Our findings introduce a multimodal speech-neuroprosthetic approach that has substantial promise to restore full, embodied communication to people living with severe paralysis.",
      "doi": "https://doi.org/10.1038/s41586-023-06443-4",
      "openalex_id": "https://openalex.org/W4386100600",
      "arxiv_id": "",
      "publication_date": "2023-08-23",
      "published": "2023-08-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge",
      "summary": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
      "doi": "https://doi.org/10.1609/aaai.v38i18.29991",
      "openalex_id": "https://openalex.org/W4393160744",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What Do Children and Parents Want and Perceive in Conversational Agents? Towards Transparent, Trustworthy, Democratized Agents",
      "summary": "Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of \"ideal future agents\" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.",
      "abstract": "Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of \"ideal future agents\" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.",
      "doi": "https://doi.org/10.1145/3585088.3589353",
      "openalex_id": "https://openalex.org/W4362707416",
      "arxiv_id": "",
      "publication_date": "2023-06-14",
      "published": "2023-06-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "‘All possible sounds’: speech, music, and the emergence of machine listening",
      "summary": "\"Machine listening\" is one common term for a fast-growing interdisciplinary field of science and engineering that \"uses signal processing and machine learning to extract useful information from sound\". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards \"more fundamental problems of audition\". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader \"auditory scene\". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is \"to cover all possible sounds\". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.",
      "abstract": "\"Machine listening\" is one common term for a fast-growing interdisciplinary field of science and engineering that \"uses signal processing and machine learning to extract useful information from sound\". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards \"more fundamental problems of audition\". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader \"auditory scene\". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is \"to cover all possible sounds\". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.",
      "doi": "https://doi.org/10.1080/20551940.2023.2195057",
      "openalex_id": "https://openalex.org/W4363676804",
      "arxiv_id": "",
      "publication_date": "2023-04-10",
      "published": "2023-04-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
      "summary": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
      "abstract": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1182",
      "openalex_id": "https://openalex.org/W3190032417",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals",
      "summary": "Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.",
      "abstract": "Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.",
      "doi": "https://doi.org/10.1101/2024.03.11.584533",
      "openalex_id": "https://openalex.org/W4392791724",
      "arxiv_id": "",
      "publication_date": "2024-03-14",
      "published": "2024-03-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Inference in State-Space Models",
      "summary": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.",
      "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.",
      "doi": "https://doi.org/10.48550/arxiv.2107.13349",
      "openalex_id": "https://openalex.org/W3201728305",
      "arxiv_id": "",
      "publication_date": "2021-07-28",
      "published": "2021-07-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How BPE Affects Memorization in Transformers",
      "summary": "Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.",
      "abstract": "Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02782",
      "openalex_id": "https://openalex.org/W3204516855",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units",
      "summary": "Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.",
      "abstract": "Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.",
      "doi": "https://doi.org/10.48550/arxiv.2111.00610",
      "openalex_id": "https://openalex.org/W3210088932",
      "arxiv_id": "",
      "publication_date": "2021-10-31",
      "published": "2021-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Simulating Early Phonetic and Word Learning Without Linguistic Categories",
      "summary": "ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.",
      "abstract": "ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.",
      "doi": "https://doi.org/10.1111/desc.13606",
      "openalex_id": "https://openalex.org/W4406132022",
      "arxiv_id": "",
      "publication_date": "2025-01-06",
      "published": "2025-01-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Partial Fake Speech Attacks in the Real World Using Deepfake Audio",
      "summary": "Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.",
      "abstract": "Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.",
      "doi": "https://doi.org/10.3390/jcp5010006",
      "openalex_id": "https://openalex.org/W4407388754",
      "arxiv_id": "",
      "publication_date": "2025-02-08",
      "published": "2025-02-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Extracting Speaker and Emotion Information from Self-Supervised Speech Models via Channel-Wise Correlations",
      "summary": "Self-supervised learning of speech representations from large amounts of unlabeled data has enabled state-of-the-art results in several speech processing tasks. Aggregating these speech representations across time is typically approached by using descriptive statistics, and in particular, using the first - and second-order statistics of representation coefficients. In this paper, we examine an alternative way of extracting speaker and emotion information from self-supervised trained models, based on the correlations between the coefficients of the representations - correlation pooling. We show improvements over mean pooling and further gains when the pooling methods are combined via fusion. The code is available at github.com/Lamomal/s3prl_correlation.",
      "abstract": "Self-supervised learning of speech representations from large amounts of unlabeled data has enabled state-of-the-art results in several speech processing tasks. Aggregating these speech representations across time is typically approached by using descriptive statistics, and in particular, using the first - and second-order statistics of representation coefficients. In this paper, we examine an alternative way of extracting speaker and emotion information from self-supervised trained models, based on the correlations between the coefficients of the representations - correlation pooling. We show improvements over mean pooling and further gains when the pooling methods are combined via fusion. The code is available at github.com/Lamomal/s3prl_correlation.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023345",
      "openalex_id": "https://openalex.org/W4319862700",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SRAG: Speech Retrieval Augmented Generation for Spoken Language Understanding",
      "summary": "Retrieval augmented generation (RAG) has shown promise for enhancing natural language understanding (NLU) capabilities of large language models (LLMs) by retrieving relevant knowledge as prompts. Extending RAG to spoken language understanding (SLU) represents an important research direction. This paper proposes a RAG approach for improving SLU. First, the encoder of a pretrained automatic speech recognition model is utilized for speech retrieval over the training set. The corresponding texts and intent labels are then formulated as prompts to guide the SLU decoder. Furthermore, a prompt attention mechanism is introduced to strengthen the attention between generation and prompts. Experiments demonstrate that the proposed speech RAG approach substantially outperforms conventional end-to-end and cascaded SLU models in intent prediction from speech. This highlights the efficacy of leveraging retrieval-based prompting to incorporate external knowledge for advancing SLU.",
      "abstract": "Retrieval augmented generation (RAG) has shown promise for enhancing natural language understanding (NLU) capabilities of large language models (LLMs) by retrieving relevant knowledge as prompts. Extending RAG to spoken language understanding (SLU) represents an important research direction. This paper proposes a RAG approach for improving SLU. First, the encoder of a pretrained automatic speech recognition model is utilized for speech retrieval over the training set. The corresponding texts and intent labels are then formulated as prompts to guide the SLU decoder. Furthermore, a prompt attention mechanism is introduced to strengthen the attention between generation and prompts. Experiments demonstrate that the proposed speech RAG approach substantially outperforms conventional end-to-end and cascaded SLU models in intent prediction from speech. This highlights the efficacy of leveraging retrieval-based prompting to incorporate external knowledge for advancing SLU.",
      "doi": "https://doi.org/10.1109/iccect60629.2024.10546001",
      "openalex_id": "https://openalex.org/W4399427593",
      "arxiv_id": "",
      "publication_date": "2024-04-26",
      "published": "2024-04-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-Speaker Training and Adaptation for Electromyography-to-Speech Conversion",
      "summary": "Surface Electromyography (EMG) signals of articulatory muscles can be used to synthesize acoustic speech with Electromyography-to-Speech (ETS) models. Recent models have improved the synthesis quality by combining training data from multiple recordings of single speakers. In this work, we evaluated whether using recordings of multiple speakers also increases performance and if cross-speaker models can be adapted to unseen speakers with limited data. We recorded the EMG-Vox corpus, which consists of EMG and audio signals of four speakers with five sessions each. We compared cross-speaker models with single-speaker counterparts and conducted adaptation experiments. Cross-speaker models achieved on average significantly better performance than single-speaker models. Experiments with balanced data indicated that this improvement stemmed from a larger training set. Performing speaker adaptation from cross-speaker models showed higher synthesis quality than training from scratch and was at least on par with session adaptation for most speakers. To the best of our knowledge, this is the first work to report that cross-speaker ETS models yielded better results than single-speaker models.",
      "abstract": "Surface Electromyography (EMG) signals of articulatory muscles can be used to synthesize acoustic speech with Electromyography-to-Speech (ETS) models. Recent models have improved the synthesis quality by combining training data from multiple recordings of single speakers. In this work, we evaluated whether using recordings of multiple speakers also increases performance and if cross-speaker models can be adapted to unseen speakers with limited data. We recorded the EMG-Vox corpus, which consists of EMG and audio signals of four speakers with five sessions each. We compared cross-speaker models with single-speaker counterparts and conducted adaptation experiments. Cross-speaker models achieved on average significantly better performance than single-speaker models. Experiments with balanced data indicated that this improvement stemmed from a larger training set. Performing speaker adaptation from cross-speaker models showed higher synthesis quality than training from scratch and was at least on par with session adaptation for most speakers. To the best of our knowledge, this is the first work to report that cross-speaker ETS models yielded better results than single-speaker models.",
      "doi": "https://doi.org/10.1109/embc53108.2024.10781707",
      "openalex_id": "https://openalex.org/W4405490053",
      "arxiv_id": "",
      "publication_date": "2024-07-15",
      "published": "2024-07-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vocoder drift compensation by x-vector alignment in speaker anonymisation",
      "summary": "For the most popular x-vector-based approaches to speaker anonymisation, the bulk of the anonymisation can stem from vocoding rather than from the core anonymisation function which is used to substitute an original speaker x-vector with that of a fictitious pseudo-speaker.This phenomenon can impede the design of better anonymisation systems since there is a lack of fine-grained control over the x-vector space.The work reported in this paper explores the origin of so-called vocoder drift and shows that it is due to the mismatch between the substituted x-vector and the original representations of the linguistic content, intonation and prosody.Also reported is an original approach to vocoder drift compensation.While anonymisation performance degrades as expected, compensation reduces vocoder drift substantially, offers improved control over the x-vector space and lays a foundation for the design of better anonymisation functions in the future.",
      "abstract": "For the most popular x-vector-based approaches to speaker anonymisation, the bulk of the anonymisation can stem from vocoding rather than from the core anonymisation function which is used to substitute an original speaker x-vector with that of a fictitious pseudo-speaker.This phenomenon can impede the design of better anonymisation systems since there is a lack of fine-grained control over the x-vector space.The work reported in this paper explores the origin of so-called vocoder drift and shows that it is due to the mismatch between the substituted x-vector and the original representations of the linguistic content, intonation and prosody.Also reported is an original approach to vocoder drift compensation.While anonymisation performance degrades as expected, compensation reduces vocoder drift substantially, offers improved control over the x-vector space and lays a foundation for the design of better anonymisation functions in the future.",
      "doi": "https://doi.org/10.21437/spsc.2023-3",
      "openalex_id": "https://openalex.org/W4386712580",
      "arxiv_id": "",
      "publication_date": "2023-08-19",
      "published": "2023-08-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
      "summary": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents.No temporal information is used in the model.However, there is often a relationship between the corresponding topics of consecutive tokens.In this paper, we present an extension to LDA that uses a Markov chain to model temporal information.We use this new model for acoustic unit discovery from speech.As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes.The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones.In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another.This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA.Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
      "abstract": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents.No temporal information is used in the model.However, there is often a relationship between the corresponding topics of consecutive tokens.In this paper, we present an extension to LDA that uses a Markov chain to model temporal information.We use this new model for acoustic unit discovery from speech.As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes.The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones.In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another.This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA.Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
      "doi": "https://doi.org/10.21437/interspeech.2022-11369",
      "openalex_id": "https://openalex.org/W4297841546",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ESVC: Combining Adaptive Style Fusion and Multi-Level Feature Disentanglement for Expressive Singing Voice Conversion",
      "summary": "Nowadays, singing voice conversion (SVC) has made great strides in both naturalness and similarity for common SVC with a neutral expression. However, besides singer identity, emotional expression is also essential to convey the singer's emotions and attitudes, but current SVC systems can not effectively support it. In this paper, we propose an expressive SVC framework called ESVC, which can convert singer identity and emotional style simultaneously. ESVC combines the ideas of style fusion and feature disentanglement, seeking to maximize fidelity in terms of emotional style and singer identity. Firstly, for style information penetration, we employ adaptive instance normalization (AdaIN) to fuse the content feature and style feature. Secondly, given the possibility of information leakage, two disentanglement-oriented methods are introduced to decouple different kinds of singing features. Mutual information (MI) is used to reduce the correlation between linguistic content, fundamental frequency (F0) and expressive feature, while adversarial triplet loss is exerted for decoupling identity and emotional elements. To the best of our knowledge, ESVC is the first SVC system to jointly convert singer identity and emotional style. Objective and subjective experiments demonstrate that our system significantly outperforms the state-of-the-art SVC model in terms of style expressiveness.",
      "abstract": "Nowadays, singing voice conversion (SVC) has made great strides in both naturalness and similarity for common SVC with a neutral expression. However, besides singer identity, emotional expression is also essential to convey the singer's emotions and attitudes, but current SVC systems can not effectively support it. In this paper, we propose an expressive SVC framework called ESVC, which can convert singer identity and emotional style simultaneously. ESVC combines the ideas of style fusion and feature disentanglement, seeking to maximize fidelity in terms of emotional style and singer identity. Firstly, for style information penetration, we employ adaptive instance normalization (AdaIN) to fuse the content feature and style feature. Secondly, given the possibility of information leakage, two disentanglement-oriented methods are introduced to decouple different kinds of singing features. Mutual information (MI) is used to reduce the correlation between linguistic content, fundamental frequency (F0) and expressive feature, while adversarial triplet loss is exerted for decoupling identity and emotional elements. To the best of our knowledge, ESVC is the first SVC system to jointly convert singer identity and emotional style. Objective and subjective experiments demonstrate that our system significantly outperforms the state-of-the-art SVC model in terms of style expressiveness.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446284",
      "openalex_id": "https://openalex.org/W4392931776",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Optimized EMG Encoder to Minimize Soft Speech Loss for Speech to EMG Conversions",
      "summary": "Electromyography (EMG) to speech conversions is a standard problem to facilitate speech impaired individuals for communication via EMG (EMG to Speech). However, dataset acquisition is a cumbersome process and highly dependent on acquisition configuration. The availability of EMG signals can be made by tackling the inverse problem (Speech to EMG). In this paper, we propose an optimized EMG encoder which enhanced EMG feature extraction and in turn leads to improvements in soft speech units' representations. To validate the efficacy of our proposed enhanced EMG encoder, we utilized state-of-the-art speech to EMG generative adversarial network (STE-GANs). We witnessed a significant improvements in synthesized EMG signals after utilizing proposed EMG encoder which improves soft speech losses by producing enhanced speech units during training of STE-GANs. The extensive results are presented on public dataset.",
      "abstract": "Electromyography (EMG) to speech conversions is a standard problem to facilitate speech impaired individuals for communication via EMG (EMG to Speech). However, dataset acquisition is a cumbersome process and highly dependent on acquisition configuration. The availability of EMG signals can be made by tackling the inverse problem (Speech to EMG). In this paper, we propose an optimized EMG encoder which enhanced EMG feature extraction and in turn leads to improvements in soft speech units' representations. To validate the efficacy of our proposed enhanced EMG encoder, we utilized state-of-the-art speech to EMG generative adversarial network (STE-GANs). We witnessed a significant improvements in synthesized EMG signals after utilizing proposed EMG encoder which improves soft speech losses by producing enhanced speech units during training of STE-GANs. The extensive results are presented on public dataset.",
      "doi": "https://doi.org/10.1109/bigcomp60711.2024.00041",
      "openalex_id": "https://openalex.org/W4394713256",
      "arxiv_id": "",
      "publication_date": "2024-02-18",
      "published": "2024-02-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Interpretable and Transferable Adversarial Attack against Synthetic Speech Detectors",
      "summary": "Existing work finds it challenging for adversarial examples to transfer among different synthetic speech detectors because of cross-feature and cross-model. To enhance the transferability of adversarial examples, we propose a spectral saliency analysis method and gain insight into the underlying detection mechanisms of existing detectors for the first time. These insights offer an interpretable basis for why adversarial examples are challenging to transfer between synthetic speech detection models. Then we further propose a two-stage adversarial attack framework. Specifically, the first stage leverages insights into the model detection mechanism to design a random time-frequency masking module, the random offset module, and 1D convolution to generate transferable and robust adversarial examples. In the second stage, to mitigate the problem of obvious noise in the low-energy frames of the carrier in existing adversarial attacks, we perform secondary optimization on frames below the Signal-Noise-Rate threshold to enhance its auditory quality. Extensive experimental results demonstrate that the proposed method significantly enhances the transferability and robustness of adversarial examples, while simultaneously preserving the acoustic quality compared to typical approaches.",
      "abstract": "Existing work finds it challenging for adversarial examples to transfer among different synthetic speech detectors because of cross-feature and cross-model. To enhance the transferability of adversarial examples, we propose a spectral saliency analysis method and gain insight into the underlying detection mechanisms of existing detectors for the first time. These insights offer an interpretable basis for why adversarial examples are challenging to transfer between synthetic speech detection models. Then we further propose a two-stage adversarial attack framework. Specifically, the first stage leverages insights into the model detection mechanism to design a random time-frequency masking module, the random offset module, and 1D convolution to generate transferable and robust adversarial examples. In the second stage, to mitigate the problem of obvious noise in the low-energy frames of the carrier in existing adversarial attacks, we perform secondary optimization on frames below the Signal-Noise-Rate threshold to enhance its auditory quality. Extensive experimental results demonstrate that the proposed method significantly enhances the transferability and robustness of adversarial examples, while simultaneously preserving the acoustic quality compared to typical approaches.",
      "doi": "https://doi.org/10.1145/3727341",
      "openalex_id": "https://openalex.org/W4409045741",
      "arxiv_id": "",
      "publication_date": "2025-04-01",
      "published": "2025-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MPFM-VC: A Voice Conversion Algorithm Based on Multi-Dimensional Perception Flow Matching",
      "summary": "Voice conversion (VC) is an advanced technology that enables the transformation of raw speech into high-quality audio resembling the target speaker’s voice while preserving the original linguistic content and prosodic patterns. In this study, we propose a voice conversion algorithm, Multi-Dimensional Perception Flow Matching (MPFM-VC). Unlike traditional approaches that directly generate waveform outputs, MPFM-VC models the evolutionary trajectory of mel spectrograms with a flow-matching framework and incorporates a multi-dimensional feature perception network to enhance the stability and quality of speech synthesis. Additionally, we introduce a content perturbation method during training to improve the model’s generalization ability and reduce inference-time artifacts. To further increase speaker similarity, an adversarial training mechanism on speaker embeddings is employed to achieve effective disentanglement between content and speaker identity representations, thereby enhancing the timbre consistency of the converted speech. Experimental results for both speech and singing voice conversion tasks show that MPFM-VC achieves competitive performance compared to existing state-of-the-art VC models in both subjective and objective evaluation metrics. The synthesized speech shows improved naturalness, clarity, and timbre fidelity in both objective and subjective evaluations, suggesting the potential effectiveness of the proposed approach.",
      "abstract": "Voice conversion (VC) is an advanced technology that enables the transformation of raw speech into high-quality audio resembling the target speaker’s voice while preserving the original linguistic content and prosodic patterns. In this study, we propose a voice conversion algorithm, Multi-Dimensional Perception Flow Matching (MPFM-VC). Unlike traditional approaches that directly generate waveform outputs, MPFM-VC models the evolutionary trajectory of mel spectrograms with a flow-matching framework and incorporates a multi-dimensional feature perception network to enhance the stability and quality of speech synthesis. Additionally, we introduce a content perturbation method during training to improve the model’s generalization ability and reduce inference-time artifacts. To further increase speaker similarity, an adversarial training mechanism on speaker embeddings is employed to achieve effective disentanglement between content and speaker identity representations, thereby enhancing the timbre consistency of the converted speech. Experimental results for both speech and singing voice conversion tasks show that MPFM-VC achieves competitive performance compared to existing state-of-the-art VC models in both subjective and objective evaluation metrics. The synthesized speech shows improved naturalness, clarity, and timbre fidelity in both objective and subjective evaluations, suggesting the potential effectiveness of the proposed approach.",
      "doi": "https://doi.org/10.3390/app15105503",
      "openalex_id": "https://openalex.org/W4410362024",
      "arxiv_id": "",
      "publication_date": "2025-05-14",
      "published": "2025-05-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model",
      "summary": "Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this paper, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used LRS3 dataset.",
      "abstract": "Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this paper, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used LRS3 dataset.",
      "doi": "https://doi.org/10.1109/tmm.2024.3352388",
      "openalex_id": "https://openalex.org/W4390691978",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering",
      "summary": "Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users.Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts.Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly.Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task.Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult.This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and finetuned by the SQA downstream task.The time intervals of spoken answers can be directly predicted from spoken documents.We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios.We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data.Our code and model will be open-sourced 1 .",
      "abstract": "Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users.Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts.Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly.Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task.Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult.This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and finetuned by the SQA downstream task.The time intervals of spoken answers can be directly predicted from spoken documents.We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios.We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data.Our code and model will be open-sourced 1 .",
      "doi": "https://doi.org/10.21437/interspeech.2022-612",
      "openalex_id": "https://openalex.org/W4221146627",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Holistic Cascade System, Benchmark, and Human Evaluation Protocol for Expressive Speech-to-Speech Translation",
      "summary": "Expressive speech-to-speech translation (S2ST) aims to transfer prosodic attributes of source speech to target speech while maintaining translation accuracy. Existing research in expressive S2ST is limited, typically focusing on a single expressivity aspect at a time. Likewise, this research area lacks standard evaluation protocols and well-curated benchmark datasets. In this work, we propose a holistic cascade system for expressive S2ST, combining multiple prosody transfer techniques previously considered only in isolation. We curate a benchmark expressivity test set in the TV series domain and explored a second dataset in the audiobook domain. Finally, we present a human evaluation protocol to assess multiple expressive dimensions across speech pairs. Experimental results indicate that bi-lingual annotators can assess the quality of expressive preservation in S2ST systems, and the holistic modeling approach outperforms single-aspect systems. Audio samples can be accessed through our demo webpage: https://facebookresearch.github.io/speech_translation/cascade_expressive_s2st.",
      "abstract": "Expressive speech-to-speech translation (S2ST) aims to transfer prosodic attributes of source speech to target speech while maintaining translation accuracy. Existing research in expressive S2ST is limited, typically focusing on a single expressivity aspect at a time. Likewise, this research area lacks standard evaluation protocols and well-curated benchmark datasets. In this work, we propose a holistic cascade system for expressive S2ST, combining multiple prosody transfer techniques previously considered only in isolation. We curate a benchmark expressivity test set in the TV series domain and explored a second dataset in the audiobook domain. Finally, we present a human evaluation protocol to assess multiple expressive dimensions across speech pairs. Experimental results indicate that bi-lingual annotators can assess the quality of expressive preservation in S2ST systems, and the holistic modeling approach outperforms single-aspect systems. Audio samples can be accessed through our demo webpage: https://facebookresearch.github.io/speech_translation/cascade_expressive_s2st.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096183",
      "openalex_id": "https://openalex.org/W4372260088",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero Resource Code-Switched Speech Benchmark Using Speech Utterance Pairs for Multiple Spoken Languages",
      "summary": "We introduce a new zero resource code-switched speech bench-mark designed to assess the code-switching capabilities of self-supervised speech encoders directly. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc., on three tracks of different code-switched language pairs: Spanish-English, French-English, and Chinese-English. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.",
      "abstract": "We introduce a new zero resource code-switched speech bench-mark designed to assess the code-switching capabilities of self-supervised speech encoders directly. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc., on three tracks of different code-switched language pairs: Spanish-English, French-English, and Chinese-English. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446737",
      "openalex_id": "https://openalex.org/W4392903468",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechDPR: End-To-End Spoken Passage Retrieval For Open-Domain Spoken Question Answering",
      "summary": "Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end frame-work, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors.",
      "abstract": "Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end frame-work, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448210",
      "openalex_id": "https://openalex.org/W4392903115",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ZeST: A Zero-Resourced Speech-to-Speech Translation Approach for Unknown, Unpaired, and Untranscribed Languages",
      "summary": "Speech-to-speech translation (S2ST) has emerged as a practical solution for overcoming linguistic barriers, enabling direct translation between spoken languages without relying on intermediate text representations. However, existing S2ST systems face significant challenges, including the requirement for extensive parallel speech data and the limitations of known written languages. This paper proposes ZeST, a novel zero-resourced approach to speech-to-speech translation that addresses the challenges of processing unknown, unpaired, and untranscribed languages. ZeST consists of two main phases: <xref ref-type=\"disp-formula\" rid=\"deqn1\">(1)</xref> Discovering semantically related speech pairs from unpaired data by leveraging self-supervised visually grounded speech (VGS) models and <xref ref-type=\"disp-formula\" rid=\"deqn2\">(2)</xref> Achieving textless speech-to-speech translation for untranscribed languages using discrete speech representations and sequence-to-sequence modeling. Experimental evaluations using three different data scenarios demonstrate that the ZeST system effectively performs direct speech-to-speech translation without relying on transcribed data or parallel corpora. The experimental results highlight the potential of ZeST in contributing to the field of zero-resourced speech processing and improving communication in multilingual societies.",
      "abstract": "Speech-to-speech translation (S2ST) has emerged as a practical solution for overcoming linguistic barriers, enabling direct translation between spoken languages without relying on intermediate text representations. However, existing S2ST systems face significant challenges, including the requirement for extensive parallel speech data and the limitations of known written languages. This paper proposes ZeST, a novel zero-resourced approach to speech-to-speech translation that addresses the challenges of processing unknown, unpaired, and untranscribed languages. ZeST consists of two main phases: <xref ref-type=\"disp-formula\" rid=\"deqn1\">(1)</xref> Discovering semantically related speech pairs from unpaired data by leveraging self-supervised visually grounded speech (VGS) models and <xref ref-type=\"disp-formula\" rid=\"deqn2\">(2)</xref> Achieving textless speech-to-speech translation for untranscribed languages using discrete speech representations and sequence-to-sequence modeling. Experimental evaluations using three different data scenarios demonstrate that the ZeST system effectively performs direct speech-to-speech translation without relying on transcribed data or parallel corpora. The experimental results highlight the potential of ZeST in contributing to the field of zero-resourced speech processing and improving communication in multilingual societies.",
      "doi": "https://doi.org/10.1109/access.2025.3527012",
      "openalex_id": "https://openalex.org/W4406163711",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task",
      "summary": "Kun Song, Yi Lei, Peikun Chen, Yiqing Cao, Kun Wei, Yongmao Zhang, Lei Xie, Ning Jiang, Guoqing Zhao. Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). 2023.",
      "abstract": "Kun Song, Yi Lei, Peikun Chen, Yiqing Cao, Kun Wei, Yongmao Zhang, Lei Xie, Ning Jiang, Guoqing Zhao. Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.iwslt-1.29",
      "openalex_id": "https://openalex.org/W4385571194",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluating Self-Supervised Speech Models on a Taiwanese Hokkien Corpus",
      "summary": "Taiwanese Hokkien is declining in use and status due to a language shift towards Mandarin in Taiwan. This is partly why it is a low resource language in NLP and speech research today. To ensure that the state of the art in speech processing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour dataset of Taiwanese Hokkien to MLSUPERB's hidden set. Evaluating ML-SUPERB's suite of self-supervised learning (SSL) speech representations on our dataset, we find that model size does not consistently determine performance. In fact, certain smaller models outperform larger ones. Furthermore, linguistic alignment between pretraining data and the target language plays a crucial role.",
      "abstract": "Taiwanese Hokkien is declining in use and status due to a language shift towards Mandarin in Taiwan. This is partly why it is a low resource language in NLP and speech research today. To ensure that the state of the art in speech processing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour dataset of Taiwanese Hokkien to MLSUPERB's hidden set. Evaluating ML-SUPERB's suite of self-supervised learning (SSL) speech representations on our dataset, we find that model size does not consistently determine performance. In fact, certain smaller models outperform larger ones. Furthermore, linguistic alignment between pretraining data and the target language plays a crucial role.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389734",
      "openalex_id": "https://openalex.org/W4391021639",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LightCodec: A High Fidelity Neural Audio Codec with Low Computation Complexity",
      "summary": "The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).",
      "abstract": "The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447532",
      "openalex_id": "https://openalex.org/W4392902628",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lace: A Light-Weight, Causal Model for Enhancing Coded Speech Through Adaptive Convolutions",
      "summary": "Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.",
      "abstract": "Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.",
      "doi": "https://doi.org/10.1109/waspaa58266.2023.10248150",
      "openalex_id": "https://openalex.org/W4386764386",
      "arxiv_id": "",
      "publication_date": "2023-09-15",
      "published": "2023-09-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Text-to-Speech and Speech-to-Text Models: A Systematic Examination of Diverse Approaches",
      "summary": "The convergence of Text-to-Speech (TTS) and Speech-to-Text (STT) models has spearheaded an era of transformative advancements in natural language processing (NLP). This article discusses transformative advances in natural language processing (NLP). It presents a comprehensive survey that systematically examines specific STT and TTS models, such as rule-based systems and deep learning architectures, and explores their structures, methodologies, and applications. The paper classifies these models based on their underlying technologies, architectural approaches, training methods, and application domains, and provides a detailed overview of their development. It highlights industry successes and challenges, covering a range of application areas, from assistive technologies to virtual assistants and language translation. This research provides new perspectives to understand the current world of STT and TTS technologies, which will especially benefit researchers, developers, and industry players. By highlighting the empowering bridge between written and spoken language, the paper uniquely contributes to the ongoing dialogue in the field. It not only sheds light on the current state of STT and TTS technologies but also lays the groundwork for future innovations and advances that could change the way we interact with language.",
      "abstract": "The convergence of Text-to-Speech (TTS) and Speech-to-Text (STT) models has spearheaded an era of transformative advancements in natural language processing (NLP). This article discusses transformative advances in natural language processing (NLP). It presents a comprehensive survey that systematically examines specific STT and TTS models, such as rule-based systems and deep learning architectures, and explores their structures, methodologies, and applications. The paper classifies these models based on their underlying technologies, architectural approaches, training methods, and application domains, and provides a detailed overview of their development. It highlights industry successes and challenges, covering a range of application areas, from assistive technologies to virtual assistants and language translation. This research provides new perspectives to understand the current world of STT and TTS technologies, which will especially benefit researchers, developers, and industry players. By highlighting the empowering bridge between written and spoken language, the paper uniquely contributes to the ongoing dialogue in the field. It not only sheds light on the current state of STT and TTS technologies but also lays the groundwork for future innovations and advances that could change the way we interact with language.",
      "doi": "https://doi.org/10.1109/i2ct61223.2024.10544015",
      "openalex_id": "https://openalex.org/W4399486076",
      "arxiv_id": "",
      "publication_date": "2024-04-05",
      "published": "2024-04-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "summary": "Message from the Program ChairsIt's hard to believe that we're actually going to be seeing the program come together in Toronto.We're really looking forward to it and to seeing you all there!Most of the work of a program chair is behind the scenes: herding reviewers and chairs, wrangling data from various sources, and answering lots and lots of email.This is a volunteer position, so the only reward we get for this is our chance to make the process of submitting and reviewing papers to our conference better.This letter will outline some of those experiments.First, we asked reviewers for two scores: soundness and excitement.Our goal was that any sound paper would be accepted to some ACL affiliated venue, but that the \"main conference\" distinction (limited by space) would be focused on the most exciting papers.Our hope was that soundness would be less noisy than a single \"overall recommendation\" score, which would help reduce the randomness of decisions.Judging by the exit surveys, this change was well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change.Next, we developed a new process for matching papers to reviewers based on keywords for not only the subject matter of the paper, but also its type of contribution and target language(s).This allowed more fine-grained control over the paper-reviewer matches, and we were also able to provide the chairs with context for the paper-reviewer matches.To improve review quality, we also updated the reviewer guidelines, and developed a system for the authors to flag specific types of issues with reviews.Finally, we have also proposed a new initiative for recognizing outstanding reviewers and chairs (73 awards at ACL'23).Finally, we have tried to give more options for presentations.Findings papers now have an in-person presentation spotlight slot and virtual posters in addition to recording videos.Virtual posters have portals to link in-person attendees to virtual posters.We have also brought back Miniconf and RocketChat to allow for better virtual communication between papers (regardless of where the authors are).This conference is a result of the joint efforts of over ten thousand people.We deeply thank them all, and apologize for the many nagging emails we had to send out.In particular: AreasTo ensure a smooth process, the submissions to ACL 2023 were divided into 26 areas.The areas mostly followed these of previous ACL, and more broadly *ACL conferences, reflecting the typical divisions in the field.Following EMNLP 2022, we split the \"Large Language Models\" track away from \"Machine learning in NLP\", reflecting the growth of submissions in the area.We also offered two new tracks (\"Linguistic diversity\" and \"Multilingualism and Cross-Lingual NLP\").For the papers authored by SACs, the final recommendation decisions were made by a separate SAC team.The most popular areas (with over 250 submissions) were \"Dialogue and Interactive Systems\", \"Information Extraction\", \"Large Language Models\", \"Machine Learning for NLP\", and \"NLP Applications\". Best Paper AwardsACL'23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding.In total, 73 papers were nominated by the reviewers or area chairs for consideration for awards.These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 3 special awards (social impact, resource, reproduction), and several dozen outstanding papers.The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023. Presentation ModeIn ACL 2023, there is no meaningful distinction between oral and poster presentations in terms of paper quality.The composition of the oral sessions were proposed by the SACs of their respective tracks, so as to compose a thematically coherent set of papers on a shared topic or method, which would allow for an engaging discussion.The decisions were not based on the authors' virtual or on-site attendance.We hope you enjoy the program and the new elements we introduced (but let us know either way).We are looking forward to a great ACL 2023!",
      "abstract": "Message from the Program ChairsIt's hard to believe that we're actually going to be seeing the program come together in Toronto.We're really looking forward to it and to seeing you all there!Most of the work of a program chair is behind the scenes: herding reviewers and chairs, wrangling data from various sources, and answering lots and lots of email.This is a volunteer position, so the only reward we get for this is our chance to make the process of submitting and reviewing papers to our conference better.This letter will outline some of those experiments.First, we asked reviewers for two scores: soundness and excitement.Our goal was that any sound paper would be accepted to some ACL affiliated venue, but that the \"main conference\" distinction (limited by space) would be focused on the most exciting papers.Our hope was that soundness would be less noisy than a single \"overall recommendation\" score, which would help reduce the randomness of decisions.Judging by the exit surveys, this change was well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change.Next, we developed a new process for matching papers to reviewers based on keywords for not only the subject matter of the paper, but also its type of contribution and target language(s).This allowed more fine-grained control over the paper-reviewer matches, and we were also able to provide the chairs with context for the paper-reviewer matches.To improve review quality, we also updated the reviewer guidelines, and developed a system for the authors to flag specific types of issues with reviews.Finally, we have also proposed a new initiative for recognizing outstanding reviewers and chairs (73 awards at ACL'23).Finally, we have tried to give more options for presentations.Findings papers now have an in-person presentation spotlight slot and virtual posters in addition to recording videos.Virtual posters have portals to link in-person attendees to virtual posters.We have also brought back Miniconf and RocketChat to allow for better virtual communication between papers (regardless of where the authors are).This conference is a result of the joint efforts of over ten thousand people.We deeply thank them all, and apologize for the many nagging emails we had to send out.In particular: AreasTo ensure a smooth process, the submissions to ACL 2023 were divided into 26 areas.The areas mostly followed these of previous ACL, and more broadly *ACL conferences, reflecting the typical divisions in the field.Following EMNLP 2022, we split the \"Large Language Models\" track away from \"Machine learning in NLP\", reflecting the growth of submissions in the area.We also offered two new tracks (\"Linguistic diversity\" and \"Multilingualism and Cross-Lingual NLP\").For the papers authored by SACs, the final recommendation decisions were made by a separate SAC team.The most popular areas (with over 250 submissions) were \"Dialogue and Interactive Systems\", \"Information Extraction\", \"Large Language Models\", \"Machine Learning for NLP\", and \"NLP Applications\". Best Paper AwardsACL'23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding.In total, 73 papers were nominated by the reviewers or area chairs for consideration for awards.These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 3 special awards (social impact, resource, reproduction), and several dozen outstanding papers.The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023. Presentation ModeIn ACL 2023, there is no meaningful distinction between oral and poster presentations in terms of paper quality.The composition of the oral sessions were proposed by the SACs of their respective tracks, so as to compose a thematically coherent set of papers on a shared topic or method, which would allow for an engaging discussion.The decisions were not based on the authors' virtual or on-site attendance.We hope you enjoy the program and the new elements we introduced (but let us know either way).We are looking forward to a great ACL 2023!",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long",
      "openalex_id": "https://openalex.org/W2148437670",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech",
      "summary": "In this paper we present VDTTS, a Visually-Driven Text-to-Speech model. Motivated by dubbing, VDTTS takes ad-vantage of video frames as an additional input alongside text, and generates speech that matches the video signal. We demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech that not only has prosodic variations like natural pauses and pitch, but is also synchronized to the input video. Experimentally, we show our model produces well-synchronized outputs, approaching the video-speech synchronization quality of the ground-truth, on several challenging benchmarks including \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody, presented at the project page. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Project page: http://google-research.github.io/lingvo-lab/vdtts",
      "abstract": "In this paper we present VDTTS, a Visually-Driven Text-to-Speech model. Motivated by dubbing, VDTTS takes ad-vantage of video frames as an additional input alongside text, and generates speech that matches the video signal. We demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech that not only has prosodic variations like natural pauses and pitch, but is also synchronized to the input video. Experimentally, we show our model produces well-synchronized outputs, approaching the video-speech synchronization quality of the ground-truth, on several challenging benchmarks including \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody, presented at the project page. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Project page: http://google-research.github.io/lingvo-lab/vdtts",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.01033",
      "openalex_id": "https://openalex.org/W3216976702",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bloom-Net: Blockwise Optimization for Masking Networks Toward Scalable and Efficient Speech Enhancement",
      "summary": "In this paper, we present a blockwise optimization method for masking-based networks (BLOOM-Net) for training scalable speech enhancement networks. Here, we design our network with a residual learning scheme and train the internal separator blocks sequentially to obtain a scalable masking-based deep neural network for speech enhancement. Its scalability lets it dynamically adjust the run-time complexity depending on the test time environment. To this end, we modularize our models in that they can flexibly accommodate varying needs for enhancement performance and constraints on the resources, incurring minimal memory or training overhead due to the added scalability. Our experiments on speech enhancement demonstrate that the proposed blockwise optimization method achieves the desired scalability with only a slight performance degradation compared to corresponding models trained end-to-end.",
      "abstract": "In this paper, we present a blockwise optimization method for masking-based networks (BLOOM-Net) for training scalable speech enhancement networks. Here, we design our network with a residual learning scheme and train the internal separator blocks sequentially to obtain a scalable masking-based deep neural network for speech enhancement. Its scalability lets it dynamically adjust the run-time complexity depending on the test time environment. To this end, we modularize our models in that they can flexibly accommodate varying needs for enhancement performance and constraints on the resources, incurring minimal memory or training overhead due to the added scalability. Our experiments on speech enhancement demonstrate that the proposed blockwise optimization method achieves the desired scalability with only a slight performance degradation compared to corresponding models trained end-to-end.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746767",
      "openalex_id": "https://openalex.org/W3213528868",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weight, Block or Unit? Exploring Sparsity Tradeoffs for Speech Enhancement on Tiny Neural Accelerators",
      "summary": "We explore network sparsification strategies with the aim of compressing neural speech enhancement (SE) down to an optimal configuration for a new generation of low power microcontroller based neural accelerators (microNPU's). We examine three unique sparsity structures: weight pruning, block pruning and unit pruning; and discuss their benefits and drawbacks when applied to SE. We focus on the interplay between computational throughput, memory footprint and model quality. Our method supports all three structures above and jointly learns integer quantized weights along with sparsity. Additionally, we demonstrate offline magnitude based pruning of integer quantized models as a performance baseline. Although efficient speech enhancement is an active area of research, our work is the first to apply block pruning to SE and the first to address SE model compression in the context of microNPU's. Using weight pruning, we show that we are able to compress an already compact model's memory footprint by a factor of 42x from 3.7MB to 87kB while only losing 0.1 dB SDR in performance. We also show a computational speedup of 6.7x with a corresponding SDR drop of only 0.59 dB SDR using block pruning.",
      "abstract": "We explore network sparsification strategies with the aim of compressing neural speech enhancement (SE) down to an optimal configuration for a new generation of low power microcontroller based neural accelerators (microNPU's). We examine three unique sparsity structures: weight pruning, block pruning and unit pruning; and discuss their benefits and drawbacks when applied to SE. We focus on the interplay between computational throughput, memory footprint and model quality. Our method supports all three structures above and jointly learns integer quantized weights along with sparsity. Additionally, we demonstrate offline magnitude based pruning of integer quantized models as a performance baseline. Although efficient speech enhancement is an active area of research, our work is the first to apply block pruning to SE and the first to address SE model compression in the context of microNPU's. Using weight pruning, we show that we are able to compress an already compact model's memory footprint by a factor of 42x from 3.7MB to 87kB while only losing 0.1 dB SDR in performance. We also show a computational speedup of 6.7x with a corresponding SDR drop of only 0.59 dB SDR using block pruning.",
      "doi": "https://doi.org/10.48550/arxiv.2111.02351",
      "openalex_id": "https://openalex.org/W3210269866",
      "arxiv_id": "",
      "publication_date": "2021-11-03",
      "published": "2021-11-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Translation Algorithm of Proper Nouns in English-Chinese Translation Based on Lexical Knowledge Spectrum Map",
      "summary": "The Lexical Knowledge Spectrum Map (LKSM) represents a comprehensive visual representation of the breadth and depth of lexical knowledge within a specific domain or language. By categorizing words and phrases along a spectrum ranging from basic to advanced levels of complexity, the LKSM provides learners and educators with a clear overview of vocabulary proficiency and progression. This dynamic tool not only helps individuals track their language learning journey but also guides instructional planning by identifying areas of focus and potential gaps in lexical understanding. With its intuitive interface and adaptable framework, the LKSM serves as a valuable resource for promoting effective vocabulary acquisition and mastery across diverse linguistic contexts and educational settings. This paper introduces a novel translation algorithm designed specifically for handling proper nouns in English-Chinese translation, leveraging the Lexical Knowledge Spectrum Map (LKSM) and Feature Vector Optimization with Statistical (FVOS) techniques. Proper nouns pose a unique challenge in translation due to their cultural and contextual significance, often requiring specialized handling to ensure accuracy and coherence in the target language. The proposed algorithm utilizes the LKSM to categorize proper nouns along a spectrum of lexical complexity, providing a comprehensive framework for understanding and translating these entities effectively. Additionally, FVOS techniques are employed to optimize feature vectors for proper noun translation, enhancing the algorithm's ability to capture and preserve semantic nuances across languages. With FVOS model English proper nouns translated into Chinese, the proposed algorithm achieves an average accuracy of 85%, outperforming baseline translation methods by 15%. Moreover, specific proper noun categories exhibit notable improvements, with names of people achieving an accuracy of 90%, followed by locations at 85%, and organizations at 80%.",
      "abstract": "The Lexical Knowledge Spectrum Map (LKSM) represents a comprehensive visual representation of the breadth and depth of lexical knowledge within a specific domain or language. By categorizing words and phrases along a spectrum ranging from basic to advanced levels of complexity, the LKSM provides learners and educators with a clear overview of vocabulary proficiency and progression. This dynamic tool not only helps individuals track their language learning journey but also guides instructional planning by identifying areas of focus and potential gaps in lexical understanding. With its intuitive interface and adaptable framework, the LKSM serves as a valuable resource for promoting effective vocabulary acquisition and mastery across diverse linguistic contexts and educational settings. This paper introduces a novel translation algorithm designed specifically for handling proper nouns in English-Chinese translation, leveraging the Lexical Knowledge Spectrum Map (LKSM) and Feature Vector Optimization with Statistical (FVOS) techniques. Proper nouns pose a unique challenge in translation due to their cultural and contextual significance, often requiring specialized handling to ensure accuracy and coherence in the target language. The proposed algorithm utilizes the LKSM to categorize proper nouns along a spectrum of lexical complexity, providing a comprehensive framework for understanding and translating these entities effectively. Additionally, FVOS techniques are employed to optimize feature vectors for proper noun translation, enhancing the algorithm's ability to capture and preserve semantic nuances across languages. With FVOS model English proper nouns translated into Chinese, the proposed algorithm achieves an average accuracy of 85%, outperforming baseline translation methods by 15%. Moreover, specific proper noun categories exhibit notable improvements, with names of people achieving an accuracy of 90%, followed by locations at 85%, and organizations at 80%.",
      "doi": "https://doi.org/10.52783/jes.1324",
      "openalex_id": "https://openalex.org/W4394818086",
      "arxiv_id": "",
      "publication_date": "2024-04-04",
      "published": "2024-04-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotion Intensity and its Control for Emotional Voice Conversion",
      "summary": "Emotional voice conversion (EVC) seeks to convert the emotional state of an\\nutterance while preserving the linguistic content and speaker identity. In EVC,\\nemotions are usually treated as discrete categories overlooking the fact that\\nspeech also conveys emotions with various intensity levels that the listener\\ncan perceive. In this paper, we aim to explicitly characterize and control the\\nintensity of emotion. We propose to disentangle the speaker style from\\nlinguistic content and encode the speaker style into a style embedding in a\\ncontinuous space that forms the prototype of emotion embedding. We further\\nlearn the actual emotion encoder from an emotion-labelled database and study\\nthe use of relative attributes to represent fine-grained emotion intensity. To\\nensure emotional intelligibility, we incorporate emotion classification loss\\nand emotion embedding similarity loss into the training of the EVC network. As\\ndesired, the proposed network controls the fine-grained emotion intensity in\\nthe output speech. Through both objective and subjective evaluations, we\\nvalidate the effectiveness of the proposed network for emotional expressiveness\\nand emotion intensity control.\\n",
      "abstract": "Emotional voice conversion (EVC) seeks to convert the emotional state of an\\nutterance while preserving the linguistic content and speaker identity. In EVC,\\nemotions are usually treated as discrete categories overlooking the fact that\\nspeech also conveys emotions with various intensity levels that the listener\\ncan perceive. In this paper, we aim to explicitly characterize and control the\\nintensity of emotion. We propose to disentangle the speaker style from\\nlinguistic content and encode the speaker style into a style embedding in a\\ncontinuous space that forms the prototype of emotion embedding. We further\\nlearn the actual emotion encoder from an emotion-labelled database and study\\nthe use of relative attributes to represent fine-grained emotion intensity. To\\nensure emotional intelligibility, we incorporate emotion classification loss\\nand emotion embedding similarity loss into the training of the EVC network. As\\ndesired, the proposed network controls the fine-grained emotion intensity in\\nthe output speech. Through both objective and subjective evaluations, we\\nvalidate the effectiveness of the proposed network for emotional expressiveness\\nand emotion intensity control.\\n",
      "doi": "https://doi.org/10.1109/taffc.2022.3175578",
      "openalex_id": "https://openalex.org/W4221147462",
      "arxiv_id": "",
      "publication_date": "2022-05-19",
      "published": "2022-05-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
      "summary": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.",
      "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.",
      "doi": "https://doi.org/10.21437/interspeech.2021-283",
      "openalex_id": "https://openalex.org/W3197659778",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Freevc: Towards High-Quality Text-Free One-Shot Voice Conversion",
      "summary": "Voice conversion (VC) can be achieved by first extracting source content information and target speaker information, and then reconstructing waveform with these information. However, current approaches normally either extract dirty content information with speaker information leaked in, or demand a large amount of annotated data for training. Besides, the quality of reconstructed waveform can be degraded by the mismatch between conversion model and vocoder. In this paper, we adopt the end-to-end framework of VITS for high-quality waveform reconstruction, and propose strategies for clean content information extraction without text annotation. We disentangle content information by imposing an information bottleneck to WavLM features, and propose the spectrogram-resize based data augmentation to improve the purity of extracted content information. Experimental results show that the proposed method outperforms the latest VC models trained with annotated data and has greater robustness.",
      "abstract": "Voice conversion (VC) can be achieved by first extracting source content information and target speaker information, and then reconstructing waveform with these information. However, current approaches normally either extract dirty content information with speaker information leaked in, or demand a large amount of annotated data for training. Besides, the quality of reconstructed waveform can be degraded by the mismatch between conversion model and vocoder. In this paper, we adopt the end-to-end framework of VITS for high-quality waveform reconstruction, and propose strategies for clean content information extraction without text annotation. We disentangle content information by imposing an information bottleneck to WavLM features, and propose the spectrogram-resize based data augmentation to improve the purity of extracted content information. Experimental results show that the proposed method outperforms the latest VC models trained with annotated data and has greater robustness.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095191",
      "openalex_id": "https://openalex.org/W4372260053",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Multi-Speaker Multi-Style Voice Cloning Challenge 2021",
      "summary": "The Multi-speaker Multi-style Voice Cloning Challenge (M2VoC) aims to provide a common sizable dataset as well as a fair testbed for the benchmarking of the popular voice cloning task. Specifically, we formulate the challenge to adapt an average TTS model to the stylistic target voice with limited data from target speaker, evaluated by speaker identity and style similarity. The challenge consists of two tracks, namely few-shot track and one-shot track, where the participants are required to clone multiple target voices with 100 and 5 samples respectively. There are also two sub-tracks in each track. For sub-track a, to fairly compare different strategies, the participants are allowed to use only the training data provided by the organizer strictly. For sub-track b, the participants are allowed to use any data publicly available. In this paper, we present a detailed explanation on the tasks and data used in the challenge, followed by a summary of submitted systems and evaluation results.",
      "abstract": "The Multi-speaker Multi-style Voice Cloning Challenge (M2VoC) aims to provide a common sizable dataset as well as a fair testbed for the benchmarking of the popular voice cloning task. Specifically, we formulate the challenge to adapt an average TTS model to the stylistic target voice with limited data from target speaker, evaluated by speaker identity and style similarity. The challenge consists of two tracks, namely few-shot track and one-shot track, where the participants are required to clone multiple target voices with 100 and 5 samples respectively. There are also two sub-tracks in each track. For sub-track a, to fairly compare different strategies, the participants are allowed to use only the training data provided by the organizer strictly. For sub-track b, the participants are allowed to use any data publicly available. In this paper, we present a detailed explanation on the tasks and data used in the challenge, followed by a summary of submitted systems and evaluation results.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414001",
      "openalex_id": "https://openalex.org/W3162794600",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How Far Are We from Robust Voice Conversion: A Survey",
      "summary": "Voice conversion technologies have been greatly improved in recent years with the help of deep learning, but their capabilities of producing natural sounding utterances in different conditions remain unclear. In this paper, we gave a thorough study of the robustness of known VC models. We also modified these models, such as the replacement of speaker embeddings, to further improve their performances. We found that the sampling rate and audio duration greatly influence voice conversion. All the VC models suffer from unseen data, but AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained is more suitable for voice conversion than those trained on speaker identification.",
      "abstract": "Voice conversion technologies have been greatly improved in recent years with the help of deep learning, but their capabilities of producing natural sounding utterances in different conditions remain unclear. In this paper, we gave a thorough study of the robustness of known VC models. We also modified these models, such as the replacement of speaker embeddings, to further improve their performances. We found that the sampling rate and audio duration greatly influence voice conversion. All the VC models suffer from unseen data, but AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained is more suitable for voice conversion than those trained on speaker identification.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383498",
      "openalex_id": "https://openalex.org/W3143523927",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UNET-TTS: Improving Unseen Speaker and Style Transfer in One-Shot Voice Cloning",
      "summary": "One-shot voice cloning aims to transform speaker voice and speaking style in speech synthesized from a text-to-speech (TTS) system, where only a shot recording from the target reference speech can be used. Out-of-domain transfer is still a challenging task, and one important aspect that impacts the accuracy and similarity of synthetic speech is the conditional representations carrying speaker or style cues extracted from the limited references. In this paper, we present a novel one-shot voice cloning algorithm called Unet-TTS that has good generalization ability for unseen speakers and styles. Based on a skip-connected U-net structure, the new model can efficiently discover speaker-level and utterance-level spectral feature details from the reference audio, enabling accurate inference of complex acoustic characteristics as well as imitation of speaking styles into the synthetic speech. According to both subjective and objective evaluations of similarity, the new model outperforms both speaker embedding and unsupervised style modeling (GST) approaches on an unseen emotional corpus.",
      "abstract": "One-shot voice cloning aims to transform speaker voice and speaking style in speech synthesized from a text-to-speech (TTS) system, where only a shot recording from the target reference speech can be used. Out-of-domain transfer is still a challenging task, and one important aspect that impacts the accuracy and similarity of synthetic speech is the conditional representations carrying speaker or style cues extracted from the limited references. In this paper, we present a novel one-shot voice cloning algorithm called Unet-TTS that has good generalization ability for unseen speakers and styles. Based on a skip-connected U-net structure, the new model can efficiently discover speaker-level and utterance-level spectral feature details from the reference audio, enabling accurate inference of complex acoustic characteristics as well as imitation of speaking styles into the synthetic speech. According to both subjective and objective evaluations of similarity, the new model outperforms both speaker embedding and unsupervised style modeling (GST) approaches on an unseen emotional corpus.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746049",
      "openalex_id": "https://openalex.org/W3200756692",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio Deepfake Detection: What Has Been Achieved and What Lies Ahead",
      "summary": "Advancements in audio synthesis and manipulation technologies have reshaped applications such as personalised virtual assistants, voice cloning for creative content, and language learning tools. However, the misuse of these technologies to create audio deepfakes has raised serious concerns about security, privacy, and trust. Studies reveal that human judgement of deepfake audio is not always reliable, highlighting the urgent need for robust detection technologies to mitigate these risks. This paper provides a comprehensive survey of recent advancements in audio deepfake detection, with a focus on cutting-edge developments in the past few years. It begins by exploring the foundational methods of audio deepfake generation, including text-to-speech (TTS) and voice conversion (VC), followed by a review of datasets driving progress in the field. The survey then delves into detection approaches, covering frontend feature extraction, backend classification models, and end-to-end systems. Additionally, emerging topics such as privacy-preserving detection, explainability, and fairness are discussed. Finally, this paper identifies key challenges and outlines future directions for developing robust and scalable audio deepfake detection systems.",
      "abstract": "Advancements in audio synthesis and manipulation technologies have reshaped applications such as personalised virtual assistants, voice cloning for creative content, and language learning tools. However, the misuse of these technologies to create audio deepfakes has raised serious concerns about security, privacy, and trust. Studies reveal that human judgement of deepfake audio is not always reliable, highlighting the urgent need for robust detection technologies to mitigate these risks. This paper provides a comprehensive survey of recent advancements in audio deepfake detection, with a focus on cutting-edge developments in the past few years. It begins by exploring the foundational methods of audio deepfake generation, including text-to-speech (TTS) and voice conversion (VC), followed by a review of datasets driving progress in the field. The survey then delves into detection approaches, covering frontend feature extraction, backend classification models, and end-to-end systems. Additionally, emerging topics such as privacy-preserving detection, explainability, and fairness are discussed. Finally, this paper identifies key challenges and outlines future directions for developing robust and scalable audio deepfake detection systems.",
      "doi": "https://doi.org/10.3390/s25071989",
      "openalex_id": "https://openalex.org/W4408799803",
      "arxiv_id": "",
      "publication_date": "2025-03-22",
      "published": "2025-03-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SIG-VC: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines",
      "summary": "Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.",
      "abstract": "Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746048",
      "openalex_id": "https://openalex.org/W3213785244",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero-Shot Face-Based Voice Conversion: Bottleneck-Free Speech Disentanglement in the Real-World Scenario",
      "summary": "Often a face has a voice. Appearance sometimes has a strong relationship with one's voice. In this work, we study how a face can be converted to a voice, which is a face-based voice conversion. Since there is no clean dataset that contains face and speech, voice conversion faces difficult learning and low-quality problems caused by background noise or echo. Too much redundant information for face-to-voice also causes synthesis of a general style of speech. Furthermore, previous work tried to disentangle speech with bottleneck adjustment. However, it is hard to decide on the size of the bottleneck. Therefore, we propose a bottleneck-free strategy for speech disentanglement. To avoid synthesizing the general style of speech, we utilize framewise facial embedding. It applied adversarial learning with a multi-scale discriminator for the model to achieve better quality. In addition, the self-attention module is added to focus on content-related features for in-the-wild data. Quantitative experiments show that our method outperforms previous work.",
      "abstract": "Often a face has a voice. Appearance sometimes has a strong relationship with one's voice. In this work, we study how a face can be converted to a voice, which is a face-based voice conversion. Since there is no clean dataset that contains face and speech, voice conversion faces difficult learning and low-quality problems caused by background noise or echo. Too much redundant information for face-to-voice also causes synthesis of a general style of speech. Furthermore, previous work tried to disentangle speech with bottleneck adjustment. However, it is hard to decide on the size of the bottleneck. Therefore, we propose a bottleneck-free strategy for speech disentanglement. To avoid synthesizing the general style of speech, we utilize framewise facial embedding. It applied adversarial learning with a multi-scale discriminator for the model to achieve better quality. In addition, the self-attention module is added to focus on content-related features for in-the-wild data. Quantitative experiments show that our method outperforms previous work.",
      "doi": "https://doi.org/10.1609/aaai.v37i11.26607",
      "openalex_id": "https://openalex.org/W4382202619",
      "arxiv_id": "",
      "publication_date": "2023-06-26",
      "published": "2023-06-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Adaptation For Enhancement Of Bone-Conducted Speech",
      "summary": "Deep neural network (DNN)-based speech enhancement models often face challenges in maintaining their performance for speakers not encountered during training. This challenge is exacerbated in applications such as enhancement and bandwidth extension of bone-conducted speech, where the distortion exhibits a close correlation with speaker-specific characteristics. We address this issue by introducing a bottleneck module aimed at disentangling speaker-specific characteristics from speech content in speech enhancement DNNs. A DNN model is trained for enhancement of bone-conducted speech and modified with the proposed bottleneck module. We evaluate the DNN’s adaptability to unseen speakers through fine-tuning the network with a limited amount of adaptation data. The results show that the proposed bottleneck module can enhance adaptation performance to new unseen speakers, especially when limited amount of speaker-specific adaptation data is available.",
      "abstract": "Deep neural network (DNN)-based speech enhancement models often face challenges in maintaining their performance for speakers not encountered during training. This challenge is exacerbated in applications such as enhancement and bandwidth extension of bone-conducted speech, where the distortion exhibits a close correlation with speaker-specific characteristics. We address this issue by introducing a bottleneck module aimed at disentangling speaker-specific characteristics from speech content in speech enhancement DNNs. A DNN model is trained for enhancement of bone-conducted speech and modified with the proposed bottleneck module. We evaluate the DNN’s adaptability to unseen speakers through fine-tuning the network with a limited amount of adaptation data. The results show that the proposed bottleneck module can enhance adaptation performance to new unseen speakers, especially when limited amount of speaker-specific adaptation data is available.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447322",
      "openalex_id": "https://openalex.org/W4392909681",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Spoofing Capability for End-to-end Any-to-many Voice Conversion",
      "summary": "Audio deep synthesis techniques have been able to generate high-quality speech whose authenticity is difficult for humans to recognize. Meanwhile, many anti-spoofing systems have been developed to capture artifacts in the synthesized speech that are imperceptible to human hearing, thus a continuous escalating race of 'attacking and defending' in voice deepfake has started. Hence, to further improve the probability of successfully cheating anti-spoofing systems, we propose a fully end-to-end, any-to-many voice conversion method based on a non-autoregressive structure with the addition of two light but strong post-processing strategies namely silence replacement and global noise perturbation. Experimental results show that the proposed method performs better than current baselines in fooling several state-of-the-art anti-spoofing systems. Better naturalness and speaker similarity are also achieved, resulting in our proposed method showing high deception performance against humans.",
      "abstract": "Audio deep synthesis techniques have been able to generate high-quality speech whose authenticity is difficult for humans to recognize. Meanwhile, many anti-spoofing systems have been developed to capture artifacts in the synthesized speech that are imperceptible to human hearing, thus a continuous escalating race of 'attacking and defending' in voice deepfake has started. Hence, to further improve the probability of successfully cheating anti-spoofing systems, we propose a fully end-to-end, any-to-many voice conversion method based on a non-autoregressive structure with the addition of two light but strong post-processing strategies namely silence replacement and global noise perturbation. Experimental results show that the proposed method performs better than current baselines in fooling several state-of-the-art anti-spoofing systems. Better naturalness and speaker similarity are also achieved, resulting in our proposed method showing high deception performance against humans.",
      "doi": "https://doi.org/10.1145/3552466.3556532",
      "openalex_id": "https://openalex.org/W4298633870",
      "arxiv_id": "",
      "publication_date": "2022-10-01",
      "published": "2022-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Level Temporal-Channel Speaker Retrieval for Zero-Shot Voice Conversion",
      "summary": "Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">ulti-level</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">emporal-</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">c</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">hannel</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">r</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">etrieval</i> , referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">emporal-</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">c</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">hannel</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">r</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">etrieval (TCR)</i> , to find out <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">when</i> and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">where</i> speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently to achieve better speech disentanglement and reconstruction. To drive this process, we adopt perceptual constraints on three aspects: content, style, and speaker. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",
      "abstract": "Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">ulti-level</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">emporal-</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">c</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">hannel</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">r</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">etrieval</i> , referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">emporal-</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">c</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">hannel</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">r</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">etrieval (TCR)</i> , to find out <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">when</i> and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">where</i> speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently to achieve better speech disentanglement and reconstruction. To drive this process, we adopt perceptual constraints on three aspects: content, style, and speaker. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",
      "doi": "https://doi.org/10.1109/taslp.2024.3407577",
      "openalex_id": "https://openalex.org/W4399168655",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A New Spoken Language Teaching Tech: Combining Multi-attention and AdaIN for One-shot Cross Language Voice Conversion",
      "summary": "Computer aided pronunciation training(CAPT) plays an important role in oral language teaching. The main methods of traditional computer-assisted oral teaching include mispronunciation detection and pronunciation scoring and assessment.However, these two techniques only give negative feedback information such as scores or error categories. In this case,it is difficult for learners to refine their pronunciation through these two indicators without the guidance of correct speech.To tackle this problem, we proposed a cross language voice conversion(VC) framework that can generate speech with template speech content and learners’ own timbre,which can guide the learner’s pronunciation.To improve VC effect,we apply AdaIN in the fore-end and after the Value matrix in multi-head attention once respectively,called attention-AdaIN,which can improve the style transfer and sequence generation ability.We used attention-AdaIN to construct VC framework based on VAE.Experiments conducted on the AISHELL-3 and VCTK corpus showed that this new aprroach improved the baseline VAE-VC.",
      "abstract": "Computer aided pronunciation training(CAPT) plays an important role in oral language teaching. The main methods of traditional computer-assisted oral teaching include mispronunciation detection and pronunciation scoring and assessment.However, these two techniques only give negative feedback information such as scores or error categories. In this case,it is difficult for learners to refine their pronunciation through these two indicators without the guidance of correct speech.To tackle this problem, we proposed a cross language voice conversion(VC) framework that can generate speech with template speech content and learners’ own timbre,which can guide the learner’s pronunciation.To improve VC effect,we apply AdaIN in the fore-end and after the Value matrix in multi-head attention once respectively,called attention-AdaIN,which can improve the style transfer and sequence generation ability.We used attention-AdaIN to construct VC framework based on VAE.Experiments conducted on the AISHELL-3 and VCTK corpus showed that this new aprroach improved the baseline VAE-VC.",
      "doi": "https://doi.org/10.1109/iscslp57327.2022.10038137",
      "openalex_id": "https://openalex.org/W4319781964",
      "arxiv_id": "",
      "publication_date": "2022-12-11",
      "published": "2022-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MetricCycleGAN-VC: Forcing CycleGAN-Based Voice Conversion Systems to Associate With Objective Quality Metrics",
      "summary": "In CycleGAN-based voice conversion (VC) systems, such as MaskCycleGAN-VC, similar to many other GAN-based VC systems, adversarial losses are not explicitly designed to optimize VC evaluation metrics. Consequently, the generator may not consistently improve the metric scores related to VC assessments, such as the Mel-Cepstral Distance (MCD). To address this, we propose MetricCycleGAN-VC, which optimizes discriminators based on one or more VC evaluation metrics in a black-box strategy. Our approach covers various metrics related to VC quality triple aspects: speaker similarity, speech naturalness, and intelligibility. We Examine different well-known speech-related metrics such as MCD, Short-Time Objective of Intelligibility (STOI), neural tools such as Non-Intrusive objective Speech Quality Assessment (NISQA), and a newly proposed metric called Speaker Embedding Cosine Similarity (SECS). We also leverage full-reference image quality metrics like Feature-based similarity index Measure (FSIM) and combine all these intrusive, and non-intrusive metrics Within a unified framework for non-parallel voice conversion. While our method allows the utilization of all existing or newly developed quality metrics, this will be done without compromising their structural integrity or requiring knowledge of their formulation. Experimental results demonstrate improved desired metrics in both inter- and intra-gender conversions, showcasing the superiority of our approach over MaskCycleGAN-VC a typical baseline and many state-of-the-art VC systems. A key strength of our proposed method is its adaptability to evolving VC quality metrics, promising even better results as more precise metrics are introduced and implemented.",
      "abstract": "In CycleGAN-based voice conversion (VC) systems, such as MaskCycleGAN-VC, similar to many other GAN-based VC systems, adversarial losses are not explicitly designed to optimize VC evaluation metrics. Consequently, the generator may not consistently improve the metric scores related to VC assessments, such as the Mel-Cepstral Distance (MCD). To address this, we propose MetricCycleGAN-VC, which optimizes discriminators based on one or more VC evaluation metrics in a black-box strategy. Our approach covers various metrics related to VC quality triple aspects: speaker similarity, speech naturalness, and intelligibility. We Examine different well-known speech-related metrics such as MCD, Short-Time Objective of Intelligibility (STOI), neural tools such as Non-Intrusive objective Speech Quality Assessment (NISQA), and a newly proposed metric called Speaker Embedding Cosine Similarity (SECS). We also leverage full-reference image quality metrics like Feature-based similarity index Measure (FSIM) and combine all these intrusive, and non-intrusive metrics Within a unified framework for non-parallel voice conversion. While our method allows the utilization of all existing or newly developed quality metrics, this will be done without compromising their structural integrity or requiring knowledge of their formulation. Experimental results demonstrate improved desired metrics in both inter- and intra-gender conversions, showcasing the superiority of our approach over MaskCycleGAN-VC a typical baseline and many state-of-the-art VC systems. A key strength of our proposed method is its adaptability to evolving VC quality metrics, promising even better results as more precise metrics are introduced and implemented.",
      "doi": "https://doi.org/10.1109/access.2024.3471926",
      "openalex_id": "https://openalex.org/W4403022196",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Quantized Prosody Representation for Controllable Speech Synthesis",
      "summary": "In this paper, we propose a novel prosody disentangle method for prosodic Text-to-Speech (TTS) model, which introduces the vector quantization (VQ) method to the auxiliary prosody encoder to obtain the decomposed prosody representations in an unsupervised manner. Rely on its advantages, the speaking styles, such as pitch, speaking velocity, local pitch variance, etc., are decomposed automatically into the latent quantize vectors. We also investigate the internal mechanism of VQ disentangle process by means of a latent variables counter and find that higher value dimensions usually represent prosody information. Experiments show that our model can control the speaking styles of synthesis results by directly manipulating the latent variables. The objective and subjective evaluations illustrated that our model outperforms the popular models.",
      "abstract": "In this paper, we propose a novel prosody disentangle method for prosodic Text-to-Speech (TTS) model, which introduces the vector quantization (VQ) method to the auxiliary prosody encoder to obtain the decomposed prosody representations in an unsupervised manner. Rely on its advantages, the speaking styles, such as pitch, speaking velocity, local pitch variance, etc., are decomposed automatically into the latent quantize vectors. We also investigate the internal mechanism of VQ disentangle process by means of a latent variables counter and find that higher value dimensions usually represent prosody information. Experiments show that our model can control the speaking styles of synthesis results by directly manipulating the latent variables. The objective and subjective evaluations illustrated that our model outperforms the popular models.",
      "doi": "https://doi.org/10.1109/icme52920.2022.9859946",
      "openalex_id": "https://openalex.org/W4226487411",
      "arxiv_id": "",
      "publication_date": "2022-07-18",
      "published": "2022-07-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Review of User Identification Methods Based on Digital Fingerprint",
      "summary": "Methods of user identification based on digital fingerprints are considered. The main approaches for the browser fingerprints creation which is installed on the user's device and characterizes the device belonging to the user are presented. The methods used to identify a person (user) during the operation of the device are also described. Methods using both the dynamics of keystrokes and interactions with the touch screen, voice and geolocation data, as well as behavioral biometrics and behavioral profile are presented. The concept of continuous authentication is described as a development of the identification approach. A list of publicly available data sets mentioned in the studies reviewed in the review is provided, with links to download them.",
      "abstract": "Methods of user identification based on digital fingerprints are considered. The main approaches for the browser fingerprints creation which is installed on the user's device and characterizes the device belonging to the user are presented. The methods used to identify a person (user) during the operation of the device are also described. Methods using both the dynamics of keystrokes and interactions with the touch screen, voice and geolocation data, as well as behavioral biometrics and behavioral profile are presented. The concept of continuous authentication is described as a development of the identification approach. A list of publicly available data sets mentioned in the studies reviewed in the review is provided, with links to download them.",
      "doi": "https://doi.org/10.31854/1813-324x-2023-9-5-91-111",
      "openalex_id": "https://openalex.org/W4388733014",
      "arxiv_id": "",
      "publication_date": "2023-11-14",
      "published": "2023-11-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized Representation",
      "summary": "Most existing neural-based text-to-speech methods rely on extensive datasets and face challenges under low-resource condition. In this paper, we introduce a novel semi-supervised text-to-speech synthesis model that learns from both paired and unpaired data to address this challenge. The key component of the proposed model is a dynamic quantized representation module, which is integrated into a sequential autoencoder. When given paired data, the module incorporates a trainable codebook that learns quantized representations under the supervision of the paired data. However, due to the limited paired data in low-resource scenario, these paired data are difficult to cover all phonemes. Then unpaired data is fed to expand the dynamic codebook by adding quantized representation vectors that are sufficiently distant from the existing ones during training. Experiments show that with less than 120 minutes of paired data, the proposed method outperforms existing methods in both subjective and objective metrics.",
      "abstract": "Most existing neural-based text-to-speech methods rely on extensive datasets and face challenges under low-resource condition. In this paper, we introduce a novel semi-supervised text-to-speech synthesis model that learns from both paired and unpaired data to address this challenge. The key component of the proposed model is a dynamic quantized representation module, which is integrated into a sequential autoencoder. When given paired data, the module incorporates a trainable codebook that learns quantized representations under the supervision of the paired data. However, due to the limited paired data in low-resource scenario, these paired data are difficult to cover all phonemes. Then unpaired data is fed to expand the dynamic codebook by adding quantized representation vectors that are sufficiently distant from the existing ones during training. Experiments show that with less than 120 minutes of paired data, the proposed method outperforms existing methods in both subjective and objective metrics.",
      "doi": "https://doi.org/10.1109/ispa-bdcloud-socialcom-sustaincom59178.2023.00154",
      "openalex_id": "https://openalex.org/W4394712318",
      "arxiv_id": "",
      "publication_date": "2023-12-21",
      "published": "2023-12-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASGAN-VC: One-Shot Voice Conversion with Additional Style Embedding and Generative Adversarial Networks",
      "summary": "In this paper, we present a voice conversion system that improves the quality of generated voice and its similarity to the target voice style significantly. Many VC systems use feature-disentangle-based learning techniques to separate speakers' voices from their linguistic content in order to translate a voice into another style. This is the approach we are taking. To prevent speaker-style information from obscuring the content embedding, some previous works quantize or reduce the dimension of the embedding. However, an imperfect disentanglement would damage the quality and similarity of the sound. In this paper, to further improve quality and similarity in voice conversion, we propose a novel style transfer method within an autoencoder-based VC system that involves generative adversarial training. The conversion process was objectively evaluated using the fair third-party speaker verification system, the results shows that ASGAN-VC outperforms VQVC + and AGAINVC in terms of speaker similarity. A subjectively observing that our proposal outperformed the VQVC + and AGAINVC in terms of naturalness and speaker similarity.",
      "abstract": "In this paper, we present a voice conversion system that improves the quality of generated voice and its similarity to the target voice style significantly. Many VC systems use feature-disentangle-based learning techniques to separate speakers' voices from their linguistic content in order to translate a voice into another style. This is the approach we are taking. To prevent speaker-style information from obscuring the content embedding, some previous works quantize or reduce the dimension of the embedding. However, an imperfect disentanglement would damage the quality and similarity of the sound. In this paper, to further improve quality and similarity in voice conversion, we propose a novel style transfer method within an autoencoder-based VC system that involves generative adversarial training. The conversion process was objectively evaluated using the fair third-party speaker verification system, the results shows that ASGAN-VC outperforms VQVC + and AGAINVC in terms of speaker similarity. A subjectively observing that our proposal outperformed the VQVC + and AGAINVC in terms of naturalness and speaker similarity.",
      "doi": "https://doi.org/10.23919/apsipaasc55919.2022.9979975",
      "openalex_id": "https://openalex.org/W4312097414",
      "arxiv_id": "",
      "publication_date": "2022-11-07",
      "published": "2022-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme",
      "summary": "Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying the target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.",
      "abstract": "Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying the target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.",
      "doi": "https://doi.org/10.48550/arxiv.2109.13821",
      "openalex_id": "https://openalex.org/W3204602440",
      "arxiv_id": "",
      "publication_date": "2021-09-28",
      "published": "2021-09-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Controllable voice conversion based on quantization of voice factor scores",
      "summary": "Various voice conversion models using deep learning have been proposed. However, most focus on speaker identity con-version from one person to another. To the best of our knowledge, there is no model that controls the factors that determine voice tone (e.g. husky and nasal) and transforms it. We propose voice factor quantized splitternetVC (VFQ-SplitterNetVC), a voice conversion model that can control voice tone by learning factors that determine voice tone, such as the degree of huskiness, and their strength and weakness, based on SplitterNetVC and vector quantized-variational auto encoder (VQ- VAE). We evaluated the naturalness of the converted voice, speaker similarity, and voice tone control performance of the proposed model in subjective experiments. We found that VFQ-SplitterNetVC showed high speaker similarity and voice tone control performance.",
      "abstract": "Various voice conversion models using deep learning have been proposed. However, most focus on speaker identity con-version from one person to another. To the best of our knowledge, there is no model that controls the factors that determine voice tone (e.g. husky and nasal) and transforms it. We propose voice factor quantized splitternetVC (VFQ-SplitterNetVC), a voice conversion model that can control voice tone by learning factors that determine voice tone, such as the degree of huskiness, and their strength and weakness, based on SplitterNetVC and vector quantized-variational auto encoder (VQ- VAE). We evaluated the naturalness of the converted voice, speaker similarity, and voice tone control performance of the proposed model in subjective experiments. We found that VFQ-SplitterNetVC showed high speaker similarity and voice tone control performance.",
      "doi": "https://doi.org/10.23919/apsipaasc55919.2022.9979912",
      "openalex_id": "https://openalex.org/W4312097058",
      "arxiv_id": "",
      "publication_date": "2022-11-07",
      "published": "2022-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Collection of Transcribed Speech for Low Resources Languages",
      "summary": "Speech is a crucial for human communication and combined with the evolution of instant messaging in voice format as well as automated chatbots, its importance is greater. While the majority of speech technologies have achieved high accuracy, they fail when tested for accents that deviate from the “standard” of a language. This becomes more concerning for languages that lack on datasets and have scarce literature, like Brazilian Portuguese. Thus, this paper proposes a methodology to collect and release a speech dataset for Brazilian Portuguese. The method explores the availability of data and information in video platforms, and automatically extracts the audio from TEDx Talks.",
      "abstract": "Speech is a crucial for human communication and combined with the evolution of instant messaging in voice format as well as automated chatbots, its importance is greater. While the majority of speech technologies have achieved high accuracy, they fail when tested for accents that deviate from the “standard” of a language. This becomes more concerning for languages that lack on datasets and have scarce literature, like Brazilian Portuguese. Thus, this paper proposes a methodology to collect and release a speech dataset for Brazilian Portuguese. The method explores the availability of data and information in video platforms, and automatically extracts the audio from TEDx Talks.",
      "doi": "https://doi.org/10.1109/icprs58416.2023.10179033",
      "openalex_id": "https://openalex.org/W4384699182",
      "arxiv_id": "",
      "publication_date": "2023-07-04",
      "published": "2023-07-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero-Shot Voice Conversion Based on Speaker Embedding Domain Generalization",
      "summary": "In this paper, a zero-shot voice conversion frame-work is constructed by effectively decoupling the semantic and speaker features in speech. The proposed method is based on the pre-trained wav2vec 2.0 model to extract semantic features from source speakers and a WavLM model to extract speaker features from target speakers. We propose the Robust-MAML model to map the speaker feature of the target speaker into a domain generalization space, making it directly applicable to any unregistered speaker domain. Finally, through transfer learning, the speech synthesis model FastSpeech2 integrates the semantic feature and domain-generalized speaker features to synthesize the target speaker's voice. Experimental results show that the proposed method outperforms the common baseline systems in both naturalness and speaker similarity.",
      "abstract": "In this paper, a zero-shot voice conversion frame-work is constructed by effectively decoupling the semantic and speaker features in speech. The proposed method is based on the pre-trained wav2vec 2.0 model to extract semantic features from source speakers and a WavLM model to extract speaker features from target speakers. We propose the Robust-MAML model to map the speaker feature of the target speaker into a domain generalization space, making it directly applicable to any unregistered speaker domain. Finally, through transfer learning, the speech synthesis model FastSpeech2 integrates the semantic feature and domain-generalized speaker features to synthesize the target speaker's voice. Experimental results show that the proposed method outperforms the common baseline systems in both naturalness and speaker similarity.",
      "doi": "https://doi.org/10.1109/rivf60135.2023.10471830",
      "openalex_id": "https://openalex.org/W4393079319",
      "arxiv_id": "",
      "publication_date": "2023-12-23",
      "published": "2023-12-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AOSR-Net: All-in-One Sandstorm Removal Network",
      "summary": "Most existing sandstorm image enhancement methods are based on traditional theory and prior knowledge, which often restrict their applicability in real-world scenarios. In addition, these approaches often adopt a strategy of color correction followed by dust removal, which makes the algorithm structure too complex. To solve the issue, we introduce a novel image restoration model, named all-in-one sandstorm removal network (AOSR-Net). This model is developed based on a re-formulated sandstorm scattering model, which directly establishes the image mapping relationship by integrating intermediate parameters. Such integration scheme effectively addresses the problems of over-enhancement and weak generalization in the field of sand dust image enhancement. Experimental results on synthetic and real-world sandstorm images demonstrate the superiority of the proposed AOSR-Net over state-of-the-art (SOTA) algorithms.",
      "abstract": "Most existing sandstorm image enhancement methods are based on traditional theory and prior knowledge, which often restrict their applicability in real-world scenarios. In addition, these approaches often adopt a strategy of color correction followed by dust removal, which makes the algorithm structure too complex. To solve the issue, we introduce a novel image restoration model, named all-in-one sandstorm removal network (AOSR-Net). This model is developed based on a re-formulated sandstorm scattering model, which directly establishes the image mapping relationship by integrating intermediate parameters. Such integration scheme effectively addresses the problems of over-enhancement and weak generalization in the field of sand dust image enhancement. Experimental results on synthetic and real-world sandstorm images demonstrate the superiority of the proposed AOSR-Net over state-of-the-art (SOTA) algorithms.",
      "doi": "https://doi.org/10.1109/ictai59109.2023.00100",
      "openalex_id": "https://openalex.org/W4389988694",
      "arxiv_id": "",
      "publication_date": "2023-11-06",
      "published": "2023-11-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring the Role of Language Families for Building Indic Speech Synthesisers",
      "summary": "&lt;p&gt;Building end-to-end speech synthesisers for Indian languages is challenging, given the lack of adequate clean training data and multiple grapheme representations across languages. This work explores the importance of training multilingual and multi-speaker text-to-speech (TTS) systems based on language families. Such a study is crucial not only from a low resource perspective but also to reduce the overall consumption of computing power. &lt;/p&gt; &lt;p&gt;TTS systems are trained separately for Indo-Aryan and Dravidian language families, and their performance is compared to that of a combined Indo-Aryan+Dravidian voice. We also investigate the amount of training data required for a language in a multilingual setting. We want to see if we can do more with less data without compromising the quality and intelligibility of the synthesised speech. The voices are easily extendable to new languages with limited data. Same-family and cross-family synthesis and adaptation to new languages are analysed. The analyses show that language family-wise training of Indic systems is the way forward for the Indian subcontinent, where a large number of languages are spoken.&lt;/p&gt; &lt;p&gt;&lt;br&gt;&lt;/p&gt;",
      "abstract": "&lt;p&gt;Building end-to-end speech synthesisers for Indian languages is challenging, given the lack of adequate clean training data and multiple grapheme representations across languages. This work explores the importance of training multilingual and multi-speaker text-to-speech (TTS) systems based on language families. Such a study is crucial not only from a low resource perspective but also to reduce the overall consumption of computing power. &lt;/p&gt; &lt;p&gt;TTS systems are trained separately for Indo-Aryan and Dravidian language families, and their performance is compared to that of a combined Indo-Aryan+Dravidian voice. We also investigate the amount of training data required for a language in a multilingual setting. We want to see if we can do more with less data without compromising the quality and intelligibility of the synthesised speech. The voices are easily extendable to new languages with limited data. Same-family and cross-family synthesis and adaptation to new languages are analysed. The analyses show that language family-wise training of Indic systems is the way forward for the Indian subcontinent, where a large number of languages are spoken.&lt;/p&gt; &lt;p&gt;&lt;br&gt;&lt;/p&gt;",
      "doi": "https://doi.org/10.36227/techrxiv.21435756",
      "openalex_id": "https://openalex.org/W4308002754",
      "arxiv_id": "",
      "publication_date": "2022-11-02",
      "published": "2022-11-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model",
      "summary": "Abstract The ChatGPT, a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs have stirred up much interest among researchers and practitioners in their impressive skills in natural language processing tasks, which profoundly impact various fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. We also present cases to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study. Overall, LLMs have the potential to revolutionize dental diagnosis and treatment, which indicates a promising avenue for clinical application and research in dentistry.",
      "abstract": "Abstract The ChatGPT, a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs have stirred up much interest among researchers and practitioners in their impressive skills in natural language processing tasks, which profoundly impact various fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. We also present cases to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study. Overall, LLMs have the potential to revolutionize dental diagnosis and treatment, which indicates a promising avenue for clinical application and research in dentistry.",
      "doi": "https://doi.org/10.1038/s41368-023-00239-y",
      "openalex_id": "https://openalex.org/W4385346108",
      "arxiv_id": "",
      "publication_date": "2023-07-28",
      "published": "2023-07-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Channel-Aware Pretraining Of Joint Encoder-Decoder Self-Supervised Model For Telephonic-Speech ASR",
      "summary": "This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of $\\sim 4$% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.",
      "abstract": "This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of $\\sim 4$% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193218",
      "openalex_id": "https://openalex.org/W4385489023",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UniEnc-CASSNAT: An Encoder-Only Non-Autoregressive ASR for Speech SSL Models",
      "summary": "Non-autoregressive automatic speech recognition (NASR) models have gained\\nattention due to their parallelism and fast inference. The encoder-based NASR,\\ne.g. connectionist temporal classification (CTC), can be initialized from the\\nspeech foundation models (SFM) but does not account for any dependencies among\\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\\ndependency problem but is not able to efficiently integrate SFM. Inspired by\\nthe success of recent work of speech-text joint pre-training with a shared\\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\\nencoder as the major module, which can be the SFM. The encoder plays the role\\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\\nof the encoder accepts the speech signal as input, while the concatenation of\\nthe speech signal and the token-level acoustic embedding is used as the input\\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\\nmodel parameters. Our codes are publicly available.\\n",
      "abstract": "Non-autoregressive automatic speech recognition (NASR) models have gained\\nattention due to their parallelism and fast inference. The encoder-based NASR,\\ne.g. connectionist temporal classification (CTC), can be initialized from the\\nspeech foundation models (SFM) but does not account for any dependencies among\\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\\ndependency problem but is not able to efficiently integrate SFM. Inspired by\\nthe success of recent work of speech-text joint pre-training with a shared\\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\\nencoder as the major module, which can be the SFM. The encoder plays the role\\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\\nof the encoder accepts the speech signal as input, while the concatenation of\\nthe speech signal and the token-level acoustic embedding is used as the input\\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\\nmodel parameters. Our codes are publicly available.\\n",
      "doi": "https://doi.org/10.1109/lsp.2024.3365036",
      "openalex_id": "https://openalex.org/W4391759660",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating images using generative adversarial networks based on text descriptions",
      "summary": "Modern developments in the fields of natural language processing (NLP) and computer vision (CV) emphasize the increasing importance of generating images from text descriptions. The presented article analyzes and compares two key methods in this area: generative adversarial network with conditional latent semantic analysis (GAN-CLS) and ultra-long transformer network (XLNet). The main components of GAN-CLS, including the generator, discriminator, and text encoder, are discussed in the context of their functional tasks—generating images from text inputs, assessing the realism of generated images, and converting text descriptions into latent spaces, respectively. A detailed comparative analysis of the performance of GAN-CLS and XLNet, the latter of which is widely used in the organic light-emitting diode (OEL) field, is carried out. The purpose of the study is to determine the effectiveness of each method in different scenarios and then provide valuable recommendations for selecting the best method for generating images from text descriptions, taking into account specific tasks and resources. Ultimately, our paper aims to be a valuable research resource by providing scientific guidance for NLP and CV experts.",
      "abstract": "Modern developments in the fields of natural language processing (NLP) and computer vision (CV) emphasize the increasing importance of generating images from text descriptions. The presented article analyzes and compares two key methods in this area: generative adversarial network with conditional latent semantic analysis (GAN-CLS) and ultra-long transformer network (XLNet). The main components of GAN-CLS, including the generator, discriminator, and text encoder, are discussed in the context of their functional tasks—generating images from text inputs, assessing the realism of generated images, and converting text descriptions into latent spaces, respectively. A detailed comparative analysis of the performance of GAN-CLS and XLNet, the latter of which is widely used in the organic light-emitting diode (OEL) field, is carried out. The purpose of the study is to determine the effectiveness of each method in different scenarios and then provide valuable recommendations for selecting the best method for generating images from text descriptions, taking into account specific tasks and resources. Ultimately, our paper aims to be a valuable research resource by providing scientific guidance for NLP and CV experts.",
      "doi": "https://doi.org/10.11591/ijece.v14i2.pp2014-2023",
      "openalex_id": "https://openalex.org/W4391260976",
      "arxiv_id": "",
      "publication_date": "2024-01-26",
      "published": "2024-01-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NTT Multi-Speaker ASR System for the DASR Task of CHiME-7 Challenge",
      "summary": "We introduce our submission to the Distant automatic speech recognition (DSAR) task of the CHiME 7 challenge.Our system uses end-to-end diarization with vector clustering (EEND-VC), guided source separation (GSS), and attention-based encoder-decoder and transducer-based ASR systems.Our submission exploits pre-trained self-supervised learning (SSL) models to build strong diarization and ASR modules.We also explore data augmentation using contrastive data selection based on representations from SSL models.Besides, we use self-supervised adaptation (SSA) to adapt these modules to the recording conditions of each session.Our DASR system achieves a 36 % diarization error rate (DER) reduction and 47 % word error rate reduction (WER) over the baseline on the main track of the evaluation set and ranked third in the challenge.",
      "abstract": "We introduce our submission to the Distant automatic speech recognition (DSAR) task of the CHiME 7 challenge.Our system uses end-to-end diarization with vector clustering (EEND-VC), guided source separation (GSS), and attention-based encoder-decoder and transducer-based ASR systems.Our submission exploits pre-trained self-supervised learning (SSL) models to build strong diarization and ASR modules.We also explore data augmentation using contrastive data selection based on representations from SSL models.Besides, we use self-supervised adaptation (SSA) to adapt these modules to the recording conditions of each session.Our DASR system achieves a 36 % diarization error rate (DER) reduction and 47 % word error rate reduction (WER) over the baseline on the main track of the evaluation set and ranked third in the challenge.",
      "doi": "https://doi.org/10.21437/chime.2023-9",
      "openalex_id": "https://openalex.org/W4389315131",
      "arxiv_id": "",
      "publication_date": "2023-08-25",
      "published": "2023-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Data Selection for Efficient Speech Processing",
      "summary": "While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.",
      "abstract": "While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.",
      "doi": "https://doi.org/10.1109/access.2025.3582395",
      "openalex_id": "https://openalex.org/W4411550778",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DFNet: Decoupled Fusion Network for Dialectal Speech Recognition",
      "summary": "Deep learning is often inadequate for achieving effective dialect recognition in situations where data are limited and model training is complex. Differences between Mandarin and dialects, such as the varied pronunciation variants and distinct linguistic features of dialects, often result in a significant decline in recognition performance. In addition, existing work often overlooks the similarities between Mandarin and its dialects and fails to leverage these connections to enhance recognition accuracy. To address these challenges, we propose the Decoupled Fusion Network (DFNet). This network extracts acoustic private and shared features of different languages through feature decoupling, which enhances adaptation to the uniqueness and similarity of these two speech patterns. In addition, we designed a heterogeneous information-weighted fusion module to effectively combine the decoupled Mandarin and dialect features. This strategy leverages the similarity between Mandarin and its dialects, enabling the sharing of multilingual information, and notably enhance the model’s recognition capabilities on low-resource dialect data. An evaluation of our method on the Henan and Guangdong datasets shows that the DFNet performance has improved by 2.64% and 2.68%, respectively. Additionally, a significant number of ablation comparison experiments demonstrate the effectiveness of the method.",
      "abstract": "Deep learning is often inadequate for achieving effective dialect recognition in situations where data are limited and model training is complex. Differences between Mandarin and dialects, such as the varied pronunciation variants and distinct linguistic features of dialects, often result in a significant decline in recognition performance. In addition, existing work often overlooks the similarities between Mandarin and its dialects and fails to leverage these connections to enhance recognition accuracy. To address these challenges, we propose the Decoupled Fusion Network (DFNet). This network extracts acoustic private and shared features of different languages through feature decoupling, which enhances adaptation to the uniqueness and similarity of these two speech patterns. In addition, we designed a heterogeneous information-weighted fusion module to effectively combine the decoupled Mandarin and dialect features. This strategy leverages the similarity between Mandarin and its dialects, enabling the sharing of multilingual information, and notably enhance the model’s recognition capabilities on low-resource dialect data. An evaluation of our method on the Henan and Guangdong datasets shows that the DFNet performance has improved by 2.64% and 2.68%, respectively. Additionally, a significant number of ablation comparison experiments demonstrate the effectiveness of the method.",
      "doi": "https://doi.org/10.3390/math12121886",
      "openalex_id": "https://openalex.org/W4399730635",
      "arxiv_id": "",
      "publication_date": "2024-06-17",
      "published": "2024-06-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Leveraging language foundation models for human mobility forecasting",
      "summary": "In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility.",
      "abstract": "In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility.",
      "doi": "https://doi.org/10.1145/3557915.3561026",
      "openalex_id": "https://openalex.org/W4309651822",
      "arxiv_id": "",
      "publication_date": "2022-11-01",
      "published": "2022-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluating Self-Supervised Speech Representations for Speech Emotion Recognition",
      "summary": "Self-supervised learning has recently been implemented widely in speech processing areas, replacing conventional acoustic feature extraction to extract meaningful information from speech. One of the challenging applications of speech processing is to extract affective information from speech, commonly called speech emotion recognition. Until now, it is not clear the position of these speech representations compared to the classical acoustic feature. This paper evaluates nineteen self-supervised speech representations and one classical acoustic feature for five distinct speech emotion recognition datasets on the same classifier. We calculate the effect size among twenty speech representations to show the magnitude of relative differences from the top to the lowest performance. The top three are WavLM Large, UniSpeech-SAT Large, and HuBERT Large, with negligible effect sizes among them. The significance test supports the difference among self-supervised speech representations. The best prediction for each dataset is shown in the form of a confusion matrix to gain insights into the best performance of speech representations for each emotion category based on the training data from balanced vs. unbalanced datasets, English vs. Japanese corpus, and five vs. six emotion categories. Despite showing their competitiveness, this exploration of self-supervised learning for speech emotion recognition also shows their limitations on models pre-trained on small data and trained on unbalanced datasets.",
      "abstract": "Self-supervised learning has recently been implemented widely in speech processing areas, replacing conventional acoustic feature extraction to extract meaningful information from speech. One of the challenging applications of speech processing is to extract affective information from speech, commonly called speech emotion recognition. Until now, it is not clear the position of these speech representations compared to the classical acoustic feature. This paper evaluates nineteen self-supervised speech representations and one classical acoustic feature for five distinct speech emotion recognition datasets on the same classifier. We calculate the effect size among twenty speech representations to show the magnitude of relative differences from the top to the lowest performance. The top three are WavLM Large, UniSpeech-SAT Large, and HuBERT Large, with negligible effect sizes among them. The significance test supports the difference among self-supervised speech representations. The best prediction for each dataset is shown in the form of a confusion matrix to gain insights into the best performance of speech representations for each emotion category based on the training data from balanced vs. unbalanced datasets, English vs. Japanese corpus, and five vs. six emotion categories. Despite showing their competitiveness, this exploration of self-supervised learning for speech emotion recognition also shows their limitations on models pre-trained on small data and trained on unbalanced datasets.",
      "doi": "https://doi.org/10.1109/access.2022.3225198",
      "openalex_id": "https://openalex.org/W4312951904",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning",
      "summary": "Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. The purification module aims at alleviating the adversarial perturbations in the samples and pulling the contaminated adversarial inputs back towards the decision boundary. Experimental results show that our proposed purification module effectively counters adversarial attacks and outperforms traditional filters from both alleviating the adversarial noise and maintaining the performance of genuine samples. The detection module aims at detecting adversarial samples from genuine ones based on the statistical properties of ASV scores derived by a unique ASV integrating with different number of SSLMs. Experimental results show that our detection module helps shield the ASV by detecting adversarial samples. Both purification and detection methods are helpful for defending against different kinds of attack algorithms. Moreover, since there is no common metric for evaluating the ASV performance under adversarial attacks, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.",
      "abstract": "Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. The purification module aims at alleviating the adversarial perturbations in the samples and pulling the contaminated adversarial inputs back towards the decision boundary. Experimental results show that our proposed purification module effectively counters adversarial attacks and outperforms traditional filters from both alleviating the adversarial noise and maintaining the performance of genuine samples. The detection module aims at detecting adversarial samples from genuine ones based on the statistical properties of ASV scores derived by a unique ASV integrating with different number of SSLMs. Experimental results show that our detection module helps shield the ASV by detecting adversarial samples. Both purification and detection methods are helpful for defending against different kinds of attack algorithms. Moreover, since there is no common metric for evaluating the ASV performance under adversarial attacks, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.",
      "doi": "https://doi.org/10.1109/taslp.2021.3133189",
      "openalex_id": "https://openalex.org/W3173081491",
      "arxiv_id": "",
      "publication_date": "2021-12-09",
      "published": "2021-12-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding the brain with attention: A survey of transformers in brain sciences",
      "summary": "Abstract Owing to their superior capabilities and advanced achievements, Transformers have gradually attracted attention with regard to understanding complex brain processing mechanisms. This study aims to comprehensively review and discuss the applications of Transformers in brain sciences. First, we present a brief introduction of the critical architecture of Transformers. Then, we overview and analyze their most relevant applications in brain sciences, including brain disease diagnosis, brain age prediction, brain anomaly detection, semantic segmentation, multi‐modal registration, functional Magnetic Resonance Imaging (fMRI) modeling, Electroencephalogram (EEG) processing, and multi‐task collaboration. We organize the model details and open sources for reference and replication. In addition, we discuss the quantitative assessments, model complexity, and optimization of Transformers, which are topics of great concern in the field. Finally, we explore possible future challenges and opportunities, exploiting some concrete and recent cases to provoke discussion and innovation. We hope that this review will stimulate interest in further research on Transformers in the context of brain sciences.",
      "abstract": "Abstract Owing to their superior capabilities and advanced achievements, Transformers have gradually attracted attention with regard to understanding complex brain processing mechanisms. This study aims to comprehensively review and discuss the applications of Transformers in brain sciences. First, we present a brief introduction of the critical architecture of Transformers. Then, we overview and analyze their most relevant applications in brain sciences, including brain disease diagnosis, brain age prediction, brain anomaly detection, semantic segmentation, multi‐modal registration, functional Magnetic Resonance Imaging (fMRI) modeling, Electroencephalogram (EEG) processing, and multi‐task collaboration. We organize the model details and open sources for reference and replication. In addition, we discuss the quantitative assessments, model complexity, and optimization of Transformers, which are topics of great concern in the field. Finally, we explore possible future challenges and opportunities, exploiting some concrete and recent cases to provoke discussion and innovation. We hope that this review will stimulate interest in further research on Transformers in the context of brain sciences.",
      "doi": "https://doi.org/10.1002/brx2.29",
      "openalex_id": "https://openalex.org/W4387620292",
      "arxiv_id": "",
      "publication_date": "2023-09-01",
      "published": "2023-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers",
      "summary": "In this paper we propose a novel virtual simulation-pilot engine for speeding up air traffic controller (ATCo) training by integrating different state-of-the-art artificial intelligence (AI)-based tools. The virtual simulation-pilot engine receives spoken communications from ATCo trainees, and it performs automatic speech recognition and understanding. Thus, it goes beyond only transcribing the communication and can also understand its meaning. The output is subsequently sent to a response generator system, which resembles the spoken read-back that pilots give to the ATCo trainees. The overall pipeline is composed of the following submodules: (i) an automatic speech recognition (ASR) system that transforms audio into a sequence of words; (ii) a high-level air traffic control (ATC)-related entity parser that understands the transcribed voice communication; and (iii) a text-to-speech submodule that generates a spoken utterance that resembles a pilot based on the situation of the dialogue. Our system employs state-of-the-art AI-based tools such as Wav2Vec 2.0, Conformer, BERT and Tacotron models. To the best of our knowledge, this is the first work fully based on open-source ATC resources and AI tools. In addition, we develop a robust and modular system with optional submodules that can enhance the system’s performance by incorporating real-time surveillance data, metadata related to exercises (such as sectors or runways), or even a deliberate read-back error to train ATCo trainees to identify them. Our ASR system can reach as low as 5.5% and 15.9% absolute word error rates (WER) on high- and low-quality ATC audio. We also demonstrate that adding surveillance data into the ASR can yield a callsign detection accuracy of more than 96%.",
      "abstract": "In this paper we propose a novel virtual simulation-pilot engine for speeding up air traffic controller (ATCo) training by integrating different state-of-the-art artificial intelligence (AI)-based tools. The virtual simulation-pilot engine receives spoken communications from ATCo trainees, and it performs automatic speech recognition and understanding. Thus, it goes beyond only transcribing the communication and can also understand its meaning. The output is subsequently sent to a response generator system, which resembles the spoken read-back that pilots give to the ATCo trainees. The overall pipeline is composed of the following submodules: (i) an automatic speech recognition (ASR) system that transforms audio into a sequence of words; (ii) a high-level air traffic control (ATC)-related entity parser that understands the transcribed voice communication; and (iii) a text-to-speech submodule that generates a spoken utterance that resembles a pilot based on the situation of the dialogue. Our system employs state-of-the-art AI-based tools such as Wav2Vec 2.0, Conformer, BERT and Tacotron models. To the best of our knowledge, this is the first work fully based on open-source ATC resources and AI tools. In addition, we develop a robust and modular system with optional submodules that can enhance the system’s performance by incorporating real-time surveillance data, metadata related to exercises (such as sectors or runways), or even a deliberate read-back error to train ATCo trainees to identify them. Our ASR system can reach as low as 5.5% and 15.9% absolute word error rates (WER) on high- and low-quality ATC audio. We also demonstrate that adding surveillance data into the ASR can yield a callsign detection accuracy of more than 96%.",
      "doi": "https://doi.org/10.3390/aerospace10050490",
      "openalex_id": "https://openalex.org/W4377289660",
      "arxiv_id": "",
      "publication_date": "2023-05-22",
      "published": "2023-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Beyond Labels: A Comprehensive Review of Self-Supervised Learning and Intrinsic Data Properties",
      "summary": "Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.",
      "abstract": "Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.",
      "doi": "https://doi.org/10.55662/jst.2023.4403",
      "openalex_id": "https://openalex.org/W4402255177",
      "arxiv_id": "",
      "publication_date": "2023-08-20",
      "published": "2023-08-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-View Collaborative Training and Self-Supervised Learning for Group Recommendation",
      "summary": "Recommendation systems offer an effective solution to information overload, finding widespread application across e-commerce, news platforms, and beyond. By analyzing interaction histories, these systems automatically filter and recommend items that are most likely to resonate with users. Recently, with the swift advancement of social networking, group recommendation has emerged as a compelling research area, enabling personalized recommendations for groups of users. Unlike individual recommendation, group recommendation must consider both individual preferences and group dynamics, thereby enhancing decision-making efficiency for groups. One of the key challenges facing recommendation algorithms is data sparsity, a limitation that is even more severe in group recommendation than in traditional recommendation tasks. While various group recommendation methods attempt to address this issue, many of them still rely on single-view modeling or fail to sufficiently account for individual user preferences within a group, limiting their effectiveness. This paper addresses the data sparsity issue to improve group recommendation performance, overcoming the limitations of overlooking individual user recommendation tasks and depending on single-view modeling. We propose MCSS (multi-view collaborative training and self-supervised learning), a novel framework that harnesses both multi-view collaborative training and self-supervised learning specifically for group recommendations. By incorporating both group and individual recommendation tasks, MCSS leverages graph convolution and attention mechanisms to generate three sets of embeddings, enhancing the model’s representational power. Additionally, we design self-supervised auxiliary tasks to maximize the data utility, further enhancing performance. Through multi-task joint training, the model generates refined recommendation lists tailored to each group and individual user. Extensive validation and comparison demonstrate the method’s robustness and effectiveness, underscoring the potential of MCSS to advance state-of-the-art group recommendation.",
      "abstract": "Recommendation systems offer an effective solution to information overload, finding widespread application across e-commerce, news platforms, and beyond. By analyzing interaction histories, these systems automatically filter and recommend items that are most likely to resonate with users. Recently, with the swift advancement of social networking, group recommendation has emerged as a compelling research area, enabling personalized recommendations for groups of users. Unlike individual recommendation, group recommendation must consider both individual preferences and group dynamics, thereby enhancing decision-making efficiency for groups. One of the key challenges facing recommendation algorithms is data sparsity, a limitation that is even more severe in group recommendation than in traditional recommendation tasks. While various group recommendation methods attempt to address this issue, many of them still rely on single-view modeling or fail to sufficiently account for individual user preferences within a group, limiting their effectiveness. This paper addresses the data sparsity issue to improve group recommendation performance, overcoming the limitations of overlooking individual user recommendation tasks and depending on single-view modeling. We propose MCSS (multi-view collaborative training and self-supervised learning), a novel framework that harnesses both multi-view collaborative training and self-supervised learning specifically for group recommendations. By incorporating both group and individual recommendation tasks, MCSS leverages graph convolution and attention mechanisms to generate three sets of embeddings, enhancing the model’s representational power. Additionally, we design self-supervised auxiliary tasks to maximize the data utility, further enhancing performance. Through multi-task joint training, the model generates refined recommendation lists tailored to each group and individual user. Extensive validation and comparison demonstrate the method’s robustness and effectiveness, underscoring the potential of MCSS to advance state-of-the-art group recommendation.",
      "doi": "https://doi.org/10.3390/math13010066",
      "openalex_id": "https://openalex.org/W4405968073",
      "arxiv_id": "",
      "publication_date": "2024-12-27",
      "published": "2024-12-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Aphasic Speech Recognition by Using Novel Semi-Supervised Learning Methods on AphasiaBank for English and Spanish",
      "summary": "Automatic speech recognition in patients with aphasia is a challenging task for which studies have been published in a few languages. Reasonably, the systems reported in the literature within this field show significantly lower performance than those focused on transcribing non-pathological clean speech. It is mainly due to the difficulty of recognizing a more unintelligible voice, as well as due to the scarcity of annotated aphasic data. This work is mainly focused on applying novel semi-supervised learning methods to the AphasiaBank dataset in order to deal with these two major issues, reporting improvements for the English language and providing the first benchmark for the Spanish language for which less than one hour of transcribed aphasic speech was used for training. In addition, the influence of reinforcing the training and decoding processes with out-of-domain acoustic and text data is described by using different strategies and configurations to fine-tune the hyperparameters and the final recognition systems. The interesting results obtained encourage extending this technological approach to other languages and scenarios where the scarcity of annotated data to train recognition models is a challenging reality.",
      "abstract": "Automatic speech recognition in patients with aphasia is a challenging task for which studies have been published in a few languages. Reasonably, the systems reported in the literature within this field show significantly lower performance than those focused on transcribing non-pathological clean speech. It is mainly due to the difficulty of recognizing a more unintelligible voice, as well as due to the scarcity of annotated aphasic data. This work is mainly focused on applying novel semi-supervised learning methods to the AphasiaBank dataset in order to deal with these two major issues, reporting improvements for the English language and providing the first benchmark for the Spanish language for which less than one hour of transcribed aphasic speech was used for training. In addition, the influence of reinforcing the training and decoding processes with out-of-domain acoustic and text data is described by using different strategies and configurations to fine-tune the hyperparameters and the final recognition systems. The interesting results obtained encourage extending this technological approach to other languages and scenarios where the scarcity of annotated data to train recognition models is a challenging reality.",
      "doi": "https://doi.org/10.3390/app11198872",
      "openalex_id": "https://openalex.org/W3203147359",
      "arxiv_id": "",
      "publication_date": "2021-09-24",
      "published": "2021-09-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluating the Performance of wav2vec Embedding for Parkinson's Disease Detection",
      "summary": "Abstract Speech is one of the most serious manifestations of Parkinson's disease (PD). Sophisticated language/speech models have already demonstrated impressive performance on a variety of tasks, including classification. By analysing large amounts of data from a given setting, these models can identify patterns that would be difficult for clinicians to detect. We focus on evaluating the performance of a large self-supervised speech representation model, wav2vec, for PD classification. Based on the computed wav2vec embedding for each available speech signal, we calculated two sets of 512 derived features, wav2vec-sum and wav2vec-mean. Unlike traditional signal processing methods, this approach can learn a suitable representation of the signal directly from the data without requiring manual or hand-crafted feature extraction. Using an ensemble random forest classifier, we evaluated the embedding-based features on three different healthy vs. PD datasets (participants rhythmically repeat syllables /pa/, Italian dataset and English dataset). The obtained results showed that the wav2vec signal representation was accurate, with a minimum area under the receiver operating characteristic curve (AUROC) of 0.77 for the /pa/ task and the best AUROC of 0.98 for the Italian speech classification. The findings highlight the potential of the generalisability of the wav2vec features and the performance of these features in the cross-database scenarios.",
      "abstract": "Abstract Speech is one of the most serious manifestations of Parkinson's disease (PD). Sophisticated language/speech models have already demonstrated impressive performance on a variety of tasks, including classification. By analysing large amounts of data from a given setting, these models can identify patterns that would be difficult for clinicians to detect. We focus on evaluating the performance of a large self-supervised speech representation model, wav2vec, for PD classification. Based on the computed wav2vec embedding for each available speech signal, we calculated two sets of 512 derived features, wav2vec-sum and wav2vec-mean. Unlike traditional signal processing methods, this approach can learn a suitable representation of the signal directly from the data without requiring manual or hand-crafted feature extraction. Using an ensemble random forest classifier, we evaluated the embedding-based features on three different healthy vs. PD datasets (participants rhythmically repeat syllables /pa/, Italian dataset and English dataset). The obtained results showed that the wav2vec signal representation was accurate, with a minimum area under the receiver operating characteristic curve (AUROC) of 0.77 for the /pa/ task and the best AUROC of 0.98 for the Italian speech classification. The findings highlight the potential of the generalisability of the wav2vec features and the performance of these features in the cross-database scenarios.",
      "doi": "https://doi.org/10.2478/msr-2023-0033",
      "openalex_id": "https://openalex.org/W4388766108",
      "arxiv_id": "",
      "publication_date": "2023-11-17",
      "published": "2023-11-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-view Self-supervised Learning and Multi-scale Feature Fusion for Automatic Speech Recognition",
      "summary": "Abstract To address the challenges of the poor representation capability and low data utilization rate of end-to-end speech recognition models in deep learning, this study proposes an end-to-end speech recognition model based on multi-scale feature fusion and multi-view self-supervised learning (MM-ASR). It adopts a multi-task learning paradigm for training. The proposed method emphasizes the importance of inter-layer information within shared encoders, aiming to enhance the model’s characterization capability via the multi-scale feature fusion module. Moreover, we apply multi-view self-supervised learning to effectively exploit data information. Our approach is rigorously evaluated on the Aishell-1 dataset and further validated its effectiveness on the English corpus WSJ. The experimental results demonstrate a noteworthy 4.6 $$\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>%</mml:mo> </mml:math> reduction in character error rate, indicating significantly improved speech recognition performance . These findings showcase the effectiveness and potential of our proposed MM-ASR model for end-to-end speech recognition tasks.",
      "abstract": "Abstract To address the challenges of the poor representation capability and low data utilization rate of end-to-end speech recognition models in deep learning, this study proposes an end-to-end speech recognition model based on multi-scale feature fusion and multi-view self-supervised learning (MM-ASR). It adopts a multi-task learning paradigm for training. The proposed method emphasizes the importance of inter-layer information within shared encoders, aiming to enhance the model’s characterization capability via the multi-scale feature fusion module. Moreover, we apply multi-view self-supervised learning to effectively exploit data information. Our approach is rigorously evaluated on the Aishell-1 dataset and further validated its effectiveness on the English corpus WSJ. The experimental results demonstrate a noteworthy 4.6 $$\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>%</mml:mo> </mml:math> reduction in character error rate, indicating significantly improved speech recognition performance . These findings showcase the effectiveness and potential of our proposed MM-ASR model for end-to-end speech recognition tasks.",
      "doi": "https://doi.org/10.1007/s11063-024-11614-z",
      "openalex_id": "https://openalex.org/W4396735698",
      "arxiv_id": "",
      "publication_date": "2024-05-08",
      "published": "2024-05-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analyzing Wav2Vec 1.0 Embeddings for Cross-Database Parkinson’s Disease Detection and Speech Features Extraction",
      "summary": "Advancements in deep learning speech representations have facilitated the effective use of extensive unlabeled speech datasets for Parkinson’s disease (PD) modeling with minimal annotated data. This study employs the non-fine-tuned wav2vec 1.0 architecture to develop machine learning models for PD speech diagnosis tasks, such as cross-database classification and regression to predict demographic and articulation characteristics. The primary aim is to analyze overlapping components within the embeddings on both classification and regression tasks, investigating whether latent speech representations in PD are shared across models, particularly for related tasks. Firstly, evaluation using three multi-language PD datasets showed that wav2vec accurately detected PD based on speech, outperforming feature extraction using mel-frequency cepstral coefficients in the proposed cross-database classification scenarios. In cross-database scenarios using Italian and English-read texts, wav2vec demonstrated performance comparable to intra-dataset evaluations. We also compared our cross-database findings against those of other related studies. Secondly, wav2vec proved effective in regression, modeling various quantitative speech characteristics related to articulation and aging. Ultimately, subsequent analysis of important features examined the presence of significant overlaps between classification and regression models. The feature importance experiments discovered shared features across trained models, with increased sharing for related tasks, further suggesting that wav2vec contributes to improved generalizability. The study proposes wav2vec embeddings as a next promising step toward a speech-based universal model to assist in the evaluation of PD.",
      "abstract": "Advancements in deep learning speech representations have facilitated the effective use of extensive unlabeled speech datasets for Parkinson’s disease (PD) modeling with minimal annotated data. This study employs the non-fine-tuned wav2vec 1.0 architecture to develop machine learning models for PD speech diagnosis tasks, such as cross-database classification and regression to predict demographic and articulation characteristics. The primary aim is to analyze overlapping components within the embeddings on both classification and regression tasks, investigating whether latent speech representations in PD are shared across models, particularly for related tasks. Firstly, evaluation using three multi-language PD datasets showed that wav2vec accurately detected PD based on speech, outperforming feature extraction using mel-frequency cepstral coefficients in the proposed cross-database classification scenarios. In cross-database scenarios using Italian and English-read texts, wav2vec demonstrated performance comparable to intra-dataset evaluations. We also compared our cross-database findings against those of other related studies. Secondly, wav2vec proved effective in regression, modeling various quantitative speech characteristics related to articulation and aging. Ultimately, subsequent analysis of important features examined the presence of significant overlaps between classification and regression models. The feature importance experiments discovered shared features across trained models, with increased sharing for related tasks, further suggesting that wav2vec contributes to improved generalizability. The study proposes wav2vec embeddings as a next promising step toward a speech-based universal model to assist in the evaluation of PD.",
      "doi": "https://doi.org/10.3390/s24175520",
      "openalex_id": "https://openalex.org/W4401886013",
      "arxiv_id": "",
      "publication_date": "2024-08-26",
      "published": "2024-08-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BENDR: Using Transformers and a Contrastive Self-Supervised Learning Task to Learn From Massive Amounts of EEG Data",
      "summary": "Deep neural networks (DNNs) used for brain–computer interface (BCI) classification are commonly expected to learn general features when trained across a variety of contexts, such that these features could be fine-tuned to specific contexts. While some success is found in such an approach, we suggest that this interpretation is limited and an alternative would better leverage the newly (publicly) available massive electroencephalography (EEG) datasets. We consider how to adapt techniques and architectures used for language modeling (LM) that appear capable of ingesting awesome amounts of data toward the development of encephalography modeling with DNNs in the same vein. We specifically adapt an approach effectively used for automatic speech recognition, which similarly (to LMs) uses a self-supervised training objective to learn compressed representations of raw data signals. After adaptation to EEG, we find that a single pre-trained model is capable of modeling completely novel raw EEG sequences recorded with differing hardware, and different subjects performing different tasks. Furthermore, both the internal representations of this model and the entire architecture can be fine-tuned to a variety of downstream BCI and EEG classification tasks, outperforming prior work in more task-specific (sleep stage classification) self-supervision.",
      "abstract": "Deep neural networks (DNNs) used for brain–computer interface (BCI) classification are commonly expected to learn general features when trained across a variety of contexts, such that these features could be fine-tuned to specific contexts. While some success is found in such an approach, we suggest that this interpretation is limited and an alternative would better leverage the newly (publicly) available massive electroencephalography (EEG) datasets. We consider how to adapt techniques and architectures used for language modeling (LM) that appear capable of ingesting awesome amounts of data toward the development of encephalography modeling with DNNs in the same vein. We specifically adapt an approach effectively used for automatic speech recognition, which similarly (to LMs) uses a self-supervised training objective to learn compressed representations of raw data signals. After adaptation to EEG, we find that a single pre-trained model is capable of modeling completely novel raw EEG sequences recorded with differing hardware, and different subjects performing different tasks. Furthermore, both the internal representations of this model and the entire architecture can be fine-tuned to a variety of downstream BCI and EEG classification tasks, outperforming prior work in more task-specific (sleep stage classification) self-supervision.",
      "doi": "https://doi.org/10.3389/fnhum.2021.653659",
      "openalex_id": "https://openalex.org/W3123796542",
      "arxiv_id": "",
      "publication_date": "2021-06-23",
      "published": "2021-06-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Deep Learning-Based Approach for Foot Placement Prediction",
      "summary": "Foot placement prediction can be important for exoskeleton and prosthesis controllers, human-robot interaction, or body-worn systems to prevent slips or trips. Previous studies investigating foot placement prediction have been limited to predicting foot placement during the swing phase, and do not fully consider contextual information such as the preceding step or the stance phase before push-off. In this study, we propose a deep learning-based foot placement prediction approach, sequentially processing data from three IMU sensors mounted on the pelvis and feet. The raw sensor data are pre-processed to generate multi-variable time-series data for training two deep learning models, where the first model estimates the gait progression and the second model subsequently predicts the next foot placement. The ground truth gait phase data and foot placement data are acquired from a motion capture system. Ten healthy subjects were invited to walk naturally at different speeds on a treadmill. In cross-subject learning, the trained models had a mean distance error of 5.93 cm for foot placement prediction. In single-subject learning, the prediction accuracy improved with additional training data, and a mean distance error of 2.60 cm was achieved by fine-tuning the cross-subject validated models with the target subject data. Even from 25–81% in the gait cycle, mean distance errors were only 6.99 cm and 3.22 cm for cross-subject learning and single-subject learning, respectively.",
      "abstract": "Foot placement prediction can be important for exoskeleton and prosthesis controllers, human-robot interaction, or body-worn systems to prevent slips or trips. Previous studies investigating foot placement prediction have been limited to predicting foot placement during the swing phase, and do not fully consider contextual information such as the preceding step or the stance phase before push-off. In this study, we propose a deep learning-based foot placement prediction approach, sequentially processing data from three IMU sensors mounted on the pelvis and feet. The raw sensor data are pre-processed to generate multi-variable time-series data for training two deep learning models, where the first model estimates the gait progression and the second model subsequently predicts the next foot placement. The ground truth gait phase data and foot placement data are acquired from a motion capture system. Ten healthy subjects were invited to walk naturally at different speeds on a treadmill. In cross-subject learning, the trained models had a mean distance error of 5.93 cm for foot placement prediction. In single-subject learning, the prediction accuracy improved with additional training data, and a mean distance error of 2.60 cm was achieved by fine-tuning the cross-subject validated models with the target subject data. Even from 25–81% in the gait cycle, mean distance errors were only 6.99 cm and 3.22 cm for cross-subject learning and single-subject learning, respectively.",
      "doi": "https://doi.org/10.1109/lra.2023.3290521",
      "openalex_id": "https://openalex.org/W4382567930",
      "arxiv_id": "",
      "publication_date": "2023-06-29",
      "published": "2023-06-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Self-Supervised Model for Language Identification Integrating Phonological Knowledge",
      "summary": "In this paper, a self-supervised learning pre-trained model is proposed and successfully applied in language identification task (LID). A Transformer encoder is employed and multi-task strategy is used to train the self-supervised model: the first task is to reconstruct the masking spans of input frames and the second task is a supervision task where the phoneme and phonological labels are used with Connectionist Temporal Classification (CTC) loss. By using this multi-task learning loss, the model is expected to capture high-level speech representation in phonological space. Meanwhile, an adaptive loss is also applied for multi-task learning to balance the weight between different tasks. After the pretraining stage, the self-supervised model is used for xvector systems. Our LID experiments are carried out on the oriental language recognition (OLR) challenge data corpus and 1 s, 3 s, Full-length test sets are selected. Experimental results show that on 1 s test set, feature extraction model approach can get best performance and in 3 s, Full-length test, the fine-tuning approach can reach the best performance. Furthermore, our results prove that the multi-task training strategy is effective and the proposed model can get the best performance.",
      "abstract": "In this paper, a self-supervised learning pre-trained model is proposed and successfully applied in language identification task (LID). A Transformer encoder is employed and multi-task strategy is used to train the self-supervised model: the first task is to reconstruct the masking spans of input frames and the second task is a supervision task where the phoneme and phonological labels are used with Connectionist Temporal Classification (CTC) loss. By using this multi-task learning loss, the model is expected to capture high-level speech representation in phonological space. Meanwhile, an adaptive loss is also applied for multi-task learning to balance the weight between different tasks. After the pretraining stage, the self-supervised model is used for xvector systems. Our LID experiments are carried out on the oriental language recognition (OLR) challenge data corpus and 1 s, 3 s, Full-length test sets are selected. Experimental results show that on 1 s test set, feature extraction model approach can get best performance and in 3 s, Full-length test, the fine-tuning approach can reach the best performance. Furthermore, our results prove that the multi-task training strategy is effective and the proposed model can get the best performance.",
      "doi": "https://doi.org/10.3390/electronics10182259",
      "openalex_id": "https://openalex.org/W3201071655",
      "arxiv_id": "",
      "publication_date": "2021-09-14",
      "published": "2021-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Securing Wireless Networks Against Emerging Threats: An Overview of Protocols and Solutions",
      "summary": "As wireless networks have become an integral part of modern communication infrastructure, ensuring their security against a rapidly evolving threat landscape is a critical concern. This research article provides a comprehensive overview of the emerging threats targeting wireless networks, including advanced persistent threats, man-in-the-middle (MitM) attacks, and AI-driven adaptive malware. With the advent of new technologies such as 5G, the Internet of Things (IoT), and artificial intelligence (AI), the attack surface for wireless networks has significantly expanded, demanding more robust and adaptive security protocols. The paper analyzes the efficacy of current wireless security protocols, such as WPA3 and the 802.11i standard, in addressing these emerging vulnerabilities. While these protocols have introduced significant improvements, they are not without limitations. The article further explores innovative solutions such as blockchain-based security frameworks, AI-powered threat detection systems, and the future potential of quantum cryptography in safeguarding wireless communications. Through a critical review of recent case studies and empirical data, the article highlights the key challenges that organizations face in securing wireless networks, particularly in IoT environments where security standards lag behind technological advancements. The research concludes that while existing protocols provide foundational security, they must be continuously updated and augmented with cutting-edge technologies to counter the growing sophistication of cyberattacks. This article aims to provide insights into the state of wireless network security and offer practical recommendations for enhancing security protocols. Future research directions are also discussed, focusing on the integration of AI-driven threat intelligence and the standardization of security protocols across various wireless technologies. The findings underscore the importance of proactive security measures to safeguard wireless networks in an increasingly interconnected world.",
      "abstract": "As wireless networks have become an integral part of modern communication infrastructure, ensuring their security against a rapidly evolving threat landscape is a critical concern. This research article provides a comprehensive overview of the emerging threats targeting wireless networks, including advanced persistent threats, man-in-the-middle (MitM) attacks, and AI-driven adaptive malware. With the advent of new technologies such as 5G, the Internet of Things (IoT), and artificial intelligence (AI), the attack surface for wireless networks has significantly expanded, demanding more robust and adaptive security protocols. The paper analyzes the efficacy of current wireless security protocols, such as WPA3 and the 802.11i standard, in addressing these emerging vulnerabilities. While these protocols have introduced significant improvements, they are not without limitations. The article further explores innovative solutions such as blockchain-based security frameworks, AI-powered threat detection systems, and the future potential of quantum cryptography in safeguarding wireless communications. Through a critical review of recent case studies and empirical data, the article highlights the key challenges that organizations face in securing wireless networks, particularly in IoT environments where security standards lag behind technological advancements. The research concludes that while existing protocols provide foundational security, they must be continuously updated and augmented with cutting-edge technologies to counter the growing sophistication of cyberattacks. This article aims to provide insights into the state of wireless network security and offer practical recommendations for enhancing security protocols. Future research directions are also discussed, focusing on the integration of AI-driven threat intelligence and the standardization of security protocols across various wireless technologies. The findings underscore the importance of proactive security measures to safeguard wireless networks in an increasingly interconnected world.",
      "doi": "https://doi.org/10.55662/jst.2024.5406",
      "openalex_id": "https://openalex.org/W4402256224",
      "arxiv_id": "",
      "publication_date": "2024-09-05",
      "published": "2024-09-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Class: Continual Learning Approach for Speech Super-Resolution",
      "summary": "Supervised deep learning has significantly improved bandwidth extension (BWE), whereas the emergence of self-supervised learning (SSL) has prompted the combined exploration of SSL and BWE. Although SSL-based deep learning models have shown to produce better representations than their supervised counterparts when trained naively, their effectiveness diminishes in when the model learns different tasks sequentially. To address this problem, we propose a continual learning framework called CLASS, which incorporates continual learning (CL) and self-supervised pretraining (SSP) to improve BWE performance. The framework integrates SSP and BWE fine-tuning tasks with CL approaches, enabling the model to retain its representation knowledge while adapting to BWE as a target task. We employ the CL fine-tuning loss or exponential moving average algorithm to gradually update model parameters and learn to resemble wideband from narrowband signals without losing information from a previous task. In addition, we present the new continual loss with extended version of elastic weight consolidation by updating fisher information matrix for better BWE performance. Our experimental results demonstrate that the proposed method outperforms the baseline approach on the TIMIT dataset. Furthermore, we explore the impact of different hyperparameter settings, contributing to a more comprehensive understanding of the performance of the proposed framework.",
      "abstract": "Supervised deep learning has significantly improved bandwidth extension (BWE), whereas the emergence of self-supervised learning (SSL) has prompted the combined exploration of SSL and BWE. Although SSL-based deep learning models have shown to produce better representations than their supervised counterparts when trained naively, their effectiveness diminishes in when the model learns different tasks sequentially. To address this problem, we propose a continual learning framework called CLASS, which incorporates continual learning (CL) and self-supervised pretraining (SSP) to improve BWE performance. The framework integrates SSP and BWE fine-tuning tasks with CL approaches, enabling the model to retain its representation knowledge while adapting to BWE as a target task. We employ the CL fine-tuning loss or exponential moving average algorithm to gradually update model parameters and learn to resemble wideband from narrowband signals without losing information from a previous task. In addition, we present the new continual loss with extended version of elastic weight consolidation by updating fisher information matrix for better BWE performance. Our experimental results demonstrate that the proposed method outperforms the baseline approach on the TIMIT dataset. Furthermore, we explore the impact of different hyperparameter settings, contributing to a more comprehensive understanding of the performance of the proposed framework.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445917",
      "openalex_id": "https://openalex.org/W4392904360",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adapting Pre-Trained Self-Supervised Learning Model for Speech Recognition with Light-Weight Adapters",
      "summary": "Self-supervised learning (SSL) is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks. However, effectively incorporating a pre-trained SSL model into an automatic speech recognition (ASR) system remains challenging. In this paper, we propose a network architecture with light-weight adapters to adapt a pre-trained SSL model for an end-to-end (E2E) ASR. An adapter is introduced in each SSL network layer and trained on the downstream ASR task, while the parameters of the pre-trained SSL network layers remain unchanged. By carrying over all pre-trained parameters, we avoid the catastrophic forgetting problem. At the same time, we allow the network to quickly adapt to ASR task with light-weight adapters. The experiments using LibriSpeech and Wall Street Journal (WSJ) datasets show that (1) the proposed adapter-based fine-tuning consistently outperforms full-fledged training in low-resource scenarios, with up to 17.5%/12.2% relative word error rate (WER) reduction on the 10 min LibriSpeech split; (2) the adapter-based adaptation also shows competitive performance in high-resource scenarios, which further validates the effectiveness of the adapters.",
      "abstract": "Self-supervised learning (SSL) is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks. However, effectively incorporating a pre-trained SSL model into an automatic speech recognition (ASR) system remains challenging. In this paper, we propose a network architecture with light-weight adapters to adapt a pre-trained SSL model for an end-to-end (E2E) ASR. An adapter is introduced in each SSL network layer and trained on the downstream ASR task, while the parameters of the pre-trained SSL network layers remain unchanged. By carrying over all pre-trained parameters, we avoid the catastrophic forgetting problem. At the same time, we allow the network to quickly adapt to ASR task with light-weight adapters. The experiments using LibriSpeech and Wall Street Journal (WSJ) datasets show that (1) the proposed adapter-based fine-tuning consistently outperforms full-fledged training in low-resource scenarios, with up to 17.5%/12.2% relative word error rate (WER) reduction on the 10 min LibriSpeech split; (2) the adapter-based adaptation also shows competitive performance in high-resource scenarios, which further validates the effectiveness of the adapters.",
      "doi": "https://doi.org/10.3390/electronics13010190",
      "openalex_id": "https://openalex.org/W4390482765",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-end Trajectory Generation - Contrasting Deep Generative Models and Language Models",
      "summary": "Due to the limited availability of actual large-scale datasets, realistic synthetic trajectory data play a crucial role in various research domains, including spatiotemporal data mining and data management, and domain-driven research related to transportation planning and urban analytics. Existing generation methods rely on predefined heuristics and cannot learn the unknown underlying generative mechanisms. This work introduces two end-to-end approaches for trajectory generation. The first approach comprises deep generative VAE-like models that factorize global and local semantics (habits vs. random routing change). We further enhance this approach by developing novel inference strategies based on variational inference and constrained optimization to ensure the validity of spatiotemporal aspects. This novel deep neural network architecture implements generative and inference models with dynamic latent priors. The second approach introduces a language model (LM) inspired generation as another benchmarking and foundational approach. The LM-inspired approach conceptualizes trajectories as sentences with the aim of predicting the likelihood of subsequent locations on a trajectory, given the locations as context. As a result, the LM-inspired approach implicitly learns the inherent spatiotemporal structure and other embedded semantics within the trajectories. These proposed methods demonstrate substantial quantitative and qualitative improvements over existing approaches, as evidenced by extensive experimental evaluations.",
      "abstract": "Due to the limited availability of actual large-scale datasets, realistic synthetic trajectory data play a crucial role in various research domains, including spatiotemporal data mining and data management, and domain-driven research related to transportation planning and urban analytics. Existing generation methods rely on predefined heuristics and cannot learn the unknown underlying generative mechanisms. This work introduces two end-to-end approaches for trajectory generation. The first approach comprises deep generative VAE-like models that factorize global and local semantics (habits vs. random routing change). We further enhance this approach by developing novel inference strategies based on variational inference and constrained optimization to ensure the validity of spatiotemporal aspects. This novel deep neural network architecture implements generative and inference models with dynamic latent priors. The second approach introduces a language model (LM) inspired generation as another benchmarking and foundational approach. The LM-inspired approach conceptualizes trajectories as sentences with the aim of predicting the likelihood of subsequent locations on a trajectory, given the locations as context. As a result, the LM-inspired approach implicitly learns the inherent spatiotemporal structure and other embedded semantics within the trajectories. These proposed methods demonstrate substantial quantitative and qualitative improvements over existing approaches, as evidenced by extensive experimental evaluations.",
      "doi": "https://doi.org/10.1145/3716892",
      "openalex_id": "https://openalex.org/W4407545490",
      "arxiv_id": "",
      "publication_date": "2025-02-13",
      "published": "2025-02-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Utilizing Self-Supervised Representations for MOS Prediction",
      "summary": "Speech quality assessment has been a critical issue in speech processing for decades. Existing automatic evaluations usually require clean references or parallel ground truth data, which is infeasible when the amount of data soars. Subjective tests, on the other hand, do not need any additional clean or parallel data and correlates better to human perception. However, such a test is expensive and time-consuming because crowd work is necessary. It thus becomes highly desired to develop an automatic evaluation approach that correlates well with human perception while not requiring ground truth data. In this paper, we use self-supervised pre-trained models for MOS prediction. We show their representations can distinguish between clean and noisy audios. Then, we fine-tune these pre-trained models followed by simple linear layers in an end-to-end manner. The experiment results showed that our framework outperforms the two previous state-of-the-art models by a significant improvement on Voice Conversion Challenge 2018 and achieves comparable or superior performance on Voice Conversion Challenge 2016. We also conducted an ablation study to further investigate how each module benefits the task. The experiment results are implemented and reproducible with publicly available toolkits.",
      "abstract": "Speech quality assessment has been a critical issue in speech processing for decades. Existing automatic evaluations usually require clean references or parallel ground truth data, which is infeasible when the amount of data soars. Subjective tests, on the other hand, do not need any additional clean or parallel data and correlates better to human perception. However, such a test is expensive and time-consuming because crowd work is necessary. It thus becomes highly desired to develop an automatic evaluation approach that correlates well with human perception while not requiring ground truth data. In this paper, we use self-supervised pre-trained models for MOS prediction. We show their representations can distinguish between clean and noisy audios. Then, we fine-tune these pre-trained models followed by simple linear layers in an end-to-end manner. The experiment results showed that our framework outperforms the two previous state-of-the-art models by a significant improvement on Voice Conversion Challenge 2018 and achieves comparable or superior performance on Voice Conversion Challenge 2016. We also conducted an ablation study to further investigate how each module benefits the task. The experiment results are implemented and reproducible with publicly available toolkits.",
      "doi": "https://doi.org/10.21437/interspeech.2021-2013",
      "openalex_id": "https://openalex.org/W3142867067",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using Multimodal Transformers in Affective Computing",
      "summary": "Having devices capable of understanding human emotions will significantly improve the way people interact with them. Moreover, if those devices are capable of influencing the emotions of users in a positive way, this will improve their quality of life, especially for frail or dependent users. A first step towards this goal is improving the performance of emotion recognition systems. Specifically, using a multimodal approach is appealing, as the availability of different signals is growing. We believe that it is important to incorporate new architectures and techniques like the Transformer and BERT, and to investigate how to use them in a multimodal setting. Also, it is essential to develop self-supervised learning techniques to take advantage of the considerable quantity of unlabeled data available nowadays. In this extended abstract, we present our research in those directions.",
      "abstract": "Having devices capable of understanding human emotions will significantly improve the way people interact with them. Moreover, if those devices are capable of influencing the emotions of users in a positive way, this will improve their quality of life, especially for frail or dependent users. A first step towards this goal is improving the performance of emotion recognition systems. Specifically, using a multimodal approach is appealing, as the availability of different signals is growing. We believe that it is important to incorporate new architectures and techniques like the Transformer and BERT, and to investigate how to use them in a multimodal setting. Also, it is essential to develop self-supervised learning techniques to take advantage of the considerable quantity of unlabeled data available nowadays. In this extended abstract, we present our research in those directions.",
      "doi": "https://doi.org/10.1109/aciiw52867.2021.9666396",
      "openalex_id": "https://openalex.org/W4206221133",
      "arxiv_id": "",
      "publication_date": "2021-09-28",
      "published": "2021-09-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding Self-supervised Learning with Dual Deep Networks",
      "summary": "We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \\emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \\emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in https://github.com/facebookresearch/luckmatters/tree/master/ssl.",
      "abstract": "We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \\emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \\emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in https://github.com/facebookresearch/luckmatters/tree/master/ssl.",
      "doi": "https://doi.org/10.48550/arxiv.2010.00578",
      "openalex_id": "https://openalex.org/W3089824566",
      "arxiv_id": "",
      "publication_date": "2020-10-01",
      "published": "2020-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition.",
      "summary": "We present a method for continual learning of speech representations for multiple languages using self-supervised learning (SSL) and applying these for automatic speech recognition. There is an abundance of unannotated speech, so creating self-supervised representations from raw audio and finetuning on a small annotated datasets is a promising direction to build speech recognition systems. Wav2vec models perform SSL on raw audio in a pretraining phase and then finetune on a small fraction of annotated data. SSL models have produced state of the art results for ASR. However, these models are very expensive to pretrain with self-supervision. We tackle the problem of learning new language representations continually from audio without forgetting a previous language representation. We use ideas from continual learning to transfer knowledge from a previous task to speed up pretraining a new language task. Our continual-wav2vec2 model can decrease pretraining times by 32% when learning a new language task, and learn this new audio-language representation without forgetting previous language representation.",
      "abstract": "We present a method for continual learning of speech representations for multiple languages using self-supervised learning (SSL) and applying these for automatic speech recognition. There is an abundance of unannotated speech, so creating self-supervised representations from raw audio and finetuning on a small annotated datasets is a promising direction to build speech recognition systems. Wav2vec models perform SSL on raw audio in a pretraining phase and then finetune on a small fraction of annotated data. SSL models have produced state of the art results for ASR. However, these models are very expensive to pretrain with self-supervision. We tackle the problem of learning new language representations continually from audio without forgetting a previous language representation. We use ideas from continual learning to transfer knowledge from a previous task to speed up pretraining a new language task. Our continual-wav2vec2 model can decrease pretraining times by 32% when learning a new language task, and learn this new audio-language representation without forgetting previous language representation.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3186596101",
      "arxiv_id": "",
      "publication_date": "2021-07-26",
      "published": "2021-07-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-supervised Learning",
      "summary": "Recently, self-supervised pre-training has gained success in automatic speech recognition (ASR). However, considering the difference between speech accents in real scenarios, how to identify accents and use accent features to improve ASR is still challenging. In this paper, we employ the self-supervised pre-training method for both accent identification and accented speech recognition tasks. For the former task, a standard deviation constraint loss (SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents under the same language. As for accented speech recognition task, we design an accent-dependent ASR system, which can utilize additional accent input features. Furthermore, we propose a frame-level accent feature, which is extracted based on the proposed accent identification model and can be dynamically adjusted. We pre-train our models using 960 hours unlabeled LibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The experimental results show that our proposed accent-dependent ASR system is significantly ahead of the AESRC2020 baseline and achieves $6.5\\%$ relative word error rate (WER) reduction compared with our accent-independent ASR system.",
      "abstract": "Recently, self-supervised pre-training has gained success in automatic speech recognition (ASR). However, considering the difference between speech accents in real scenarios, how to identify accents and use accent features to improve ASR is still challenging. In this paper, we employ the self-supervised pre-training method for both accent identification and accented speech recognition tasks. For the former task, a standard deviation constraint loss (SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents under the same language. As for accented speech recognition task, we design an accent-dependent ASR system, which can utilize additional accent input features. Furthermore, we propose a frame-level accent feature, which is extracted based on the proposed accent identification model and can be dynamically adjusted. We pre-train our models using 960 hours unlabeled LibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The experimental results show that our proposed accent-dependent ASR system is significantly ahead of the AESRC2020 baseline and achieves $6.5\\%$ relative word error rate (WER) reduction compared with our accent-independent ASR system.",
      "doi": "https://doi.org/10.48550/arxiv.2109.07349",
      "openalex_id": "https://openalex.org/W3199443835",
      "arxiv_id": "",
      "publication_date": "2021-09-15",
      "published": "2021-09-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improves Neural Acoustic Word Embeddings Query by Example Spoken Term Detection with Wav2vec Pretraining and Circle Loss",
      "summary": "Query by example spoken term detection (QbE-STD) is a popular keyword detection method in the absence of speech resources. It can build a keyword query system with decent performance when there are few labeled speeches and a lack of pronunciation dictionaries. In recent years, neural acoustic word embeddings (NAWEs) has become a commonly used QbE-STD method. To make the embedded features extracted by the neural network contain more accurate context information, we use wav2vec pre-training to improve the performance of the network. Compared with the Mel-frequency cepstral coefficients(MFCC) system, the average precision (AP) is relatively improved by 11.1%. We also find that the AP of the wav2vec and MFCC splicing system is better, demonstrating that wav2vec cannot contain all spectrum information. To accelerate the convergence speed of the splicing system, we use circle loss to replace the triplet loss, making the convergence about 40% epochs earlier on average. The circle loss also relatively increases AP by more than 4.9%. The AP of our best-performing system is 7.7% better than the wav2vec baseline system and 19.7% better than the MFCC baseline system.",
      "abstract": "Query by example spoken term detection (QbE-STD) is a popular keyword detection method in the absence of speech resources. It can build a keyword query system with decent performance when there are few labeled speeches and a lack of pronunciation dictionaries. In recent years, neural acoustic word embeddings (NAWEs) has become a commonly used QbE-STD method. To make the embedded features extracted by the neural network contain more accurate context information, we use wav2vec pre-training to improve the performance of the network. Compared with the Mel-frequency cepstral coefficients(MFCC) system, the average precision (AP) is relatively improved by 11.1%. We also find that the AP of the wav2vec and MFCC splicing system is better, demonstrating that wav2vec cannot contain all spectrum information. To accelerate the convergence speed of the splicing system, we use circle loss to replace the triplet loss, making the convergence about 40% epochs earlier on average. The circle loss also relatively increases AP by more than 4.9%. The AP of our best-performing system is 7.7% better than the wav2vec baseline system and 19.7% better than the MFCC baseline system.",
      "doi": "https://doi.org/10.1109/iscslp49672.2021.9362065",
      "openalex_id": "https://openalex.org/W3133501470",
      "arxiv_id": "",
      "publication_date": "2021-01-24",
      "published": "2021-01-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SCaLa: Supervised Contrastive Learning for End-to-End Automatic Speech Recognition.",
      "summary": "End-to-end Automatic Speech Recognition (ASR) models are usually trained to reduce the losses of the whole token sequences, while neglecting explicit phonemic-granularity supervision. This could lead to recognition errors due to similar-phoneme confusion or phoneme reduction. To alleviate this problem, this paper proposes a novel framework of Supervised Contrastive Learning (SCaLa) to enhance phonemic information learning for end-to-end ASR systems. Specifically, we introduce the self-supervised Masked Contrastive Predictive Coding (MCPC) into the fully-supervised setting. To supervise phoneme learning explicitly, SCaLa first masks the variable-length encoder features corresponding to phonemes given phoneme forced-alignment extracted from a pre-trained acoustic model, and then predicts the masked phonemes via contrastive learning. The phoneme forced-alignment can mitigate the noise of positive-negative pairs in self-supervised MCPC. Experimental results conducted on reading and spontaneous speech datasets show that the proposed approach achieves 2.84% and 1.38% Character Error Rate (CER) reductions compared to the baseline, respectively.",
      "abstract": "End-to-end Automatic Speech Recognition (ASR) models are usually trained to reduce the losses of the whole token sequences, while neglecting explicit phonemic-granularity supervision. This could lead to recognition errors due to similar-phoneme confusion or phoneme reduction. To alleviate this problem, this paper proposes a novel framework of Supervised Contrastive Learning (SCaLa) to enhance phonemic information learning for end-to-end ASR systems. Specifically, we introduce the self-supervised Masked Contrastive Predictive Coding (MCPC) into the fully-supervised setting. To supervise phoneme learning explicitly, SCaLa first masks the variable-length encoder features corresponding to phonemes given phoneme forced-alignment extracted from a pre-trained acoustic model, and then predicts the masked phonemes via contrastive learning. The phoneme forced-alignment can mitigate the noise of positive-negative pairs in self-supervised MCPC. Experimental results conducted on reading and spontaneous speech datasets show that the proposed approach achieves 2.84% and 1.38% Character Error Rate (CER) reductions compared to the baseline, respectively.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3204996224",
      "arxiv_id": "",
      "publication_date": "2021-10-08",
      "published": "2021-10-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "These new \"artificial intelligence\" programs don't know what they're talking about",
      "summary": "I'm sure you've seen things like ChatGPT in the news: programs that can carry out pretty convincing conversations. They are known as Large Language Models (LLMs) and are frequently referred to as being Artificial Intelligence (AI) — but I really don't like that designation as it implies some understanding.",
      "abstract": "I'm sure you've seen things like ChatGPT in the news: programs that can carry out pretty convincing conversations. They are known as Large Language Models (LLMs) and are frequently referred to as being Artificial Intelligence (AI) — but I really don't like that designation as it implies some understanding.",
      "doi": "https://doi.org/10.59350/tnrhy-3dq84",
      "openalex_id": "https://openalex.org/W4385388413",
      "arxiv_id": "",
      "publication_date": "2023-01-15",
      "published": "2023-01-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Systematic Review of Advancing Machine Learning Through Cross-Domain Analysis of Unlabeled Data",
      "summary": "Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.",
      "abstract": "Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.",
      "doi": "https://doi.org/10.55662/jst.2023.4104",
      "openalex_id": "https://openalex.org/W4403908485",
      "arxiv_id": "",
      "publication_date": "2023-01-20",
      "published": "2023-01-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FusionX: A Symbolic-fused Multimodal Emotion Interaction Framework",
      "summary": "Understanding human emotion through multimodal signals—such as linguistic content, vocal acoustics, and facial expressions—remains a complex and nuanced challenge for artificial systems. Unlike humans, who intuitively infer emotions through intricate cross-modal cues, machines must systematically decode heterogeneous information. To address this gap, we propose a novel multimodal emotion recognition framework, \\textbf{FusionX}, that systematically models inter-modal dynamics from multiple perspectives. FusionX decomposes multimodal input signals into three complementary types of interaction representations: modality-complete (preserving full unimodal information), modality-synergistic (capturing shared inter-modal contributions), and modality-unique (highlighting distinctive aspects of each modality). To further refine the integration of these representations, we introduce a text-prioritized fusion mechanism named \\textbf{Text-Centric Hierarchical Tensor Fusion} (TCHF). This module constructs a deep hierarchical tensor network that accentuates the semantic richness of textual modality while harmonizing its contribution with the audio and visual streams. To validate FusionX, we conduct extensive evaluations across three widely-used benchmarks: MOSEI, MOSI, and IEMOCAP. Results reveal that our method significantly surpasses previous state-of-the-art baselines in both classification accuracy and regression metrics, demonstrating the superiority of hierarchical and perspective-aware interaction modeling in emotion understanding.",
      "abstract": "Understanding human emotion through multimodal signals—such as linguistic content, vocal acoustics, and facial expressions—remains a complex and nuanced challenge for artificial systems. Unlike humans, who intuitively infer emotions through intricate cross-modal cues, machines must systematically decode heterogeneous information. To address this gap, we propose a novel multimodal emotion recognition framework, \\textbf{FusionX}, that systematically models inter-modal dynamics from multiple perspectives. FusionX decomposes multimodal input signals into three complementary types of interaction representations: modality-complete (preserving full unimodal information), modality-synergistic (capturing shared inter-modal contributions), and modality-unique (highlighting distinctive aspects of each modality). To further refine the integration of these representations, we introduce a text-prioritized fusion mechanism named \\textbf{Text-Centric Hierarchical Tensor Fusion} (TCHF). This module constructs a deep hierarchical tensor network that accentuates the semantic richness of textual modality while harmonizing its contribution with the audio and visual streams. To validate FusionX, we conduct extensive evaluations across three widely-used benchmarks: MOSEI, MOSI, and IEMOCAP. Results reveal that our method significantly surpasses previous state-of-the-art baselines in both classification accuracy and regression metrics, demonstrating the superiority of hierarchical and perspective-aware interaction modeling in emotion understanding.",
      "doi": "https://doi.org/10.20944/preprints202504.0397.v1",
      "openalex_id": "https://openalex.org/W4409203934",
      "arxiv_id": "",
      "publication_date": "2025-04-06",
      "published": "2025-04-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Learning for Vehicle Noise Prediction With Limited Labeled Data",
      "summary": "With advancements in artificial intelligence, its application for developing quieter vehicles is increasingly researched in the automotive industry. Vehicle noise is highly impacted by the acceleration from electric power steering (EPS); therefore, determining the relationship between EPS-induced acceleration and vehicle noise is important. However, collecting labeled data for noise prediction is challenging because it requires expensive acceleration sensors attached to the EPS, and experts must measure the noise directly by ear. In contrast, obtaining unlabeled acceleration data from environments similar to those of the EPS is manageable. This study proposes an autoencoder-based self-supervised learning with information maximization (ASSIM) method to predict vehicle noise. ASSIM allows robust feature learning via pretraining to reconstruct unlabeled data with necessary augmentations. The experimental results demonstrate that ASSIM performs better than the other comparative methods in predicting vehicle noise in environments with less labeled data. The proposed method may aid in the design of quieter vehicles while reducing the data collection time.",
      "abstract": "With advancements in artificial intelligence, its application for developing quieter vehicles is increasingly researched in the automotive industry. Vehicle noise is highly impacted by the acceleration from electric power steering (EPS); therefore, determining the relationship between EPS-induced acceleration and vehicle noise is important. However, collecting labeled data for noise prediction is challenging because it requires expensive acceleration sensors attached to the EPS, and experts must measure the noise directly by ear. In contrast, obtaining unlabeled acceleration data from environments similar to those of the EPS is manageable. This study proposes an autoencoder-based self-supervised learning with information maximization (ASSIM) method to predict vehicle noise. ASSIM allows robust feature learning via pretraining to reconstruct unlabeled data with necessary augmentations. The experimental results demonstrate that ASSIM performs better than the other comparative methods in predicting vehicle noise in environments with less labeled data. The proposed method may aid in the design of quieter vehicles while reducing the data collection time.",
      "doi": "https://doi.org/10.1109/access.2025.3558172",
      "openalex_id": "https://openalex.org/W4409223142",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Interaction Modeling with Intelligent Coordination for Multimodal Emotion Recognition",
      "summary": "Emotion recognition through multimodal signals—such as speech, text, and facial cues—has garnered increasing attention due to its pivotal role in enhancing human-computer interaction and intelligent communication systems. However, existing approaches often struggle to thoroughly capture the intricacies of multimodal interactions, primarily due to the challenges in effectively fusing heterogeneous modalities while mitigating redundancy and preserving complementary information. In this study, we introduce \\textbf{MIMIC}, a novel framework designed to comprehensively model complex multimodal interactions from diverse perspectives. Specifically, MIMIC introduces three parallel latent representations: a modality-preserving full interaction representation, a cross-modal shared interaction representation, and individualized modality-specific representations. Furthermore, a hierarchical semantic-driven fusion strategy is proposed to seamlessly integrate these representations into a cohesive multimodal interaction space. Extensive experiments demonstrate that our MIMIC framework not only surpasses prior state-of-the-art methods but also achieves this with remarkable efficiency, involving lower computational complexity and significantly fewer trainable parameters. Our contributions are twofold: (1) advancing a multi-perspective interaction modeling approach that enhances the depth of multimodal emotion analysis, and (2) offering a streamlined, resource-efficient framework suitable for practical deployments in emotion-aware systems.",
      "abstract": "Emotion recognition through multimodal signals—such as speech, text, and facial cues—has garnered increasing attention due to its pivotal role in enhancing human-computer interaction and intelligent communication systems. However, existing approaches often struggle to thoroughly capture the intricacies of multimodal interactions, primarily due to the challenges in effectively fusing heterogeneous modalities while mitigating redundancy and preserving complementary information. In this study, we introduce \\textbf{MIMIC}, a novel framework designed to comprehensively model complex multimodal interactions from diverse perspectives. Specifically, MIMIC introduces three parallel latent representations: a modality-preserving full interaction representation, a cross-modal shared interaction representation, and individualized modality-specific representations. Furthermore, a hierarchical semantic-driven fusion strategy is proposed to seamlessly integrate these representations into a cohesive multimodal interaction space. Extensive experiments demonstrate that our MIMIC framework not only surpasses prior state-of-the-art methods but also achieves this with remarkable efficiency, involving lower computational complexity and significantly fewer trainable parameters. Our contributions are twofold: (1) advancing a multi-perspective interaction modeling approach that enhances the depth of multimodal emotion analysis, and (2) offering a streamlined, resource-efficient framework suitable for practical deployments in emotion-aware systems.",
      "doi": "https://doi.org/10.20944/preprints202505.1219.v1",
      "openalex_id": "https://openalex.org/W4410474598",
      "arxiv_id": "",
      "publication_date": "2025-05-16",
      "published": "2025-05-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator",
      "summary": "We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both.The proposed model uses an integrated auxiliary block for text-based training.This block combines a non-autoregressive multi-speaker text-to-melspectrogram generator with a GAN-based enhancer to improve the spectrogram quality.The proposed system can generate a mel-spectrogram dynamically during training.It can be used to adapt the ASR model to a new domain by using text-only data from this domain.We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only.It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.",
      "abstract": "We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both.The proposed model uses an integrated auxiliary block for text-based training.This block combines a non-autoregressive multi-speaker text-to-melspectrogram generator with a GAN-based enhancer to improve the spectrogram quality.The proposed system can generate a mel-spectrogram dynamically during training.It can be used to adapt the ASR model to a new domain by using text-only data from this domain.We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only.It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.",
      "doi": "https://doi.org/10.21437/interspeech.2023-906",
      "openalex_id": "https://openalex.org/W4385823377",
      "arxiv_id": "",
      "publication_date": "2023-08-14",
      "published": "2023-08-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Automatic Speech Recognition: Effects of Semantic Audio Filtering on Models Performance",
      "summary": "This paper presents a novel methodology for enhancing Automatic Speech Recognition (ASR) performance by utilizing contrastive learning to filter synthetic audio data. We address the challenge of incorporating synthetic data into ASR training, especially in scenarios with limited real-world data or unique linguistic characteristics. The method utilizes a contrastive learning model to align representations of synthetic audio and its corresponding text transcripts, enabling the identification and removal of low-quality samples that do not align well semantically. We evaluate the methodology on a medium-resource language across two distinct datasets: a general-domain dataset and a regionally specific dataset characterized by unique pronunciation patterns. Experimental results reveal that the optimal filtering strategy depends on both model capacity and dataset characteristics. Larger models, like Whisper Large V3, particularly benefit from aggressive filtering, while smaller models may not require such stringent filtering, especially on non-normalized text. This work highlights the importance of adjusting synthetic data augmentation and filtering to specific model architectures and target domains. The proposed method, robust and adaptable, enhances ASR performance across diverse language settings. We have open-sourced the entire work, which includes 140 hours of synthetically generated Portuguese speech, as well as the pipeline and parameter settings used to create these samples. Additionally, we provide the fine-tuned Whisper models and the code required to reproduce this research. Our code will be available at <uri>https://github.com/my-north-ai/semantic_audio_filtering</uri>.",
      "abstract": "This paper presents a novel methodology for enhancing Automatic Speech Recognition (ASR) performance by utilizing contrastive learning to filter synthetic audio data. We address the challenge of incorporating synthetic data into ASR training, especially in scenarios with limited real-world data or unique linguistic characteristics. The method utilizes a contrastive learning model to align representations of synthetic audio and its corresponding text transcripts, enabling the identification and removal of low-quality samples that do not align well semantically. We evaluate the methodology on a medium-resource language across two distinct datasets: a general-domain dataset and a regionally specific dataset characterized by unique pronunciation patterns. Experimental results reveal that the optimal filtering strategy depends on both model capacity and dataset characteristics. Larger models, like Whisper Large V3, particularly benefit from aggressive filtering, while smaller models may not require such stringent filtering, especially on non-normalized text. This work highlights the importance of adjusting synthetic data augmentation and filtering to specific model architectures and target domains. The proposed method, robust and adaptable, enhances ASR performance across diverse language settings. We have open-sourced the entire work, which includes 140 hours of synthetically generated Portuguese speech, as well as the pipeline and parameter settings used to create these samples. Additionally, we provide the fine-tuned Whisper models and the code required to reproduce this research. Our code will be available at <uri>https://github.com/my-north-ai/semantic_audio_filtering</uri>.",
      "doi": "https://doi.org/10.1109/access.2024.3482970",
      "openalex_id": "https://openalex.org/W4403510311",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Leveraging Large Text Corpora For End-To-End Speech Summarization",
      "summary": "End-to-end speech summarization (E2E SSum) is a technique to directly generate summary sentences from speech. Compared with the cascade approach, which combines automatic speech recognition (ASR) and text summarization models, the E2E approach is more promising because it mitigates ASR errors, incorporates nonverbal information, and simplifies the overall system. However, since collecting a large amount of paired data (i.e., speech and summary) is difficult, the training data is usually insufficient to train a robust E2E SSum system. In this paper, we present two novel methods that leverage a large amount of external text summarization data for E2E SSum training. The first technique is to utilize a text-to-speech (TTS) system to generate synthesized speech, which is used for E2E SSum training with the text summary. The second is a TTS-free method that directly inputs phoneme sequence instead of synthesized speech to the E2E SSum model. Experiments show that our proposed TTS- and phoneme-based methods improve several metrics on the How2 dataset. In particular, our best system outperforms a previous state-of-the-art one by a large margin (i.e., METEOR score improvements of more than 6 points). To the best of our knowledge, this is the first work to use external language resources for E2E SSum. Moreover, we report a detailed analysis of the How2 dataset to confirm the validity of our proposed E2E SSum system.",
      "abstract": "End-to-end speech summarization (E2E SSum) is a technique to directly generate summary sentences from speech. Compared with the cascade approach, which combines automatic speech recognition (ASR) and text summarization models, the E2E approach is more promising because it mitigates ASR errors, incorporates nonverbal information, and simplifies the overall system. However, since collecting a large amount of paired data (i.e., speech and summary) is difficult, the training data is usually insufficient to train a robust E2E SSum system. In this paper, we present two novel methods that leverage a large amount of external text summarization data for E2E SSum training. The first technique is to utilize a text-to-speech (TTS) system to generate synthesized speech, which is used for E2E SSum training with the text summary. The second is a TTS-free method that directly inputs phoneme sequence instead of synthesized speech to the E2E SSum model. Experiments show that our proposed TTS- and phoneme-based methods improve several metrics on the How2 dataset. In particular, our best system outperforms a previous state-of-the-art one by a large margin (i.e., METEOR score improvements of more than 6 points). To the best of our knowledge, this is the first work to use external language resources for E2E SSum. Moreover, we report a detailed analysis of the How2 dataset to confirm the validity of our proposed E2E SSum system.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094993",
      "openalex_id": "https://openalex.org/W4372183461",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improved Children’s Automatic Speech Recognition Combining Adapters and Synthetic Data Augmentation",
      "summary": "Children's automatic speech recognition (ASR) poses a significant challenge due to the high variability nature of children's speech. The limited availability of training datasets hampers the effective modelling of this variability, which can be partially addressed using a text-to-speech (TTS) system for data augmentation. However, generated data may contain imperfections, potentially impacting performance. In this work, we use Adapters to handle the domain mismatch when fine-tuning with TTS data. This involves a two-step training process: training adapter layers with a frozen pre-trained model using synthetic data, then fine-tuning both adapters and the entire model with a mix of synthetic and real data, where only synthetic data passes through the adapters. Experimental results demonstrate up to 6% relative reduction in WER compared to the straightforward use of synthetic data, indicating the effectiveness of adapter-based architectures in learning from imperfect synthetic data.",
      "abstract": "Children's automatic speech recognition (ASR) poses a significant challenge due to the high variability nature of children's speech. The limited availability of training datasets hampers the effective modelling of this variability, which can be partially addressed using a text-to-speech (TTS) system for data augmentation. However, generated data may contain imperfections, potentially impacting performance. In this work, we use Adapters to handle the domain mismatch when fine-tuning with TTS data. This involves a two-step training process: training adapter layers with a frozen pre-trained model using synthetic data, then fine-tuning both adapters and the entire model with a mix of synthetic and real data, where only synthetic data passes through the adapters. Experimental results demonstrate up to 6% relative reduction in WER compared to the straightforward use of synthetic data, indicating the effectiveness of adapter-based architectures in learning from imperfect synthetic data.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446889",
      "openalex_id": "https://openalex.org/W4392909810",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Speech Recognition with Augmented Synthesized Data and Conditional Model Training",
      "summary": "With recent advances in end-to-end text to speech (TTS), the quality of synthetic data has been significantly improved. Synthesized speech is becoming a feasible alternative to human speech for training speech recognizers. By using multi-speaker TTS, the synthesis architecture manipulations the combination of various speakers, prosodies and speaking styles. It helps to enrich the acoustic diversity and benefits the robustness of automatic speech recognition (ASR). However, the improvement of building acoustic model in ASR system with synthetic data is still limited due to the mismatch between the synthetic and the real data. Human speech is more natural and contains infoxrmation that not exist in synthetic data, such as ambient noise and frequency warping inspired by channel. In this paper, we propose two novel techniques to mitigate the problem: (i) Pre-train TTS model with large dataset, and then transfer it to each speaker for generating synthetic data which is more suitable on the ASR task. (ii) A conditional training method that improves the performance of augmenting real data with synthesized materials. Experimental results show that these methods can significantly improve the building of speech recognition systems using synthetic data. For example, the results on AISHELL-1 dataset show the proposed methods can achieve up to 41.7% relative error reduction in character error rate (CER) compared to the traditional method of building ASR model with synthetic data. Moreover, we observe up to 12.7% relative error reduction by augmenting human speech and synthesized speech with our conditional training method compared to naively using the human speech.",
      "abstract": "With recent advances in end-to-end text to speech (TTS), the quality of synthetic data has been significantly improved. Synthesized speech is becoming a feasible alternative to human speech for training speech recognizers. By using multi-speaker TTS, the synthesis architecture manipulations the combination of various speakers, prosodies and speaking styles. It helps to enrich the acoustic diversity and benefits the robustness of automatic speech recognition (ASR). However, the improvement of building acoustic model in ASR system with synthetic data is still limited due to the mismatch between the synthetic and the real data. Human speech is more natural and contains infoxrmation that not exist in synthetic data, such as ambient noise and frequency warping inspired by channel. In this paper, we propose two novel techniques to mitigate the problem: (i) Pre-train TTS model with large dataset, and then transfer it to each speaker for generating synthetic data which is more suitable on the ASR task. (ii) A conditional training method that improves the performance of augmenting real data with synthesized materials. Experimental results show that these methods can significantly improve the building of speech recognition systems using synthetic data. For example, the results on AISHELL-1 dataset show the proposed methods can achieve up to 41.7% relative error reduction in character error rate (CER) compared to the traditional method of building ASR model with synthetic data. Moreover, we observe up to 12.7% relative error reduction by augmenting human speech and synthesized speech with our conditional training method compared to naively using the human speech.",
      "doi": "https://doi.org/10.1109/iscslp57327.2022.10037977",
      "openalex_id": "https://openalex.org/W4319586610",
      "arxiv_id": "",
      "publication_date": "2022-12-11",
      "published": "2022-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASR Model Adaptation for Rare Words Using Synthetic Data Generated by Multiple Text-To-Speech Systems",
      "summary": "Automatic speech recognition (ASR) for rare words is difficult as there are little relevant text-audio data pairs to train an ASR model. To obtain more text-audio pairs, text-only data are fed to Text-To-Speech (TTS) systems to generate synthetic audio. Previous works use a single TTS system conditioned on multiple speakers to produce different speaker voices to improve the output data's speaker diversity, and they show that training an ASR model on the more diverse data can avoid overfitting and improve the model's robustness. As an alternative way to improve the diversity, we study the speaker embedding distribution of audios synthesized by different TTS systems and found that the audios synthesized by different TTS systems have different speaker distributions even when they are conditioned on the same speaker. Inspired by this, this paper proposes to condition multiple TTS systems repeatedly on a single speaker to synthesize more diverse speaker data, so ASR models can be trained more robustly. When we apply our method to a rare word dataset partitioned from National Speech Corpus SG, which contains mostly road names and addresses in its text transcripts, experiments show that a pretrained ASR model adapted to our multi-TTS-same-SPK data gives relatively 9.8% lower word error rate (WER) compared to the ASR models adapted to same-TTS-multi-SPK data of the same data size, and our overall adaptation improves the model's WER from 57.6% to 16.5% without using any real audio as training data.",
      "abstract": "Automatic speech recognition (ASR) for rare words is difficult as there are little relevant text-audio data pairs to train an ASR model. To obtain more text-audio pairs, text-only data are fed to Text-To-Speech (TTS) systems to generate synthetic audio. Previous works use a single TTS system conditioned on multiple speakers to produce different speaker voices to improve the output data's speaker diversity, and they show that training an ASR model on the more diverse data can avoid overfitting and improve the model's robustness. As an alternative way to improve the diversity, we study the speaker embedding distribution of audios synthesized by different TTS systems and found that the audios synthesized by different TTS systems have different speaker distributions even when they are conditioned on the same speaker. Inspired by this, this paper proposes to condition multiple TTS systems repeatedly on a single speaker to synthesize more diverse speaker data, so ASR models can be trained more robustly. When we apply our method to a rare word dataset partitioned from National Speech Corpus SG, which contains mostly road names and addresses in its text transcripts, experiments show that a pretrained ASR model adapted to our multi-TTS-same-SPK data gives relatively 9.8% lower word error rate (WER) compared to the ASR models adapted to same-TTS-multi-SPK data of the same data size, and our overall adaptation improves the model's WER from 57.6% to 16.5% without using any real audio as training data.",
      "doi": "https://doi.org/10.1109/apsipaasc58517.2023.10317116",
      "openalex_id": "https://openalex.org/W4388820459",
      "arxiv_id": "",
      "publication_date": "2023-10-31",
      "published": "2023-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CAMP: A Unified Data Solution for Mandarin Speech Recognition Tasks",
      "summary": "Speech recognition, the transformation of spoken language into written text, is becoming increasingly vital across a broad range of applications. Despite the advancements in end-to-end Neural Network (NN) based speech recognition systems, the requirement for large volumes of annotated audio data tailored to specific scenarios remains a significant challenge. To address this, we introduce a novel approach, the Character Audio Mix-up (CAMP), which synthesizes scenario-specific audio data for Mandarin at a significantly reduced cost and effort. This method concatenates the audio segments of each character’s Pinyin in the text, obtained through force alignment on an existing annotated dataset, to synthesize the audio. These synthesized audios are then used to train the Automatic Speech Recognition (ASR) models. Experiments conducted on the AISHELL-3, and AIDATATANG datasets validate the effectiveness of CAMP, with ASR models trained on CAMP synthesized data performing relatively well compared to those trained with actual data from these datasets. Further, our ablation study reveals that while synthesized audio data can significantly reduce the need for real annotated audio specific to each scenario, it cannot entirely replace real audio. Thus, the importance of real annotated audio data in specific application scenarios is emphasized.",
      "abstract": "Speech recognition, the transformation of spoken language into written text, is becoming increasingly vital across a broad range of applications. Despite the advancements in end-to-end Neural Network (NN) based speech recognition systems, the requirement for large volumes of annotated audio data tailored to specific scenarios remains a significant challenge. To address this, we introduce a novel approach, the Character Audio Mix-up (CAMP), which synthesizes scenario-specific audio data for Mandarin at a significantly reduced cost and effort. This method concatenates the audio segments of each character’s Pinyin in the text, obtained through force alignment on an existing annotated dataset, to synthesize the audio. These synthesized audios are then used to train the Automatic Speech Recognition (ASR) models. Experiments conducted on the AISHELL-3, and AIDATATANG datasets validate the effectiveness of CAMP, with ASR models trained on CAMP synthesized data performing relatively well compared to those trained with actual data from these datasets. Further, our ablation study reveals that while synthesized audio data can significantly reduce the need for real annotated audio specific to each scenario, it cannot entirely replace real audio. Thus, the importance of real annotated audio data in specific application scenarios is emphasized.",
      "doi": "https://doi.org/10.3233/atde230552",
      "openalex_id": "https://openalex.org/W4387707208",
      "arxiv_id": "",
      "publication_date": "2023-10-17",
      "published": "2023-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the Relevance of Phoneme Duration Variability of Synthesized Training Data for Automatic Speech Recognition",
      "summary": "Synthetic data generated by text-to-speech (TTS) systems can be used to improve automatic speech recognition (ASR) systems in low-resource or domain mismatch tasks. It has been shown that TTS-generated outputs still do not have the same qualities as real data. In this work we focus on the temporal structure of synthetic data and its relation to ASR training. By using a novel oracle setup we show how much the degradation of synthetic data quality is influenced by duration modeling in non-autoregressive (NAR) TTS. To get reference phoneme durations we use two common alignment methods, a hidden Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist temporal classification (CTC) aligner. Using a simple algorithm based on random walks we shift phoneme duration distributions of the TTS system closer to real durations, resulting in an improvement of an ASR system using synthetic data in a semi-supervised setting.",
      "abstract": "Synthetic data generated by text-to-speech (TTS) systems can be used to improve automatic speech recognition (ASR) systems in low-resource or domain mismatch tasks. It has been shown that TTS-generated outputs still do not have the same qualities as real data. In this work we focus on the temporal structure of synthetic data and its relation to ASR training. By using a novel oracle setup we show how much the degradation of synthetic data quality is influenced by duration modeling in non-autoregressive (NAR) TTS. To get reference phoneme durations we use two common alignment methods, a hidden Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist temporal classification (CTC) aligner. Using a simple algorithm based on random walks we shift phoneme duration distributions of the TTS system closer to real durations, resulting in an improvement of an ASR system using synthetic data in a semi-supervised setting.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389782",
      "openalex_id": "https://openalex.org/W4391021730",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation",
      "summary": "Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...",
      "abstract": "Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...",
      "doi": "https://doi.org/10.48550/arxiv.2109.14200",
      "openalex_id": "https://openalex.org/W3164946614",
      "arxiv_id": "",
      "publication_date": "2021-09-29",
      "published": "2021-09-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LipLearner: Customizable Silent Speech Interactions on Mobile Devices",
      "summary": "Silent speech interface is a promising technology that enables private communications in natural language. However, previous approaches only support a small and inflexible vocabulary, which leads to limited expressiveness. We leverage contrastive learning to learn efficient lipreading representations, enabling few-shot command customization with minimal user effort. Our model exhibits high robustness to different lighting, posture, and gesture conditions on an in-the-wild dataset. For 25-command classification, an F1-score of 0.8947 is achievable only using one shot, and its performance can be further boosted by adaptively learning from more data. This generalizability allowed us to develop a mobile silent speech interface empowered with on-device fine-tuning and visual keyword spotting. A user study demonstrated that with LipLearner, users could define their own commands with high reliability guaranteed by an online incremental learning scheme. Subjective feedback indicated that our system provides essential functionalities for customizable silent speech interactions with high usability and learnability.",
      "abstract": "Silent speech interface is a promising technology that enables private communications in natural language. However, previous approaches only support a small and inflexible vocabulary, which leads to limited expressiveness. We leverage contrastive learning to learn efficient lipreading representations, enabling few-shot command customization with minimal user effort. Our model exhibits high robustness to different lighting, posture, and gesture conditions on an in-the-wild dataset. For 25-command classification, an F1-score of 0.8947 is achievable only using one shot, and its performance can be further boosted by adaptively learning from more data. This generalizability allowed us to develop a mobile silent speech interface empowered with on-device fine-tuning and visual keyword spotting. A user study demonstrated that with LipLearner, users could define their own commands with high reliability guaranteed by an online incremental learning scheme. Subjective feedback indicated that our system provides essential functionalities for customizable silent speech interactions with high usability and learnability.",
      "doi": "https://doi.org/10.1145/3544548.3581465",
      "openalex_id": "https://openalex.org/W4321009928",
      "arxiv_id": "",
      "publication_date": "2023-04-19",
      "published": "2023-04-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lip2Speech: Lightweight Multi-Speaker Speech Reconstruction with Gabor Features",
      "summary": "In environments characterised by noise or the absence of audio signals, visual cues, notably facial and lip movements, serve as valuable substitutes for missing or corrupted speech signals. In these scenarios, speech reconstruction can potentially generate speech from visual data. Recent advancements in this domain have predominantly relied on end-to-end deep learning models, like Convolutional Neural Networks (CNN) or Generative Adversarial Networks (GAN). However, these models are encumbered by their intricate and opaque architectures, coupled with their lack of speaker independence. Consequently, achieving multi-speaker speech reconstruction without supplementary information is challenging. This research introduces an innovative Gabor-based speech reconstruction system tailored for lightweight and efficient multi-speaker speech restoration. Using our Gabor feature extraction technique, we propose two novel models: GaborCNN2Speech and GaborFea2Speech. These models employ a rapid Gabor feature extraction method to derive lowdimensional mouth region features, encompassing filtered Gabor mouth images and low-dimensional Gabor features as visual inputs. An encoded spectrogram serves as the audio target, and a Long Short-Term Memory (LSTM)-based model is harnessed to generate coherent speech output. Through comprehensive experiments conducted on the GRID corpus, our proposed Gabor-based models have showcased superior performance in sentence and vocabulary reconstruction when compared to traditional end-to-end CNN models. These models stand out for their lightweight design and rapid processing capabilities. Notably, the GaborFea2Speech model presented in this study achieves robust multi-speaker speech reconstruction without necessitating supplementary information, thereby marking a significant milestone in the field of speech reconstruction.",
      "abstract": "In environments characterised by noise or the absence of audio signals, visual cues, notably facial and lip movements, serve as valuable substitutes for missing or corrupted speech signals. In these scenarios, speech reconstruction can potentially generate speech from visual data. Recent advancements in this domain have predominantly relied on end-to-end deep learning models, like Convolutional Neural Networks (CNN) or Generative Adversarial Networks (GAN). However, these models are encumbered by their intricate and opaque architectures, coupled with their lack of speaker independence. Consequently, achieving multi-speaker speech reconstruction without supplementary information is challenging. This research introduces an innovative Gabor-based speech reconstruction system tailored for lightweight and efficient multi-speaker speech restoration. Using our Gabor feature extraction technique, we propose two novel models: GaborCNN2Speech and GaborFea2Speech. These models employ a rapid Gabor feature extraction method to derive lowdimensional mouth region features, encompassing filtered Gabor mouth images and low-dimensional Gabor features as visual inputs. An encoded spectrogram serves as the audio target, and a Long Short-Term Memory (LSTM)-based model is harnessed to generate coherent speech output. Through comprehensive experiments conducted on the GRID corpus, our proposed Gabor-based models have showcased superior performance in sentence and vocabulary reconstruction when compared to traditional end-to-end CNN models. These models stand out for their lightweight design and rapid processing capabilities. Notably, the GaborFea2Speech model presented in this study achieves robust multi-speaker speech reconstruction without necessitating supplementary information, thereby marking a significant milestone in the field of speech reconstruction.",
      "doi": "https://doi.org/10.3390/app14020798",
      "openalex_id": "https://openalex.org/W4390943488",
      "arxiv_id": "",
      "publication_date": "2024-01-17",
      "published": "2024-01-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero-Shot Personalized Lip-To-Speech Synthesis with Face Image Based Voice Control",
      "summary": "Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the effectiveness of the proposed method whose synthetic utterances are more natural and matching with the personality of input video than the compared methods. To our best knowledge, this paper makes the first attempt on zero-shot personalized Lip2Speech synthesis with a face image rather than reference audio to control voice characteristics.",
      "abstract": "Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the effectiveness of the proposed method whose synthetic utterances are more natural and matching with the personality of input video than the compared methods. To our best knowledge, this paper makes the first attempt on zero-shot personalized Lip2Speech synthesis with a face image rather than reference audio to control voice characteristics.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096464",
      "openalex_id": "https://openalex.org/W4372348072",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Facetron: A Multi-Speaker Face-to-Speech Model Based on Cross-Modal Latent Representations",
      "summary": "In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using a face encoder trained through cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results.",
      "abstract": "In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using a face encoder trained through cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10290115",
      "openalex_id": "https://openalex.org/W4388117482",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improvements of Silent Speech Interface Algorithms",
      "summary": "Speech is a vital mode of communication, but for some individuals, speaking out loud may not be an option.Silent speech interfaces are a promising technology that allows for speech generation from articulatory signals, enabling individuals who are unable to speak to communicate.The focus of this topic is on the development and improvement of silent speech interfaces, which involves generating speech from data gathered from the movement of the tongue, lips, and jaw.To explore this topic, I have planned a comprehensive agenda that covers various aspects of silent speech interface development.The agenda includes the following topics:Preparing Data: This topic covers the collection and processing of data from the articulatory movements of a speaker.It includes data acquisition techniques such as Electromagnetic Articulography (EMA), Ultrasound Imaging, and Magnetic Resonance Imaging (MRI).Different Model Implementations: This topic covers various models that have been implemented for generating speech from articulatory signals, such as deep neural networks, support vector machines, and Hidden Markov Models.New Evaluation Metrics: This topic covers the development of new evaluation metrics for assessing the performance of silent speech interfaces.These metrics aim to provide a more accurate measure of speech quality and intelligibility, as traditional metrics may not be suitable for evaluating speech generated from articulatory signals.Model Improvement: This topic covers various techniques for improving the performance of silent speech interfaces, such as regularization, transfer learning, and data augmentation.Generalization of the Model for Unseen Data: This topic covers the development of models that can generalize well to new data and unseen speakers, which is essential for practical applications of silent speech interfaces.Throughout this topic, I have relied on extensive research and consultations with experts in the field.Their insights and guidance have been invaluable in developing a comprehensive understanding of silent speech interfaces.I am grateful to my professor and colleagues who have provided invaluable support and guidance throughout the research process.Their feedback and insights have been instrumental in shaping this topic, and I am grateful for their contributions.Last i but not least, I would like to express my gratitude to my family, friends, colleagues, and anyone who played a significant role in helping me reach this point in my life with their support.\"As one",
      "abstract": "Speech is a vital mode of communication, but for some individuals, speaking out loud may not be an option.Silent speech interfaces are a promising technology that allows for speech generation from articulatory signals, enabling individuals who are unable to speak to communicate.The focus of this topic is on the development and improvement of silent speech interfaces, which involves generating speech from data gathered from the movement of the tongue, lips, and jaw.To explore this topic, I have planned a comprehensive agenda that covers various aspects of silent speech interface development.The agenda includes the following topics:Preparing Data: This topic covers the collection and processing of data from the articulatory movements of a speaker.It includes data acquisition techniques such as Electromagnetic Articulography (EMA), Ultrasound Imaging, and Magnetic Resonance Imaging (MRI).Different Model Implementations: This topic covers various models that have been implemented for generating speech from articulatory signals, such as deep neural networks, support vector machines, and Hidden Markov Models.New Evaluation Metrics: This topic covers the development of new evaluation metrics for assessing the performance of silent speech interfaces.These metrics aim to provide a more accurate measure of speech quality and intelligibility, as traditional metrics may not be suitable for evaluating speech generated from articulatory signals.Model Improvement: This topic covers various techniques for improving the performance of silent speech interfaces, such as regularization, transfer learning, and data augmentation.Generalization of the Model for Unseen Data: This topic covers the development of models that can generalize well to new data and unseen speakers, which is essential for practical applications of silent speech interfaces.Throughout this topic, I have relied on extensive research and consultations with experts in the field.Their insights and guidance have been invaluable in developing a comprehensive understanding of silent speech interfaces.I am grateful to my professor and colleagues who have provided invaluable support and guidance throughout the research process.Their feedback and insights have been instrumental in shaping this topic, and I am grateful for their contributions.Last i but not least, I would like to express my gratitude to my family, friends, colleagues, and anyone who played a significant role in helping me reach this point in my life with their support.\"As one",
      "doi": "https://doi.org/10.14232/phd.11818",
      "openalex_id": "https://openalex.org/W4396804723",
      "arxiv_id": "",
      "publication_date": "2024-02-05",
      "published": "2024-02-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR",
      "summary": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling global and local dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
      "abstract": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling global and local dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
      "doi": "https://doi.org/10.1109/taslp.2023.3332545",
      "openalex_id": "https://openalex.org/W4389474152",
      "arxiv_id": "",
      "publication_date": "2023-12-08",
      "published": "2023-12-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sub-8-Bit Quantization for On-Device Speech Recognition: A Regularization-Free Approach",
      "summary": "For on-device automatic speech recognition (ASR), quantization aware training (QAT) is ubiquitous to achieve the trade-off between model predictive performance and efficiency. Among existing QAT methods, one major drawback is that the quantization centroids have to be predetermined and fixed. To overcome this limitation, we introduce a regularization-free, \"soft-to-hard\" compression mechanism with self-adjustable centroids in a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mu$</tex> -Law constrained space, resulting in a simpler yet more versatile quantization scheme, called General Quantizer (GQ). We apply GQ to ASR tasks using Recurrent Neural Network Transducer (RNN-T) and Conformer architectures on both LibriSpeech and de-identified far-field datasets. Without accuracy degradation, GQ can compress both RNN-T and Conformer into sub-8-bit, and for some RNN-T layers, to 1-bit for fast and accurate inference. We observe a 30.73% memory footprint saving and 31.75% user-perceived latency reduction compared to 8-bit QAT via physical device benchmarking.",
      "abstract": "For on-device automatic speech recognition (ASR), quantization aware training (QAT) is ubiquitous to achieve the trade-off between model predictive performance and efficiency. Among existing QAT methods, one major drawback is that the quantization centroids have to be predetermined and fixed. To overcome this limitation, we introduce a regularization-free, \"soft-to-hard\" compression mechanism with self-adjustable centroids in a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mu$</tex> -Law constrained space, resulting in a simpler yet more versatile quantization scheme, called General Quantizer (GQ). We apply GQ to ASR tasks using Recurrent Neural Network Transducer (RNN-T) and Conformer architectures on both LibriSpeech and de-identified far-field datasets. Without accuracy degradation, GQ can compress both RNN-T and Conformer into sub-8-bit, and for some RNN-T layers, to 1-bit for fast and accurate inference. We observe a 30.73% memory footprint saving and 31.75% user-perceived latency reduction compared to 8-bit QAT via physical device benchmarking.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022821",
      "openalex_id": "https://openalex.org/W4319862415",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Audio Coding with Deep Complex Networks",
      "summary": "Abstract This paper proposes a transform domain audio coding method based on deep complex networks. In the proposed codec, the time-frequency spectrum of the audio signal is fed to the encoder which consists of complex convolutional blocks and a frequency-temporal modeling module to obtain the extracted features which are then quantized with a target bitrate by the vector quantizer. The structure of the decoder which reconstruct the time-frequency spectrum of the audio from quantized features is symmetrical to the encoder. In this paper, a structure combining the complex multi-head self-attention module and the complex long short-term memory is proposed to capture both frequency and temporal dependencies. Subjective and objective evaluation tests show the advantage of the proposed method.",
      "abstract": "Abstract This paper proposes a transform domain audio coding method based on deep complex networks. In the proposed codec, the time-frequency spectrum of the audio signal is fed to the encoder which consists of complex convolutional blocks and a frequency-temporal modeling module to obtain the extracted features which are then quantized with a target bitrate by the vector quantizer. The structure of the decoder which reconstruct the time-frequency spectrum of the audio from quantized features is symmetrical to the encoder. In this paper, a structure combining the complex multi-head self-attention module and the complex long short-term memory is proposed to capture both frequency and temporal dependencies. Subjective and objective evaluation tests show the advantage of the proposed method.",
      "doi": "https://doi.org/10.1088/1742-6596/2759/1/012005",
      "openalex_id": "https://openalex.org/W4397002960",
      "arxiv_id": "",
      "publication_date": "2024-05-01",
      "published": "2024-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation",
      "summary": "Unsupervised representation learning for speech audios attained impressive performances for speech recognition tasks, particularly when annotated speech is limited. However, the unsupervised paradigm needs to be carefully designed and little is known about what properties these representations acquire. There is no guarantee that the model learns meaningful representations for valuable information for recognition. Moreover, the adaptation ability of the learned representations to other domains still needs to be estimated. In this work, we explore learning domain-invariant representations via a direct mapping of speech representations to their corresponding high-level linguistic informations. Results prove that the learned latents not only capture the articulatory feature of each phoneme but also enhance the adaptation ability, outperforming the baseline largely on accented benchmarks.",
      "abstract": "Unsupervised representation learning for speech audios attained impressive performances for speech recognition tasks, particularly when annotated speech is limited. However, the unsupervised paradigm needs to be carefully designed and little is known about what properties these representations acquire. There is no guarantee that the model learns meaningful representations for valuable information for recognition. Moreover, the adaptation ability of the learned representations to other domains still needs to be estimated. In this work, we explore learning domain-invariant representations via a direct mapping of speech representations to their corresponding high-level linguistic informations. Results prove that the learned latents not only capture the articulatory feature of each phoneme but also enhance the adaptation ability, outperforming the baseline largely on accented benchmarks.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022892",
      "openalex_id": "https://openalex.org/W4319862412",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes",
      "summary": "Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time.However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term.This paper introduces a new approach to continual audio representation learning called DeCoR.Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook.We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning.Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.",
      "abstract": "Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time.However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term.This paper introduces a new approach to continual audio representation learning called DeCoR.Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook.We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning.Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.",
      "doi": "https://doi.org/10.21437/interspeech.2023-2297",
      "openalex_id": "https://openalex.org/W4385822985",
      "arxiv_id": "",
      "publication_date": "2023-08-14",
      "published": "2023-08-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis Based on Disentanglement Between Prosody and Timbre",
      "summary": "The capability of generating speech with a specific type of emotion is desired for many human-computer interaction applications. Cross-speaker emotion transfer is a common approach to generating emotional speech when speech data with emotion labels from target speakers is not available for model training. This paper presents a novel cross-speaker emotion transfer system named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type and the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. The prosody predictor is used to provide prosodic features for emotion transfer. The timbre encoder provides timbre-related information for the system. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered the primary carrier of emotion-related speech characteristics, and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that the speech of target speakers is not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion types and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to transfer emotional information to a new speaker effectively. Audio samples are publicly available.",
      "abstract": "The capability of generating speech with a specific type of emotion is desired for many human-computer interaction applications. Cross-speaker emotion transfer is a common approach to generating emotional speech when speech data with emotion labels from target speakers is not available for model training. This paper presents a novel cross-speaker emotion transfer system named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type and the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. The prosody predictor is used to provide prosodic features for emotion transfer. The timbre encoder provides timbre-related information for the system. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered the primary carrier of emotion-related speech characteristics, and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that the speech of target speakers is not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion types and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to transfer emotional information to a new speaker effectively. Audio samples are publicly available.",
      "doi": "https://doi.org/10.1109/taslp.2023.3268571",
      "openalex_id": "https://openalex.org/W4366493008",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representation Learning With Hidden Unit Clustering for Low Resource Speech Applications",
      "summary": "In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned \"time-frequency\" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula> -means algorithm. We explore techniques that improve the speaker invariance of the learned representations and illustrate the effectiveness of the proposed approach on two settings, i) completely unsupervised speech applications on the sub-tasks described as part of the ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition (ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi dataset. In these experiments, we achieve state-of-art results for various Zerospeech tasks. Further, on the ASR experiments, the HUC representations are shown to improve over other established benchmarks based on wav2vec, HuBERT and Best-RQ.",
      "abstract": "In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned \"time-frequency\" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula> -means algorithm. We explore techniques that improve the speaker invariance of the learned representations and illustrate the effectiveness of the proposed approach on two settings, i) completely unsupervised speech applications on the sub-tasks described as part of the ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition (ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi dataset. In these experiments, we achieve state-of-art results for various Zerospeech tasks. Further, on the ASR experiments, the HUC representations are shown to improve over other established benchmarks based on wav2vec, HuBERT and Best-RQ.",
      "doi": "https://doi.org/10.1109/taslp.2023.3337670",
      "openalex_id": "https://openalex.org/W4389317789",
      "arxiv_id": "",
      "publication_date": "2023-12-04",
      "published": "2023-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models",
      "summary": "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., \"A is capable of but not good at B\"). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities.",
      "abstract": "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., \"A is capable of but not good at B\"). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.309",
      "openalex_id": "https://openalex.org/W4385570599",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Regeneration Learning: A Learning Paradigm for Data Generation",
      "summary": "Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X--&gt;Y' and Y'--&gt;Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'--&gt;Y in regeneration learning and X--&gt;X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.",
      "abstract": "Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X--&gt;Y' and Y'--&gt;Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'--&gt;Y in regeneration learning and X--&gt;X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.",
      "doi": "https://doi.org/10.1609/aaai.v38i20.30271",
      "openalex_id": "https://openalex.org/W4393161149",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Masked Segmental Language Model for Unsupervised Natural Language Segmentation",
      "summary": "We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world's languages are both morphologically complex, and have no large dataset of \"gold\" segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.",
      "abstract": "We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world's languages are both morphologically complex, and have no large dataset of \"gold\" segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.",
      "doi": "https://doi.org/10.18653/v1/2022.sigmorphon-1.5",
      "openalex_id": "https://openalex.org/W3156299562",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation",
      "summary": "Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses based on harmonized multimodal representations. Comprehensive experiments conducted on both text-based and image-grounded dialogue datasets demonstrate ZRIGF's efficacy in generating contextually pertinent and informative responses. Furthermore, we adopt a fully zero-resource scenario in the image-grounded dialogue dataset to demonstrate our framework's robust generalization capabilities in novel domains. The code is available at https://github.com/zhangbo-nlp/ZRIGF.",
      "abstract": "Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses based on harmonized multimodal representations. Comprehensive experiments conducted on both text-based and image-grounded dialogue datasets demonstrate ZRIGF's efficacy in generating contextually pertinent and informative responses. Furthermore, we adopt a fully zero-resource scenario in the image-grounded dialogue dataset to demonstrate our framework's robust generalization capabilities in novel domains. The code is available at https://github.com/zhangbo-nlp/ZRIGF.",
      "doi": "https://doi.org/10.1145/3581783.3611810",
      "openalex_id": "https://openalex.org/W4385959364",
      "arxiv_id": "",
      "publication_date": "2023-10-26",
      "published": "2023-10-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks",
      "summary": "We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.",
      "abstract": "We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.",
      "doi": "https://doi.org/10.21437/interspeech.2021-50",
      "openalex_id": "https://openalex.org/W3112613336",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Proceedings of the 2020 Joint Conference on AI Music Creativity",
      "summary": "Modern approaches to sound synthesis using deep neural networks are hard to\\ncontrol, especially when fine-grained conditioning information is not\\navailable, hindering their adoption by musicians.\\n In this paper, we cast the generation of individual instrumental notes as an\\ninpainting-based task, introducing novel and unique ways to iteratively shape\\nsounds. To this end, we propose a two-step approach: first, we adapt the\\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\\nreal-valued spectrograms into compact discrete codemaps, we then implement\\ntoken-masked Transformers for the inpainting-based generation of these\\ncodemaps.\\n We apply the proposed architecture on the NSynth dataset on masked resampling\\ntasks. Most crucially, we open-source an interactive web interface to transform\\nsounds by inpainting, for artists and practitioners alike, opening up to new,\\ncreative uses.\\n",
      "abstract": "Modern approaches to sound synthesis using deep neural networks are hard to\\ncontrol, especially when fine-grained conditioning information is not\\navailable, hindering their adoption by musicians.\\n In this paper, we cast the generation of individual instrumental notes as an\\ninpainting-based task, introducing novel and unique ways to iteratively shape\\nsounds. To this end, we propose a two-step approach: first, we adapt the\\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\\nreal-valued spectrograms into compact discrete codemaps, we then implement\\ntoken-masked Transformers for the inpainting-based generation of these\\ncodemaps.\\n We apply the proposed architecture on the NSynth dataset on masked resampling\\ntasks. Most crucially, we open-source an interactive web interface to transform\\nsounds by inpainting, for artists and practitioners alike, opening up to new,\\ncreative uses.\\n",
      "doi": "https://doi.org/10.30746/978-91-519-5560-5",
      "openalex_id": "https://openalex.org/W3213967396",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Probe-Assisted Fine-Grained Control for Non-Differentiable Features in Symbolic Music Generation",
      "summary": "As symbolic music generation evolves, research interest is shifting toward more controlled and steerable generative processes to support creative decisions. Previous methods focus on global conditioning or fine-grained control through input sequences but often limit flexibility for real-time interventions and require modifications to the model&#x2019;s architecture. We introduce a novel symbolic music generation framework by combining a Transformer encoder-decoder with probe models, which enable us to interpret the encoder hidden state using pre-defined non-differentiable musical features, and subsequently manipulate the hidden state to achieve a set of desired attributes in the generated music. This method allows fine-grained control over specific musical features without altering the underlying model architecture. Probes can be trained jointly with the generative model or applied post-training, enabling adaptable control without retraining the model. Our experiments demonstrate that this intervention effectively influences the model output without hindering the music quality. This approach enhances both the flexibility and interpretability of symbolic music generation, enabling better real-world applicability for music generation models.",
      "abstract": "As symbolic music generation evolves, research interest is shifting toward more controlled and steerable generative processes to support creative decisions. Previous methods focus on global conditioning or fine-grained control through input sequences but often limit flexibility for real-time interventions and require modifications to the model&#x2019;s architecture. We introduce a novel symbolic music generation framework by combining a Transformer encoder-decoder with probe models, which enable us to interpret the encoder hidden state using pre-defined non-differentiable musical features, and subsequently manipulate the hidden state to achieve a set of desired attributes in the generated music. This method allows fine-grained control over specific musical features without altering the underlying model architecture. Probes can be trained jointly with the generative model or applied post-training, enabling adaptable control without retraining the model. Our experiments demonstrate that this intervention effectively influences the model output without hindering the music quality. This approach enhances both the flexibility and interpretability of symbolic music generation, enabling better real-world applicability for music generation models.",
      "doi": "https://doi.org/10.1109/access.2025.3540543",
      "openalex_id": "https://openalex.org/W4407304302",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions",
      "summary": "The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.",
      "abstract": "The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.",
      "doi": "https://doi.org/10.48550/arxiv.2011.06801",
      "openalex_id": "https://openalex.org/W3099378280",
      "arxiv_id": "",
      "publication_date": "2020-11-13",
      "published": "2020-11-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Piano Inpainting Application",
      "summary": "Autoregressive models are now capable of generating high-quality minute-long expressive MIDI piano performances. Even though this progress suggests new tools to assist music composition, we observe that generative algorithms are still not widely used by artists due to the limited control they offer, prohibitive inference times or the lack of integration within musicians' workflows. In this work, we present the Piano Inpainting Application (PIA), a generative model focused on inpainting piano performances, as we believe that this elementary operation (restoring missing parts of a piano performance) encourages human-machine interaction and opens up new ways to approach music composition. Our approach relies on an encoder-decoder Linear Transformer architecture trained on a novel representation for MIDI piano performances termed Structured MIDI Encoding. By uncovering an interesting synergy between Linear Transformers and our inpainting task, we are able to efficiently inpaint contiguous regions of a piano performance, which makes our model suitable for interactive and responsive A.I.-assisted composition. Finally, we introduce our freely-available Ableton Live PIA plugin, which allows musicians to smoothly generate or modify any MIDI clip using PIA within a widely-used professional Digital Audio Workstation.",
      "abstract": "Autoregressive models are now capable of generating high-quality minute-long expressive MIDI piano performances. Even though this progress suggests new tools to assist music composition, we observe that generative algorithms are still not widely used by artists due to the limited control they offer, prohibitive inference times or the lack of integration within musicians' workflows. In this work, we present the Piano Inpainting Application (PIA), a generative model focused on inpainting piano performances, as we believe that this elementary operation (restoring missing parts of a piano performance) encourages human-machine interaction and opens up new ways to approach music composition. Our approach relies on an encoder-decoder Linear Transformer architecture trained on a novel representation for MIDI piano performances termed Structured MIDI Encoding. By uncovering an interesting synergy between Linear Transformers and our inpainting task, we are able to efficiently inpaint contiguous regions of a piano performance, which makes our model suitable for interactive and responsive A.I.-assisted composition. Finally, we introduce our freely-available Ableton Live PIA plugin, which allows musicians to smoothly generate or modify any MIDI clip using PIA within a widely-used professional Digital Audio Workstation.",
      "doi": "https://doi.org/10.48550/arxiv.2107.05944",
      "openalex_id": "https://openalex.org/W3182466123",
      "arxiv_id": "",
      "publication_date": "2021-07-13",
      "published": "2021-07-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Incorporating Music Knowledge in Continual Dataset Augmentation for Music Generation",
      "summary": "Deep learning has rapidly become the state-of-the-art approach for music generation. However, training a deep model typically requires a large training set, which is often not available for specific musical styles. In this paper, we present augmentative generation (Aug-Gen), a method of dataset augmentation for any music generation system trained on a resource-constrained domain. The key intuition of this method is that the training data for a generative system can be augmented by examples the system produces during the course of training, provided these examples are of sufficiently high quality and variety. We apply Aug-Gen to Transformer-based chorale generation in the style of J.S. Bach, and show that this allows for longer training and results in better generative output.",
      "abstract": "Deep learning has rapidly become the state-of-the-art approach for music generation. However, training a deep model typically requires a large training set, which is often not available for specific musical styles. In this paper, we present augmentative generation (Aug-Gen), a method of dataset augmentation for any music generation system trained on a resource-constrained domain. The key intuition of this method is that the training data for a generative system can be augmented by examples the system produces during the course of training, provided these examples are of sufficiently high quality and variety. We apply Aug-Gen to Transformer-based chorale generation in the style of J.S. Bach, and show that this allows for longer training and results in better generative output.",
      "doi": "https://doi.org/10.48550/arxiv.2006.13331",
      "openalex_id": "https://openalex.org/W3038090892",
      "arxiv_id": "",
      "publication_date": "2020-06-23",
      "published": "2020-06-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Symbolic Music Loop Generation with VQ-VAE",
      "summary": "Music is a repetition of patterns and rhythms. It can be composed by repeating a certain number of bars in a structured way. In this paper, the objective is to generate a loop of 8 bars that can be used as a building block of music. Even considering musical diversity, we assume that music patterns familiar to humans can be defined in a finite set. With explicit rules to extract loops from music, we found that discrete representations are sufficient to model symbolic music sequences. Among VAE family, musical properties from VQ-VAE are better observed rather than other models. Further, to emphasize musical structure, we have manipulated discrete latent features to be repetitive so that the properties are more strengthened. Quantitative and qualitative experiments are extensively conducted to verify our assumptions.",
      "abstract": "Music is a repetition of patterns and rhythms. It can be composed by repeating a certain number of bars in a structured way. In this paper, the objective is to generate a loop of 8 bars that can be used as a building block of music. Even considering musical diversity, we assume that music patterns familiar to humans can be defined in a finite set. With explicit rules to extract loops from music, we found that discrete representations are sufficient to model symbolic music sequences. Among VAE family, musical properties from VQ-VAE are better observed rather than other models. Further, to emphasize musical structure, we have manipulated discrete latent features to be repetitive so that the properties are more strengthened. Quantitative and qualitative experiments are extensively conducted to verify our assumptions.",
      "doi": "https://doi.org/10.48550/arxiv.2111.07657",
      "openalex_id": "https://openalex.org/W3212222439",
      "arxiv_id": "",
      "publication_date": "2021-11-15",
      "published": "2021-11-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CallTran: Voice Translation for End-to-End Communication over the Internet",
      "summary": "The importance of language translation becomes evident when two individuals, each speaking a different mother tongue and lacking a common language, seek to communicate effectively. This article presents the development and evaluation of a mobile application for voice-to-voice translation over the Internet. The application employs three main technologies: speech recognition, machine translation, and speech synthesis. Google APIs (speech-to-text API, text-to-text-translator API, and text-to-speech synthesizer API) were used to implement the system. The evaluation showed that the system achieved an overall accuracy of 85% in recognizing and translating speech input from users. However, the accuracy varied across different languages. The system was also found to be effective in facilitating communication between users who speak different languages. The limitations of the system were identified in its performance in noisy or crowded environments and the handling of regional accents and dialects. Overall, the developed system has the potential to bridge language barriers and facilitate communication among people speaking different languages.",
      "abstract": "The importance of language translation becomes evident when two individuals, each speaking a different mother tongue and lacking a common language, seek to communicate effectively. This article presents the development and evaluation of a mobile application for voice-to-voice translation over the Internet. The application employs three main technologies: speech recognition, machine translation, and speech synthesis. Google APIs (speech-to-text API, text-to-text-translator API, and text-to-speech synthesizer API) were used to implement the system. The evaluation showed that the system achieved an overall accuracy of 85% in recognizing and translating speech input from users. However, the accuracy varied across different languages. The system was also found to be effective in facilitating communication between users who speak different languages. The limitations of the system were identified in its performance in noisy or crowded environments and the handling of regional accents and dialects. Overall, the developed system has the potential to bridge language barriers and facilitate communication among people speaking different languages.",
      "doi": "https://doi.org/10.1109/ic-etite58242.2024.10493835",
      "openalex_id": "https://openalex.org/W4394937522",
      "arxiv_id": "",
      "publication_date": "2024-02-22",
      "published": "2024-02-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiscHAR: A Discrete Approach to Enhance Human Activity Recognition in Cyber Physical Systems: Smart Homes",
      "summary": "The main challenges in smart home systems and cyber-physical systems come from not having enough data and unclear interpretation; thus, there is still a lot to be done in this field. In this work, we propose a practical approach called Discrete Human Activity Recognition (DiscHAR) based on prior research to enhance Human Activity Recognition (HAR). Our goal is to generate diverse data to build better models for activity classification. To tackle overfitting, which often occurs with small datasets, we generate data and convert them into discrete forms, improving classification accuracy. Our methodology includes advanced techniques like the R-Frame method for sampling and the Mixed-up approach for data generation. We apply K-means vector quantization to categorize the data, and through the elbow method, we determine the optimal number of clusters. The discrete sequences are converted into one-hot encoded vectors and fed into a CNN model to ensure precise recognition of human activities. Evaluations on the OPP79, PAMAP2, and WISDM datasets show that our approach outperforms existing models, achieving 89% accuracy for OPP79, 93.24% for PAMAP2, and 100% for WISDM. These results demonstrate the model’s effectiveness in identifying complex activities captured by wearable devices. Our work combines theory and practice to address ongoing challenges in this field, aiming to improve the reliability and performance of activity recognition systems in dynamic environments.",
      "abstract": "The main challenges in smart home systems and cyber-physical systems come from not having enough data and unclear interpretation; thus, there is still a lot to be done in this field. In this work, we propose a practical approach called Discrete Human Activity Recognition (DiscHAR) based on prior research to enhance Human Activity Recognition (HAR). Our goal is to generate diverse data to build better models for activity classification. To tackle overfitting, which often occurs with small datasets, we generate data and convert them into discrete forms, improving classification accuracy. Our methodology includes advanced techniques like the R-Frame method for sampling and the Mixed-up approach for data generation. We apply K-means vector quantization to categorize the data, and through the elbow method, we determine the optimal number of clusters. The discrete sequences are converted into one-hot encoded vectors and fed into a CNN model to ensure precise recognition of human activities. Evaluations on the OPP79, PAMAP2, and WISDM datasets show that our approach outperforms existing models, achieving 89% accuracy for OPP79, 93.24% for PAMAP2, and 100% for WISDM. These results demonstrate the model’s effectiveness in identifying complex activities captured by wearable devices. Our work combines theory and practice to address ongoing challenges in this field, aiming to improve the reliability and performance of activity recognition systems in dynamic environments.",
      "doi": "https://doi.org/10.3390/computers13110300",
      "openalex_id": "https://openalex.org/W4404503586",
      "arxiv_id": "",
      "publication_date": "2024-11-19",
      "published": "2024-11-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI-Powered Heritage Exploration in Tamil Nadu Historical Wonders",
      "summary": "Heritage tourism is being transformed by AI-driven solutions that offer real-time, personalized, and interactive experiences. This project introduces an intelligent platform that integrates multilingual voice assistance, GPS-enabled navigation, and AI-generated historical content to enrich visitor engagement at cultural sites. The system delivers dynamic insights using advanced language models, making historical exploration more accessible, engaging, and informative. Unlike conventional approaches that rely on static information and manual translations, this platform provides instant voice-guided explanations and adaptive recommendations based on user preferences. Tourists can explore heritage sites with location-based storytelling and interactive itineraries, enhancing their cultural journey. The system ensures seamless accessibility for diverse audiences by supporting multiple languages and AI-driven narration. Beyond improving the tourist experience, this innovation contributes to heritage preservation and digital accessibility. Designed for scalability and adaptability, the platform can be extended to various historical locations, ensuring long-term sustainability and broader cultural education and tourism outreach...",
      "abstract": "Heritage tourism is being transformed by AI-driven solutions that offer real-time, personalized, and interactive experiences. This project introduces an intelligent platform that integrates multilingual voice assistance, GPS-enabled navigation, and AI-generated historical content to enrich visitor engagement at cultural sites. The system delivers dynamic insights using advanced language models, making historical exploration more accessible, engaging, and informative. Unlike conventional approaches that rely on static information and manual translations, this platform provides instant voice-guided explanations and adaptive recommendations based on user preferences. Tourists can explore heritage sites with location-based storytelling and interactive itineraries, enhancing their cultural journey. The system ensures seamless accessibility for diverse audiences by supporting multiple languages and AI-driven narration. Beyond improving the tourist experience, this innovation contributes to heritage preservation and digital accessibility. Designed for scalability and adaptability, the platform can be extended to various historical locations, ensuring long-term sustainability and broader cultural education and tourism outreach...",
      "doi": "https://doi.org/10.48175/ijarsct-23429",
      "openalex_id": "https://openalex.org/W4408779662",
      "arxiv_id": "",
      "publication_date": "2025-03-24",
      "published": "2025-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentanglement of Prosody Representations via Diffusion Models and Scheduled Gradient Reversal",
      "summary": "Prosody plays a fundamental role in human speech and communication, facilitating intelligibility and conveying emotional and cognitive states. Extracting accurate prosodic information from speech is vital for building assistive technology, such as controllable speech synthesis, speaking style transfer, and speech emotion recognition (SER). However, it is challenging to disentangle speaker-independent prosody representations since prosodic attributes, such as intonation, excessively entangle with speaker-specific attributes, e.g., pitch. In this article, we propose a novel model, called Diffsody, to disentangle and refine prosody representations: 1) to disentangle prosody representations, we leverage the expressive generative ability of a diffusion model by conditioning it on quantified semantic information and pretrained speaker embeddings. Additionally, a prosody encoder automatically learns prosody representations used for spectrogram reconstruction in an unsupervised fashion; and 2) to refine and learn speaker-invariant prosody representations, a scheduled gradient reversal layer (sGRL) is proposed and integrated into the prosody encoder of Diffsody. We extensively evaluate Diffsody through qualitative and quantitative means. t-SNE visualization and speaker verification experiments demonstrate the efficacy of the sGRL method in preventing speaker-specific information leakage. Experimental results on speaker-independent SER and automatic depression detection (ADD) tasks demonstrate that Diffsody can efficiently factorize speaker-independent prosody representations, resulting in a significant boost in SER and ADD. In addition, Diffsody synergistically integrates with the semantic representation model WavLM, which leads to a discernibly elevated performance, outperforming contemporary methods in both SER and ADD tasks. Furthermore, the Diffsody model exhibits promising potential for various practical applications, such as voice or style conversion. Some audio samples can be found on our https://leyuanqu.github.io/Diffsody/demo website.",
      "abstract": "Prosody plays a fundamental role in human speech and communication, facilitating intelligibility and conveying emotional and cognitive states. Extracting accurate prosodic information from speech is vital for building assistive technology, such as controllable speech synthesis, speaking style transfer, and speech emotion recognition (SER). However, it is challenging to disentangle speaker-independent prosody representations since prosodic attributes, such as intonation, excessively entangle with speaker-specific attributes, e.g., pitch. In this article, we propose a novel model, called Diffsody, to disentangle and refine prosody representations: 1) to disentangle prosody representations, we leverage the expressive generative ability of a diffusion model by conditioning it on quantified semantic information and pretrained speaker embeddings. Additionally, a prosody encoder automatically learns prosody representations used for spectrogram reconstruction in an unsupervised fashion; and 2) to refine and learn speaker-invariant prosody representations, a scheduled gradient reversal layer (sGRL) is proposed and integrated into the prosody encoder of Diffsody. We extensively evaluate Diffsody through qualitative and quantitative means. t-SNE visualization and speaker verification experiments demonstrate the efficacy of the sGRL method in preventing speaker-specific information leakage. Experimental results on speaker-independent SER and automatic depression detection (ADD) tasks demonstrate that Diffsody can efficiently factorize speaker-independent prosody representations, resulting in a significant boost in SER and ADD. In addition, Diffsody synergistically integrates with the semantic representation model WavLM, which leads to a discernibly elevated performance, outperforming contemporary methods in both SER and ADD tasks. Furthermore, the Diffsody model exhibits promising potential for various practical applications, such as voice or style conversion. Some audio samples can be found on our https://leyuanqu.github.io/Diffsody/demo website.",
      "doi": "https://doi.org/10.1109/tnnls.2025.3534822",
      "openalex_id": "https://openalex.org/W4407900542",
      "arxiv_id": "",
      "publication_date": "2025-02-24",
      "published": "2025-02-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Representation Learning for Basecalling Nanopore Sequencing Data",
      "summary": "Basecalling is a complex task that involves translating noisy raw electrical signals into their corresponding DNA sequences. Several deep learning architectures have been successful in improving basecalling accuracy, but all of them rely on a supervised training scheme and require large annotated datasets to achieve high accuracy. However, obtaining labeled data for some species can be extremely challenging, making it difficult to generate a large amount of ground truth labels for training basecalling models. Self-supervised representation learning (SSL) has been shown to alleviate the need for large annotated datasets and, in some cases, enhance model performance. In this work, we investigate the effectiveness of self-supervised representation learning frameworks on the basecalling task. We consider SSL basecallers based on two well-known SSL frameworks, SimCLR and wav2vec2.0, and show that the self-supervised trained basecaller outperforms its supervised counterparts in both low and high data regimes, showing up to a 3% increase in performance when trained on only 1% of the total labeled data. Our results suggest that learning strong representations from unlabeled data can improve basecalling accuracy compared to state-of-the-art models across different architectures. Furthermore, we provide insights into representation learning for the basecalling task and discuss the role of continuous representations during SSL pretraining. Our code is publicly available at <uri>https://github.com/carlosvint/SSLBasecalling</uri>.",
      "abstract": "Basecalling is a complex task that involves translating noisy raw electrical signals into their corresponding DNA sequences. Several deep learning architectures have been successful in improving basecalling accuracy, but all of them rely on a supervised training scheme and require large annotated datasets to achieve high accuracy. However, obtaining labeled data for some species can be extremely challenging, making it difficult to generate a large amount of ground truth labels for training basecalling models. Self-supervised representation learning (SSL) has been shown to alleviate the need for large annotated datasets and, in some cases, enhance model performance. In this work, we investigate the effectiveness of self-supervised representation learning frameworks on the basecalling task. We consider SSL basecallers based on two well-known SSL frameworks, SimCLR and wav2vec2.0, and show that the self-supervised trained basecaller outperforms its supervised counterparts in both low and high data regimes, showing up to a 3% increase in performance when trained on only 1% of the total labeled data. Our results suggest that learning strong representations from unlabeled data can improve basecalling accuracy compared to state-of-the-art models across different architectures. Furthermore, we provide insights into representation learning for the basecalling task and discuss the role of continuous representations during SSL pretraining. Our code is publicly available at <uri>https://github.com/carlosvint/SSLBasecalling</uri>.",
      "doi": "https://doi.org/10.1109/access.2024.3440882",
      "openalex_id": "https://openalex.org/W4401415707",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The problem of prediction of the transmission coefficient using neural networks with a limited quantity of data",
      "summary": "The article discusses available approaches to predicting the transmission coefficient of metamaterials. In the paper was proposed different approaches that create the possibility of using data from various open sources, as well as the possibility of encoding complete structural information about the composition of metamaterials. A neural network with two inputs was designed, which is based on a three-dimensional convolution operation. Using these approaches, the training of an artificial neural network was carried out, and the results of transmission coefficient prediction were presented. The nature of metamaterial use can be determined by the predicted coefficient, but the resulting root mean square error still does not allow using such a neural network as a substitute for existing approaches. The paper presents an analysis of the obtained results, in which possible approaches to solving the problem of the amount of data are proposed, as well as solving the problem of different intervals of electromagnetic radiation in the dataset using the architecture of a three-dimensional transformer.",
      "abstract": "The article discusses available approaches to predicting the transmission coefficient of metamaterials. In the paper was proposed different approaches that create the possibility of using data from various open sources, as well as the possibility of encoding complete structural information about the composition of metamaterials. A neural network with two inputs was designed, which is based on a three-dimensional convolution operation. Using these approaches, the training of an artificial neural network was carried out, and the results of transmission coefficient prediction were presented. The nature of metamaterial use can be determined by the predicted coefficient, but the resulting root mean square error still does not allow using such a neural network as a substitute for existing approaches. The paper presents an analysis of the obtained results, in which possible approaches to solving the problem of the amount of data are proposed, as well as solving the problem of different intervals of electromagnetic radiation in the dataset using the architecture of a three-dimensional transformer.",
      "doi": "https://doi.org/10.32347/2411-4049.2025.1.155-163",
      "openalex_id": "https://openalex.org/W4410093233",
      "arxiv_id": "",
      "publication_date": "2025-03-28",
      "published": "2025-03-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Gradformer: A Framework for Multi-Aspect Multi-Granularity Pronunciation Assessment",
      "summary": "Automatic pronunciation assessment is an indispensable technology in computer-assisted pronunciation training systems. To further evaluate the quality of pronunciation, multi-task learning with simultaneous output of multi-granularity and multi-aspect has become a mainstream solution. Existing methods either predict scores at all granularity levels simultaneously through a parallel structure, or predict individual granularity scores layer by layer through a hierarchical structure. However, these methods do not fully understand and take advantage of the correlation between the three granularity levels of phoneme, word, and utterance. To address this issue, we propose a novel method, Granularity-decoupled Transformer (Gradformer), which is able to model the relationships between multiple granularity levels. Specifically, we first use a convolution-augmented transformer encoder to encode acoustic features, where the convolution module helps the model better capture local information. The model outputs both phoneme- and word-level granularity scores with high correlation by the encoder. Then, we use utterance queries to interact with the output of the encoder through the transformer decoder, ultimately obtaining the utterance scores. Through unique encoder and decoder architecture, we achieve decoupling at three granularity levels, and handling the relationship between each granularity. Experiments on the speachocean762 dataset show that our model has advantages over state-of-the-art methods in various metrics, especially in key metrics such as phoneme accuracy, word accuracy, and total score.",
      "abstract": "Automatic pronunciation assessment is an indispensable technology in computer-assisted pronunciation training systems. To further evaluate the quality of pronunciation, multi-task learning with simultaneous output of multi-granularity and multi-aspect has become a mainstream solution. Existing methods either predict scores at all granularity levels simultaneously through a parallel structure, or predict individual granularity scores layer by layer through a hierarchical structure. However, these methods do not fully understand and take advantage of the correlation between the three granularity levels of phoneme, word, and utterance. To address this issue, we propose a novel method, Granularity-decoupled Transformer (Gradformer), which is able to model the relationships between multiple granularity levels. Specifically, we first use a convolution-augmented transformer encoder to encode acoustic features, where the convolution module helps the model better capture local information. The model outputs both phoneme- and word-level granularity scores with high correlation by the encoder. Then, we use utterance queries to interact with the output of the encoder through the transformer decoder, ultimately obtaining the utterance scores. Through unique encoder and decoder architecture, we achieve decoupling at three granularity levels, and handling the relationship between each granularity. Experiments on the speachocean762 dataset show that our model has advantages over state-of-the-art methods in various metrics, especially in key metrics such as phoneme accuracy, word accuracy, and total score.",
      "doi": "https://doi.org/10.1109/taslp.2023.3335807",
      "openalex_id": "https://openalex.org/W4388936667",
      "arxiv_id": "",
      "publication_date": "2023-11-23",
      "published": "2023-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Preserving Phonemic Distinctions For Ordinal Regression: A Novel Loss Function For Automatic Pronunciation Assessment",
      "summary": "Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encourages feature representations of different phoneme categories to be far apart while simultaneously pulling closer the representations belonging to the same phoneme category by means of weighted distances. An extensive set of experiments carried out on the speechocean 762 benchmark dataset demonstrate the feasibility and effectiveness of our model in relation to some existing state-of-the-art models.",
      "abstract": "Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encourages feature representations of different phoneme categories to be far apart while simultaneously pulling closer the representations belonging to the same phoneme category by means of weighted distances. An extensive set of experiments carried out on the speechocean 762 benchmark dataset demonstrate the feasibility and effectiveness of our model in relation to some existing state-of-the-art models.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389777",
      "openalex_id": "https://openalex.org/W4391021541",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
      "summary": "Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l=20 using two SER benchmark datasets: IEMOCAP and MSP-Improv.",
      "abstract": "Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l=20 using two SER benchmark datasets: IEMOCAP and MSP-Improv.",
      "doi": "https://doi.org/10.21437/interspeech.2022-141",
      "openalex_id": "https://openalex.org/W4221161839",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scaling Laws for Acoustic Models",
      "summary": "There is a recent trend in machine learning to increase model quality by growing models to sizes previously thought to be unreasonable. Recent work has shown that autoregressive generative models with cross-entropy objective functions exhibit smooth power-law relationships, or scaling laws, that predict model quality from model size, training set size, and the available compute budget. These scaling laws allow one to choose nearly optimal hyper-parameters given constraints on available training data, model parameter count, or training computation budget. In this paper, we demonstrate that acoustic models trained with an auto-predictive coding loss behave as if they are subject to similar scaling laws. We extend previous work to jointly predict loss due to model size, to training set size, and to the inherent \"irreducible loss\" of the task. We find that the scaling laws accurately match model performance over two orders of magnitude in both model size and training set size, and make predictions about the limits of model performance.",
      "abstract": "There is a recent trend in machine learning to increase model quality by growing models to sizes previously thought to be unreasonable. Recent work has shown that autoregressive generative models with cross-entropy objective functions exhibit smooth power-law relationships, or scaling laws, that predict model quality from model size, training set size, and the available compute budget. These scaling laws allow one to choose nearly optimal hyper-parameters given constraints on available training data, model parameter count, or training computation budget. In this paper, we demonstrate that acoustic models trained with an auto-predictive coding loss behave as if they are subject to similar scaling laws. We extend previous work to jointly predict loss due to model size, to training set size, and to the inherent \"irreducible loss\" of the task. We find that the scaling laws accurately match model performance over two orders of magnitude in both model size and training set size, and make predictions about the limits of model performance.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1644",
      "openalex_id": "https://openalex.org/W3166129584",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis",
      "summary": "Embedding paralinguistic properties is a challenging task as there are only a few hours of training data available for domains such as emotional speech. One solution to this problem is to pretrain a general self-supervised speech representation model on large amounts of unlabeled speech. This pretrained model is then finetuned to a specific task. Paralinguistic properties however have notoriously high class variance, making the finetuning ineffective. In this work, we propose a two step approach to this. First we improve the embedding space, then we train an adapter to bridge the gap from the embedding space to a classification task. In order to improve the class invariance we use a combination of contrastive and non-contrastive losses to explicitly optimize for class invariant, yet discriminative features. Our approach consistently outperforms baselines that are finetuned end-to-end on multiple tasks and surpasses a benchmark on state-of-the-art emotion classification.",
      "abstract": "Embedding paralinguistic properties is a challenging task as there are only a few hours of training data available for domains such as emotional speech. One solution to this problem is to pretrain a general self-supervised speech representation model on large amounts of unlabeled speech. This pretrained model is then finetuned to a specific task. Paralinguistic properties however have notoriously high class variance, making the finetuning ineffective. In this work, we propose a two step approach to this. First we improve the embedding space, then we train an adapter to bridge the gap from the embedding space to a classification task. In order to improve the class invariance we use a combination of contrastive and non-contrastive losses to explicitly optimize for class invariant, yet discriminative features. Our approach consistently outperforms baselines that are finetuned end-to-end on multiple tasks and surpasses a benchmark on state-of-the-art emotion classification.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022897",
      "openalex_id": "https://openalex.org/W4319862658",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "XLST: Cross-lingual Self-training to Learn Multilingual Representation for Low Resource Speech Recognition",
      "summary": "In this paper, we propose a weakly supervised multilingual representation learning framework, called cross-lingual self-training (XLST). XLST is able to utilize a small amount of annotated data from high-resource languages to improve the representation learning on multilingual un-annotated data. Specifically, XLST uses a supervised trained model to produce initial representations and another model to learn from them, by maximizing the similarity between output embeddings of these two models. Furthermore, the moving average mechanism and multi-view data augmentation are employed, which are experimentally shown to be crucial to XLST. Comprehensive experiments have been conducted on the CommonVoice corpus to evaluate the effectiveness of XLST. Results on 5 downstream low-resource ASR tasks shows that our multilingual pretrained model achieves relatively 18.6% PER reduction over the state-of-the-art self-supervised method, with leveraging additional 100 hours of annotated English data.",
      "abstract": "In this paper, we propose a weakly supervised multilingual representation learning framework, called cross-lingual self-training (XLST). XLST is able to utilize a small amount of annotated data from high-resource languages to improve the representation learning on multilingual un-annotated data. Specifically, XLST uses a supervised trained model to produce initial representations and another model to learn from them, by maximizing the similarity between output embeddings of these two models. Furthermore, the moving average mechanism and multi-view data augmentation are employed, which are experimentally shown to be crucial to XLST. Comprehensive experiments have been conducted on the CommonVoice corpus to evaluate the effectiveness of XLST. Results on 5 downstream low-resource ASR tasks shows that our multilingual pretrained model achieves relatively 18.6% PER reduction over the state-of-the-art self-supervised method, with leveraging additional 100 hours of annotated English data.",
      "doi": "https://doi.org/10.48550/arxiv.2103.08207",
      "openalex_id": "https://openalex.org/W3137720654",
      "arxiv_id": "",
      "publication_date": "2021-03-15",
      "published": "2021-03-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Toxicity Analysis: A New Spoken Language Processing Task.",
      "summary": "Toxic speech, also known as hate speech, is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text with no existing work on toxicity detection from spoken utterances. In this paper, we propose a new Spoken Language Processing task of detecting toxicity from spoken speech. We introduce DeToxy, the first publicly available toxicity annotated dataset for English speech, sourced from various openly available speech databases, consisting of over 2 million utterances. Finally, we also provide analysis on how a spoken speech corpus annotated for toxicity can help facilitate the development of E2E models which better capture various prosodic cues in speech, thereby boosting toxicity classification on spoken utterances.",
      "abstract": "Toxic speech, also known as hate speech, is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text with no existing work on toxicity detection from spoken utterances. In this paper, we propose a new Spoken Language Processing task of detecting toxicity from spoken speech. We introduce DeToxy, the first publicly available toxicity annotated dataset for English speech, sourced from various openly available speech databases, consisting of over 2 million utterances. Finally, we also provide analysis on how a spoken speech corpus annotated for toxicity can help facilitate the development of E2E models which better capture various prosodic cues in speech, thereby boosting toxicity classification on spoken utterances.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3207431783",
      "arxiv_id": "",
      "publication_date": "2021-10-14",
      "published": "2021-10-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Recognition System Based on Mel Frequency Cepstral Coefficient and Four Features",
      "summary": "Biometrics signs are the most important factor in the human recognition field and considered an effective technique for person authentication systems. Voice recognition is a popular method to use due to its ease of implementation and acceptable effectiveness. This research paper will introduce a speaker recognition system that consists of preprocessing techniques to eliminate noise and make the sound smoother. For the feature extraction stage, the method Mel Frequency Cepstral Coefficient (MFCC) is used, and in the second step, the four features (FF) Mean, Standard Division, Zero-Cross and Amplitude, which added to (MFCC) to improve the results. For data representation, vector quantization has been used. The evaluation method (k-fold cross-validation) has been used. Supervised machine learning (SML) is proposed using Quadratic Discriminant Analysis (QDA) classification algorithms. And the results obtained by the algorithm (QDA) varied between 98 percent and 98.43 percent, depending on the way of features extraction that was used. These results are satisfactory and reliable. Index Terms— SML, QDA, Voice Recognition, MFCC, FF.",
      "abstract": "Biometrics signs are the most important factor in the human recognition field and considered an effective technique for person authentication systems. Voice recognition is a popular method to use due to its ease of implementation and acceptable effectiveness. This research paper will introduce a speaker recognition system that consists of preprocessing techniques to eliminate noise and make the sound smoother. For the feature extraction stage, the method Mel Frequency Cepstral Coefficient (MFCC) is used, and in the second step, the four features (FF) Mean, Standard Division, Zero-Cross and Amplitude, which added to (MFCC) to improve the results. For data representation, vector quantization has been used. The evaluation method (k-fold cross-validation) has been used. Supervised machine learning (SML) is proposed using Quadratic Discriminant Analysis (QDA) classification algorithms. And the results obtained by the algorithm (QDA) varied between 98 percent and 98.43 percent, depending on the way of features extraction that was used. These results are satisfactory and reliable. Index Terms— SML, QDA, Voice Recognition, MFCC, FF.",
      "doi": "https://doi.org/10.33103/uot.ijccce.21.4.8",
      "openalex_id": "https://openalex.org/W4313060917",
      "arxiv_id": "",
      "publication_date": "2021-12-30",
      "published": "2021-12-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EDGE: Editable Dance Generation From Music",
      "summary": "Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.",
      "abstract": "Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.00051",
      "openalex_id": "https://openalex.org/W4386065807",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "S3T: Self-Supervised Pre-Training with Swin Transformer For Music Classification",
      "summary": "In this paper, we propose S3T, a self-supervised pre-training method with Swin Transformer for music classification, aiming to learn meaningful music representations from massive easily accessible unlabeled music data. S3T introduces a momentum-based paradigm, MoCo, with Swin Transformer as its feature extractor to music time-frequency domain. For better music representations learning, S3T contributes a music data augmentation pipeline and two specially designed pre-processors. To our knowledge, S3T is the first method combining the Swin Transformer with a self-supervised learning method for music classification. We evaluate S3T on music genre classification and music tagging tasks with linear classifiers trained on learned representations. Experimental results show that S3T outperforms the previous self-supervised method (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two tasks respectively, and also surpasses the task-specific state-of-the-art supervised methods. Besides, S3T shows advances in label efficiency using only 10% labeled data exceeding CLMR on both tasks with 100% labeled data.",
      "abstract": "In this paper, we propose S3T, a self-supervised pre-training method with Swin Transformer for music classification, aiming to learn meaningful music representations from massive easily accessible unlabeled music data. S3T introduces a momentum-based paradigm, MoCo, with Swin Transformer as its feature extractor to music time-frequency domain. For better music representations learning, S3T contributes a music data augmentation pipeline and two specially designed pre-processors. To our knowledge, S3T is the first method combining the Swin Transformer with a self-supervised learning method for music classification. We evaluate S3T on music genre classification and music tagging tasks with linear classifiers trained on learned representations. Experimental results show that S3T outperforms the previous self-supervised method (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two tasks respectively, and also surpasses the task-specific state-of-the-art supervised methods. Besides, S3T shows advances in label efficiency using only 10% labeled data exceeding CLMR on both tasks with 100% labeled data.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746056",
      "openalex_id": "https://openalex.org/W4221161255",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning",
      "summary": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.",
      "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447027",
      "openalex_id": "https://openalex.org/W4392909390",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Review of the opportunities and challenges to accelerate mass‐scale application of smart grids with large‐language models",
      "summary": "Abstract Smart grids represent a paradigm shift in the electricity industry, moving from traditional one‐way systems to more dynamic, interconnected networks. These grids are characterised by their intelligent automation, robust structure, and enhanced interaction with customers, backed by comprehensive monitoring and data analytics. The key of this transformation is the integration of data‐driven methods into smart grids. Compared to previous big data solutions, large language models (LLMs), with their advanced generalisation abilities and multi‐modal competencies, are crucial in effectively managing and integrating diverse data sources. They address challenges such as data inconsistency, inadequate quality, and heterogeneity, thereby enhancing the operational efficiency and reliability of smart grids. Furthermore, at the system level, LLMs improve human–system interactions, making smart grids more user‐friendly and intuitive. Last but not the least, the structure of LLMs performs inherent advantages in bolstering system security and privacy, alongside in resolving issues related to system compatibility and integration. The paper reviews the data‐empowered smart grids and for the first time finds and proposes opportunities and future directions for adopting LLMs to accelerate the mass‐scale application of Smart Grids.",
      "abstract": "Abstract Smart grids represent a paradigm shift in the electricity industry, moving from traditional one‐way systems to more dynamic, interconnected networks. These grids are characterised by their intelligent automation, robust structure, and enhanced interaction with customers, backed by comprehensive monitoring and data analytics. The key of this transformation is the integration of data‐driven methods into smart grids. Compared to previous big data solutions, large language models (LLMs), with their advanced generalisation abilities and multi‐modal competencies, are crucial in effectively managing and integrating diverse data sources. They address challenges such as data inconsistency, inadequate quality, and heterogeneity, thereby enhancing the operational efficiency and reliability of smart grids. Furthermore, at the system level, LLMs improve human–system interactions, making smart grids more user‐friendly and intuitive. Last but not the least, the structure of LLMs performs inherent advantages in bolstering system security and privacy, alongside in resolving issues related to system compatibility and integration. The paper reviews the data‐empowered smart grids and for the first time finds and proposes opportunities and future directions for adopting LLMs to accelerate the mass‐scale application of Smart Grids.",
      "doi": "https://doi.org/10.1049/stg2.12191",
      "openalex_id": "https://openalex.org/W4404111691",
      "arxiv_id": "",
      "publication_date": "2024-11-06",
      "published": "2024-11-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Toward Universal Text-To-Music Retrieval",
      "summary": "This paper introduces effective design choices for text-to-music retrieval systems. An ideal text-based retrieval system would support various input queries such as pre-defined tags, unseen tags, and sentence-level descriptions. In reality, most previous works mainly focused on a single query type (tag or sentence) which may not generalize to another input type. Hence, we review recent text-based music retrieval systems using our proposed benchmark in two main aspects: input text representation and training objectives. Our findings enable a universal text-to-music retrieval system that achieves comparable retrieval performances in both tag- and sentence-level inputs. Furthermore, the proposed multimodal representation generalizes to 9 different downstream music classification tasks. We present the code and demo online. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "This paper introduces effective design choices for text-to-music retrieval systems. An ideal text-based retrieval system would support various input queries such as pre-defined tags, unseen tags, and sentence-level descriptions. In reality, most previous works mainly focused on a single query type (tag or sentence) which may not generalize to another input type. Hence, we review recent text-based music retrieval systems using our proposed benchmark in two main aspects: input text representation and training objectives. Our findings enable a universal text-to-music retrieval system that achieves comparable retrieval performances in both tag- and sentence-level inputs. Furthermore, the proposed multimodal representation generalizes to 9 different downstream music classification tasks. We present the code and demo online. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094670",
      "openalex_id": "https://openalex.org/W4372259760",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming",
      "summary": "Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained speech models for low-resource music classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained models for cross-modal adaptation. In addition to the known, input-independent, re-programming method, we propose an new reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outperform fine-tuning-based TL methods on a small genre classification dataset.",
      "abstract": "Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained speech models for low-resource music classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained models for cross-modal adaptation. In addition to the known, input-independent, re-programming method, we propose an new reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outperform fine-tuning-based TL methods on a small genre classification dataset.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096568",
      "openalex_id": "https://openalex.org/W4372260195",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Music Sequence Representation From Text Supervision",
      "summary": "Music representation learning is notoriously difficult for its complex human-related concepts contained in the sequence of numerical signals. To excavate better MUsic SEquence Representation from labeled audio, we propose a novel text-supervision pre-training method, namely MUSER. MUSER adopts an audio-spectrum-text tri-modal contrastive learning framework, where the text input could be any form of meta-data with the help of text templates while the spectrum is derived from an audio sequence. Our experiments reveal that MUSER could be more flexibly adapted to downstream tasks compared with the current data-hungry pre-training method, and it only requires 0.056% of pre-training data to achieve the state-of-the-art performance.",
      "abstract": "Music representation learning is notoriously difficult for its complex human-related concepts contained in the sequence of numerical signals. To excavate better MUsic SEquence Representation from labeled audio, we propose a novel text-supervision pre-training method, namely MUSER. MUSER adopts an audio-spectrum-text tri-modal contrastive learning framework, where the text input could be any form of meta-data with the help of text templates while the spectrum is derived from an audio sequence. Our experiments reveal that MUSER could be more flexibly adapted to downstream tasks compared with the current data-hungry pre-training method, and it only requires 0.056% of pre-training data to achieve the state-of-the-art performance.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746131",
      "openalex_id": "https://openalex.org/W4225328971",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the encoding of natural music in computational models and human brains",
      "summary": "This article discusses recent developments and advances in the neuroscience of music to understand the nature of musical emotion. In particular, it highlights how system identification techniques and computational models of music have advanced our understanding of how the human brain processes the textures and structures of music and how the processed information evokes emotions. Musical models relate physical properties of stimuli to internal representations called features, and predictive models relate features to neural or behavioral responses and test their predictions against independent unseen data. The new frameworks do not require orthogonalized stimuli in controlled experiments to establish reproducible knowledge, which has opened up a new wave of naturalistic neuroscience. The current review focuses on how this trend has transformed the domain of the neuroscience of music.",
      "abstract": "This article discusses recent developments and advances in the neuroscience of music to understand the nature of musical emotion. In particular, it highlights how system identification techniques and computational models of music have advanced our understanding of how the human brain processes the textures and structures of music and how the processed information evokes emotions. Musical models relate physical properties of stimuli to internal representations called features, and predictive models relate features to neural or behavioral responses and test their predictions against independent unseen data. The new frameworks do not require orthogonalized stimuli in controlled experiments to establish reproducible knowledge, which has opened up a new wave of naturalistic neuroscience. The current review focuses on how this trend has transformed the domain of the neuroscience of music.",
      "doi": "https://doi.org/10.3389/fnins.2022.928841",
      "openalex_id": "https://openalex.org/W4297906698",
      "arxiv_id": "",
      "publication_date": "2022-09-20",
      "published": "2022-09-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Source Separation By Steering Pretrained Music Models",
      "summary": "We showcase a method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "abstract": "We showcase a method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747909",
      "openalex_id": "https://openalex.org/W4225326921",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pre-Training Strategies Using Contrastive Learning and Playlist Information for Music Classification and Similarity",
      "summary": "In this work, we investigate an approach that relies on contrastive learning and music metadata as a weak source of supervision to train music representation models. Recent studies show that contrastive learning can be used with editorial metadata (e.g., artist or album name) to learn audio representations that are useful for different classification tasks. In this paper, we extend this idea to using playlist data as a source of music similarity information and investigate three approaches to generate anchor and positive track pairs. We evaluate these approaches by fine-tuning the pre-trained models for music multi-label classification tasks (genre, mood, and instrument tagging) and music similarity. We find that creating anchor and positive track pairs by relying on co-occurrences in playlists provides better music similarity and competitive classification results compared to choosing tracks from the same artist as in previous works. Additionally, our best pre-training approach based on playlists provides superior classification performance for most datasets.",
      "abstract": "In this work, we investigate an approach that relies on contrastive learning and music metadata as a weak source of supervision to train music representation models. Recent studies show that contrastive learning can be used with editorial metadata (e.g., artist or album name) to learn audio representations that are useful for different classification tasks. In this paper, we extend this idea to using playlist data as a source of music similarity information and investigate three approaches to generate anchor and positive track pairs. We evaluate these approaches by fine-tuning the pre-trained models for music multi-label classification tasks (genre, mood, and instrument tagging) and music similarity. We find that creating anchor and positive track pairs by relying on co-occurrences in playlists provides better music similarity and competitive classification results compared to choosing tracks from the same artist as in previous works. Additionally, our best pre-training approach based on playlists provides superior classification performance for most datasets.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095058",
      "openalex_id": "https://openalex.org/W4372259862",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Feature-informed Embedding Space Regularization For Audio Classification",
      "summary": "Feature representations derived from models pre-trained on large-scale datasets have shown their generalizability on a variety of audio analysis tasks. Despite this generalizability, however, task-specific features can outperform if sufficient training data is available, as specific task-relevant properties can be learned. Furthermore, the complex pre-trained models bring considerable computational burdens during inference. We propose to leverage both detailed task-specific features from spectrogram input and generic pre-trained features by introducing two regularization methods that integrate the information of both feature classes. The workload is kept low during inference as the pre-trained features are only necessary for training. In experiments with the pre-trained features VGGish, OpenL3, and a combination of both, we show that the proposed methods not only outperform baseline methods, but also can improve state-of-the-art models on several audio classification tasks. The results also suggest that using the mixture of features performs better than using individual features.",
      "abstract": "Feature representations derived from models pre-trained on large-scale datasets have shown their generalizability on a variety of audio analysis tasks. Despite this generalizability, however, task-specific features can outperform if sufficient training data is available, as specific task-relevant properties can be learned. Furthermore, the complex pre-trained models bring considerable computational burdens during inference. We propose to leverage both detailed task-specific features from spectrogram input and generic pre-trained features by introducing two regularization methods that integrate the information of both feature classes. The workload is kept low during inference as the pre-trained features are only necessary for training. In experiments with the pre-trained features VGGish, OpenL3, and a combination of both, we show that the proposed methods not only outperform baseline methods, but also can improve state-of-the-art models on several audio classification tasks. The results also suggest that using the mixture of features performs better than using individual features.",
      "doi": "https://doi.org/10.23919/eusipco55093.2022.9909904",
      "openalex_id": "https://openalex.org/W4312592048",
      "arxiv_id": "",
      "publication_date": "2022-08-29",
      "published": "2022-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Auto-tagging system based on song’s latent representations for inferring contextual user information",
      "summary": "Currently in the field of Recommender Systems for the music domain, there is active research about approaches for inferring the user context. Moreover, in the Music Information Retrieval there have been great advances in the generation of latent representations of songs including approaches such as contrastive learning as pretrain strategy or other approaches related to Natural Language Modeling like codified audio language modeling (CALM). Such advances are especially useful for Music Information Retrieval discriminative tasks such as genre classification, key detection, emotion recognition and music tagging. This last task attracts the interest of music streaming services that seek to tag their catalogs, especially with tags related to the user's context as this has a great impact on their tastes and influences the developed recommender systems. These tags are usually provided by users on social networks and are frequently found only for popular songs in the catalog. However, recently added songs to the catalog or songs belonging to the long tail do not have these tags and the need to create new systems called auto-taggers capable of tagging these songs arises. This paper proposes an auto-tagging system and presents an evaluation of different multi-label classification approaches included in it for contextual label auto-tagging. These approaches use different latent representations of songs, employing a recent published dataset with user context tags. The results obtained from the case study conducted to evaluate the proposed system show a clear improvement in the classification metrics by using new latent representations compared to the use of simpler features in traditional state-of-the-art approaches.",
      "abstract": "Currently in the field of Recommender Systems for the music domain, there is active research about approaches for inferring the user context. Moreover, in the Music Information Retrieval there have been great advances in the generation of latent representations of songs including approaches such as contrastive learning as pretrain strategy or other approaches related to Natural Language Modeling like codified audio language modeling (CALM). Such advances are especially useful for Music Information Retrieval discriminative tasks such as genre classification, key detection, emotion recognition and music tagging. This last task attracts the interest of music streaming services that seek to tag their catalogs, especially with tags related to the user's context as this has a great impact on their tastes and influences the developed recommender systems. These tags are usually provided by users on social networks and are frequently found only for popular songs in the catalog. However, recently added songs to the catalog or songs belonging to the long tail do not have these tags and the need to create new systems called auto-taggers capable of tagging these songs arises. This paper proposes an auto-tagging system and presents an evaluation of different multi-label classification approaches included in it for contextual label auto-tagging. These approaches use different latent representations of songs, employing a recent published dataset with user context tags. The results obtained from the case study conducted to evaluate the proposed system show a clear improvement in the classification metrics by using new latent representations compared to the use of simpler features in traditional state-of-the-art approaches.",
      "doi": "https://doi.org/10.1109/wi-iat55865.2022.00040",
      "openalex_id": "https://openalex.org/W4366967398",
      "arxiv_id": "",
      "publication_date": "2022-11-01",
      "published": "2022-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Source Separation By Steering Pretrained Music Models",
      "summary": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "abstract": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "doi": "https://doi.org/10.48550/arxiv.2110.13071",
      "openalex_id": "https://openalex.org/W3209109096",
      "arxiv_id": "",
      "publication_date": "2021-10-25",
      "published": "2021-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Nuanced Music Emotion Recognition via a Semi-Supervised Multi-Relational Graph Neural Network",
      "summary": "Music emotion recognition (MER) seeks to understand the complex emotional landscapes elicited by music, acknowledging music’s profound social and psychological roles beyond traditional tasks such as genre classification or content similarity. MER relies heavily on high‑quality emotional annotations, which serve as the foundation for training models to recognize emotions. However, collecting these annotations is both complex and costly, leading to limited availability of large‑scale datasets for MER. Recent efforts in MER for automatically extracting emotion have focused on learning track representations in a supervised manner. However, these approaches mainly use simplified emotion models due to limited datasets or a lack of necessity for sophisticated emotion models and ignore hidden inter‑track relations, which are beneficial in a semi‑supervised learning setting. This paper proposes a novel approach to MER by constructing a multi‑relational graph that encapsulates different facets of music. We leverage graph neural networks to model intricate inter‑track relationships and capture structurally induced representations from user data, such as listening histories, genres, and tags. Our model, the semi‑supervised multi‑relational graph neural network for emotion recognition (SRGNN‑Emo), innovates by combining graph‑based modeling with semi‑supervised learning, using rich user data to extract nuanced emotional profiles from music tracks. Through extensive experimentation, SRGNN‑Emo demonstrates significant improvements in R2 and root mean squared error metrics for predicting the intensity of nine continuous emotions (Geneva Emotional Music Scale), demonstrating its superior capability in capturing and predicting complex emotional expressions in music.",
      "abstract": "Music emotion recognition (MER) seeks to understand the complex emotional landscapes elicited by music, acknowledging music’s profound social and psychological roles beyond traditional tasks such as genre classification or content similarity. MER relies heavily on high‑quality emotional annotations, which serve as the foundation for training models to recognize emotions. However, collecting these annotations is both complex and costly, leading to limited availability of large‑scale datasets for MER. Recent efforts in MER for automatically extracting emotion have focused on learning track representations in a supervised manner. However, these approaches mainly use simplified emotion models due to limited datasets or a lack of necessity for sophisticated emotion models and ignore hidden inter‑track relations, which are beneficial in a semi‑supervised learning setting. This paper proposes a novel approach to MER by constructing a multi‑relational graph that encapsulates different facets of music. We leverage graph neural networks to model intricate inter‑track relationships and capture structurally induced representations from user data, such as listening histories, genres, and tags. Our model, the semi‑supervised multi‑relational graph neural network for emotion recognition (SRGNN‑Emo), innovates by combining graph‑based modeling with semi‑supervised learning, using rich user data to extract nuanced emotional profiles from music tracks. Through extensive experimentation, SRGNN‑Emo demonstrates significant improvements in R2 and root mean squared error metrics for predicting the intensity of nine continuous emotions (Geneva Emotional Music Scale), demonstrating its superior capability in capturing and predicting complex emotional expressions in music.",
      "doi": "https://doi.org/10.5334/tismir.235",
      "openalex_id": "https://openalex.org/W4411216760",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Typing on Any Surface: Real-Time Keystroke Detection in Augmented Reality",
      "summary": "The ineffectiveness of text entry interfaces remains a significant barrier to social engagement in augmented reality (AR). Popular options, such as mid-air keyboard interface, wireless keyboards or voice input face challenges such as ergonomic issues, limited accuracy, or social discomfort in public use. This paper introduces a deep-learning method allowing AR applications to predict keystrokes from the user perspective video stream captured by any AR headset. This enables users to type on flat surfaces without a physical or virtual keyboard. Our two-stage model combines an off-the-shelf hand landmark extractor with an innovative adaptive Convolutional Recurrent Neural Network (C-RNN). It was trained on a newly built dataset, enabling prediction of 27 keys (alphabet and space) at approximately 32 FPS. With practice, users can reach a 91.0% accuracy at 40 words per minute, comparable to typing on a physical keyboard. The encouraging results demonstrate our method's feasibility and potential for integration in diverse applications. We also explore limitations and future research directions for production system implementation.",
      "abstract": "The ineffectiveness of text entry interfaces remains a significant barrier to social engagement in augmented reality (AR). Popular options, such as mid-air keyboard interface, wireless keyboards or voice input face challenges such as ergonomic issues, limited accuracy, or social discomfort in public use. This paper introduces a deep-learning method allowing AR applications to predict keystrokes from the user perspective video stream captured by any AR headset. This enables users to type on flat surfaces without a physical or virtual keyboard. Our two-stage model combines an off-the-shelf hand landmark extractor with an innovative adaptive Convolutional Recurrent Neural Network (C-RNN). It was trained on a newly built dataset, enabling prediction of 27 keys (alphabet and space) at approximately 32 FPS. With practice, users can reach a 91.0% accuracy at 40 words per minute, comparable to typing on a physical keyboard. The encouraging results demonstrate our method's feasibility and potential for integration in diverse applications. We also explore limitations and future research directions for production system implementation.",
      "doi": "https://doi.org/10.1109/aixvr59861.2024.00060",
      "openalex_id": "https://openalex.org/W4392248135",
      "arxiv_id": "",
      "publication_date": "2024-01-17",
      "published": "2024-01-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Binaural Speech Synthesis",
      "summary": "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb.The network is a modified vectorquantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss.We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study.Results show that the proposed approach matches the ground truth data more closely than previous methods.In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene.",
      "abstract": "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb.The network is a modified vectorquantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss.We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study.Results show that the proposed approach matches the ground truth data more closely than previous methods.In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10603",
      "openalex_id": "https://openalex.org/W4285044837",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Neural Network (DNN) Audio Coder Using A Perceptually Improved Training Method",
      "summary": "A new end-to-end audio coder based on a deep neural network (DNN) is proposed. To compensate for the perceptual distortion that occurred by quantization, the proposed coder is optimized to minimize distortions in both signal and perceptual domains. The distortion in the perceptual domain is measured using the psychoacoustic model (PAM), and a loss function is obtained through the two-stage compensation approach. Also, the scalar uniform quantization was approximated using a uniform stochastic noise, together with a compression-decompression scheme, which provides simpler but more stable learning without an additional penalty than the softmax quantizer. Test results showed that the proposed coder achieves more accurate noise-masking than the previous PAM-based method and better perceptual quality then the MP3 audio coder.",
      "abstract": "A new end-to-end audio coder based on a deep neural network (DNN) is proposed. To compensate for the perceptual distortion that occurred by quantization, the proposed coder is optimized to minimize distortions in both signal and perceptual domains. The distortion in the perceptual domain is measured using the psychoacoustic model (PAM), and a loss function is obtained through the two-stage compensation approach. Also, the scalar uniform quantization was approximated using a uniform stochastic noise, together with a compression-decompression scheme, which provides simpler but more stable learning without an additional penalty than the softmax quantizer. Test results showed that the proposed coder achieves more accurate noise-masking than the previous PAM-based method and better perceptual quality then the MP3 audio coder.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747575",
      "openalex_id": "https://openalex.org/W4224919007",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Perceptual Neural Audio Coder with a Mean-Scale Hyperprior",
      "summary": "This paper proposes an end-to-end neural audio coder based on a mean-scale hyperprior model together with a perceptual optimization using a psychoacoustic model (PAM)-based loss function. The proposed coder estimates the mean and scale hyperpriors using a sub-network after assuming that the probability distribution of latent samples is Gaussian. The main network is an autoencoder based on Resnet-type gated linear units (ResGLUs), each comprising a generalized divisive normalization (GDN) layer. We train both networks to optimize perceptual attributes estimated using a multi-timescale scheme to obtain high perceptual quality. Experimental results show that the proposed model accurately predicts the mean and scale hyperpriors. Also, it obtains consistently higher audio quality than the commercial MP3 audio coder at all bitrates.",
      "abstract": "This paper proposes an end-to-end neural audio coder based on a mean-scale hyperprior model together with a perceptual optimization using a psychoacoustic model (PAM)-based loss function. The proposed coder estimates the mean and scale hyperpriors using a sub-network after assuming that the probability distribution of latent samples is Gaussian. The main network is an autoencoder based on Resnet-type gated linear units (ResGLUs), each comprising a generalized divisive normalization (GDN) layer. We train both networks to optimize perceptual attributes estimated using a multi-timescale scheme to obtain high perceptual quality. Experimental results show that the proposed model accurately predicts the mean and scale hyperpriors. Also, it obtains consistently higher audio quality than the commercial MP3 audio coder at all bitrates.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096009",
      "openalex_id": "https://openalex.org/W4372267191",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mitigating Information Interruptions by COVID-19 Face Masks: A Three-Stage Speech Enhancement Scheme",
      "summary": "The coronavirus disease 2019 (COVID-19) preventive measures have resulted in significant lifestyle changes. One of the COVID-19 new normal is the usage of face masks for protection against airborne aerosol which creates distractions and interruptions in voice communication. It has a different influence on speech than the standard concept of noise affecting speech communication. Furthermore, it has varied effects on speech in different frequency bands. To provide a solution to this problem, a three-stage adaptive speech enhancement (SE) scheme is developed in this article. In the first stage, the tunable <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$Q$</tex-math> </inline-formula> -factor wavelet transform (TQWT) features are extracted by properly setting the quality factor values and the number of levels from the input speech signal. In the second stage, the adjustable parameters of the preemphasis filter and modified multiband spectral subtraction (MBSS) are determined using bio-inspired techniques for different masking and signal-to-noise ratio (SNR) conditions. In the third stage, the weights, center values, standard deviation of the Gaussian radial basis functions, and input patterns of the radial basis function neural networks (RBFNNs) are updated to predict the optimized parameters from the input TQWT-based cepstral features (TQCFs). In the end, the performance of the proposed algorithm is compared with the standard SE algorithms using two speech datasets.",
      "abstract": "The coronavirus disease 2019 (COVID-19) preventive measures have resulted in significant lifestyle changes. One of the COVID-19 new normal is the usage of face masks for protection against airborne aerosol which creates distractions and interruptions in voice communication. It has a different influence on speech than the standard concept of noise affecting speech communication. Furthermore, it has varied effects on speech in different frequency bands. To provide a solution to this problem, a three-stage adaptive speech enhancement (SE) scheme is developed in this article. In the first stage, the tunable <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$Q$</tex-math> </inline-formula> -factor wavelet transform (TQWT) features are extracted by properly setting the quality factor values and the number of levels from the input speech signal. In the second stage, the adjustable parameters of the preemphasis filter and modified multiband spectral subtraction (MBSS) are determined using bio-inspired techniques for different masking and signal-to-noise ratio (SNR) conditions. In the third stage, the weights, center values, standard deviation of the Gaussian radial basis functions, and input patterns of the radial basis function neural networks (RBFNNs) are updated to predict the optimized parameters from the input TQWT-based cepstral features (TQCFs). In the end, the performance of the proposed algorithm is compared with the standard SE algorithms using two speech datasets.",
      "doi": "https://doi.org/10.1109/tcss.2022.3210988",
      "openalex_id": "https://openalex.org/W4312812805",
      "arxiv_id": "",
      "publication_date": "2022-10-14",
      "published": "2022-10-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Coding Using Discrete Cosine Transform and Chaotic Map",
      "summary": "Recently, data of multimedia performs an exponentially blowing tendency, saturating daily life of humans. Various modalities of data, includes images, texts and video, plays important role in different aspects and has wide. However, the key problem of utilizing data of large scale is cost of processing and massive storage. Therefore, for efficient communications and for economical storage requires effective techniques of data compression to reduce the volume of data. Speech coding is a main problem in the area of digital speech processing. The process of converting the voice signals into a more compressed form is speech coding. In this work, we demonstrate that a DCT with a chaotic system combined with run-length coding can be utilized to implement speech coding of very low bit-rate with high reconstruction quality. Experimental result show that compression ratio is about 13% when implemented on Librispeech dataset.",
      "abstract": "Recently, data of multimedia performs an exponentially blowing tendency, saturating daily life of humans. Various modalities of data, includes images, texts and video, plays important role in different aspects and has wide. However, the key problem of utilizing data of large scale is cost of processing and massive storage. Therefore, for efficient communications and for economical storage requires effective techniques of data compression to reduce the volume of data. Speech coding is a main problem in the area of digital speech processing. The process of converting the voice signals into a more compressed form is speech coding. In this work, we demonstrate that a DCT with a chaotic system combined with run-length coding can be utilized to implement speech coding of very low bit-rate with high reconstruction quality. Experimental result show that compression ratio is about 13% when implemented on Librispeech dataset.",
      "doi": "https://doi.org/10.18280/isi.270419",
      "openalex_id": "https://openalex.org/W4296887698",
      "arxiv_id": "",
      "publication_date": "2022-08-31",
      "published": "2022-08-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Six-Dimensional Hyperchaotic Pseudorandom Sequence for Enhanced Voice Encryption",
      "summary": "Over recent decades, the demand for robust voice encryption algorithms has escalated to fortify the security of speech transmission over vulnerable channels such as the internet.Among the myriad of available methodologies, those underpinned by chaos theory have garnered significant attention due to their inherent pseudorandomness, acute sensitivity to initial conditions, and control parameters.These attributes render them capable of encrypting a variety of data types, encompassing but not limited to videos, images, and audio.This study presents a novel voice encryption approach predicated on a sixdimensional (6D) hyperchaotic system.In the proposed method, six unique keys are generated from the 6D hyperchaotic system.The initial three keys are employed to permute the human voice signal, while the subsequent trio is engaged in the diffusion process.The efficacy of this scheme is evaluated on several parameters: Mean Square Error (MSE), Signal-To-Noise Ratio (SNR), correlation coefficient, Peak Signal-To-Noise Ratio (PSNR), key sensitivity, key space, and entropy analysis.The Libri-Speech dataset serves as the test bench for the proposed system.The key space has been determined to be 2465.The system's performance is notable, with correlation coefficients ranging between -0.00276 and 0.002759, entropy values from 14.74399 to 14.74942, PSNR values from 4.2814 to 4.7875, SNR values from -30.3854 to -9.2364, and a nearly zero MSE range of 0.3321 to 0.3731 between original and extracted signals.This study underscores the potential of the 6D hyperchaotic system in enhancing information security, specifically for voice encryption.The findings may pave the way for more secure communication protocols in an increasingly interconnected digital world.",
      "abstract": "Over recent decades, the demand for robust voice encryption algorithms has escalated to fortify the security of speech transmission over vulnerable channels such as the internet.Among the myriad of available methodologies, those underpinned by chaos theory have garnered significant attention due to their inherent pseudorandomness, acute sensitivity to initial conditions, and control parameters.These attributes render them capable of encrypting a variety of data types, encompassing but not limited to videos, images, and audio.This study presents a novel voice encryption approach predicated on a sixdimensional (6D) hyperchaotic system.In the proposed method, six unique keys are generated from the 6D hyperchaotic system.The initial three keys are employed to permute the human voice signal, while the subsequent trio is engaged in the diffusion process.The efficacy of this scheme is evaluated on several parameters: Mean Square Error (MSE), Signal-To-Noise Ratio (SNR), correlation coefficient, Peak Signal-To-Noise Ratio (PSNR), key sensitivity, key space, and entropy analysis.The Libri-Speech dataset serves as the test bench for the proposed system.The key space has been determined to be 2465.The system's performance is notable, with correlation coefficients ranging between -0.00276 and 0.002759, entropy values from 14.74399 to 14.74942, PSNR values from 4.2814 to 4.7875, SNR values from -30.3854 to -9.2364, and a nearly zero MSE range of 0.3321 to 0.3731 between original and extracted signals.This study underscores the potential of the 6D hyperchaotic system in enhancing information security, specifically for voice encryption.The findings may pave the way for more secure communication protocols in an increasingly interconnected digital world.",
      "doi": "https://doi.org/10.18280/isi.280425",
      "openalex_id": "https://openalex.org/W4386649189",
      "arxiv_id": "",
      "publication_date": "2023-08-31",
      "published": "2023-08-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SoundSpring: Loss-Resilient Audio Transceiver With Dual-Functional Masked Language Modeling",
      "summary": "In this paper, we propose \"SoundSpring\", a cutting-edge error-resilient audio transceiver that marries the robustness benefits of joint source-channel coding (JSCC) while also being compatible with current digital communication systems. Unlike recent deep JSCC transceivers, which learn to directly map audio signals to analog channel-input symbols via neural networks, our SoundSpring adopts the layered architecture that delineates audio compression from digital coded transmission, but it sufficiently exploits the impressive in-context predictive capabilities of large language (foundation) models. Integrated with the casual-order mask learning strategy, our single model operates on the latent feature domain and serve dual-functionalities: as efficient audio compressors at the transmitter and as effective mechanisms for packet loss concealment at the receiver. By jointly optimizing towards both audio compression efficiency and transmission error resiliency, we show that mask-learned language models are indeed powerful contextual predictors, and our dual-functional compression and concealment framework offers fresh perspectives on the application of foundation language models in audio communication. Through extensive experimental evaluations, we establish that SoundSpring apparently outperforms contemporary audio transmission systems in terms of signal fidelity metrics and perceptual quality scores. These new findings not only advocate for the practical deployment of SoundSpring in learning-based audio communication systems but also inspire the development of future audio semantic transceivers.",
      "abstract": "In this paper, we propose \"SoundSpring\", a cutting-edge error-resilient audio transceiver that marries the robustness benefits of joint source-channel coding (JSCC) while also being compatible with current digital communication systems. Unlike recent deep JSCC transceivers, which learn to directly map audio signals to analog channel-input symbols via neural networks, our SoundSpring adopts the layered architecture that delineates audio compression from digital coded transmission, but it sufficiently exploits the impressive in-context predictive capabilities of large language (foundation) models. Integrated with the casual-order mask learning strategy, our single model operates on the latent feature domain and serve dual-functionalities: as efficient audio compressors at the transmitter and as effective mechanisms for packet loss concealment at the receiver. By jointly optimizing towards both audio compression efficiency and transmission error resiliency, we show that mask-learned language models are indeed powerful contextual predictors, and our dual-functional compression and concealment framework offers fresh perspectives on the application of foundation language models in audio communication. Through extensive experimental evaluations, we establish that SoundSpring apparently outperforms contemporary audio transmission systems in terms of signal fidelity metrics and perceptual quality scores. These new findings not only advocate for the practical deployment of SoundSpring in learning-based audio communication systems but also inspire the development of future audio semantic transceivers.",
      "doi": "https://doi.org/10.1109/jsac.2025.3531406",
      "openalex_id": "https://openalex.org/W4406610604",
      "arxiv_id": "",
      "publication_date": "2025-01-20",
      "published": "2025-01-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scalable and Efficient Neural Speech Coding.",
      "summary": "This work presents a scalable and efficient neural waveform codec (NWC) for speech compression. We formulate the speech coding problem as an autoencoding task, where a convolutional neural network (CNN) performs encoding and decoding as its feedforward routine. The proposed CNN autoencoder also defines quantization and entropy coding as a trainable module, so the coding artifacts and bitrate control are handled during the optimization process. We achieve efficiency by introducing compact model architectures to our fully convolutional network model, such as gated residual networks and depthwise separable convolution. Furthermore, the proposed models are with a scalable architecture, cross-module residual learning (CMRL), to cover a wide range of bitrates. To this end, we employ the residual coding concept to concatenate multiple NWC autoencoding modules, where an NWC module performs residual coding to restore any reconstruction loss that its preceding modules have created. CMRL can scale down to cover lower bitrates as well, for which it employs linear predictive coding (LPC) module as its first autoencoder. We redefine LPC's quantization as a trainable module to enhance the bit allocation tradeoff between LPC and its following NWC modules. Compared to the other autoregressive decoder-based neural speech coders, our decoder has significantly smaller architecture, e.g., with only 0.12 million parameters, more than 100 times smaller than a WaveNet decoder. Compared to the LPCNet-based speech codec, which leverages the speech production model to reduce the network complexity in low bitrates, ours can scale up to higher bitrates to achieve transparent performance. Our lightweight neural speech coding model achieves comparable subjective scores against AMR-WB at the low bitrate range and provides transparent coding quality at 32 kbps.",
      "abstract": "This work presents a scalable and efficient neural waveform codec (NWC) for speech compression. We formulate the speech coding problem as an autoencoding task, where a convolutional neural network (CNN) performs encoding and decoding as its feedforward routine. The proposed CNN autoencoder also defines quantization and entropy coding as a trainable module, so the coding artifacts and bitrate control are handled during the optimization process. We achieve efficiency by introducing compact model architectures to our fully convolutional network model, such as gated residual networks and depthwise separable convolution. Furthermore, the proposed models are with a scalable architecture, cross-module residual learning (CMRL), to cover a wide range of bitrates. To this end, we employ the residual coding concept to concatenate multiple NWC autoencoding modules, where an NWC module performs residual coding to restore any reconstruction loss that its preceding modules have created. CMRL can scale down to cover lower bitrates as well, for which it employs linear predictive coding (LPC) module as its first autoencoder. We redefine LPC's quantization as a trainable module to enhance the bit allocation tradeoff between LPC and its following NWC modules. Compared to the other autoregressive decoder-based neural speech coders, our decoder has significantly smaller architecture, e.g., with only 0.12 million parameters, more than 100 times smaller than a WaveNet decoder. Compared to the LPCNet-based speech codec, which leverages the speech production model to reduce the network complexity in low bitrates, ours can scale up to higher bitrates to achieve transparent performance. Our lightweight neural speech coding model achieves comparable subjective scores against AMR-WB at the low bitrate range and provides transparent coding quality at 32 kbps.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3148531355",
      "arxiv_id": "",
      "publication_date": "2021-03-27",
      "published": "2021-03-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GD-Conformer: a Conformer-based gated dense encoder-decoder for monaural speech enhancement",
      "summary": "<title>Abstract</title> Speech enhancement improves speech quality by mitigating noise, dereverberation, and echo. Existing methods struggle with amplitude-phase compensation, capturing temporal-frequency features, and high complexity. To address these issues, a gated dense encoder-decoder architecture with a two-stage Conformer, abbreviated as GD-Conformer, is proposed. It integrates a gated dense module, a two-stage residual Conformer module, a mask decoder and a complex decoder. The gated dense module consists of two parts: a dilated dense convolution and a gated convolution, where the former captures both global and local dependencies features, while the latter refines these distinct features accordingly. The two-stage residual Conformer focuses on the time-frequency dependence of speech, it also reduces the computational complexity. The mask decoder and the complex decoder restore spectral resolution while preserving speech fidelity. The outcomes of experiments conducted on the public dataset VoiceBank+DEMAND and DNS Challenge 2020 demonstrate that, compared with those state-of-the-art methods, the proposed GD-Conformer achieves comparable performance in terms of denoising and generalization with fewer parameters and lower computation complexity.",
      "abstract": "<title>Abstract</title> Speech enhancement improves speech quality by mitigating noise, dereverberation, and echo. Existing methods struggle with amplitude-phase compensation, capturing temporal-frequency features, and high complexity. To address these issues, a gated dense encoder-decoder architecture with a two-stage Conformer, abbreviated as GD-Conformer, is proposed. It integrates a gated dense module, a two-stage residual Conformer module, a mask decoder and a complex decoder. The gated dense module consists of two parts: a dilated dense convolution and a gated convolution, where the former captures both global and local dependencies features, while the latter refines these distinct features accordingly. The two-stage residual Conformer focuses on the time-frequency dependence of speech, it also reduces the computational complexity. The mask decoder and the complex decoder restore spectral resolution while preserving speech fidelity. The outcomes of experiments conducted on the public dataset VoiceBank+DEMAND and DNS Challenge 2020 demonstrate that, compared with those state-of-the-art methods, the proposed GD-Conformer achieves comparable performance in terms of denoising and generalization with fewer parameters and lower computation complexity.",
      "doi": "https://doi.org/10.21203/rs.3.rs-6111294/v1",
      "openalex_id": "https://openalex.org/W4409947375",
      "arxiv_id": "",
      "publication_date": "2025-04-29",
      "published": "2025-04-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech",
      "summary": "Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.",
      "abstract": "Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383526",
      "openalex_id": "https://openalex.org/W3097112431",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Expressive TTS Training with Frame and Style Reconstruction Loss",
      "summary": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system to improve the expressiveness of speech. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. Our proposed idea marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. The proposed training strategy adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The proposed style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms a state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.",
      "abstract": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system to improve the expressiveness of speech. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. Our proposed idea marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. The proposed training strategy adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The proposed style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms a state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.",
      "doi": "https://doi.org/10.48550/arxiv.2008.01490",
      "openalex_id": "https://openalex.org/W3047107405",
      "arxiv_id": "",
      "publication_date": "2020-08-04",
      "published": "2020-08-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Weighted Cross-Modal Attention Mechanism With Sentiment Prediction Auxiliary Task for Multimodal Sentiment Analysis",
      "summary": "Human brain extracts the spatial and temporal semantic information by processing the multi-modalities, which has contextually meaningful for perceiving and understanding the emotional state of an individual. However, there are two main challenges in modeling multimodal sequences: 1) the different sampling rates from multimodal data make the cross-modal interactions very difficult; 2) how to efficiently fuse unimodal representations and effectively capture relationships among multimodal data. In this paper, we design the weighted cross-modal attention mechanism, which not only captures the temporal correlation information and the spatial dependence information of each modality, but also dynamically adjusts the weight of each modality across different time steps. And the unimodal subtasks are led for assisting the representation learning of specific modality to jointly train the multimodal tasks and unimodal subtasks to explore the complementary relationships of each modality. Our model gets a new state-of-the-art record on the CMU-MOSI dataset and brings noticeable performance improvements on all the metrics. For the CMU-MOSEI dataset, the F1 score of the binary classification, the 7-class task, and the regression task of our model are still the highest among all models and the proposed model is only lower than the multimodal split attention fusion (MSAF) model with aligned data on the accuracy of the binary classification, showing the great performance of the suggested method.",
      "abstract": "Human brain extracts the spatial and temporal semantic information by processing the multi-modalities, which has contextually meaningful for perceiving and understanding the emotional state of an individual. However, there are two main challenges in modeling multimodal sequences: 1) the different sampling rates from multimodal data make the cross-modal interactions very difficult; 2) how to efficiently fuse unimodal representations and effectively capture relationships among multimodal data. In this paper, we design the weighted cross-modal attention mechanism, which not only captures the temporal correlation information and the spatial dependence information of each modality, but also dynamically adjusts the weight of each modality across different time steps. And the unimodal subtasks are led for assisting the representation learning of specific modality to jointly train the multimodal tasks and unimodal subtasks to explore the complementary relationships of each modality. Our model gets a new state-of-the-art record on the CMU-MOSI dataset and brings noticeable performance improvements on all the metrics. For the CMU-MOSEI dataset, the F1 score of the binary classification, the 7-class task, and the regression task of our model are still the highest among all models and the proposed model is only lower than the multimodal split attention fusion (MSAF) model with aligned data on the accuracy of the binary classification, showing the great performance of the suggested method.",
      "doi": "https://doi.org/10.1109/taslp.2022.3192728",
      "openalex_id": "https://openalex.org/W4286370724",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tunet: A Block-Online Bandwidth Extension Model Based On Transformers And Self-Supervised Pretraining",
      "summary": "We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in both intrusive and non-intrusive metrics. Pretraining and filter augmentation also help stabilize and enhance the overall performance.",
      "abstract": "We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in both intrusive and non-intrusive metrics. Pretraining and filter augmentation also help stabilize and enhance the overall performance.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747699",
      "openalex_id": "https://openalex.org/W3211264909",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Investigating Self-Supervised Pre-Training for End-to-End Speech Translation",
      "summary": "Self-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predic-tive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that self-supervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC representations leads to near state-of-the-art models without using any ASR pre-training. This might be particularly beneficial when one needs to develop a system that translates from speech in a language with poorly standardized orthography or even from speech in an unwritten language. Index Terms: self-supervised learning from speech, automatic speech translation, end-to-end models, low resource settings.",
      "abstract": "Self-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predic-tive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that self-supervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC representations leads to near state-of-the-art models without using any ASR pre-training. This might be particularly beneficial when one needs to develop a system that translates from speech in a language with poorly standardized orthography or even from speech in an unwritten language. Index Terms: self-supervised learning from speech, automatic speech translation, end-to-end models, low resource settings.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1835",
      "openalex_id": "https://openalex.org/W3049256661",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Jointly Fine-Tuning “BERT-Like” Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "summary": "Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality-specific \"BERT-like\" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning \"BERT-like\" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.",
      "abstract": "Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality-specific \"BERT-like\" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning \"BERT-like\" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1212",
      "openalex_id": "https://openalex.org/W3049723069",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "summary": "Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. On a large-scale competitive setup, we show that pre-training on unlabeled in-domain data reduces the gap between models trained on in-domain and out-of-domain labeled data by 66%-73%. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. Code and models will be made available at https://github.com/pytorch/fairseq.",
      "abstract": "Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. On a large-scale competitive setup, we show that pre-training on unlabeled in-domain data reduces the gap between models trained on in-domain and out-of-domain labeled data by 66%-73%. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. Code and models will be made available at https://github.com/pytorch/fairseq.",
      "doi": "https://doi.org/10.21437/interspeech.2021-236",
      "openalex_id": "https://openalex.org/W3144173820",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Guided Generative Adversarial Neural Network for Representation Learning and Audio Generation Using Fewer Labelled Audio Data",
      "summary": "The Generation power of Generative Adversarial Neural Networks (GANs) has shown great promise to learn representations from unlabelled data while guided by a small amount of labelled data. We aim to utilise the generation power of GANs to learn Audio Representations. Most existing studies are, however, focused on images. Some studies use GANs for speech generation, but they are conditioned on text or acoustic features, limiting their use for other audio, such as instruments, and even for speech where transcripts are limited. This paper proposes a novel GAN-based model that we named Guided Generative Adversarial Neural Network (GGAN), which can learn powerful representations and generate good-quality samples using a small amount of labelled data as guidance. Experimental results based on a speech [Speech Command Dataset (S09)] and a non-speech [Musical Instrument Sound dataset (Nsyth)] dataset demonstrate that using only 5\\% of labelled data as guidance, GGAN learns significantly better representations than the state-of-the-art models.",
      "abstract": "The Generation power of Generative Adversarial Neural Networks (GANs) has shown great promise to learn representations from unlabelled data while guided by a small amount of labelled data. We aim to utilise the generation power of GANs to learn Audio Representations. Most existing studies are, however, focused on images. Some studies use GANs for speech generation, but they are conditioned on text or acoustic features, limiting their use for other audio, such as instruments, and even for speech where transcripts are limited. This paper proposes a novel GAN-based model that we named Guided Generative Adversarial Neural Network (GGAN), which can learn powerful representations and generate good-quality samples using a small amount of labelled data as guidance. Experimental results based on a speech [Speech Command Dataset (S09)] and a non-speech [Musical Instrument Sound dataset (Nsyth)] dataset demonstrate that using only 5\\% of labelled data as guidance, GGAN learns significantly better representations than the state-of-the-art models.",
      "doi": "https://doi.org/10.1109/taslp.2021.3098764",
      "openalex_id": "https://openalex.org/W3184415155",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised Pre-Training and its Application to Children’s ASR",
      "summary": "We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.",
      "abstract": "We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414970",
      "openalex_id": "https://openalex.org/W3161005563",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Representation Learning for Document Image Classification",
      "summary": "Supervised learning, despite being extremely effective, relies on expensive, time-consuming, and error-prone annotations. Self-supervised learning has recently emerged as a strong alternate to supervised learning in a range of different domains as collecting a large amount of unlabeled data can be achieved by simply crawling the internet. These self-supervised methods automatically discover features relevant to represent an input example by using self-defined proxy tasks. In this paper, we question the potential of commonly employed purely supervised training (starting either from ImageNet pretrained networks or pure random initialization) in contrast to self-supervised representations that can be learned directly using self-supervised representation learning methods on large document image datasets. For this purpose, we leverage a large-scale document image collection (RVL-CDIP) to train ResNet-50 image encoder using two different self-supervision methods (SimCLR and Barlow Twins). Employing a linear classifier on top of self-supervised embeddings from ResNet-50 results in an accuracy of 86.75&#x0025; as compared to 71.43&#x0025; from the corresponding ImageNet pretrained embeddings. Similarly, evaluating on Tobacco-3482 dataset using self-supervised embeddings from ResNet-50 yields an accuracy of 88.52&#x0025; in contrast to 74.16&#x0025; from the corresponding ImageNet pretrained embeddings. We show that in the case of limited labeled data, this wide gap in performance between self-supervised and fully supervised models persists even after fine-tuning pretrained models. However, a significant reduction in this gap is observed with an increasing amount of data including the case where the model is trained from scratch. Our results show that representations learned using self-supervised representation learning techniques are a viable option for document image classification, specifically in the context of limited labeled data, which is a usual restriction in industrial use cases.",
      "abstract": "Supervised learning, despite being extremely effective, relies on expensive, time-consuming, and error-prone annotations. Self-supervised learning has recently emerged as a strong alternate to supervised learning in a range of different domains as collecting a large amount of unlabeled data can be achieved by simply crawling the internet. These self-supervised methods automatically discover features relevant to represent an input example by using self-defined proxy tasks. In this paper, we question the potential of commonly employed purely supervised training (starting either from ImageNet pretrained networks or pure random initialization) in contrast to self-supervised representations that can be learned directly using self-supervised representation learning methods on large document image datasets. For this purpose, we leverage a large-scale document image collection (RVL-CDIP) to train ResNet-50 image encoder using two different self-supervision methods (SimCLR and Barlow Twins). Employing a linear classifier on top of self-supervised embeddings from ResNet-50 results in an accuracy of 86.75&#x0025; as compared to 71.43&#x0025; from the corresponding ImageNet pretrained embeddings. Similarly, evaluating on Tobacco-3482 dataset using self-supervised embeddings from ResNet-50 yields an accuracy of 88.52&#x0025; in contrast to 74.16&#x0025; from the corresponding ImageNet pretrained embeddings. We show that in the case of limited labeled data, this wide gap in performance between self-supervised and fully supervised models persists even after fine-tuning pretrained models. However, a significant reduction in this gap is observed with an increasing amount of data including the case where the model is trained from scratch. Our results show that representations learned using self-supervised representation learning techniques are a viable option for document image classification, specifically in the context of limited labeled data, which is a usual restriction in industrial use cases.",
      "doi": "https://doi.org/10.1109/access.2021.3133200",
      "openalex_id": "https://openalex.org/W4205377807",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Universal Cross-Lingual Data Generation for Low Resource ASR",
      "summary": "Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CommonVoice</small> dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.",
      "abstract": "Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CommonVoice</small> dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.",
      "doi": "https://doi.org/10.1109/taslp.2023.3345150",
      "openalex_id": "https://openalex.org/W4390096798",
      "arxiv_id": "",
      "publication_date": "2023-12-22",
      "published": "2023-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for\\n unsupervised spoken language modeling",
      "summary": "We introduce a new unsupervised task, spoken language modeling: the learning\\nof linguistic representations from raw audio signals without any labels, along\\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\\nmetrics probing for the quality of the learned models at 4 linguistic levels:\\nphonetics, lexicon, syntax and semantics. We present the results and analyses\\nof a composite baseline made of the concatenation of three unsupervised\\nsystems: self-supervised contrastive representation learning (CPC), clustering\\n(k-means) and language modeling (LSTM or BERT). The language models learn on\\nthe basis of the pseudo-text derived from clustering the learned\\nrepresentations. This simple pipeline shows better than chance performance on\\nall four metrics, demonstrating the feasibility of spoken language modeling\\nfrom raw speech. It also yields worse performance compared to text-based\\n'topline' systems trained on the same data, delineating the space to be\\nexplored by more sophisticated end-to-end models.\\n",
      "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning\\nof linguistic representations from raw audio signals without any labels, along\\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\\nmetrics probing for the quality of the learned models at 4 linguistic levels:\\nphonetics, lexicon, syntax and semantics. We present the results and analyses\\nof a composite baseline made of the concatenation of three unsupervised\\nsystems: self-supervised contrastive representation learning (CPC), clustering\\n(k-means) and language modeling (LSTM or BERT). The language models learn on\\nthe basis of the pseudo-text derived from clustering the learned\\nrepresentations. This simple pipeline shows better than chance performance on\\nall four metrics, demonstrating the feasibility of spoken language modeling\\nfrom raw speech. It also yields worse performance compared to text-based\\n'topline' systems trained on the same data, delineating the space to be\\nexplored by more sophisticated end-to-end models.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2011.11588",
      "openalex_id": "https://openalex.org/W3110458199",
      "arxiv_id": "",
      "publication_date": "2020-11-23",
      "published": "2020-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition",
      "summary": "Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.",
      "abstract": "Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.",
      "doi": "https://doi.org/10.48550/arxiv.2008.03687",
      "openalex_id": "https://openalex.org/W3047866127",
      "arxiv_id": "",
      "publication_date": "2020-08-09",
      "published": "2020-08-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Further Study of Unsupervised Pre-training for Transformer Based Speech Recognition",
      "summary": "Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.",
      "abstract": "Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.",
      "doi": "https://doi.org/10.48550/arxiv.2005.09862",
      "openalex_id": "https://openalex.org/W3026957705",
      "arxiv_id": "",
      "publication_date": "2020-05-20",
      "published": "2020-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "OkwuGbé: End-to-End Speech Recognition for Fon and Igbo",
      "summary": "Language is inherent and compulsory for human communication. Whether expressed in a written or spoken way, it ensures understanding between people of the same and different regions. With the growing awareness and effort to include more low-resourced languages in NLP research, African languages have recently been a major subject of research in machine translation, and other text-based areas of NLP. However, there is still very little comparable research in speech recognition for African languages. Interestingly, some of the unique properties of African languages affecting NLP, like their diacritical and tonal complexities, have a major root in their speech, suggesting that careful speech interpretation could provide more intuition on how to deal with the linguistic complexities of African languages for text-based NLP. OkwuGbé is a step towards building speech recognition systems for African low-resourced languages. Using Fon and Igbo as our case study, we conduct a comprehensive linguistic analysis of each language and describe the creation of end-to-end, deep neural network-based speech recognition models for both languages. We present a state-of-art ASR model for Fon, as well as benchmark ASR model results for Igbo. Our linguistic analyses (for Fon and Igbo) provide valuable insights and guidance into the creation of speech recognition models for other African low-resourced languages, as well as guide future NLP research for Fon and Igbo. The Fon and Igbo models source code have been made publicly available.",
      "abstract": "Language is inherent and compulsory for human communication. Whether expressed in a written or spoken way, it ensures understanding between people of the same and different regions. With the growing awareness and effort to include more low-resourced languages in NLP research, African languages have recently been a major subject of research in machine translation, and other text-based areas of NLP. However, there is still very little comparable research in speech recognition for African languages. Interestingly, some of the unique properties of African languages affecting NLP, like their diacritical and tonal complexities, have a major root in their speech, suggesting that careful speech interpretation could provide more intuition on how to deal with the linguistic complexities of African languages for text-based NLP. OkwuGbé is a step towards building speech recognition systems for African low-resourced languages. Using Fon and Igbo as our case study, we conduct a comprehensive linguistic analysis of each language and describe the creation of end-to-end, deep neural network-based speech recognition models for both languages. We present a state-of-art ASR model for Fon, as well as benchmark ASR model results for Igbo. Our linguistic analyses (for Fon and Igbo) provide valuable insights and guidance into the creation of speech recognition models for other African low-resourced languages, as well as guide future NLP research for Fon and Igbo. The Fon and Igbo models source code have been made publicly available.",
      "doi": "https://doi.org/10.48550/arxiv.2103.07762",
      "openalex_id": "https://openalex.org/W3136219906",
      "arxiv_id": "",
      "publication_date": "2021-03-13",
      "published": "2021-03-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised Pre-training and Its Application to Children's ASR",
      "summary": "We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.",
      "abstract": "We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.",
      "doi": "https://doi.org/10.48550/arxiv.2102.06816",
      "openalex_id": "https://openalex.org/W3132108706",
      "arxiv_id": "",
      "publication_date": "2021-02-12",
      "published": "2021-02-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large Scale Weakly and Semi-Supervised Learning for Low-Resource Video ASR",
      "summary": "Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pretraining using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only CTC-based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline WERs by more than 8%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20% compared to the strongest data-augmented supervised baseline.",
      "abstract": "Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pretraining using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only CTC-based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline WERs by more than 8%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20% compared to the strongest data-augmented supervised baseline.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1917",
      "openalex_id": "https://openalex.org/W3025896989",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Input-independent Attention Weights Are Expressive Enough: A Study of Attention in Self-supervised Audio Transformers",
      "summary": "In this paper, we seek solutions for reducing the computation complexity of transformer-based models for speech representation learning. We evaluate 10 attention algorithms; then, we pre-train the transformer-based model with those attention algorithms in a self-supervised fashion and treat them as feature extractors on downstream tasks, including phoneme classification and speaker classification. With the assistance of t-SNE, PCA and some observation, the attention weights in self-supervised audio transformers can be categorized into four general cases. Based on these cases and some analyses, we are able to use a specific set of attention weights to initialize the model. Our approach shows comparable performance to the typical self-attention yet requires 20% less time in both training and inference.",
      "abstract": "In this paper, we seek solutions for reducing the computation complexity of transformer-based models for speech representation learning. We evaluate 10 attention algorithms; then, we pre-train the transformer-based model with those attention algorithms in a self-supervised fashion and treat them as feature extractors on downstream tasks, including phoneme classification and speaker classification. With the assistance of t-SNE, PCA and some observation, the attention weights in self-supervised audio transformers can be categorized into four general cases. Based on these cases and some analyses, we are able to use a specific set of attention weights to initialize the model. Our approach shows comparable performance to the typical self-attention yet requires 20% less time in both training and inference.",
      "doi": "https://doi.org/10.48550/arxiv.2006.05174",
      "openalex_id": "https://openalex.org/W3096703500",
      "arxiv_id": "",
      "publication_date": "2020-06-09",
      "published": "2020-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spiker-Converter: A Semi-Supervised Framework for Low-Resource Speech Recognition with Stable Adversarial Training",
      "summary": "Labeling large amounts of speech is laborious and expensive. The scarcity of speech with the accent or in specific scenes hangs the further applications of the ASR system in practice. On the contrary, collecting speech and domain-related text corpus is more achievable. In this work, we propose an end-to-end model called Spiker-Converter for the low-resource speech recognition task. It decomposes the ASR task by introducing additional acoustic supervision, dramatically reduce the demand for labeled samples. Besides, we provide a semi-supervised training method, which consumes a few labeled speech samples but large amounts of unlabeled speech and domain-related text. Specifically, we use a Discriminator to produce learning signals for the ASR model with unlabeled speech as input. Note that we apply adversarial training to part of the ASR model, ensuring stability. Experiments show the significant effectiveness of our semi-supervised training method. For now, our method can only be used for Chinese-like languages, but it shows a potential direction to solve low-resource speech recognition tasks.",
      "abstract": "Labeling large amounts of speech is laborious and expensive. The scarcity of speech with the accent or in specific scenes hangs the further applications of the ASR system in practice. On the contrary, collecting speech and domain-related text corpus is more achievable. In this work, we propose an end-to-end model called Spiker-Converter for the low-resource speech recognition task. It decomposes the ASR task by introducing additional acoustic supervision, dramatically reduce the demand for labeled samples. Besides, we provide a semi-supervised training method, which consumes a few labeled speech samples but large amounts of unlabeled speech and domain-related text. Specifically, we use a Discriminator to produce learning signals for the ASR model with unlabeled speech as input. Note that we apply adversarial training to part of the ASR model, ensuring stability. Experiments show the significant effectiveness of our semi-supervised training method. For now, our method can only be used for Chinese-like languages, but it shows a potential direction to solve low-resource speech recognition tasks.",
      "doi": "https://doi.org/10.1109/icme51207.2021.9428111",
      "openalex_id": "https://openalex.org/W3166405250",
      "arxiv_id": "",
      "publication_date": "2021-06-09",
      "published": "2021-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improved Noisy Iterative Pseudo-Labeling for Semi-Supervised Speech Recognition",
      "summary": "Due to the high annotation cost in ASR, the implementation of semi-supervised training has been a hot issue in research and industry. In a multitude of recent investigations, it has been established that pseudo-labeling, a fundamental sub-direction of semi-supervised learning, is effective in ASR. However, if the iterative PL is utilized, the expense of doing data experiments is prohibitively high, making the promotion to diverse situations of ASR tasks problematic. In this paper, we propose an empirical scoring method based on hypothesis distribution testing to guide iterative PL training, therefore lowering the cost of data experiments and boosting ASR performance. Meanwhile, we conducted extensive experiments to determine the necessity and limitation of model perturbation in the initial training and the PL stages. On the Librispeech 100/860 task, our method improves the 12+6 transformer-based CTC+S2S architecture performance from 4.8%/10.1 % to 3.9%/9.6% on test-clean and test-other.",
      "abstract": "Due to the high annotation cost in ASR, the implementation of semi-supervised training has been a hot issue in research and industry. In a multitude of recent investigations, it has been established that pseudo-labeling, a fundamental sub-direction of semi-supervised learning, is effective in ASR. However, if the iterative PL is utilized, the expense of doing data experiments is prohibitively high, making the promotion to diverse situations of ASR tasks problematic. In this paper, we propose an empirical scoring method based on hypothesis distribution testing to guide iterative PL training, therefore lowering the cost of data experiments and boosting ASR performance. Meanwhile, we conducted extensive experiments to determine the necessity and limitation of model perturbation in the initial training and the PL stages. On the Librispeech 100/860 task, our method improves the 12+6 transformer-based CTC+S2S architecture performance from 4.8%/10.1 % to 3.9%/9.6% on test-clean and test-other.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022417",
      "openalex_id": "https://openalex.org/W4319862240",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LFM Signal Sources Classification Based on Self-supervised Learning",
      "summary": "Linear Frequency Modulation (LFM) signals are widely used in radar and sonar technology.Many applications are interested in determining the source of an LFM signal.In recent years, the rapid development of machine learning has facilitated research in various fields, including signal recognition.The neural networks can extract the implicit features of the signals, which can help the system to sort and recognize the signal sources quickly and accurately.High performance of neural networks requires large amounts of high-quality labeled data.However, it is difficult and expensive to obtain a large amount of high-quality labeled data.Simultaneously, some features will be lost during data preprocessing, and feature extraction and classification tasks will be inefficient.The self-supervised network is proposed in this paper for pre-training the signal waveform and fine-tuning the classification with a small amount of labeled data.The proposed method can extract more signal waveform features, save labeling costs, and has higher precision.This method can provide up to 99.7% recognition accuracy at 20 dB.",
      "abstract": "Linear Frequency Modulation (LFM) signals are widely used in radar and sonar technology.Many applications are interested in determining the source of an LFM signal.In recent years, the rapid development of machine learning has facilitated research in various fields, including signal recognition.The neural networks can extract the implicit features of the signals, which can help the system to sort and recognize the signal sources quickly and accurately.High performance of neural networks requires large amounts of high-quality labeled data.However, it is difficult and expensive to obtain a large amount of high-quality labeled data.Simultaneously, some features will be lost during data preprocessing, and feature extraction and classification tasks will be inefficient.The self-supervised network is proposed in this paper for pre-training the signal waveform and fine-tuning the classification with a small amount of labeled data.The proposed method can extract more signal waveform features, save labeling costs, and has higher precision.This method can provide up to 99.7% recognition accuracy at 20 dB.",
      "doi": "https://doi.org/10.2528/pierl23073102",
      "openalex_id": "https://openalex.org/W4387010942",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep learning in electron microscopy",
      "summary": "Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.",
      "abstract": "Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.",
      "doi": "https://doi.org/10.1088/2632-2153/abd614",
      "openalex_id": "https://openalex.org/W3084979415",
      "arxiv_id": "",
      "publication_date": "2020-12-22",
      "published": "2020-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the Convergence of Adam and Adagrad",
      "summary": "We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\\ell_\\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.",
      "abstract": "We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\\ell_\\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3009948090",
      "arxiv_id": "",
      "publication_date": "2020-03-05",
      "published": "2020-03-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stochastic Restoration of Heavily Compressed Musical Audio Using Generative Adversarial Networks",
      "summary": "Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.",
      "abstract": "Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.",
      "doi": "https://doi.org/10.3390/electronics10111349",
      "openalex_id": "https://openalex.org/W3169882120",
      "arxiv_id": "",
      "publication_date": "2021-06-05",
      "published": "2021-06-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network",
      "summary": "Abstract Amongst the various characteristics of a speech signal, the expression of emotion is one of the characteristics that exhibits the slowest temporal dynamics. Hence, a performant speech emotion recognition (SER) system requires a predictive model that is capable of learning sufficiently long temporal dependencies in the analysed speech signal. Therefore, in this work, we propose a novel end-to-end neural network architecture based on the concept of dilated causal convolution with context stacking. Firstly, the proposed model consists only of parallelisable layers and is hence suitable for parallel processing, while avoiding the inherent lack of parallelisability occurring with recurrent neural network (RNN) layers. Secondly, the design of a dedicated dilated causal convolution block allows the model to have a receptive field as large as the input sequence length, while maintaining a reasonably low computational cost. Thirdly, by introducing a context stacking structure, the proposed model is capable of exploiting long-term temporal dependencies hence providing an alternative to the use of RNN layers. We evaluate the proposed model in SER regression and classification tasks and provide a comparison with a state-of-the-art end-to-end SER model. Experimental results indicate that the proposed model requires only 1/3 of the number of model parameters used in the state-of-the-art model, while also significantly improving SER performance. Further experiments are reported to understand the impact of using various types of input representations (i.e. raw audio samples vs log mel-spectrograms) and to illustrate the benefits of an end-to-end approach over the use of hand-crafted audio features. Moreover, we show that the proposed model can efficiently learn intermediate embeddings preserving speech emotion information.",
      "abstract": "Abstract Amongst the various characteristics of a speech signal, the expression of emotion is one of the characteristics that exhibits the slowest temporal dynamics. Hence, a performant speech emotion recognition (SER) system requires a predictive model that is capable of learning sufficiently long temporal dependencies in the analysed speech signal. Therefore, in this work, we propose a novel end-to-end neural network architecture based on the concept of dilated causal convolution with context stacking. Firstly, the proposed model consists only of parallelisable layers and is hence suitable for parallel processing, while avoiding the inherent lack of parallelisability occurring with recurrent neural network (RNN) layers. Secondly, the design of a dedicated dilated causal convolution block allows the model to have a receptive field as large as the input sequence length, while maintaining a reasonably low computational cost. Thirdly, by introducing a context stacking structure, the proposed model is capable of exploiting long-term temporal dependencies hence providing an alternative to the use of RNN layers. We evaluate the proposed model in SER regression and classification tasks and provide a comparison with a state-of-the-art end-to-end SER model. Experimental results indicate that the proposed model requires only 1/3 of the number of model parameters used in the state-of-the-art model, while also significantly improving SER performance. Further experiments are reported to understand the impact of using various types of input representations (i.e. raw audio samples vs log mel-spectrograms) and to illustrate the benefits of an end-to-end approach over the use of hand-crafted audio features. Moreover, we show that the proposed model can efficiently learn intermediate embeddings preserving speech emotion information.",
      "doi": "https://doi.org/10.1186/s13636-021-00208-5",
      "openalex_id": "https://openalex.org/W3164680725",
      "arxiv_id": "",
      "publication_date": "2021-05-12",
      "published": "2021-05-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Environmental Sound Classification with Tiny Transformers in Noisy Edge Environments",
      "summary": "The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.",
      "abstract": "The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.",
      "doi": "https://doi.org/10.1109/wf-iot51360.2021.9596007",
      "openalex_id": "https://openalex.org/W3212947338",
      "arxiv_id": "",
      "publication_date": "2021-06-14",
      "published": "2021-06-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "One Billion Audio Sounds from GPU-Enabled Modular Synthesis",
      "summary": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.",
      "abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.",
      "doi": "https://doi.org/10.23919/dafx51585.2021.9768246",
      "openalex_id": "https://openalex.org/W3159239022",
      "arxiv_id": "",
      "publication_date": "2021-09-08",
      "published": "2021-09-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI song contest: Human-AI co-creation in songwriting",
      "summary": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",
      "abstract": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",
      "doi": "https://doi.org/10.5281/zenodo.4245530",
      "openalex_id": "https://openalex.org/W3092135915",
      "arxiv_id": "",
      "publication_date": "2020-10-11",
      "published": "2020-10-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks",
      "summary": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
      "abstract": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
      "doi": "https://doi.org/10.5281/zenodo.4245503",
      "openalex_id": "https://openalex.org/W3081378361",
      "arxiv_id": "",
      "publication_date": "2020-10-11",
      "published": "2020-10-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Generative Models in Engineering Design: A Review",
      "summary": "Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.",
      "abstract": "Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.",
      "doi": "https://doi.org/10.1115/1.4053859",
      "openalex_id": "https://openalex.org/W3206790237",
      "arxiv_id": "",
      "publication_date": "2022-02-16",
      "published": "2022-02-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BassNet: A Variational Gated Autoencoder for Conditional Generation of Bass Guitar Tracks with Learned Interactive Control",
      "summary": "Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.",
      "abstract": "Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.",
      "doi": "https://doi.org/10.3390/app10186627",
      "openalex_id": "https://openalex.org/W3088329082",
      "arxiv_id": "",
      "publication_date": "2020-09-22",
      "published": "2020-09-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Symbolic Music Generation with Diffusion Models",
      "summary": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
      "abstract": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
      "doi": "https://doi.org/10.5281/zenodo.5624363",
      "openalex_id": "https://openalex.org/W3148695041",
      "arxiv_id": "",
      "publication_date": "2021-11-07",
      "published": "2021-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformer Neural Networks for Automated Rhythm Generation",
      "summary": "Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.",
      "abstract": "Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.",
      "doi": "https://doi.org/10.21428/92fbeb44.fe9a0d82",
      "openalex_id": "https://openalex.org/W3185297410",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders",
      "summary": "The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.",
      "abstract": "The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.",
      "doi": "https://doi.org/10.18653/v1/2020.inlg-1.42",
      "openalex_id": "https://openalex.org/W3112789166",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Watching a Language Model Learning Chess",
      "summary": "We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.",
      "abstract": "We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.",
      "doi": "https://doi.org/10.26615/978-954-452-072-4_153",
      "openalex_id": "https://openalex.org/W3211354215",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control",
      "summary": "In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.",
      "abstract": "In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.",
      "doi": "https://doi.org/10.21437/ssw.2021-21",
      "openalex_id": "https://openalex.org/W3193323418",
      "arxiv_id": "",
      "publication_date": "2021-08-24",
      "published": "2021-08-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What to Play and How to Play it: Guiding Generative Music Models with Multiple Demonstrations",
      "summary": "We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both \"what to play\" (via scores in MIDI format) and \"how to play it\" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.",
      "abstract": "We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both \"what to play\" (via scores in MIDI format) and \"how to play it\" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.",
      "doi": "https://doi.org/10.21428/92fbeb44.06e2d5f4",
      "openalex_id": "https://openalex.org/W3209221083",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Upsampling Layers for Music Source Separation",
      "summary": "Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.",
      "abstract": "Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10289768",
      "openalex_id": "https://openalex.org/W3216722162",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models",
      "summary": "Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",
      "abstract": "Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",
      "doi": "https://doi.org/10.5281/zenodo.5624597",
      "openalex_id": "https://openalex.org/W3189022627",
      "arxiv_id": "",
      "publication_date": "2021-11-07",
      "published": "2021-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Geometry-Free View Synthesis: Transformers and no 3D Priors",
      "summary": "Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn",
      "abstract": "Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn",
      "doi": "https://doi.org/10.1109/iccv48922.2021.01409",
      "openalex_id": "https://openalex.org/W3154159596",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "summary": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "doi": "https://doi.org/10.48550/arxiv.2103.00020",
      "openalex_id": "https://openalex.org/W3135367836",
      "arxiv_id": "",
      "publication_date": "2021-02-26",
      "published": "2021-02-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero-Shot Text-to-Image Generation",
      "summary": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
      "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
      "doi": "https://doi.org/10.48550/arxiv.2102.12092",
      "openalex_id": "https://openalex.org/W3129576130",
      "arxiv_id": "",
      "publication_date": "2021-02-24",
      "published": "2021-02-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scaling Laws for Autoregressive Generative Modeling",
      "summary": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
      "abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
      "doi": "https://doi.org/10.48550/arxiv.2010.14701",
      "openalex_id": "https://openalex.org/W3095645723",
      "arxiv_id": "",
      "publication_date": "2020-10-28",
      "published": "2020-10-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
      "summary": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
      "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
      "doi": "https://doi.org/10.48550/arxiv.2104.10157",
      "openalex_id": "https://openalex.org/W3152733922",
      "arxiv_id": "",
      "publication_date": "2021-04-20",
      "published": "2021-04-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them\\n on Images",
      "summary": "We present a hierarchical VAE that, for the first time, generates samples\\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\\nautoregressive models, as well as faster, better models if they exist, when\\nmade sufficiently deep. Despite this, autoregressive models have historically\\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\\nby scaling a VAE to greater stochastic depth than previously explored and\\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\\nsamples thousands of times faster, and are more easily applied to\\nhigh-resolution images. Qualitative studies suggest this is because the VAE\\nlearns efficient hierarchical visual representations. We release our source\\ncode and models at https://github.com/openai/vdvae.\\n",
      "abstract": "We present a hierarchical VAE that, for the first time, generates samples\\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\\nautoregressive models, as well as faster, better models if they exist, when\\nmade sufficiently deep. Despite this, autoregressive models have historically\\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\\nby scaling a VAE to greater stochastic depth than previously explored and\\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\\nsamples thousands of times faster, and are more easily applied to\\nhigh-resolution images. Qualitative studies suggest this is because the VAE\\nlearns efficient hierarchical visual representations. We release our source\\ncode and models at https://github.com/openai/vdvae.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2011.10650",
      "openalex_id": "https://openalex.org/W3120243996",
      "arxiv_id": "",
      "publication_date": "2020-11-20",
      "published": "2020-11-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "POP909: A Pop-song Dataset for Music Arrangement Generation",
      "summary": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
      "abstract": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
      "doi": "https://doi.org/10.48550/arxiv.2008.07142",
      "openalex_id": "https://openalex.org/W3049247973",
      "arxiv_id": "",
      "publication_date": "2020-08-17",
      "published": "2020-08-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis",
      "summary": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",
      "abstract": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",
      "doi": "https://doi.org/10.48550/arxiv.2108.08827",
      "openalex_id": "https://openalex.org/W3196163807",
      "arxiv_id": "",
      "publication_date": "2021-08-19",
      "published": "2021-08-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
      "summary": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
      "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
      "doi": "https://doi.org/10.48550/arxiv.2110.03675",
      "openalex_id": "https://openalex.org/W3204896549",
      "arxiv_id": "",
      "publication_date": "2021-10-07",
      "published": "2021-10-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Computer-Aided Design as Language",
      "summary": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
      "abstract": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
      "doi": "https://doi.org/10.48550/arxiv.2105.02769",
      "openalex_id": "https://openalex.org/W3159309302",
      "arxiv_id": "",
      "publication_date": "2021-05-06",
      "published": "2021-05-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based\\n Models",
      "summary": "Energy-based models (EBMs) have recently been successful in representing\\ncomplex distributions of small images. However, sampling from them requires\\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\\nsamples quickly and are equipped with a latent space that enables fast\\ntraversal of the data manifold. However, VAEs tend to assign high probability\\ndensity to regions in data space outside the actual data distribution and often\\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\\ncaptures the overall mode structure of the data distribution using a\\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\\nnon-data-like regions from the model and refine the image samples. Moreover,\\nthe VAE component in VAEBM allows us to speed up MCMC updates by\\nreparameterizing them in the VAE's latent space. Our experimental results show\\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\\nseveral benchmark image datasets by a large margin. It can generate\\nhigh-quality images as large as 256$\\\\times$256 pixels with short MCMC chains.\\nWe also demonstrate that VAEBM provides complete mode coverage and performs\\nwell in out-of-distribution detection. The source code is available at\\nhttps://github.com/NVlabs/VAEBM\\n",
      "abstract": "Energy-based models (EBMs) have recently been successful in representing\\ncomplex distributions of small images. However, sampling from them requires\\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\\nsamples quickly and are equipped with a latent space that enables fast\\ntraversal of the data manifold. However, VAEs tend to assign high probability\\ndensity to regions in data space outside the actual data distribution and often\\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\\ncaptures the overall mode structure of the data distribution using a\\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\\nnon-data-like regions from the model and refine the image samples. Moreover,\\nthe VAE component in VAEBM allows us to speed up MCMC updates by\\nreparameterizing them in the VAE's latent space. Our experimental results show\\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\\nseveral benchmark image datasets by a large margin. It can generate\\nhigh-quality images as large as 256$\\\\times$256 pixels with short MCMC chains.\\nWe also demonstrate that VAEBM provides complete mode coverage and performs\\nwell in out-of-distribution detection. The source code is available at\\nhttps://github.com/NVlabs/VAEBM\\n",
      "doi": "https://doi.org/10.48550/arxiv.2010.00654",
      "openalex_id": "https://openalex.org/W3118605064",
      "arxiv_id": "",
      "publication_date": "2020-10-01",
      "published": "2020-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Simple and Effective VAE Training with Calibrated Decoders",
      "summary": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/",
      "abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/",
      "doi": "https://doi.org/10.48550/arxiv.2006.13202",
      "openalex_id": "https://openalex.org/W3036520878",
      "arxiv_id": "",
      "publication_date": "2020-06-23",
      "published": "2020-06-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Simple Convergence Proof of Adam and Adagrad",
      "summary": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.",
      "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.",
      "doi": "https://doi.org/10.48550/arxiv.2003.02395",
      "openalex_id": "https://openalex.org/W3096312061",
      "arxiv_id": "",
      "publication_date": "2020-03-05",
      "published": "2020-03-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Predicting Video with VQVAE",
      "summary": "In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.",
      "abstract": "In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.",
      "doi": "https://doi.org/10.48550/arxiv.2103.01950",
      "openalex_id": "https://openalex.org/W3133405188",
      "arxiv_id": "",
      "publication_date": "2021-03-02",
      "published": "2021-03-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "summary": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "doi": "https://doi.org/10.48550/arxiv.2107.12979",
      "openalex_id": "https://openalex.org/W3186883833",
      "arxiv_id": "",
      "publication_date": "2021-07-27",
      "published": "2021-07-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Spectral Energy Distance for Parallel Speech Synthesis",
      "summary": "Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.",
      "abstract": "Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.",
      "doi": "https://doi.org/10.48550/arxiv.2008.01160",
      "openalex_id": "https://openalex.org/W3046970875",
      "arxiv_id": "",
      "publication_date": "2020-08-03",
      "published": "2020-08-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks",
      "summary": "Deep neural networks are typically trained under a supervised learning framework where a model learns a single task using labeled data. Instead of relying solely on labeled data, practitioners can harness unlabeled or related data to improve model performance, which is often more accessible and ubiquitous. Self-supervised pre-training for transfer learning is becoming an increasingly popular technique to improve state-of-the-art results using unlabeled data. It involves first pre-training a model on a large amount of unlabeled data, then adapting the model to target tasks of interest. In this review, we survey self-supervised learning methods and their applications within the sequential transfer learning framework. We provide an overview of the taxonomy for self-supervised learning and transfer learning, and highlight some prominent methods for designing pre-training tasks across different domains. Finally, we discuss recent trends and suggest areas for future investigation.",
      "abstract": "Deep neural networks are typically trained under a supervised learning framework where a model learns a single task using labeled data. Instead of relying solely on labeled data, practitioners can harness unlabeled or related data to improve model performance, which is often more accessible and ubiquitous. Self-supervised pre-training for transfer learning is becoming an increasingly popular technique to improve state-of-the-art results using unlabeled data. It involves first pre-training a model on a large amount of unlabeled data, then adapting the model to target tasks of interest. In this review, we survey self-supervised learning methods and their applications within the sequential transfer learning framework. We provide an overview of the taxonomy for self-supervised learning and transfer learning, and highlight some prominent methods for designing pre-training tasks across different domains. Finally, we discuss recent trends and suggest areas for future investigation.",
      "doi": "https://doi.org/10.48550/arxiv.2007.00800",
      "openalex_id": "https://openalex.org/W3039049049",
      "arxiv_id": "",
      "publication_date": "2020-07-01",
      "published": "2020-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements",
      "summary": "We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.",
      "abstract": "We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.",
      "doi": "https://doi.org/10.48550/arxiv.2012.03478",
      "openalex_id": "https://openalex.org/W3111853169",
      "arxiv_id": "",
      "publication_date": "2020-12-07",
      "published": "2020-12-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tiny Transformers for Environmental Sound Classification at the Edge",
      "summary": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
      "abstract": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
      "doi": "https://doi.org/10.48550/arxiv.2103.12157",
      "openalex_id": "https://openalex.org/W3136991969",
      "arxiv_id": "",
      "publication_date": "2021-03-22",
      "published": "2021-03-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Score-based Generative Modeling in Latent Space",
      "summary": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
      "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
      "doi": "https://doi.org/10.48550/arxiv.2106.05931",
      "openalex_id": "https://openalex.org/W3168452307",
      "arxiv_id": "",
      "publication_date": "2021-06-10",
      "published": "2021-06-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain",
      "summary": "The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",
      "abstract": "The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",
      "doi": "https://doi.org/10.48550/arxiv.2009.05359",
      "openalex_id": "https://openalex.org/W3086298921",
      "arxiv_id": "",
      "publication_date": "2020-09-11",
      "published": "2020-09-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Images with Sparse Representations",
      "summary": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.",
      "abstract": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.",
      "doi": "https://doi.org/10.48550/arxiv.2103.03841",
      "openalex_id": "https://openalex.org/W3135058862",
      "arxiv_id": "",
      "publication_date": "2021-03-05",
      "published": "2021-03-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Contrastive Learning Approach for Training Variational Autoencoder Priors",
      "summary": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
      "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
      "doi": "https://doi.org/10.48550/arxiv.2010.02917",
      "openalex_id": "https://openalex.org/W3209501356",
      "arxiv_id": "",
      "publication_date": "2020-10-06",
      "published": "2020-10-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Crispr2vec: Machine Learning Model Predicts Off-Target Cuts of CRISPR systems",
      "summary": "1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.",
      "abstract": "1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.",
      "doi": "https://doi.org/10.1101/2020.10.28.359885",
      "openalex_id": "https://openalex.org/W3097306158",
      "arxiv_id": "",
      "publication_date": "2020-10-29",
      "published": "2020-10-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MTCRNN: A multi-scale RNN for directed audio texture synthesis",
      "summary": "Audio textures are a subset of environmental sounds, often defined as having stable statistical characteristics within an adequately large window of time but may be unstructured locally. They include common everyday sounds such as from rain, wind, and engines. Given that these complex sounds contain patterns on multiple timescales, they are a challenge to model with traditional methods. We introduce a novel modelling approach for textures, combining recurrent neural networks trained at different levels of abstraction with a conditioning strategy that allows for user-directed synthesis. We demonstrate the model's performance on a variety of datasets, examine its performance on various metrics, and discuss some potential applications.",
      "abstract": "Audio textures are a subset of environmental sounds, often defined as having stable statistical characteristics within an adequately large window of time but may be unstructured locally. They include common everyday sounds such as from rain, wind, and engines. Given that these complex sounds contain patterns on multiple timescales, they are a challenge to model with traditional methods. We introduce a novel modelling approach for textures, combining recurrent neural networks trained at different levels of abstraction with a conditioning strategy that allows for user-directed synthesis. We demonstrate the model's performance on a variety of datasets, examine its performance on various metrics, and discuss some potential applications.",
      "doi": "https://doi.org/10.48550/arxiv.2011.12596",
      "openalex_id": "https://openalex.org/W3110624927",
      "arxiv_id": "",
      "publication_date": "2020-11-25",
      "published": "2020-11-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention",
      "summary": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
      "abstract": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
      "doi": "https://doi.org/10.48550/arxiv.2105.03928",
      "openalex_id": "https://openalex.org/W3163120468",
      "arxiv_id": "",
      "publication_date": "2021-05-09",
      "published": "2021-05-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative Models as a Data Source for Multiview Representation Learning",
      "summary": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
      "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
      "doi": "https://doi.org/10.48550/arxiv.2106.05258",
      "openalex_id": "https://openalex.org/W3171895902",
      "arxiv_id": "",
      "publication_date": "2021-06-09",
      "published": "2021-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis",
      "summary": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.",
      "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.",
      "doi": "https://doi.org/10.48550/arxiv.2110.10139",
      "openalex_id": "https://openalex.org/W3206916870",
      "arxiv_id": "",
      "publication_date": "2021-10-19",
      "published": "2021-10-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures",
      "summary": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
      "abstract": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
      "doi": "https://doi.org/10.48550/arxiv.2006.12878",
      "openalex_id": "https://openalex.org/W3036165773",
      "arxiv_id": "",
      "publication_date": "2020-06-23",
      "published": "2020-06-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
      "summary": "We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.",
      "abstract": "We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.",
      "doi": "https://doi.org/10.48550/arxiv.2101.04785",
      "openalex_id": "https://openalex.org/W3119914886",
      "arxiv_id": "",
      "publication_date": "2021-01-12",
      "published": "2021-01-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Catch-A-Waveform: Learning to Generate Audio from a Single Short Example",
      "summary": "Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.",
      "abstract": "Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.",
      "doi": "https://doi.org/10.48550/arxiv.2106.06426",
      "openalex_id": "https://openalex.org/W3171443854",
      "arxiv_id": "",
      "publication_date": "2021-06-11",
      "published": "2021-06-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Audio Synthesis and Audio-Visual Multimodal Processing",
      "summary": "With the development of deep learning and artificial intelligence, audio synthesis has a pivotal role in the area of machine learning and shows strong applicability in the industry. Meanwhile, significant efforts have been dedicated by researchers to handle multimodal tasks at present such as audio-visual multimodal processing. In this paper, we conduct a survey on audio synthesis and audio-visual multimodal processing, which helps understand current research and future trends. This review focuses on text to speech(TTS), music generation and some tasks that combine visual and acoustic information. The corresponding technical methods are comprehensively classified and introduced, and their future development trends are prospected. This survey can provide some guidance for researchers who are interested in the areas like audio synthesis and audio-visual multimodal processing.",
      "abstract": "With the development of deep learning and artificial intelligence, audio synthesis has a pivotal role in the area of machine learning and shows strong applicability in the industry. Meanwhile, significant efforts have been dedicated by researchers to handle multimodal tasks at present such as audio-visual multimodal processing. In this paper, we conduct a survey on audio synthesis and audio-visual multimodal processing, which helps understand current research and future trends. This review focuses on text to speech(TTS), music generation and some tasks that combine visual and acoustic information. The corresponding technical methods are comprehensively classified and introduced, and their future development trends are prospected. This survey can provide some guidance for researchers who are interested in the areas like audio synthesis and audio-visual multimodal processing.",
      "doi": "https://doi.org/10.48550/arxiv.2108.00443",
      "openalex_id": "https://openalex.org/W3192307151",
      "arxiv_id": "",
      "publication_date": "2021-08-01",
      "published": "2021-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DeepSinger: Singing Voice Synthesis with Data Mined From the Web",
      "summary": "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)",
      "abstract": "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)",
      "doi": "https://doi.org/10.48550/arxiv.2007.04590",
      "openalex_id": "https://openalex.org/W3041199652",
      "arxiv_id": "",
      "publication_date": "2020-07-09",
      "published": "2020-07-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer",
      "summary": "Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",
      "abstract": "Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",
      "doi": "https://doi.org/10.48550/arxiv.2108.11213",
      "openalex_id": "https://openalex.org/W3196114627",
      "arxiv_id": "",
      "publication_date": "2021-08-25",
      "published": "2021-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks",
      "summary": "As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.",
      "abstract": "As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.",
      "doi": "https://doi.org/10.48550/arxiv.2101.08919",
      "openalex_id": "https://openalex.org/W3209092256",
      "arxiv_id": "",
      "publication_date": "2021-01-22",
      "published": "2021-01-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics",
      "summary": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
      "abstract": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
      "doi": "https://doi.org/10.48550/arxiv.2105.08164",
      "openalex_id": "https://openalex.org/W3160695487",
      "arxiv_id": "",
      "publication_date": "2021-05-17",
      "published": "2021-05-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models",
      "summary": "Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
      "abstract": "Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02891",
      "openalex_id": "https://openalex.org/W3201970400",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech-to-Singing Conversion based on Boundary Equilibrium GAN",
      "summary": "This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.",
      "abstract": "This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.",
      "doi": "https://doi.org/10.48550/arxiv.2005.13835",
      "openalex_id": "https://openalex.org/W3028988798",
      "arxiv_id": "",
      "publication_date": "2020-05-28",
      "published": "2020-05-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Melody Classifier with Stacked-LSTM",
      "summary": "Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.",
      "abstract": "Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.",
      "doi": "https://doi.org/10.48550/arxiv.2010.08123",
      "openalex_id": "https://openalex.org/W3092769093",
      "arxiv_id": "",
      "publication_date": "2020-10-16",
      "published": "2020-10-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis with GANs",
      "summary": "Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called \"soft labels\") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",
      "abstract": "Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called \"soft labels\") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",
      "doi": "https://doi.org/10.48550/arxiv.2108.01216",
      "openalex_id": "https://openalex.org/W3191340970",
      "arxiv_id": "",
      "publication_date": "2021-08-03",
      "published": "2021-08-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset",
      "summary": "This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.",
      "abstract": "This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.",
      "doi": "https://doi.org/10.48550/arxiv.2108.01576",
      "openalex_id": "https://openalex.org/W3191781868",
      "arxiv_id": "",
      "publication_date": "2021-08-03",
      "published": "2021-08-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Network Bending: Manipulating The Inner Representations of Deep Generative Models.",
      "summary": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.",
      "abstract": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3030072033",
      "arxiv_id": "",
      "publication_date": "2020-05-25",
      "published": "2020-05-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hierarchical Timbre-Painting and Articulation Generation",
      "summary": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
      "abstract": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
      "doi": "https://doi.org/10.48550/arxiv.2008.13095",
      "openalex_id": "https://openalex.org/W3082087924",
      "arxiv_id": "",
      "publication_date": "2020-08-30",
      "published": "2020-08-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation",
      "summary": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
      "abstract": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
      "doi": "https://doi.org/10.48550/arxiv.2106.06819",
      "openalex_id": "https://openalex.org/W3170636531",
      "arxiv_id": "",
      "publication_date": "2021-06-12",
      "published": "2021-06-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Source Separation via Bayesian Inference in the Latent Domain",
      "summary": "State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.",
      "abstract": "State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.",
      "doi": "https://doi.org/10.48550/arxiv.2110.05313",
      "openalex_id": "https://openalex.org/W3204992386",
      "arxiv_id": "",
      "publication_date": "2021-10-11",
      "published": "2021-10-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers",
      "summary": "Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.",
      "abstract": "Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.",
      "doi": "https://doi.org/10.48550/arxiv.2101.00169",
      "openalex_id": "https://openalex.org/W3120142428",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Neural Networks and End-to-End Learning for Audio Compression",
      "summary": "Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",
      "abstract": "Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",
      "doi": "https://doi.org/10.5626/jok.2021.48.8.940",
      "openalex_id": "https://openalex.org/W3163985612",
      "arxiv_id": "",
      "publication_date": "2021-08-31",
      "published": "2021-08-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral",
      "summary": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
      "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3170685585",
      "arxiv_id": "",
      "publication_date": "2021-06-15",
      "published": "2021-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PocketVAE: A Two-step Model for Groove Generation and Control",
      "summary": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.",
      "abstract": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.",
      "doi": "https://doi.org/10.48550/arxiv.2107.05009",
      "openalex_id": "https://openalex.org/W3178354449",
      "arxiv_id": "",
      "publication_date": "2021-07-11",
      "published": "2021-07-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BumbleBee: A Transformer for Music",
      "summary": "We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)",
      "abstract": "We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)",
      "doi": "https://doi.org/10.48550/arxiv.2107.03443",
      "openalex_id": "https://openalex.org/W3181311346",
      "arxiv_id": "",
      "publication_date": "2021-07-07",
      "published": "2021-07-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Musical Speech: A Transformer-based Composition Tool",
      "summary": "In this paper, we propose a new compositional tool that will generate a musical outline of speech recorded/provided by the user for use as a musical building block in their compositions. The tool allows any user to use their own speech to generate musical material, while still being able to hear the direct connection between their recorded speech and the resulting music. The tool is built on our proposed pipeline. This pipeline begins with speech-based signal processing, after which some simple musical heuristics are applied, and finally these pre-processed signals are passed through Transformer models trained on new musical tasks. We illustrate the effectiveness of our pipeline -- which does not require a paired dataset for training -- through examples of music created by musicians making use of our tool.",
      "abstract": "In this paper, we propose a new compositional tool that will generate a musical outline of speech recorded/provided by the user for use as a musical building block in their compositions. The tool allows any user to use their own speech to generate musical material, while still being able to hear the direct connection between their recorded speech and the resulting music. The tool is built on our proposed pipeline. This pipeline begins with speech-based signal processing, after which some simple musical heuristics are applied, and finally these pre-processed signals are passed through Transformer models trained on new musical tasks. We illustrate the effectiveness of our pipeline -- which does not require a paired dataset for training -- through examples of music created by musicians making use of our tool.",
      "doi": "https://doi.org/10.48550/arxiv.2108.01043",
      "openalex_id": "https://openalex.org/W3192483709",
      "arxiv_id": "",
      "publication_date": "2021-08-02",
      "published": "2021-08-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ownership and Creativity in Generative Models",
      "summary": "Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.",
      "abstract": "Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.",
      "doi": "https://doi.org/10.48550/arxiv.2112.01516",
      "openalex_id": "https://openalex.org/W3217025373",
      "arxiv_id": "",
      "publication_date": "2021-12-02",
      "published": "2021-12-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input",
      "summary": "Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.",
      "abstract": "Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.",
      "doi": "https://doi.org/10.1073/pnas.2001844118",
      "openalex_id": "https://openalex.org/W3125087428",
      "arxiv_id": "",
      "publication_date": "2021-01-28",
      "published": "2021-01-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Zero-Shot Learning for Automatic Phonemic Transcription",
      "summary": "Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.",
      "abstract": "Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6341",
      "openalex_id": "https://openalex.org/W2998284473",
      "arxiv_id": "",
      "publication_date": "2020-04-03",
      "published": "2020-04-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders",
      "summary": "Cory Shain, Micha Elsner. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
      "abstract": "Cory Shain, Micha Elsner. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
      "doi": "https://doi.org/10.18653/v1/n19-1007",
      "openalex_id": "https://openalex.org/W2945769669",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The zero resource speech challenge 2017",
      "summary": "We describe a new challenge aimed at discovering subword and word units from raw speech. This challenge is the followup to the Zero Resource Speech Challenge 2015. It aims at constructing systems that generalize across languages and adapt to new speakers. The design features and evaluation metrics of the challenge are presented and the results of seventeen models are discussed.",
      "abstract": "We describe a new challenge aimed at discovering subword and word units from raw speech. This challenge is the followup to the Zero Resource Speech Challenge 2015. It aims at constructing systems that generalize across languages and adapt to new speakers. The design features and evaluation metrics of the challenge are presented and the results of seventeen models are discussed.",
      "doi": "https://doi.org/10.1109/asru.2017.8268953",
      "openalex_id": "https://openalex.org/W2296607128",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
      "summary": "This study tackles unsupervised subword modeling in the zero-resource\\nscenario, learning frame-level speech representation that is phonetically\\ndiscriminative and speaker-invariant, using only untranscribed speech for\\ntarget languages. Frame label acquisition is an essential step in solving this\\nproblem. High quality frame labels should be in good consistency with golden\\ntranscriptions and robust to speaker variation. We propose to improve frame\\nlabel acquisition in our previously adopted deep neural network-bottleneck\\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\\nand speaker identity information encoded in speech. By discarding or unifying\\nspeaker information, speaker-invariant features are learned and fed as inputs\\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\\\%$ and $0.6\\\\%$\\nabsolute ABX error rate reductions in across- and within-speaker conditions,\\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\\napproaches significantly outperform vocal tract length normalization in\\nimproving frame labeling and subword modeling.\\n",
      "abstract": "This study tackles unsupervised subword modeling in the zero-resource\\nscenario, learning frame-level speech representation that is phonetically\\ndiscriminative and speaker-invariant, using only untranscribed speech for\\ntarget languages. Frame label acquisition is an essential step in solving this\\nproblem. High quality frame labels should be in good consistency with golden\\ntranscriptions and robust to speaker variation. We propose to improve frame\\nlabel acquisition in our previously adopted deep neural network-bottleneck\\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\\nand speaker identity information encoded in speech. By discarding or unifying\\nspeaker information, speaker-invariant features are learned and fed as inputs\\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\\\%$ and $0.6\\\\%$\\nabsolute ABX error rate reductions in across- and within-speaker conditions,\\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\\napproaches significantly outperform vocal tract length normalization in\\nimproving frame labeling and subword modeling.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-1338",
      "openalex_id": "https://openalex.org/W2949510815",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling",
      "summary": "This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n",
      "abstract": "This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n",
      "doi": "https://doi.org/10.1109/taslp.2019.2937953",
      "openalex_id": "https://openalex.org/W2971041032",
      "arxiv_id": "",
      "publication_date": "2019-08-28",
      "published": "2019-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Acquiring language from speech by learning to remember and predict",
      "summary": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.",
      "abstract": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.",
      "doi": "https://doi.org/10.18653/v1/2020.conll-1.15",
      "openalex_id": "https://openalex.org/W3102519966",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling",
      "summary": "We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.",
      "abstract": "We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.",
      "doi": "https://doi.org/10.1109/taslp.2018.2852500",
      "openalex_id": "https://openalex.org/W2810166208",
      "arxiv_id": "",
      "publication_date": "2018-07-02",
      "published": "2018-07-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
      "summary": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.",
      "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1664",
      "openalex_id": "https://openalex.org/W3145811386",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Feature Representation Learning using Sequence-to-sequence Autoencoder Architecture for Low-resource Language",
      "summary": "In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.",
      "abstract": "In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.",
      "doi": "https://doi.org/10.1109/ccai50917.2021.9447504",
      "openalex_id": "https://openalex.org/W3171005929",
      "arxiv_id": "",
      "publication_date": "2021-05-07",
      "published": "2021-05-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bayesian Subspace HMM for the Zerospeech 2020 Challenge",
      "summary": "In this paper we describe our submission to the Zerospeech 2020 challenge, where the participants are required to discover latent representations from unannotated speech, and to use those representations to perform speech synthesis, with synthesis quality used as a proxy metric for the unit quality. In our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit discovery. The SHMM models each unit as an HMM whose parameters are constrained to lie in a low dimensional subspace of the total parameter space which is trained to model phonetic variability. Our system compares favorably with the baseline on the human-evaluated character error rate while maintaining significantly lower unit bitrate.",
      "abstract": "In this paper we describe our submission to the Zerospeech 2020 challenge, where the participants are required to discover latent representations from unannotated speech, and to use those representations to perform speech synthesis, with synthesis quality used as a proxy metric for the unit quality. In our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit discovery. The SHMM models each unit as an HMM whose parameters are constrained to lie in a low dimensional subspace of the total parameter space which is trained to model phonetic variability. Our system compares favorably with the baseline on the human-evaluated character error rate while maintaining significantly lower unit bitrate.",
      "doi": "https://doi.org/10.48550/arxiv.2005.09282",
      "openalex_id": "https://openalex.org/W3026505300",
      "arxiv_id": "",
      "publication_date": "2020-05-19",
      "published": "2020-05-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages",
      "summary": "(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.",
      "abstract": "(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.",
      "doi": "https://doi.org/10.48550/arxiv.2007.15074",
      "openalex_id": "https://openalex.org/W3045592404",
      "arxiv_id": "",
      "publication_date": "2020-07-29",
      "published": "2020-07-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Spoken Term Discovery on Untranscribed Speech",
      "summary": "(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived \"phonemes\". The audio are labelled with these \"phonemes\" to obtain \"phoneme\" sequences. Unsupervised pattern discovery searches for repetitive patterns in the \"phoneme\" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.",
      "abstract": "(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived \"phonemes\". The audio are labelled with these \"phonemes\" to obtain \"phoneme\" sequences. Unsupervised pattern discovery searches for repetitive patterns in the \"phoneme\" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.",
      "doi": "https://doi.org/10.48550/arxiv.2011.14060",
      "openalex_id": "https://openalex.org/W3110585608",
      "arxiv_id": "",
      "publication_date": "2020-11-28",
      "published": "2020-11-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The effectiveness of self-supervised representation learning in zero-resource subword modeling",
      "summary": "For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.",
      "abstract": "For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.",
      "doi": "https://doi.org/10.1109/ieeeconf53345.2021.9723318",
      "openalex_id": "https://openalex.org/W4214942696",
      "arxiv_id": "",
      "publication_date": "2021-10-31",
      "published": "2021-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MIPAD: Mini Program Analysis for Clone Detection using Static Analysis Techniques",
      "summary": "In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.",
      "abstract": "In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.",
      "doi": "https://doi.org/10.1109/frse58934.2023.00052",
      "openalex_id": "https://openalex.org/W4386597471",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS",
      "summary": "End-to-end neural TTS training has shown improved performance in speech style transfer.However, the improvement is still limited by the training data in both target styles and speakers.Inadequate style transfer performance occurs when the trained TTS tries to transfer the speech to a target style from a new speaker with an unknown, arbitrary style.In this paper, we propose a new approach to style transfer for both seen and unseen styles, with disjoint, multi-style datasets, i.e., datasets of different styles are recorded, each individual style is by one speaker with multiple utterances.To encode the style information, we adopt an inverse autoregressive flow (IAF) structure to improve the variational inference.The whole system is optimized to minimize a weighed sum of four different loss functions: 1) a reconstruction loss to measure the distortions in both source and target reconstructions; 2) an adversarial loss to \"fool\" a well-trained discriminator; 3) a style distortion loss to measure the expected style loss after the transfer; 4) a cycle consistency loss to preserve the speaker identity of the source after the transfer.Experiments demonstrate, both objectively and subjectively, the effectiveness of the proposed approach for seen and unseen style transfer tasks.The performance of the new approach is better and more robust than those of four baseline systems of the prior art.",
      "abstract": "End-to-end neural TTS training has shown improved performance in speech style transfer.However, the improvement is still limited by the training data in both target styles and speakers.Inadequate style transfer performance occurs when the trained TTS tries to transfer the speech to a target style from a new speaker with an unknown, arbitrary style.In this paper, we propose a new approach to style transfer for both seen and unseen styles, with disjoint, multi-style datasets, i.e., datasets of different styles are recorded, each individual style is by one speaker with multiple utterances.To encode the style information, we adopt an inverse autoregressive flow (IAF) structure to improve the variational inference.The whole system is optimized to minimize a weighed sum of four different loss functions: 1) a reconstruction loss to measure the distortions in both source and target reconstructions; 2) an adversarial loss to \"fool\" a well-trained discriminator; 3) a style distortion loss to measure the expected style loss after the transfer; 4) a cycle consistency loss to preserve the speaker identity of the source after the transfer.Experiments demonstrate, both objectively and subjectively, the effectiveness of the proposed approach for seen and unseen style transfer tasks.The performance of the new approach is better and more robust than those of four baseline systems of the prior art.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1407",
      "openalex_id": "https://openalex.org/W3194613004",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Controlling Prosody in End-to-End TTS: A Case Study on Contrastive Focus Generation",
      "summary": "While End-2-End Text-to-Speech (TTS) has made significant progresses over the past few years, these systems still lack intuitive user controls over prosody. For instance, generating speech with fine-grained prosody control (prosodic prominence, contextually appropriate emotions) is still an open challenge. In this paper, we investigate whether we can control prosody directly from the input text, in order to code information related to contrastive focus which emphasizes a specific word that is contrary to the presuppositions of the interlocutor. We build and share a specific dataset for this purpose and show that it allows to train a TTS system were this fine-grained prosodic feature can be correctly conveyed using control tokens. Our evaluation compares synthetic and natural utterances and shows that prosodic patterns of contrastive focus (variations of Fo, Intensity and Duration) can be learnt accurately. Such a milestone is important to allow, for example, smart speakers to be programmatically controlled in terms of output prosody.",
      "abstract": "While End-2-End Text-to-Speech (TTS) has made significant progresses over the past few years, these systems still lack intuitive user controls over prosody. For instance, generating speech with fine-grained prosody control (prosodic prominence, contextually appropriate emotions) is still an open challenge. In this paper, we investigate whether we can control prosody directly from the input text, in order to code information related to contrastive focus which emphasizes a specific word that is contrary to the presuppositions of the interlocutor. We build and share a specific dataset for this purpose and show that it allows to train a TTS system were this fine-grained prosodic feature can be correctly conveyed using control tokens. Our evaluation compares synthetic and natural utterances and shows that prosodic patterns of contrastive focus (variations of Fo, Intensity and Duration) can be learnt accurately. Such a milestone is important to allow, for example, smart speakers to be programmatically controlled in terms of output prosody.",
      "doi": "https://doi.org/10.18653/v1/2021.conll-1.42",
      "openalex_id": "https://openalex.org/W3214634826",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Prosodic Clustering for Phoneme-Level Prosody Control in End-to-End Speech Synthesis",
      "summary": "This paper presents a method for controlling the prosody at the phoneme level\\nin an autoregressive attention-based text-to-speech system. Instead of learning\\nlatent prosodic features with a variational framework as is commonly done, we\\ndirectly extract phoneme-level F0 and duration features from the speech data in\\nthe training set. Each prosodic feature is discretized using unsupervised\\nclustering in order to produce a sequence of prosodic labels for each\\nutterance. This sequence is used in parallel to the phoneme sequence in order\\nto condition the decoder with the utilization of a prosodic encoder and a\\ncorresponding attention module. Experimental results show that the proposed\\nmethod retains the high quality of generated speech, while allowing\\nphoneme-level control of F0 and duration. By replacing the F0 cluster centroids\\nwith musical notes, the model can also provide control over the note and octave\\nwithin the range of the speaker.\\n",
      "abstract": "This paper presents a method for controlling the prosody at the phoneme level\\nin an autoregressive attention-based text-to-speech system. Instead of learning\\nlatent prosodic features with a variational framework as is commonly done, we\\ndirectly extract phoneme-level F0 and duration features from the speech data in\\nthe training set. Each prosodic feature is discretized using unsupervised\\nclustering in order to produce a sequence of prosodic labels for each\\nutterance. This sequence is used in parallel to the phoneme sequence in order\\nto condition the decoder with the utilization of a prosodic encoder and a\\ncorresponding attention module. Experimental results show that the proposed\\nmethod retains the high quality of generated speech, while allowing\\nphoneme-level control of F0 and duration. By replacing the F0 cluster centroids\\nwith musical notes, the model can also provide control over the note and octave\\nwithin the range of the speaker.\\n",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9413604",
      "openalex_id": "https://openalex.org/W3163003432",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech",
      "summary": "Yang Li, Cheng Yu, Guangzhi Sun, Hua Jiang, Fanglei Sun, Weiqin Zu, Ying Wen, Yang Yang, Jun Wang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Yang Li, Cheng Yu, Guangzhi Sun, Hua Jiang, Fanglei Sun, Weiqin Zu, Ying Wen, Yang Yang, Jun Wang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.30",
      "openalex_id": "https://openalex.org/W4280604450",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GAN-Based Fine-Grained Feature Modeling For Zero-Shot Voice Cloning",
      "summary": "With the continuous development of deep learning and speech signal processing, speech synthesis technology has greatly improved in naturalness and comprehensibility, and many application technologies such as artificial intelligence voice assistant and personalized navigation have been widely used in real life, and the demand for personalized speech synthesis is increasing.Personalized speech synthesis requires models that can achieve speech timbre migration, also known as speech reproduction, with only a small number of target speaker speech samples.However, since human speech is highly expressive and contains rich information, including speaker identity information, prosody, rhythm, emotion and other factors, the limited speech data will lead to poor similarity and rhythmic performance of the model-generated speech, and the model needs to be fine-tuned to improve the quality of the synthesized speech.Therefore, personalized speech synthesis with few samples is a very challenging task.To achieve the goal of speech cloning, this paper proposes a personalized speech synthesis method based on FastSpeech2.By using fine-grained feature modeling module containing prosody extractor and prosody predictor, and a training strategy based on Generative adversarial network (GAN) and meta-learning, it is realized that personalized speech with high similarity and naturalness can be generated with a very short reference audio.The subjective and objective experiments also demonstrate that the model proposed in this paper can achieve high quality speech replication without fine-tuning the model under a few or even a single reference audio of the target speaker.",
      "abstract": "With the continuous development of deep learning and speech signal processing, speech synthesis technology has greatly improved in naturalness and comprehensibility, and many application technologies such as artificial intelligence voice assistant and personalized navigation have been widely used in real life, and the demand for personalized speech synthesis is increasing.Personalized speech synthesis requires models that can achieve speech timbre migration, also known as speech reproduction, with only a small number of target speaker speech samples.However, since human speech is highly expressive and contains rich information, including speaker identity information, prosody, rhythm, emotion and other factors, the limited speech data will lead to poor similarity and rhythmic performance of the model-generated speech, and the model needs to be fine-tuned to improve the quality of the synthesized speech.Therefore, personalized speech synthesis with few samples is a very challenging task.To achieve the goal of speech cloning, this paper proposes a personalized speech synthesis method based on FastSpeech2.By using fine-grained feature modeling module containing prosody extractor and prosody predictor, and a training strategy based on Generative adversarial network (GAN) and meta-learning, it is realized that personalized speech with high similarity and naturalness can be generated with a very short reference audio.The subjective and objective experiments also demonstrate that the model proposed in this paper can achieve high quality speech replication without fine-tuning the model under a few or even a single reference audio of the target speaker.",
      "doi": "https://doi.org/10.11159/mhci23.111",
      "openalex_id": "https://openalex.org/W4386074659",
      "arxiv_id": "",
      "publication_date": "2023-08-01",
      "published": "2023-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Modified Magnitude-Phase Spectrum Information for Spoofing Detection",
      "summary": "Most of the existing feature representations for spoofing countermeasures consider information either from the magnitude or phase spectrum. We hypothesize that both magnitude and phase spectra can be beneficial for spoofing detection (SD) when collectively used to capture the signal artifacts. In this work, we propose a novel feature referred to as modified magnitude-phase spectrum (MMPS) to capture both magnitude and phase information from the speech signal. The constant-Q transform is used to obtain the magnitude and phase information in terms of MMPS, which can be denoted as CQT-MMPS. We then use this information for the proposal of a handcrafted feature, namely, constant-Q modified octave coefficients (CQMOC). To evaluate the proposed CQT-MMPS and CQMOC features, three classic anti-spoofing models are adopted, including the Gaussian mixture model (GMM), the light CNN (LCNN) and the ResNet. Additionally, since there is usually no prior knowledge about the spoofing kind in real-world applications, two novel methods referred to as three-class classifiers with maximum spoofing-score (TCMS) and multi-task learning (MTL) are designed for unknown-kind SD (UKSD). The experimental results on ASVspoof 2019 corpus show that CQMOC outperforms most of the commonly-used handcrafted features, and the CQT-based MMPS performs better than the magnitude-phase spectrum and the commonly-used log power spectrum. Further, the MMPS-based systems can achieve comparable or even better performance when compared with the state-of-the-art systems. We find that the newly-designed TCMS and MTL methods outperform the combination-based method for UKSD and meanwhile, generalize much better than the respective-kind-based methods in cross-spoofing-kind evaluation scenarios.",
      "abstract": "Most of the existing feature representations for spoofing countermeasures consider information either from the magnitude or phase spectrum. We hypothesize that both magnitude and phase spectra can be beneficial for spoofing detection (SD) when collectively used to capture the signal artifacts. In this work, we propose a novel feature referred to as modified magnitude-phase spectrum (MMPS) to capture both magnitude and phase information from the speech signal. The constant-Q transform is used to obtain the magnitude and phase information in terms of MMPS, which can be denoted as CQT-MMPS. We then use this information for the proposal of a handcrafted feature, namely, constant-Q modified octave coefficients (CQMOC). To evaluate the proposed CQT-MMPS and CQMOC features, three classic anti-spoofing models are adopted, including the Gaussian mixture model (GMM), the light CNN (LCNN) and the ResNet. Additionally, since there is usually no prior knowledge about the spoofing kind in real-world applications, two novel methods referred to as three-class classifiers with maximum spoofing-score (TCMS) and multi-task learning (MTL) are designed for unknown-kind SD (UKSD). The experimental results on ASVspoof 2019 corpus show that CQMOC outperforms most of the commonly-used handcrafted features, and the CQT-based MMPS performs better than the magnitude-phase spectrum and the commonly-used log power spectrum. Further, the MMPS-based systems can achieve comparable or even better performance when compared with the state-of-the-art systems. We find that the newly-designed TCMS and MTL methods outperform the combination-based method for UKSD and meanwhile, generalize much better than the respective-kind-based methods in cross-spoofing-kind evaluation scenarios.",
      "doi": "https://doi.org/10.1109/taslp.2021.3060810",
      "openalex_id": "https://openalex.org/W3132550238",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weakly-Supervised Semantic Segmentation for Histopathology Images Based on Dataset Synthesis and Feature Consistency Constraint",
      "summary": "Tissue segmentation is a critical task in computational pathology due to its desirable ability to indicate the prognosis of cancer patients. Currently, numerous studies attempt to use image-level labels to achieve pixel-level segmentation to reduce the need for fine annotations. However, most of these methods are based on class activation map, which suffers from inaccurate segmentation boundaries. To address this problem, we propose a novel weakly-supervised tissue segmentation framework named PistoSeg, which is implemented under a fully-supervised manner by transferring tissue category labels to pixel-level masks. Firstly, a dataset synthesis method is proposed based on Mosaic transformation to generate synthesized images with pixel-level masks. Next, considering the difference between synthesized and real images, this paper devises an attention-based feature consistency, which directs the training process of a proposed pseudo-mask refining module. Finally, the refined pseudo-masks are used to train a precise segmentation model for testing. Experiments based on WSSS4LUAD and BCSS-WSSS validate that PistoSeg outperforms the state-of-the-art methods. The code is released at https://github.com/Vison307/PistoSeg.",
      "abstract": "Tissue segmentation is a critical task in computational pathology due to its desirable ability to indicate the prognosis of cancer patients. Currently, numerous studies attempt to use image-level labels to achieve pixel-level segmentation to reduce the need for fine annotations. However, most of these methods are based on class activation map, which suffers from inaccurate segmentation boundaries. To address this problem, we propose a novel weakly-supervised tissue segmentation framework named PistoSeg, which is implemented under a fully-supervised manner by transferring tissue category labels to pixel-level masks. Firstly, a dataset synthesis method is proposed based on Mosaic transformation to generate synthesized images with pixel-level masks. Next, considering the difference between synthesized and real images, this paper devises an attention-based feature consistency, which directs the training process of a proposed pseudo-mask refining module. Finally, the refined pseudo-masks are used to train a precise segmentation model for testing. Experiments based on WSSS4LUAD and BCSS-WSSS validate that PistoSeg outperforms the state-of-the-art methods. The code is released at https://github.com/Vison307/PistoSeg.",
      "doi": "https://doi.org/10.1609/aaai.v37i1.25136",
      "openalex_id": "https://openalex.org/W4382240846",
      "arxiv_id": "",
      "publication_date": "2023-06-26",
      "published": "2023-06-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Machine Learning in Short-Reach Optical Systems: A Comprehensive Survey",
      "summary": "Recently, extensive research has been conducted to explore the utilization of machine learning (ML) algorithms in various direct-detected and (self)-coherent short-reach communication applications. These applications encompass a wide range of tasks, including bandwidth request prediction, signal quality monitoring, fault detection, traffic prediction, and digital signal processing (DSP)-based equalization. As a versatile approach, ML demonstrates the ability to address stochastic phenomena in optical systems networks where deterministic methods may fall short. However, when it comes to DSP equalization algorithms such as feed-forward/decision-feedback equalizers (FFEs/DFEs) and Volterra-based nonlinear equalizers, their performance improvements are often marginal, and their complexity is prohibitively high, especially in cost-sensitive short-reach communications scenarios such as passive optical networks (PONs). Time-series ML models offer distinct advantages over frequency-domain models in specific contexts. They excel in capturing temporal dependencies, handling irregular or nonlinear patterns effectively, and accommodating variable time intervals. Within this survey, we outline the application of ML techniques in short-reach communications, specifically emphasizing their utilization in high-bandwidth demanding PONs. We introduce a novel taxonomy for time-series methods employed in ML signal processing, providing a structured classification framework. Our taxonomy categorizes current time-series methods into four distinct groups: traditional methods, Fourier convolution-based methods, transformer-based models, and time-series convolutional networks. Finally, we highlight prospective research directions within this rapidly evolving field and outline specific solutions to mitigate the complexity associated with hardware implementations. We aim to pave the way for more practical and efficient deployment of ML approaches in short-reach optical communication systems by addressing complexity concerns.",
      "abstract": "Recently, extensive research has been conducted to explore the utilization of machine learning (ML) algorithms in various direct-detected and (self)-coherent short-reach communication applications. These applications encompass a wide range of tasks, including bandwidth request prediction, signal quality monitoring, fault detection, traffic prediction, and digital signal processing (DSP)-based equalization. As a versatile approach, ML demonstrates the ability to address stochastic phenomena in optical systems networks where deterministic methods may fall short. However, when it comes to DSP equalization algorithms such as feed-forward/decision-feedback equalizers (FFEs/DFEs) and Volterra-based nonlinear equalizers, their performance improvements are often marginal, and their complexity is prohibitively high, especially in cost-sensitive short-reach communications scenarios such as passive optical networks (PONs). Time-series ML models offer distinct advantages over frequency-domain models in specific contexts. They excel in capturing temporal dependencies, handling irregular or nonlinear patterns effectively, and accommodating variable time intervals. Within this survey, we outline the application of ML techniques in short-reach communications, specifically emphasizing their utilization in high-bandwidth demanding PONs. We introduce a novel taxonomy for time-series methods employed in ML signal processing, providing a structured classification framework. Our taxonomy categorizes current time-series methods into four distinct groups: traditional methods, Fourier convolution-based methods, transformer-based models, and time-series convolutional networks. Finally, we highlight prospective research directions within this rapidly evolving field and outline specific solutions to mitigate the complexity associated with hardware implementations. We aim to pave the way for more practical and efficient deployment of ML approaches in short-reach optical communication systems by addressing complexity concerns.",
      "doi": "https://doi.org/10.3390/photonics11070613",
      "openalex_id": "https://openalex.org/W4400235716",
      "arxiv_id": "",
      "publication_date": "2024-06-28",
      "published": "2024-06-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BDTwin: An Integrated Framework for Enhancing Security and Privacy in Cybertwin-Driven Automotive Industrial Internet of Things",
      "summary": "The rapid development of the automotive Industrial Internet of Things requires secure networking infrastructure toward digitalization. Cybertwin (CT) is a next-generation networking architecture that serves as a communication, and digital asset owner, and can make the Vehicle-to-Everything (V2X) network flexible and secure. However, CT itself can publish end users’ digital assets to other entities as a service, making data security and privacy major obstacles in the realization of V2X applications. Motivated from the aforementioned discussion, this article presents BDTwin, a blockchain and deep-learning-based integrated framework to enhance security and privacy in CT-driven V2X applications. Specifically, a blockchain scheme is designed to ensure secure communication among vehicles, roadside units, CT-edge server, and cloud server using a smart contract-based enhance-Proof-of-Work (ePoW) and Zero Knowledge Proof (ZKP)-based verification process. Smart contracts are used to enforce rules and regulations that govern the behavior of V2X entities in a nondeniable and automated manner. In a deep-learning scheme, an autoregressive-deep variational autoencoder model is combined with attention-based bidirectional long short-term memory (A-BLSTM) for automatic feature extraction and attack detection by analyzing CT-edge servers data in a V2X environment. Security analysis and experimental results using two different sources, ToN-IoT and CICIDS-2017 show the superiority of the proposed BDTwin framework over some baseline and recent state-of-the-art techniques.",
      "abstract": "The rapid development of the automotive Industrial Internet of Things requires secure networking infrastructure toward digitalization. Cybertwin (CT) is a next-generation networking architecture that serves as a communication, and digital asset owner, and can make the Vehicle-to-Everything (V2X) network flexible and secure. However, CT itself can publish end users’ digital assets to other entities as a service, making data security and privacy major obstacles in the realization of V2X applications. Motivated from the aforementioned discussion, this article presents BDTwin, a blockchain and deep-learning-based integrated framework to enhance security and privacy in CT-driven V2X applications. Specifically, a blockchain scheme is designed to ensure secure communication among vehicles, roadside units, CT-edge server, and cloud server using a smart contract-based enhance-Proof-of-Work (ePoW) and Zero Knowledge Proof (ZKP)-based verification process. Smart contracts are used to enforce rules and regulations that govern the behavior of V2X entities in a nondeniable and automated manner. In a deep-learning scheme, an autoregressive-deep variational autoencoder model is combined with attention-based bidirectional long short-term memory (A-BLSTM) for automatic feature extraction and attack detection by analyzing CT-edge servers data in a V2X environment. Security analysis and experimental results using two different sources, ToN-IoT and CICIDS-2017 show the superiority of the proposed BDTwin framework over some baseline and recent state-of-the-art techniques.",
      "doi": "https://doi.org/10.1109/jiot.2021.3122021",
      "openalex_id": "https://openalex.org/W4205548770",
      "arxiv_id": "",
      "publication_date": "2021-12-14",
      "published": "2021-12-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phase-Aware Spoof Speech Detection Based On Res2net with Phase Network",
      "summary": "For automatic speaker verification systems, spoof speech detection (SSD) is an essential countermeasure. Although SSD with magnitude features in the frequency domain has shown promising results, phase information can also be useful in capturing the artefacts of certain spoofing attacks. Thus, both magnitude and phase features must be considered to ensure the ability to generalize diverse types of spoofing attacks. In this study, we discovered that the randomness difference between magnitude and phase features is large, which can interrupt the feature-level fusion via backend neural network. In this regard, we propose a phase network to reduce that difference, which makes the Res2Net-based feature-level fusion feasible. To validate our SSD system for practical environment, both known- and unknown-type SSD scenarios are considered. As a result, our SSD system delivers competitive results compared to other state-of-the-art SSD systems in all scenarios.",
      "abstract": "For automatic speaker verification systems, spoof speech detection (SSD) is an essential countermeasure. Although SSD with magnitude features in the frequency domain has shown promising results, phase information can also be useful in capturing the artefacts of certain spoofing attacks. Thus, both magnitude and phase features must be considered to ensure the ability to generalize diverse types of spoofing attacks. In this study, we discovered that the randomness difference between magnitude and phase features is large, which can interrupt the feature-level fusion via backend neural network. In this regard, we propose a phase network to reduce that difference, which makes the Res2Net-based feature-level fusion feasible. To validate our SSD system for practical environment, both known- and unknown-type SSD scenarios are considered. As a result, our SSD system delivers competitive results compared to other state-of-the-art SSD systems in all scenarios.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096672",
      "openalex_id": "https://openalex.org/W4375869011",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perception of prosodic variation for speech synthesis using an unsupervised discrete representation of F0",
      "summary": "In English, prosody adds a broad range of information to segment sequences,\\nfrom information structure (e.g. contrast) to stylistic variation (e.g.\\nexpression of emotion). However, when learning to control prosody in\\ntext-to-speech voices, it is not clear what exactly the control is modifying.\\nExisting research on discrete representation learning for prosody has\\ndemonstrated high naturalness, but no analysis has been performed on what these\\nrepresentations capture, or if they can generate meaningfully-distinct variants\\nof an utterance. We present a phrase-level variational autoencoder with a\\nmulti-modal prior, using the mode centres as \"intonation codes\". Our evaluation\\nestablishes which intonation codes are perceptually distinct, finding that the\\nintonation codes from our multi-modal latent model were significantly more\\ndistinct than a baseline using k-means clustering. We carry out a follow-up\\nqualitative study to determine what information the codes are carrying. Most\\ncommonly, listeners commented on the intonation codes having a statement or\\nquestion style. However, many other affect-related styles were also reported,\\nincluding: emotional, uncertain, surprised, sarcastic, passive aggressive, and\\nupset.\\n",
      "abstract": "In English, prosody adds a broad range of information to segment sequences,\\nfrom information structure (e.g. contrast) to stylistic variation (e.g.\\nexpression of emotion). However, when learning to control prosody in\\ntext-to-speech voices, it is not clear what exactly the control is modifying.\\nExisting research on discrete representation learning for prosody has\\ndemonstrated high naturalness, but no analysis has been performed on what these\\nrepresentations capture, or if they can generate meaningfully-distinct variants\\nof an utterance. We present a phrase-level variational autoencoder with a\\nmulti-modal prior, using the mode centres as \"intonation codes\". Our evaluation\\nestablishes which intonation codes are perceptually distinct, finding that the\\nintonation codes from our multi-modal latent model were significantly more\\ndistinct than a baseline using k-means clustering. We carry out a follow-up\\nqualitative study to determine what information the codes are carrying. Most\\ncommonly, listeners commented on the intonation codes having a statement or\\nquestion style. However, many other affect-related styles were also reported,\\nincluding: emotional, uncertain, surprised, sarcastic, passive aggressive, and\\nupset.\\n",
      "doi": "https://doi.org/10.21437/speechprosody.2020-197",
      "openalex_id": "https://openalex.org/W3026278778",
      "arxiv_id": "",
      "publication_date": "2020-05-21",
      "published": "2020-05-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-deterministic and Contrastive Variational Graph Autoencoder for Recommendation",
      "summary": "Variational AutoEncoder (VAE) is a popular deep generative framework with a solid theoretical basis. There are many research efforts on improving VAE. Among the existing works, a recently proposed deterministic Regularized AutoEncoder (RAE) provides a new scheme for generative modeling. RAE fixes the variance of the inferred Gaussian approximate posterior distribution as a hyperparameter, and substitutes the stochastic encoder by injecting noise into the input of a deterministic decoder. However, the deterministic RAE has three limitations: 1) RAE needs to fit the variance; 2) RAE requires ex-post density estimation to ensure sample quality; 3) RAE employs an additional gradient regularization to ensure training smoothness. Thus, it raises an interesting research question: Can we maintain the flexibility of variational inference while simplifying VAE, and at the same time ensuring a smooth training process to obtain good generative performance? Based on the above motivation, in this paper, we propose a novel Semi-deterministic and Contrastive Variational Graph autoencoder (SCVG) for item recommendation. The core design of SCVG is to learn the variance of the approximate Gaussian posterior distribution in a semi-deterministic manner by aggregating inferred mean vectors from other connected nodes via graph convolution operation. We analyze the expressive power of SCVG for the Weisfeiler-Lehman graph isomorphism test, and we deduce the simplified form of the evidence lower bound of SCVG. Besides, we introduce an efficient contrastive regularization instead of gradient regularization. We empirically show that the contrastive regularization makes learned user/item latent representation more personalized and helps to smooth the training process. We conduct extensive experiments on three real-world datasets to show the superiority of our model over state-of-the-art methods for the item recommendation task. Codes are available at https://github.com/syxkason/SCVG.",
      "abstract": "Variational AutoEncoder (VAE) is a popular deep generative framework with a solid theoretical basis. There are many research efforts on improving VAE. Among the existing works, a recently proposed deterministic Regularized AutoEncoder (RAE) provides a new scheme for generative modeling. RAE fixes the variance of the inferred Gaussian approximate posterior distribution as a hyperparameter, and substitutes the stochastic encoder by injecting noise into the input of a deterministic decoder. However, the deterministic RAE has three limitations: 1) RAE needs to fit the variance; 2) RAE requires ex-post density estimation to ensure sample quality; 3) RAE employs an additional gradient regularization to ensure training smoothness. Thus, it raises an interesting research question: Can we maintain the flexibility of variational inference while simplifying VAE, and at the same time ensuring a smooth training process to obtain good generative performance? Based on the above motivation, in this paper, we propose a novel Semi-deterministic and Contrastive Variational Graph autoencoder (SCVG) for item recommendation. The core design of SCVG is to learn the variance of the approximate Gaussian posterior distribution in a semi-deterministic manner by aggregating inferred mean vectors from other connected nodes via graph convolution operation. We analyze the expressive power of SCVG for the Weisfeiler-Lehman graph isomorphism test, and we deduce the simplified form of the evidence lower bound of SCVG. Besides, we introduce an efficient contrastive regularization instead of gradient regularization. We empirically show that the contrastive regularization makes learned user/item latent representation more personalized and helps to smooth the training process. We conduct extensive experiments on three real-world datasets to show the superiority of our model over state-of-the-art methods for the item recommendation task. Codes are available at https://github.com/syxkason/SCVG.",
      "doi": "https://doi.org/10.1145/3459637.3482390",
      "openalex_id": "https://openalex.org/W3209495512",
      "arxiv_id": "",
      "publication_date": "2021-10-26",
      "published": "2021-10-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ensemble Prosody Prediction For Expressive Speech Synthesis",
      "summary": "Generating expressive speech with rich and varied prosody continues to be a challenge for Text-to-Speech. Most efforts have focused on sophisticated neural architectures intended to better model the data distribution. Yet, in evaluations it is generally found that no single model is preferred for all input texts. This suggests an approach that has rarely been used before for Text-to-Speech: an ensemble of models.We apply ensemble learning to prosody prediction. We construct simple ensembles of prosody predictors by varying either model architecture or model parameter values.To automatically select amongst the models in the ensemble when performing Text-to-Speech, we propose a novel, and computationally trivial, variance-based criterion. We demonstrate that even a small ensemble of prosody predictors yields useful diversity, which, combined with the proposed selection criterion, outperforms any individual model from the ensemble.",
      "abstract": "Generating expressive speech with rich and varied prosody continues to be a challenge for Text-to-Speech. Most efforts have focused on sophisticated neural architectures intended to better model the data distribution. Yet, in evaluations it is generally found that no single model is preferred for all input texts. This suggests an approach that has rarely been used before for Text-to-Speech: an ensemble of models.We apply ensemble learning to prosody prediction. We construct simple ensembles of prosody predictors by varying either model architecture or model parameter values.To automatically select amongst the models in the ensemble when performing Text-to-Speech, we propose a novel, and computationally trivial, variance-based criterion. We demonstrate that even a small ensemble of prosody predictors yields useful diversity, which, combined with the proposed selection criterion, outperforms any individual model from the ensemble.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096962",
      "openalex_id": "https://openalex.org/W4372260289",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Strata-NeRF : Neural Radiance Fields for Stratified Scenes",
      "summary": "Neural Radiance Field (NeRF) approaches learn the underlying 3D representation of a scene and generate photo-realistic novel views with high fidelity. However, most proposed settings concentrate on modelling a single object or a single level of a scene. However, in the real world, we may capture a scene at multiple levels, resulting in a layered capture. For example, tourists usually capture a monument's exterior structure before capturing the inner structure. Modelling such scenes in 3D with seamless switching between levels can drastically improve immersive experiences. However, most existing techniques struggle in modelling such scenes. We propose Strata-NeRF, a single neural radiance field that implicitly captures a scene with multiple levels. Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ) latent representations which allow sudden changes in scene structure. We evaluate the effectiveness of our approach in multi-layered synthetic dataset comprising diverse scenes and then further validate its generalization on the real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures stratified scenes, minimizes artifacts, and synthesizes high-fidelity views compared to existing approaches. https://ankitatiisc.github.io/Strata-NeRF/",
      "abstract": "Neural Radiance Field (NeRF) approaches learn the underlying 3D representation of a scene and generate photo-realistic novel views with high fidelity. However, most proposed settings concentrate on modelling a single object or a single level of a scene. However, in the real world, we may capture a scene at multiple levels, resulting in a layered capture. For example, tourists usually capture a monument's exterior structure before capturing the inner structure. Modelling such scenes in 3D with seamless switching between levels can drastically improve immersive experiences. However, most existing techniques struggle in modelling such scenes. We propose Strata-NeRF, a single neural radiance field that implicitly captures a scene with multiple levels. Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ) latent representations which allow sudden changes in scene structure. We evaluate the effectiveness of our approach in multi-layered synthetic dataset comprising diverse scenes and then further validate its generalization on the real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures stratified scenes, minimizes artifacts, and synthesizes high-fidelity views compared to existing approaches. https://ankitatiisc.github.io/Strata-NeRF/",
      "doi": "https://doi.org/10.1109/iccv51070.2023.01614",
      "openalex_id": "https://openalex.org/W4390874236",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lyrics to Music Generator: Statistical Approach",
      "summary": "Natural Language Processing is in growing demand with recent developments. This Generator model is one such example of a music generation system conditioned on lyrics. The model proposed has been tested on songs having lyrics written only in English, but the idea can be generalized to various languages. This paper’s objective is to mainly explain how one can create a music generator using statistical machine learning methods. This paper also explains how effectively outputs can be formulated, which are the music signals as they are million sized over a short period frame. The parameters mentioned in the paper only serve an explanatory purpose. This paper discusses the effective statistical formulation of output thereby decreasing the vast amount of estimation of output parameters, and how to reconstruct the audio signals from predicted parameters by using ‘phase-shift algorithm’.",
      "abstract": "Natural Language Processing is in growing demand with recent developments. This Generator model is one such example of a music generation system conditioned on lyrics. The model proposed has been tested on songs having lyrics written only in English, but the idea can be generalized to various languages. This paper’s objective is to mainly explain how one can create a music generator using statistical machine learning methods. This paper also explains how effectively outputs can be formulated, which are the music signals as they are million sized over a short period frame. The parameters mentioned in the paper only serve an explanatory purpose. This paper discusses the effective statistical formulation of output thereby decreasing the vast amount of estimation of output parameters, and how to reconstruct the audio signals from predicted parameters by using ‘phase-shift algorithm’.",
      "doi": "https://doi.org/10.5121/csit.2021.111209",
      "openalex_id": "https://openalex.org/W3196232151",
      "arxiv_id": "",
      "publication_date": "2021-08-21",
      "published": "2021-08-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Signal Factorization for Speech: Identity, Content, and Style",
      "summary": "Preliminary experiments in this dissertation show that it is possible to factorize specific types of information from the speech signal in an abstract embedding space using machine learning. This information includes characteristics of the recording environment, speaking style, and speech quality. Based on these findings, a new technique is proposed to factorize multiple types of information from the speech signal simultaneously using a combination of state-of-the-art machine learning methods for speech processing. Successful speech signal factorization will lead to advances across many speech technologies, including improved speaker identification, detection of speech audio deep fakes, and controllable expression in speech synthesis.",
      "abstract": "Preliminary experiments in this dissertation show that it is possible to factorize specific types of information from the speech signal in an abstract embedding space using machine learning. This information includes characteristics of the recording environment, speaking style, and speech quality. Based on these findings, a new technique is proposed to factorize multiple types of information from the speech signal simultaneously using a combination of state-of-the-art machine learning methods for speech processing. Successful speech signal factorization will lead to advances across many speech technologies, including improved speaker identification, detection of speech audio deep fakes, and controllable expression in speech synthesis.",
      "doi": "https://doi.org/10.24963/ijcai.2020/746",
      "openalex_id": "https://openalex.org/W3041694242",
      "arxiv_id": "",
      "publication_date": "2020-07-01",
      "published": "2020-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A survey on deep learning based reenactment methods for deepfake applications",
      "summary": "Abstract Among the sectors that deep learning has transformed, deepfake, a novel method of manipulating multimedia, deserves particular attention. The long‐term objective of many researchers is to seamlessly mimic human facial movement or whole‐body activity, referred to as reenactment. Deepfake progress has made this goal much more feasible in recent years. Yet, achieving more realistic facial and body reenactment remains a challenging task. The primary focus of this study is to explore the current capability of the reenactment techniques and expand them further to attain greater results. The analysis offers a thorough overview of the various techniques involved, the challenges addressed, the datasets utilized, and the metrics employed by the underlying methods of reenactment technologies. The study also addresses the potential risks and their mitigating strategies to ensure responsible reenactment techniques. To the best of the authors' knowledge, this is the first survey paper that delves deeper into the topic of deepfake reenactment.",
      "abstract": "Abstract Among the sectors that deep learning has transformed, deepfake, a novel method of manipulating multimedia, deserves particular attention. The long‐term objective of many researchers is to seamlessly mimic human facial movement or whole‐body activity, referred to as reenactment. Deepfake progress has made this goal much more feasible in recent years. Yet, achieving more realistic facial and body reenactment remains a challenging task. The primary focus of this study is to explore the current capability of the reenactment techniques and expand them further to attain greater results. The analysis offers a thorough overview of the various techniques involved, the challenges addressed, the datasets utilized, and the metrics employed by the underlying methods of reenactment technologies. The study also addresses the potential risks and their mitigating strategies to ensure responsible reenactment techniques. To the best of the authors' knowledge, this is the first survey paper that delves deeper into the topic of deepfake reenactment.",
      "doi": "https://doi.org/10.1049/ipr2.13201",
      "openalex_id": "https://openalex.org/W4401704640",
      "arxiv_id": "",
      "publication_date": "2024-08-19",
      "published": "2024-08-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EDG‐CDM: A New Encoder‐Guided Conditional Diffusion Model‐Based Image Synthesis Method for Limited Data",
      "summary": "ABSTRACT The Diffusion Probabilistic Model (DM) has emerged as a powerful generative model in the field of image synthesis, capable of producing high‐quality and realistic images. However, training DM requires a large and diverse dataset, which can be challenging to obtain. This limitation weakens the model's generalisation and robustness when training data is limited. To address this issue, EDG‐CDM, an innovative encoder‐guided conditional diffusion model was proposed for image synthesis with limited data. Firstly, the authors pre‐train the encoder by introducing noise to capture the distribution of image features and generate the condition vector through contrastive learning and KL divergence. Next, the encoder undergoes further training with classification to integrate image class information, providing more favourable and versatile conditions for the diffusion model. Subsequently, the encoder is connected to the diffusion model, which is trained using all available data with encoder‐provided conditions. Finally, the authors evaluate EDG‐CDM on various public datasets with limited data, conducting extensive experiments and comparing our results with state‐of‐the‐art methods using metrics such as Fréchet Inception Distance and Inception Score. Our experiments demonstrate that EDG‐CDM outperforms existing models by consistently achieving the lowest FID scores and the highest IS scores, highlighting its effectiveness in generating high‐quality and diverse images with limited training data. These results underscore the significance of EDG‐CDM in advancing image synthesis techniques under data‐constrained scenarios.",
      "abstract": "ABSTRACT The Diffusion Probabilistic Model (DM) has emerged as a powerful generative model in the field of image synthesis, capable of producing high‐quality and realistic images. However, training DM requires a large and diverse dataset, which can be challenging to obtain. This limitation weakens the model's generalisation and robustness when training data is limited. To address this issue, EDG‐CDM, an innovative encoder‐guided conditional diffusion model was proposed for image synthesis with limited data. Firstly, the authors pre‐train the encoder by introducing noise to capture the distribution of image features and generate the condition vector through contrastive learning and KL divergence. Next, the encoder undergoes further training with classification to integrate image class information, providing more favourable and versatile conditions for the diffusion model. Subsequently, the encoder is connected to the diffusion model, which is trained using all available data with encoder‐provided conditions. Finally, the authors evaluate EDG‐CDM on various public datasets with limited data, conducting extensive experiments and comparing our results with state‐of‐the‐art methods using metrics such as Fréchet Inception Distance and Inception Score. Our experiments demonstrate that EDG‐CDM outperforms existing models by consistently achieving the lowest FID scores and the highest IS scores, highlighting its effectiveness in generating high‐quality and diverse images with limited training data. These results underscore the significance of EDG‐CDM in advancing image synthesis techniques under data‐constrained scenarios.",
      "doi": "https://doi.org/10.1049/cvi2.70018",
      "openalex_id": "https://openalex.org/W4409309516",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ten years of generative adversarial nets (GANs): a survey of the state-of-the-art",
      "summary": "Abstract Generative adversarial networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas, since their inception in 2014. Consisting of a discriminative network and a generative network engaged in a minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ‘Top Ten Global Breakthrough Technologies List’ issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, cycle-consistent GAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adversarial principle underlying GAN and Jensen–Shannon divergence while discussing the optimality characteristics of the GAN framework. The efficiency of GAN variants and their model architectures will be evaluated along with training obstacles as well as training solutions. In addition, a detailed discussion will be provided, examining the integration of GANs with newly developed deep learning frameworks such as transformers, physics-informed neural networks, large language models, and diffusion models. Finally, we reveal several issues as well as future research outlines in this field.",
      "abstract": "Abstract Generative adversarial networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas, since their inception in 2014. Consisting of a discriminative network and a generative network engaged in a minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ‘Top Ten Global Breakthrough Technologies List’ issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, cycle-consistent GAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adversarial principle underlying GAN and Jensen–Shannon divergence while discussing the optimality characteristics of the GAN framework. The efficiency of GAN variants and their model architectures will be evaluated along with training obstacles as well as training solutions. In addition, a detailed discussion will be provided, examining the integration of GANs with newly developed deep learning frameworks such as transformers, physics-informed neural networks, large language models, and diffusion models. Finally, we reveal several issues as well as future research outlines in this field.",
      "doi": "https://doi.org/10.1088/2632-2153/ad1f77",
      "openalex_id": "https://openalex.org/W4390937897",
      "arxiv_id": "",
      "publication_date": "2024-01-17",
      "published": "2024-01-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Prosody-Aware Speecht5 for Expressive Neural TTS",
      "summary": "SpeechT5, a multimodal learning framework which explores encoder-decoder pre-training by leveraging both unlabeled speech and text, has been proven to be effective on a wide variety of speech processing tasks. In this paper, we enhance SpeechT5 by adding a new sub-task on prosody modeling (prosody-aware SpeechT5) for neural text-to-speech (TTS), which can improve the model capability to learn richer contextual representations through multi-task learning. In the prosody-aware SpeechT5 training framework, most modules in neural TTS can be pre-trained with large-scale unlabeled speech and text corpus, including encoder, decoder, and variance adaptor. Experimental results show that the proposed prosody-aware SpeechT5 is effective at improving the expressiveness of neural TTS <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> : 1) the CMOS (comparison mean opinion score) gain is 0.154 for texts from news domain and 0.114 for texts from audiobook domain; 2) the prosody related issues in synthetic speech are reduced by 19.02% in subjective evaluation.",
      "abstract": "SpeechT5, a multimodal learning framework which explores encoder-decoder pre-training by leveraging both unlabeled speech and text, has been proven to be effective on a wide variety of speech processing tasks. In this paper, we enhance SpeechT5 by adding a new sub-task on prosody modeling (prosody-aware SpeechT5) for neural text-to-speech (TTS), which can improve the model capability to learn richer contextual representations through multi-task learning. In the prosody-aware SpeechT5 training framework, most modules in neural TTS can be pre-trained with large-scale unlabeled speech and text corpus, including encoder, decoder, and variance adaptor. Experimental results show that the proposed prosody-aware SpeechT5 is effective at improving the expressiveness of neural TTS <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> : 1) the CMOS (comparison mean opinion score) gain is 0.154 for texts from news domain and 0.114 for texts from audiobook domain; 2) the prosody related issues in synthetic speech are reduced by 19.02% in subjective evaluation.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096530",
      "openalex_id": "https://openalex.org/W4375869201",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation",
      "summary": "Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity prior is defined to distinguish between strong and weak facial activity, obtained by statistically analyzing facial animations. Based on the facial activity intensity prior, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/.",
      "abstract": "Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity prior is defined to distinguish between strong and weak facial activity, obtained by statistically analyzing facial animations. Based on the facial activity intensity prior, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/.",
      "doi": "https://doi.org/10.1109/tcsvt.2024.3386836",
      "openalex_id": "https://openalex.org/W4394698479",
      "arxiv_id": "",
      "publication_date": "2024-04-10",
      "published": "2024-04-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MSStyleTTS: Multi-Scale Style Modeling With Hierarchical Context Information for Expressive Speech Synthesis",
      "summary": "Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.",
      "abstract": "Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.",
      "doi": "https://doi.org/10.1109/taslp.2023.3301217",
      "openalex_id": "https://openalex.org/W4385486424",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Context-Aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis",
      "summary": "Recent advances in text-to-speech have significantly improved the expressiveness of synthesized speech. However, it is still challenging to generate speech with contextually appropriate and coherent speaking style for multi-sentence text in audiobooks. In this paper, we propose a context-aware coherent speaking style prediction method for audiobook speech synthesis. To predict the style embedding of the current utterance, a hierarchical transformer-based context-aware style predictor with a mixture attention mask is designed, considering both text-side context information and speech- side style information of previous speeches. Based on this, we can generate long-form speech with coherent style and prosody sentence by sentence. Objective and subjective evaluations on a Mandarin audiobook dataset demonstrate that our proposed model can generate speech with more expressive and coherent speaking style than baselines, for both single-sentence and multi-sentence test <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "Recent advances in text-to-speech have significantly improved the expressiveness of synthesized speech. However, it is still challenging to generate speech with contextually appropriate and coherent speaking style for multi-sentence text in audiobooks. In this paper, we propose a context-aware coherent speaking style prediction method for audiobook speech synthesis. To predict the style embedding of the current utterance, a hierarchical transformer-based context-aware style predictor with a mixture attention mask is designed, considering both text-side context information and speech- side style information of previous speeches. Based on this, we can generate long-form speech with coherent style and prosody sentence by sentence. Objective and subjective evaluations on a Mandarin audiobook dataset demonstrate that our proposed model can generate speech with more expressive and coherent speaking style than baselines, for both single-sentence and multi-sentence test <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095866",
      "openalex_id": "https://openalex.org/W4372266971",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Granularity of Prosodic Representations in Expressive Text-to-Speech",
      "summary": "In expressive speech synthesis it is widely adopted to use latent prosody representations to deal with variability of the data during training. Same text may correspond to various acoustic realizations, which is known as a one-to-many mapping problem in text-to-speech. Utterance, word, or phoneme-level representations are extracted from target signal in an auto-encoding setup, to complement phonetic input and simplify that mapping. This paper compares prosodic embeddings at different levels of granularity and examines their prediction from text. We show that utterance-level embeddings have insufficient capacity and phoneme-level tend to introduce instabilities when predicted from text. Word-level representations impose balance between capacity and predictability. As a result, we close the gap in naturalness by 90% between synthetic speech and recordings on LibriTTS dataset, without sacrificing intelligibility.",
      "abstract": "In expressive speech synthesis it is widely adopted to use latent prosody representations to deal with variability of the data during training. Same text may correspond to various acoustic realizations, which is known as a one-to-many mapping problem in text-to-speech. Utterance, word, or phoneme-level representations are extracted from target signal in an auto-encoding setup, to complement phonetic input and simplify that mapping. This paper compares prosodic embeddings at different levels of granularity and examines their prediction from text. We show that utterance-level embeddings have insufficient capacity and phoneme-level tend to introduce instabilities when predicted from text. Word-level representations impose balance between capacity and predictability. As a result, we close the gap in naturalness by 90% between synthetic speech and recordings on LibriTTS dataset, without sacrificing intelligibility.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022793",
      "openalex_id": "https://openalex.org/W4319862228",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative Adversarial Networks in Business and Social Science",
      "summary": "Generative adversarial networks (GANs) have become a recent and rapidly developing research topic in machine learning. Since their inception in 2014, a significant number of variants have been proposed to address various topics across many fields, and they have particularly excelled not only in image and language processing but also in the medical and data science domains. In this paper, we aim to highlight the significance of and advancements that these GAN models can introduce in the field of Business Economics, where they have yet to be fully developed. To this end, a review of the literature of GANs is presented in general together with a more specific review in the field of Business Economics, for which only a few papers can be found. Furthermore, the most relevant papers are analysed in order to provide approaches for the opportunity to research GANs in the field of Business Economics.",
      "abstract": "Generative adversarial networks (GANs) have become a recent and rapidly developing research topic in machine learning. Since their inception in 2014, a significant number of variants have been proposed to address various topics across many fields, and they have particularly excelled not only in image and language processing but also in the medical and data science domains. In this paper, we aim to highlight the significance of and advancements that these GAN models can introduce in the field of Business Economics, where they have yet to be fully developed. To this end, a review of the literature of GANs is presented in general together with a more specific review in the field of Business Economics, for which only a few papers can be found. Furthermore, the most relevant papers are analysed in order to provide approaches for the opportunity to research GANs in the field of Business Economics.",
      "doi": "https://doi.org/10.3390/app14177438",
      "openalex_id": "https://openalex.org/W4401889190",
      "arxiv_id": "",
      "publication_date": "2024-08-23",
      "published": "2024-08-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancement of Text-Predicting Style Token With Generative Adversarial Network for Expressive Speech Synthesis",
      "summary": "This work proposes an advanced text-predicting style embedding for expressive speech synthesis. Text-predicting global style token (TPGST) predicts style embedding from text instead of reference speech and uses it to condition a text-to-speech synthesis (TTS) model, resulting in style TTS without reference speech. Although this minimizes the style embedding's L1-loss between that extracted from reference speech and that predicted during training, predicted embedding tends to be over-smoothed. To overcome this issue, the proposed method uses the generative adversarial network (GAN) in training style predictors. This not only improves style reproduction, but also aims to reduce style conditioning mismatch during TTS model training. We also utilize TTS text embeddings as in other related work, as well as word information via BERT in order to find better style distributions in GAN. An evaluation of subjective style reproduction demonstrates that 1) the proposed method outperforms conventional TPGST, and 2) the use of words yielded by BERT provides even better performance. Our style predictor is also effective in attaining unseen style TTS for \"seen\" and \"unseen\" speakers.",
      "abstract": "This work proposes an advanced text-predicting style embedding for expressive speech synthesis. Text-predicting global style token (TPGST) predicts style embedding from text instead of reference speech and uses it to condition a text-to-speech synthesis (TTS) model, resulting in style TTS without reference speech. Although this minimizes the style embedding's L1-loss between that extracted from reference speech and that predicted during training, predicted embedding tends to be over-smoothed. To overcome this issue, the proposed method uses the generative adversarial network (GAN) in training style predictors. This not only improves style reproduction, but also aims to reduce style conditioning mismatch during TTS model training. We also utilize TTS text embeddings as in other related work, as well as word information via BERT in order to find better style distributions in GAN. An evaluation of subjective style reproduction demonstrates that 1) the proposed method outperforms conventional TPGST, and 2) the use of words yielded by BERT provides even better performance. Our style predictor is also effective in attaining unseen style TTS for \"seen\" and \"unseen\" speakers.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095513",
      "openalex_id": "https://openalex.org/W4375868902",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voicifier-LN: An Novel Approach to Elevate the Speaker Similarity for General Zero-shot Multi-Speaker TTS",
      "summary": "Speeches generated from neural network-based Text-to-Speech (TTS) have been becoming more natural and intelligible. However, the evident dropping performance still exists when synthesizing multi-speaker speeches in zero-shot manner, especially for those from different countries with different accents. To bridge this gap, we propose a novel method, called Voicifier. It firstly operates on high frequency mel-spectrogram bins to approximately remove the content and rhythm. Then Voicifier uses two strategies, from the shallow to the deep mixing, to further destroy the content and rhythm but retain the timbre. Furthermore, for better zero-shot performance, we propose Voice-Pin Layer Normalization (VPLN) which pins down the timbre according with the text feature. During inference, the model is allowed to synthesize high quality and similarity speeches with just around 1 sec target speech audio. Experiments and ablation studies prove that the methods are able to retain more target timbre while abandoning much more of the content and rhythm-related information. To our best knowledge, the methods are found to be universal that is to say it can be applied to most of the existing TTS systems to enhance the ability of cross-speaker synthesis.",
      "abstract": "Speeches generated from neural network-based Text-to-Speech (TTS) have been becoming more natural and intelligible. However, the evident dropping performance still exists when synthesizing multi-speaker speeches in zero-shot manner, especially for those from different countries with different accents. To bridge this gap, we propose a novel method, called Voicifier. It firstly operates on high frequency mel-spectrogram bins to approximately remove the content and rhythm. Then Voicifier uses two strategies, from the shallow to the deep mixing, to further destroy the content and rhythm but retain the timbre. Furthermore, for better zero-shot performance, we propose Voice-Pin Layer Normalization (VPLN) which pins down the timbre according with the text feature. During inference, the model is allowed to synthesize high quality and similarity speeches with just around 1 sec target speech audio. Experiments and ablation studies prove that the methods are able to retain more target timbre while abandoning much more of the content and rhythm-related information. To our best knowledge, the methods are found to be universal that is to say it can be applied to most of the existing TTS systems to enhance the ability of cross-speaker synthesis.",
      "doi": "https://doi.org/10.1145/3573942.3574120",
      "openalex_id": "https://openalex.org/W4376852275",
      "arxiv_id": "",
      "publication_date": "2022-09-23",
      "published": "2022-09-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Transmission of Voice in Real-Time Applications",
      "summary": "In today’s telecommunication world sharing the data becomes very easy. It is a bit-complicated in converting the text documents to voice assistance even proposed a lot of resources. Giving the correct information to the right person in the right way is essential on both a personal and professional level. Numerous applications have developed with the purpose of enabling two individuals to communicate instantly. The major objective of this effort is to address the issues that dysarthria, business meetings, and regular travelers face. To solve this issue, proposing a gadget that will aid in the translation of written language into speech. The majority of these applications include, language translation, signal conversion from text to synthetic voice, and articulators. In this project, proposing the development in a wide range of strategies and algorithms needed to make text to speech a reality (TTS)",
      "abstract": "In today’s telecommunication world sharing the data becomes very easy. It is a bit-complicated in converting the text documents to voice assistance even proposed a lot of resources. Giving the correct information to the right person in the right way is essential on both a personal and professional level. Numerous applications have developed with the purpose of enabling two individuals to communicate instantly. The major objective of this effort is to address the issues that dysarthria, business meetings, and regular travelers face. To solve this issue, proposing a gadget that will aid in the translation of written language into speech. The majority of these applications include, language translation, signal conversion from text to synthetic voice, and articulators. In this project, proposing the development in a wide range of strategies and algorithms needed to make text to speech a reality (TTS)",
      "doi": "https://doi.org/10.47392/irjash.2023.s037",
      "openalex_id": "https://openalex.org/W4383109451",
      "arxiv_id": "",
      "publication_date": "2023-05-28",
      "published": "2023-05-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry",
      "summary": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "abstract": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "doi": "https://doi.org/10.1016/j.enbenv.2024.03.010",
      "openalex_id": "https://openalex.org/W4393236964",
      "arxiv_id": "",
      "publication_date": "2024-03-27",
      "published": "2024-03-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adapting Frechet Audio Distance for Generative Music Evaluation",
      "summary": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
      "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446663",
      "openalex_id": "https://openalex.org/W4392902957",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Maskmark: Robust Neuralwatermarking for Real and Synthetic Speech",
      "summary": "High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.",
      "abstract": "High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447253",
      "openalex_id": "https://openalex.org/W4392904158",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Determination of optimal formats for digital image compression",
      "summary": "Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.",
      "abstract": "Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.",
      "doi": "https://doi.org/10.17163/ings.n33.2025.01",
      "openalex_id": "https://openalex.org/W4406233769",
      "arxiv_id": "",
      "publication_date": "2025-01-10",
      "published": "2025-01-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CMM: Code-Switching with Manifold Mixup for Cross-Lingual Spoken Language Understanding",
      "summary": "Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.",
      "abstract": "Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.",
      "doi": "https://doi.org/10.1109/smc53992.2023.10393998",
      "openalex_id": "https://openalex.org/W4391331299",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview",
      "summary": "We present a structured overview of adaptation algorithms for neural\\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\\nneural network systems and end-to-end neural network systems, with a focus on\\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\\ncharacterizes adaptation algorithms as based on embeddings, model parameter\\nadaptation, or data augmentation. We present a meta-analysis of the performance\\nof speech recognition adaptation algorithms, based on relative error rate\\nreductions as reported in the literature.\\n",
      "abstract": "We present a structured overview of adaptation algorithms for neural\\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\\nneural network systems and end-to-end neural network systems, with a focus on\\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\\ncharacterizes adaptation algorithms as based on embeddings, model parameter\\nadaptation, or data augmentation. We present a meta-analysis of the performance\\nof speech recognition adaptation algorithms, based on relative error rate\\nreductions as reported in the literature.\\n",
      "doi": "https://doi.org/10.1109/ojsp.2020.3045349",
      "openalex_id": "https://openalex.org/W3112702554",
      "arxiv_id": "",
      "publication_date": "2020-12-17",
      "published": "2020-12-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transforming English language learning: Advanced speech recognition with MLP-LSTM for personalized education",
      "summary": "Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.",
      "abstract": "Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.",
      "doi": "https://doi.org/10.1016/j.aej.2024.10.065",
      "openalex_id": "https://openalex.org/W4403558103",
      "arxiv_id": "",
      "publication_date": "2024-10-19",
      "published": "2024-10-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Validity, feasibility, and effectiveness of a voice‐recognition based digital cognitive screener for dementia and mild cognitive impairment in community‐dwelling older Chinese adults: A large‐scale implementation study",
      "summary": "Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.",
      "abstract": "Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.",
      "doi": "https://doi.org/10.1002/alz.13668",
      "openalex_id": "https://openalex.org/W4391436317",
      "arxiv_id": "",
      "publication_date": "2024-02-01",
      "published": "2024-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Building a Language-Independent Speech Scoring Assessment",
      "summary": "Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.",
      "abstract": "Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.",
      "doi": "https://doi.org/10.1609/aaai.v38i21.30366",
      "openalex_id": "https://openalex.org/W4393157347",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
      "summary": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.",
      "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.",
      "doi": "https://doi.org/10.1613/jair.1.13083",
      "openalex_id": "https://openalex.org/W3166631396",
      "arxiv_id": "",
      "publication_date": "2022-07-13",
      "published": "2022-07-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Temporal Knowledge Distillation for on-device Audio Classification",
      "summary": "Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",
      "abstract": "Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747908",
      "openalex_id": "https://openalex.org/W3210574093",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
      "summary": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.",
      "abstract": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1390",
      "openalex_id": "https://openalex.org/W3110524561",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adaptation Algorithms for Speech Recognition: An Overview",
      "summary": "We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.",
      "abstract": "We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3049397270",
      "arxiv_id": "",
      "publication_date": "2021-02-01",
      "published": "2021-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dynamic Acoustic Unit Augmentation with BPE-Dropout for Low-Resource End-to-End Speech Recognition",
      "summary": "With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.",
      "abstract": "With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.",
      "doi": "https://doi.org/10.3390/s21093063",
      "openalex_id": "https://openalex.org/W3136717460",
      "arxiv_id": "",
      "publication_date": "2021-04-28",
      "published": "2021-04-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
      "summary": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.",
      "abstract": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.",
      "doi": "https://doi.org/10.21437/interspeech.2022-547",
      "openalex_id": "https://openalex.org/W3207547029",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging",
      "summary": "Machine Learning (ML) is more than just training models, the whole workflow\\nmust be considered. Once deployed, a ML model needs to be watched and\\nconstantly supervised and debugged to guarantee its validity and robustness in\\nunexpected situations. Debugging in ML aims to identify (and address) the model\\nweaknesses in not trivial contexts. Several techniques have been proposed to\\nidentify different types of model weaknesses, such as bias in classification,\\nmodel decay, adversarial attacks, etc., yet there is not a generic framework\\nthat allows them to work in a collaborative, modular, portable, iterative way\\nand, more importantly, flexible enough to allow both human- and machine-driven\\ntechniques. In this paper, we propose a novel containerized directed graph\\nframework to support and accelerate end-to-end ML workflow management,\\nsupervision, and debugging. The framework allows defining and deploying ML\\nworkflows in containers, tracking their metadata, checking their behavior in\\nproduction, and improving the models by using both learned and human-provided\\nknowledge. We demonstrate these capabilities by integrating in the framework\\ntwo hybrid systems to detect data drift distribution which identify the samples\\nthat are far from the latent space of the original distribution, ask for human\\nintervention, and whether retrain the model or wrap it with a filter to remove\\nthe noise of corrupted data at inference time. We test these systems on\\nMNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy\\nresults with the help of human involvement.\\n",
      "abstract": "Machine Learning (ML) is more than just training models, the whole workflow\\nmust be considered. Once deployed, a ML model needs to be watched and\\nconstantly supervised and debugged to guarantee its validity and robustness in\\nunexpected situations. Debugging in ML aims to identify (and address) the model\\nweaknesses in not trivial contexts. Several techniques have been proposed to\\nidentify different types of model weaknesses, such as bias in classification,\\nmodel decay, adversarial attacks, etc., yet there is not a generic framework\\nthat allows them to work in a collaborative, modular, portable, iterative way\\nand, more importantly, flexible enough to allow both human- and machine-driven\\ntechniques. In this paper, we propose a novel containerized directed graph\\nframework to support and accelerate end-to-end ML workflow management,\\nsupervision, and debugging. The framework allows defining and deploying ML\\nworkflows in containers, tracking their metadata, checking their behavior in\\nproduction, and improving the models by using both learned and human-provided\\nknowledge. We demonstrate these capabilities by integrating in the framework\\ntwo hybrid systems to detect data drift distribution which identify the samples\\nthat are far from the latent space of the original distribution, ask for human\\nintervention, and whether retrain the model or wrap it with a filter to remove\\nthe noise of corrupted data at inference time. We test these systems on\\nMNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy\\nresults with the help of human involvement.\\n",
      "doi": "https://doi.org/10.1016/j.eswa.2022.117232",
      "openalex_id": "https://openalex.org/W3211110040",
      "arxiv_id": "",
      "publication_date": "2022-04-19",
      "published": "2022-04-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals",
      "summary": "Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based \"AI Listener\" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",
      "abstract": "Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based \"AI Listener\" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",
      "doi": "https://doi.org/10.1109/ner52421.2023.10123751",
      "openalex_id": "https://openalex.org/W4377089543",
      "arxiv_id": "",
      "publication_date": "2023-04-24",
      "published": "2023-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CDPAM: Contrastive Learning for Perceptual Audio Similarity",
      "summary": "Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.",
      "abstract": "Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9413711",
      "openalex_id": "https://openalex.org/W3128311069",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The CogALex shared task on monolingual and multilingual identification of semantic relations",
      "summary": "The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.",
      "abstract": "The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3120335524",
      "arxiv_id": "",
      "publication_date": "2020-12-01",
      "published": "2020-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformer Based Speech to Text Translation for Indic Languages",
      "summary": "In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.",
      "abstract": "In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.",
      "doi": "https://doi.org/10.1109/icaisc58445.2023.10200222",
      "openalex_id": "https://openalex.org/W4385695089",
      "arxiv_id": "",
      "publication_date": "2023-06-16",
      "published": "2023-06-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
      "summary": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.",
      "abstract": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1683",
      "openalex_id": "https://openalex.org/W3145189018",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data",
      "summary": "We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.",
      "abstract": "We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.591",
      "openalex_id": "https://openalex.org/W3153532013",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
      "summary": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
      "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2011.01403",
      "openalex_id": "https://openalex.org/W3096565276",
      "arxiv_id": "",
      "publication_date": "2020-11-03",
      "published": "2020-11-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data",
      "summary": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.",
      "abstract": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.",
      "doi": "https://doi.org/10.48550/arxiv.2101.07597",
      "openalex_id": "https://openalex.org/W3121299949",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset",
      "summary": "Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.",
      "abstract": "Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.",
      "doi": "https://doi.org/10.48550/arxiv.2107.05233",
      "openalex_id": "https://openalex.org/W3178203035",
      "arxiv_id": "",
      "publication_date": "2021-07-12",
      "published": "2021-07-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
      "summary": "In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.",
      "abstract": "In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.",
      "doi": "https://doi.org/10.48550/arxiv.2104.06678",
      "openalex_id": "https://openalex.org/W3153287399",
      "arxiv_id": "",
      "publication_date": "2021-04-14",
      "published": "2021-04-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech Recognition Initiative for African Languages",
      "summary": "Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.",
      "abstract": "Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.",
      "doi": "https://doi.org/10.21203/rs.3.rs-2708355/v1",
      "openalex_id": "https://openalex.org/W4327858845",
      "arxiv_id": "",
      "publication_date": "2023-03-20",
      "published": "2023-03-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatically Identifying Language Family from Acoustic Examples in Low Resource Scenarios",
      "summary": "Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.",
      "abstract": "Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.",
      "doi": "https://doi.org/10.48550/arxiv.2012.00876",
      "openalex_id": "https://openalex.org/W3109527089",
      "arxiv_id": "",
      "publication_date": "2020-12-01",
      "published": "2020-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition",
      "summary": "Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often include faulty phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classifier to distinguish Anglicisms from native German words. With this approach, the model learns to generate pronunciations differently depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries that are added to an existing German speech recognition model. Tested on a dedicated Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by 1 % and the Anglicism error rate by 3 %. We show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.",
      "abstract": "Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often include faulty phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classifier to distinguish Anglicisms from native German words. With this approach, the model learns to generate pronunciations differently depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries that are added to an existing German speech recognition model. Tested on a dedicated Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by 1 % and the Anglicism error rate by 3 %. We show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.",
      "doi": "https://doi.org/10.48550/arxiv.2105.12708",
      "openalex_id": "https://openalex.org/W3165619625",
      "arxiv_id": "",
      "publication_date": "2021-05-26",
      "published": "2021-05-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages",
      "summary": "We present a CLSRIL-23, a self supervised learning based audio pre-trained model which learns cross lingual speech representations from raw audio across 23 Indic languages. It is built on top of wav2vec 2.0 which is solved by training a contrastive task over masked latent speech representations and jointly learns the quantization of latents shared across all languages. We compare the language wise loss during pretraining to compare effects of monolingual and multilingual pretraining. Performance on some downstream fine-tuning tasks for speech recognition is also compared and our experiments show that multilingual pretraining outperforms monolingual training, in terms of learning speech representations which encodes phonetic similarity of languages and also in terms of performance on down stream tasks. A decrease of 5% is observed in WER and 9.5% in CER when a multilingual pretrained model is used for finetuning in Hindi. All the code models are also open sourced. CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio data to facilitate research in speech recognition for Indic languages. We hope that new state of the art systems will be created using the self supervised approach, especially for low resources Indic languages.",
      "abstract": "We present a CLSRIL-23, a self supervised learning based audio pre-trained model which learns cross lingual speech representations from raw audio across 23 Indic languages. It is built on top of wav2vec 2.0 which is solved by training a contrastive task over masked latent speech representations and jointly learns the quantization of latents shared across all languages. We compare the language wise loss during pretraining to compare effects of monolingual and multilingual pretraining. Performance on some downstream fine-tuning tasks for speech recognition is also compared and our experiments show that multilingual pretraining outperforms monolingual training, in terms of learning speech representations which encodes phonetic similarity of languages and also in terms of performance on down stream tasks. A decrease of 5% is observed in WER and 9.5% in CER when a multilingual pretrained model is used for finetuning in Hindi. All the code models are also open sourced. CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio data to facilitate research in speech recognition for Indic languages. We hope that new state of the art systems will be created using the self supervised approach, especially for low resources Indic languages.",
      "doi": "https://doi.org/10.48550/arxiv.2107.07402",
      "openalex_id": "https://openalex.org/W3177999760",
      "arxiv_id": "",
      "publication_date": "2021-07-15",
      "published": "2021-07-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19",
      "summary": "Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.",
      "abstract": "Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.",
      "doi": "https://doi.org/10.48550/arxiv.2108.10791",
      "openalex_id": "https://openalex.org/W3196174113",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASR4REAL: An extended benchmark for speech models",
      "summary": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",
      "abstract": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",
      "doi": "https://doi.org/10.48550/arxiv.2110.08583",
      "openalex_id": "https://openalex.org/W3206083773",
      "arxiv_id": "",
      "publication_date": "2021-10-16",
      "published": "2021-10-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals",
      "summary": "Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.",
      "abstract": "Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.",
      "doi": "https://doi.org/10.48550/arxiv.2103.11011",
      "openalex_id": "https://openalex.org/W3136649840",
      "arxiv_id": "",
      "publication_date": "2021-03-19",
      "published": "2021-03-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pretext Tasks selection for multitask self-supervised speech\\n representation learning",
      "summary": "Through solving pretext tasks, self-supervised learning leverages unlabeled\\ndata to extract useful latent representations replacing traditional input\\nfeatures in the downstream task. In audio/speech signal processing, a wide\\nrange of features where engineered through decades of research efforts. As it\\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\\nto be a particularly relevant pretext task, leading to useful self-supervised\\nrepresentations which prove to be effective for downstream tasks. However,\\nmethods and common practices for combining such pretext tasks for better\\nperformance on the downstream task have not been explored and understood\\nproperly. In fact, the process relies almost exclusively on a computationally\\nheavy experimental procedure, which becomes intractable with the increase of\\nthe number of pretext tasks. This paper introduces a method to select a group\\nof pretext tasks among a set of candidates. The method we propose estimates\\ncalibrated weights for the partial losses corresponding to the considered\\npretext tasks during the self-supervised training process. The experiments\\nconducted on automatic speech recognition, speaker and emotion recognition\\nvalidate our approach, as the groups selected and weighted with our method\\nperform better than classic baselines, thus facilitating the selection and\\ncombination of relevant pseudo-labels for self-supervised representation\\nlearning.\\n",
      "abstract": "Through solving pretext tasks, self-supervised learning leverages unlabeled\\ndata to extract useful latent representations replacing traditional input\\nfeatures in the downstream task. In audio/speech signal processing, a wide\\nrange of features where engineered through decades of research efforts. As it\\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\\nto be a particularly relevant pretext task, leading to useful self-supervised\\nrepresentations which prove to be effective for downstream tasks. However,\\nmethods and common practices for combining such pretext tasks for better\\nperformance on the downstream task have not been explored and understood\\nproperly. In fact, the process relies almost exclusively on a computationally\\nheavy experimental procedure, which becomes intractable with the increase of\\nthe number of pretext tasks. This paper introduces a method to select a group\\nof pretext tasks among a set of candidates. The method we propose estimates\\ncalibrated weights for the partial losses corresponding to the considered\\npretext tasks during the self-supervised training process. The experiments\\nconducted on automatic speech recognition, speaker and emotion recognition\\nvalidate our approach, as the groups selected and weighted with our method\\nperform better than classic baselines, thus facilitating the selection and\\ncombination of relevant pseudo-labels for self-supervised representation\\nlearning.\\n",
      "doi": "https://doi.org/10.1109/jstsp.2022.3195430",
      "openalex_id": "https://openalex.org/W3176481516",
      "arxiv_id": "",
      "publication_date": "2021-07-01",
      "published": "2021-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "IMS’ Systems for the IWSLT 2021 Low-Resource Speech Translation Task",
      "summary": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.",
      "abstract": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.",
      "doi": "https://doi.org/10.18653/v1/2021.iwslt-1.21",
      "openalex_id": "https://openalex.org/W3177457454",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-lingual Transfer for Speech Processing using Acoustic Language Similarity",
      "summary": "Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.",
      "abstract": "Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.",
      "doi": "https://doi.org/10.48550/arxiv.2111.01326",
      "openalex_id": "https://openalex.org/W3211231337",
      "arxiv_id": "",
      "publication_date": "2021-11-02",
      "published": "2021-11-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
      "summary": "In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.",
      "abstract": "In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.",
      "doi": "https://doi.org/10.48550/arxiv.2111.04823",
      "openalex_id": "https://openalex.org/W3211624279",
      "arxiv_id": "",
      "publication_date": "2021-11-08",
      "published": "2021-11-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Emotion Recognition in Italian Using Wav2Vec 2.0 and the Novel Crowdsourced Emotional Speech Corpus Emozionalmente",
      "summary": "&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;",
      "doi": "https://doi.org/10.36227/techrxiv.22821992",
      "openalex_id": "https://openalex.org/W4377088036",
      "arxiv_id": "",
      "publication_date": "2023-05-19",
      "published": "2023-05-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Gloss Attention for Gloss-free Sign Language Translation",
      "summary": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.",
      "abstract": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.00251",
      "openalex_id": "https://openalex.org/W4386076575",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation",
      "summary": "Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \\url{https://SingGAN.github.io/}",
      "abstract": "Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \\url{https://SingGAN.github.io/}",
      "doi": "https://doi.org/10.1145/3503161.3547854",
      "openalex_id": "https://openalex.org/W3206191467",
      "arxiv_id": "",
      "publication_date": "2022-10-10",
      "published": "2022-10-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment",
      "summary": "The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performance in terms of both objective and subjective metrics. Audio samples are available at https://alignsts.github.io.",
      "abstract": "The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performance in terms of both objective and subjective metrics. Audio samples are available at https://alignsts.github.io.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.442",
      "openalex_id": "https://openalex.org/W4385571990",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Multi-Intent Spoken Language Understanding via Hierarchical Attention and Optimal Transport",
      "summary": "Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.",
      "abstract": "Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29738",
      "openalex_id": "https://openalex.org/W4393160357",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
      "summary": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.",
      "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.406",
      "openalex_id": "https://openalex.org/W4385572615",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Speech Translation with Mutual Knowledge Distillation",
      "summary": "Multi-task learning (MTL) is widely used to improve end-to-end speech translation (ST), which implicitly transfer knowledge from auxiliary automatic speech recognition (ASR) and/or machine translation (MT) to ST through shared modules. In this study, we find that triple-task MTL (ST+MT+ASR) suffers from a knowledge transfer limitation that leads to performance stagnation compared with dual-task MTL (ST+MT or ST+ASR). To address this issue, we propose a simple yet effective method, ST-MKD (Speech Translation with Mutual Knowledge Distillation). In ST-MKD, we employ a mutual knowledge distillation framework to mutually enhance dual-task MTL models with different knowledge bases, and explore regularization to maintain the consistency of the task representations. Experiments on the ST benchmark dataset MuST-C show that ST-MKD significantly outperforms strong MTL baseline and achieves state-of-the-art performance under three speech pre-training settings. Further analyses confirm that our approach effectively overcomes the knowledge transfer limitation of triple-task MTL.",
      "abstract": "Multi-task learning (MTL) is widely used to improve end-to-end speech translation (ST), which implicitly transfer knowledge from auxiliary automatic speech recognition (ASR) and/or machine translation (MT) to ST through shared modules. In this study, we find that triple-task MTL (ST+MT+ASR) suffers from a knowledge transfer limitation that leads to performance stagnation compared with dual-task MTL (ST+MT or ST+ASR). To address this issue, we propose a simple yet effective method, ST-MKD (Speech Translation with Mutual Knowledge Distillation). In ST-MKD, we employ a mutual knowledge distillation framework to mutually enhance dual-task MTL models with different knowledge bases, and explore regularization to maintain the consistency of the task representations. Experiments on the ST benchmark dataset MuST-C show that ST-MKD significantly outperforms strong MTL baseline and achieves state-of-the-art performance under three speech pre-training settings. Further analyses confirm that our approach effectively overcomes the knowledge transfer limitation of triple-task MTL.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445811",
      "openalex_id": "https://openalex.org/W4392904172",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Indic-ST: A Large-Scale Multilingual Corpus for Low-Resource Speech-to-Text Translation",
      "summary": "We introduce Indic-ST, a novel dataset for speech-to-text translation (ST) task from English to Indic languages to bridge the performance gap. ST involves converting spoken input in one language into written text in another, playing a key role in real-world applications like subtitling, lecture transcription, and multilingual communication systems. Despite several efforts like Meta’s seamless m4t, OpenAI’s Whisper, or Google USM model, the performance of ST models on low-resource languages lags to that of English (or high-resource languages like European languages). Indic-ST is compiled from four distinct domains: conversational audio, religious texts, education, and news, which combined results in the Indic-ST dataset. To the best of our knowledge, this is the largest low-resource ST data covering approximately 6,800 hours of English speech in the real human voice and text in 15 Indic languages with diverse scripts totaling approximately 900 GB in size. To assess the usefulness of the dataset, we present the baseline performance of individual language pairs using state-of-the-art ST models. We also present a unified multilingual English-to-Indic-ST model. The code and dataset are available at https://github.com/Nivedita5/Indic-ST .",
      "abstract": "We introduce Indic-ST, a novel dataset for speech-to-text translation (ST) task from English to Indic languages to bridge the performance gap. ST involves converting spoken input in one language into written text in another, playing a key role in real-world applications like subtitling, lecture transcription, and multilingual communication systems. Despite several efforts like Meta’s seamless m4t, OpenAI’s Whisper, or Google USM model, the performance of ST models on low-resource languages lags to that of English (or high-resource languages like European languages). Indic-ST is compiled from four distinct domains: conversational audio, religious texts, education, and news, which combined results in the Indic-ST dataset. To the best of our knowledge, this is the largest low-resource ST data covering approximately 6,800 hours of English speech in the real human voice and text in 15 Indic languages with diverse scripts totaling approximately 900 GB in size. To assess the usefulness of the dataset, we present the baseline performance of individual language pairs using state-of-the-art ST models. We also present a unified multilingual English-to-Indic-ST model. The code and dataset are available at https://github.com/Nivedita5/Indic-ST .",
      "doi": "https://doi.org/10.1145/3736720",
      "openalex_id": "https://openalex.org/W4410829339",
      "arxiv_id": "",
      "publication_date": "2025-05-28",
      "published": "2025-05-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
      "summary": "Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.",
      "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445891",
      "openalex_id": "https://openalex.org/W4392904484",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal Fish Feeding Intensity Assessment in Aquaculture",
      "summary": "Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management.",
      "abstract": "Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management.",
      "doi": "https://doi.org/10.48550/arxiv.2309.05058",
      "openalex_id": "https://openalex.org/W4386648419",
      "arxiv_id": "",
      "publication_date": "2023-09-10",
      "published": "2023-09-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parameter-Efficient Cross-Language Transfer Learning for a Language-Modular Audiovisual Speech Recognition",
      "summary": "In audiovisual speech recognition (AV-ASR), for many languages only few audiovisual data is available. Building upon an English model, in this work, we first apply and analyze various adapters for cross-language transfer learning to build a parameter-efficient and easy-to-extend AV-ASR in multiple languages. Fine-tuning only the bottleneck adapter with 4% of encoder's parameters and the decoder shows comparable performance to full fine-tuning in French and Spanish AV-ASR. Second, we investigate the effectiveness of various encoder components in cross-language transfer learning. Our proposed modular linguistic transfer learning approach excels the full fine-tuning method for German, French, and Spanish AV-ASR in almost all clean and noisy conditions (8/9). On low-resourced German AV data (13h), our proposed linguistic transfer learning achieves a 4.1% abs. WER reduction on average for clean and noisy speech, while fine-tuning only 50% of the encoder's parameters. Our code is at GitHub. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/ifnspaml/Cross_Language_Transfer_Learning_AVASR.git",
      "abstract": "In audiovisual speech recognition (AV-ASR), for many languages only few audiovisual data is available. Building upon an English model, in this work, we first apply and analyze various adapters for cross-language transfer learning to build a parameter-efficient and easy-to-extend AV-ASR in multiple languages. Fine-tuning only the bottleneck adapter with 4% of encoder's parameters and the decoder shows comparable performance to full fine-tuning in French and Spanish AV-ASR. Second, we investigate the effectiveness of various encoder components in cross-language transfer learning. Our proposed modular linguistic transfer learning approach excels the full fine-tuning method for German, French, and Spanish AV-ASR in almost all clean and noisy conditions (8/9). On low-resourced German AV data (13h), our proposed linguistic transfer learning achieves a 4.1% abs. WER reduction on average for clean and noisy speech, while fine-tuning only 50% of the encoder's parameters. Our code is at GitHub. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/ifnspaml/Cross_Language_Transfer_Learning_AVASR.git",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389795",
      "openalex_id": "https://openalex.org/W4391021793",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Working with troubles and failures in conversation between humans and robots: workshop report",
      "summary": "This paper summarizes the structure and findings from the first Workshop on Troubles and Failures in Conversations between Humans and Robots . The workshop was organized to bring together a small, interdisciplinary group of researchers working on miscommunication from two complementary perspectives. One group of technology-oriented researchers was made up of roboticists, Human-Robot Interaction (HRI) researchers and dialogue system experts. The second group involved experts from conversation analysis, cognitive science, and linguistics. Uniting both groups of researchers is the belief that communication failures between humans and machines need to be taken seriously and that a systematic analysis of such failures may open fruitful avenues in research beyond current practices to improve such systems, including both speech-centric and multimodal interfaces. This workshop represents a starting point for this endeavour. The aim of the workshop was threefold: Firstly, to establish an interdisciplinary network of researchers that share a common interest in investigating communicative failures with a particular view towards robotic speech interfaces; secondly, to gain a partial overview of the “failure landscape” as experienced by roboticists and HRI researchers; and thirdly, to determine the potential for creating a robotic benchmark scenario for testing future speech interfaces with respect to the identified failures. The present article summarizes both the “failure landscape” surveyed during the workshop as well as the outcomes of the attempt to define a benchmark scenario.",
      "abstract": "This paper summarizes the structure and findings from the first Workshop on Troubles and Failures in Conversations between Humans and Robots . The workshop was organized to bring together a small, interdisciplinary group of researchers working on miscommunication from two complementary perspectives. One group of technology-oriented researchers was made up of roboticists, Human-Robot Interaction (HRI) researchers and dialogue system experts. The second group involved experts from conversation analysis, cognitive science, and linguistics. Uniting both groups of researchers is the belief that communication failures between humans and machines need to be taken seriously and that a systematic analysis of such failures may open fruitful avenues in research beyond current practices to improve such systems, including both speech-centric and multimodal interfaces. This workshop represents a starting point for this endeavour. The aim of the workshop was threefold: Firstly, to establish an interdisciplinary network of researchers that share a common interest in investigating communicative failures with a particular view towards robotic speech interfaces; secondly, to gain a partial overview of the “failure landscape” as experienced by roboticists and HRI researchers; and thirdly, to determine the potential for creating a robotic benchmark scenario for testing future speech interfaces with respect to the identified failures. The present article summarizes both the “failure landscape” surveyed during the workshop as well as the outcomes of the attempt to define a benchmark scenario.",
      "doi": "https://doi.org/10.3389/frobt.2023.1202306",
      "openalex_id": "https://openalex.org/W4389223778",
      "arxiv_id": "",
      "publication_date": "2023-12-01",
      "published": "2023-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dictionary Attacks on Speaker Verification",
      "summary": "In this paper, we propose dictionary attacks against speaker verification - a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.",
      "abstract": "In this paper, we propose dictionary attacks against speaker verification - a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.",
      "doi": "https://doi.org/10.1109/tifs.2022.3229583",
      "openalex_id": "https://openalex.org/W4313506319",
      "arxiv_id": "",
      "publication_date": "2022-12-15",
      "published": "2022-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Zero-Shot Voice Conversion Using a DDSP Vocoder",
      "summary": "In this paper, we propose a zero-shot voice conversion algorithm using a neural vocoder based on differential digital signal processing. The vocoder does not require auto-regression, and its lightweight, differentiable nature allows the proposed system to be trained in an end-to-end fashion. This enables the use of more perceptually relevant objective functions for model training, and allows feature conversion and vocoder sub-networks to internally learn their own acoustic representation in a data-driven manner. We illustrate the effectiveness of the proposed algorithm by both qualitative and quantitative means, with comparisons to some of our previous works.",
      "abstract": "In this paper, we propose a zero-shot voice conversion algorithm using a neural vocoder based on differential digital signal processing. The vocoder does not require auto-regression, and its lightweight, differentiable nature allows the proposed system to be trained in an end-to-end fashion. This enables the use of more perceptually relevant objective functions for model training, and allows feature conversion and vocoder sub-networks to internally learn their own acoustic representation in a data-driven manner. We illustrate the effectiveness of the proposed algorithm by both qualitative and quantitative means, with comparisons to some of our previous works.",
      "doi": "https://doi.org/10.1109/waspaa52581.2021.9632754",
      "openalex_id": "https://openalex.org/W4200027410",
      "arxiv_id": "",
      "publication_date": "2021-10-17",
      "published": "2021-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stylespeech: Self-Supervised Style Enhancing with VQ-VAE-Based Pre-Training for Expressive Audiobook Speech Synthesis",
      "summary": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446352",
      "openalex_id": "https://openalex.org/W4392903361",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentanglement of Latent Representations via Causal Interventions",
      "summary": "The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consists of finding the action responsible for the transition between two images. We test our method on standard synthetic and real-world disentanglement datasets. We show that it can effectively disentangle the factors of variation and perform precise interventions on high-level semantic attributes of an image without affecting its quality, even with imbalanced data distributions.",
      "abstract": "The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consists of finding the action responsible for the transition between two images. We test our method on standard synthetic and real-world disentanglement datasets. We show that it can effectively disentangle the factors of variation and perform precise interventions on high-level semantic attributes of an image without affecting its quality, even with imbalanced data distributions.",
      "doi": "https://doi.org/10.24963/ijcai.2023/361",
      "openalex_id": "https://openalex.org/W4385764366",
      "arxiv_id": "",
      "publication_date": "2023-08-01",
      "published": "2023-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI-Based Affective Music Generation Systems: A Review of Methods and Challenges",
      "summary": "Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.",
      "abstract": "Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.",
      "doi": "https://doi.org/10.1145/3672554",
      "openalex_id": "https://openalex.org/W4399743538",
      "arxiv_id": "",
      "publication_date": "2024-06-17",
      "published": "2024-06-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audiosr: Versatile Audio Super-Resolution at Scale",
      "summary": "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.",
      "abstract": "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447246",
      "openalex_id": "https://openalex.org/W4392903177",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How voice and helpfulness shape perceptions in human–agent teams",
      "summary": "Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.",
      "abstract": "Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.",
      "doi": "https://doi.org/10.1016/j.chbah.2024.100101",
      "openalex_id": "https://openalex.org/W4404135183",
      "arxiv_id": "",
      "publication_date": "2024-08-01",
      "published": "2024-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Investigating Personalization Methods in Text to Music Generation",
      "summary": "In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446869",
      "openalex_id": "https://openalex.org/W4393138539",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Harmonizing AI-Generated Music: Integrating Symbolic and Audio Models for Text-to-Music Generation",
      "summary": "The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.",
      "abstract": "The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.",
      "doi": "https://doi.org/10.1109/aiiip61647.2023.00030",
      "openalex_id": "https://openalex.org/W4391584535",
      "arxiv_id": "",
      "publication_date": "2023-10-27",
      "published": "2023-10-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation",
      "summary": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448249",
      "openalex_id": "https://openalex.org/W4392902987",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI for Musical Discovery",
      "summary": "What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.",
      "abstract": "What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.",
      "doi": "https://doi.org/10.21428/e4baedd9.8fa181e9",
      "openalex_id": "https://openalex.org/W4393228744",
      "arxiv_id": "",
      "publication_date": "2024-03-27",
      "published": "2024-03-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Learning for Style Transfer and Experimentation with Audio Effects and Music Creation",
      "summary": "Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.",
      "abstract": "Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.",
      "doi": "https://doi.org/10.1609/aaai.v38i21.30558",
      "openalex_id": "https://openalex.org/W4393145938",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input",
      "summary": "Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.",
      "abstract": "Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.",
      "doi": "https://doi.org/10.1109/nice61972.2024.10549580",
      "openalex_id": "https://openalex.org/W4399530928",
      "arxiv_id": "",
      "publication_date": "2024-04-23",
      "published": "2024-04-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative AI for Short Sound Message Transmission in the Internet of Things",
      "summary": "We leverage the latest advancements in generative AI for music creation to develop an automated system producing short sound messages. These sound-based messages, referred to as Transmit In Sound code (TIScode), are brief audio sequences lasting 5 seconds that carrying digital information. They can be recognized by a specific smartphone application in an Internet of Audio Things (IoAuT) scenario. We describe the methodologies of the TIScode pipeline, which includes generation, transmission, and ultimately, reception and decoding. For the generation phase, we use MusicGen, a state-of-the-art autoregressive transformer model, and we introduce a channel coding system based on the quantization of sound features and high-level features extracted through convolutional neural networks (CNNs). The extracted features are mapped to create a unique bitmap for each TIScode, simplifying the decoding process. We present an algorithm for the recognition phase, combining sound feature analysis with frequency-based peak analysis to enhance detection accuracy. Experimental results, obtained through simulation and field tests, demonstrate the effectiveness of the system in retrieving the digital information encoded within sound messages.",
      "abstract": "We leverage the latest advancements in generative AI for music creation to develop an automated system producing short sound messages. These sound-based messages, referred to as Transmit In Sound code (TIScode), are brief audio sequences lasting 5 seconds that carrying digital information. They can be recognized by a specific smartphone application in an Internet of Audio Things (IoAuT) scenario. We describe the methodologies of the TIScode pipeline, which includes generation, transmission, and ultimately, reception and decoding. For the generation phase, we use MusicGen, a state-of-the-art autoregressive transformer model, and we introduce a channel coding system based on the quantization of sound features and high-level features extracted through convolutional neural networks (CNNs). The extracted features are mapped to create a unique bitmap for each TIScode, simplifying the decoding process. We present an algorithm for the recognition phase, combining sound feature analysis with frequency-based peak analysis to enhance detection accuracy. Experimental results, obtained through simulation and field tests, demonstrate the effectiveness of the system in retrieving the digital information encoded within sound messages.",
      "doi": "https://doi.org/10.1109/icmlcn64995.2025.11140271",
      "openalex_id": "https://openalex.org/W4413967603",
      "arxiv_id": "",
      "publication_date": "2025-05-26",
      "published": "2025-05-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning latent representations for controllable combinational creativity and game design",
      "summary": "Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract",
      "abstract": "Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract",
      "doi": "https://doi.org/10.17760/d20581905",
      "openalex_id": "https://openalex.org/W4388761715",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation",
      "summary": "Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/",
      "abstract": "Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/",
      "doi": "https://doi.org/10.1109/bigdata59044.2023.10386748",
      "openalex_id": "https://openalex.org/W4391093895",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm",
      "summary": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.",
      "abstract": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.",
      "doi": "https://doi.org/10.1109/wacv57701.2024.00702",
      "openalex_id": "https://openalex.org/W4394625798",
      "arxiv_id": "",
      "publication_date": "2024-01-03",
      "published": "2024-01-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Raging with the Machine in the Uncanny Valley: Human–AI Cocreativity in the Eurovision-Themed AI Song Contest",
      "summary": "Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.",
      "abstract": "Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.",
      "doi": "https://doi.org/10.1162/comj_a_00674",
      "openalex_id": "https://openalex.org/W4396606329",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI Assistants in Media Production and Management: A Survey of Workflow Optimizations for Enhancing Creativity",
      "summary": "This paper delves into AI's transformative role in media production and management, examining its application in media asset management, video editing, audio production, and music composition. It investigates how AI, through technologies like semantic embedding and large language models, significantly impacts creative processes, workflow optimization, and ideation. AI aids in automating mundane tasks, enhancing contextual searches, and providing recommended editorial choices, thus allowing creators to concentrate on more complex creative tasks. The survey highlights the use of AI in content management, semantic media search, transcript-based video editing, sound design, and chord symbol auto-completion, illustrating AI's role as a collaborative partner that enhances human ingenuity. The paper underscores the symbiotic relationship between AI and creators, emphasizing the potential for AI to usher in a new era of innovative media content creation and management, positioning AI as a central component of the modern media landscape.",
      "abstract": "This paper delves into AI's transformative role in media production and management, examining its application in media asset management, video editing, audio production, and music composition. It investigates how AI, through technologies like semantic embedding and large language models, significantly impacts creative processes, workflow optimization, and ideation. AI aids in automating mundane tasks, enhancing contextual searches, and providing recommended editorial choices, thus allowing creators to concentrate on more complex creative tasks. The survey highlights the use of AI in content management, semantic media search, transcript-based video editing, sound design, and chord symbol auto-completion, illustrating AI's role as a collaborative partner that enhances human ingenuity. The paper underscores the symbiotic relationship between AI and creators, emphasizing the potential for AI to usher in a new era of innovative media content creation and management, positioning AI as a central component of the modern media landscape.",
      "doi": "https://doi.org/10.5594/jmi.2024/wren8857",
      "openalex_id": "https://openalex.org/W4398199707",
      "arxiv_id": "",
      "publication_date": "2024-05-15",
      "published": "2024-05-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms",
      "summary": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "abstract": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "doi": "https://doi.org/10.14569/ijacsa.2024.01505101",
      "openalex_id": "https://openalex.org/W4399260034",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Benchmarking Music Generation Models and Metrics via Human Preference Studies",
      "summary": "Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.",
      "abstract": "Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.",
      "doi": "https://doi.org/10.1109/icassp49660.2025.10887745",
      "openalex_id": "https://openalex.org/W4408345698",
      "arxiv_id": "",
      "publication_date": "2025-03-12",
      "published": "2025-03-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Variations in Variational Autoencoders - A Comparative Evaluation",
      "summary": "Variational Auto-Encoders (VAEs) are deep latent space generative models which have been immensely successful in many applications such as image generation, image captioning, protein design, mutation prediction, and language models among others. The fundamental idea in VAEs is to learn the distribution of data in such a way that new meaningful data can be generated from the encoded distribution. This concept has led to tremendous research and variations in the design of VAEs in the last few years creating a field of its own, referred to as unsupervised representation learning. This paper provides a much-needed comprehensive evaluation of the variations of the VAEs based on their end goals and resulting architectures. It further provides intuition as well as mathematical formulation and quantitative results of each popular variation, presents a concise comparison of these variations, and concludes with challenges and future opportunities for research in VAEs.",
      "abstract": "Variational Auto-Encoders (VAEs) are deep latent space generative models which have been immensely successful in many applications such as image generation, image captioning, protein design, mutation prediction, and language models among others. The fundamental idea in VAEs is to learn the distribution of data in such a way that new meaningful data can be generated from the encoded distribution. This concept has led to tremendous research and variations in the design of VAEs in the last few years creating a field of its own, referred to as unsupervised representation learning. This paper provides a much-needed comprehensive evaluation of the variations of the VAEs based on their end goals and resulting architectures. It further provides intuition as well as mathematical formulation and quantitative results of each popular variation, presents a concise comparison of these variations, and concludes with challenges and future opportunities for research in VAEs.",
      "doi": "https://doi.org/10.1109/access.2020.3018151",
      "openalex_id": "https://openalex.org/W3080764280",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Quasi-Periodic WaveNet: An Autoregressive Raw Waveform Generative Model With Pitch-Dependent Dilated Convolution Neural Network",
      "summary": "In this paper, a pitch-adaptive waveform generative model named\\nQuasi-Periodic WaveNet (QPNet) is proposed to improve the limited pitch\\ncontrollability of vanilla WaveNet (WN) using pitch-dependent dilated\\nconvolution neural networks (PDCNNs). Specifically, as a probabilistic\\nautoregressive generation model with stacked dilated convolution layers, WN\\nachieves high-fidelity audio waveform generation. However, the pure-data-driven\\nnature and the lack of prior knowledge of audio signals degrade the pitch\\ncontrollability of WN. For instance, it is difficult for WN to precisely\\ngenerate the periodic components of audio signals when the given auxiliary\\nfundamental frequency ($F_{0}$) features are outside the $F_{0}$ range observed\\nin the training data. To address this problem, QPNet with two novel designs is\\nproposed. First, the PDCNN component is applied to dynamically change the\\nnetwork architecture of WN according to the given auxiliary $F_{0}$ features.\\nSecond, a cascaded network structure is utilized to simultaneously model the\\nlong- and short-term dependencies of quasi-periodic signals such as speech. The\\nperformances of single-tone sinusoid and speech generations are evaluated. The\\nexperimental results show the effectiveness of the PDCNNs for unseen auxiliary\\n$F_{0}$ features and the effectiveness of the cascaded structure for speech\\ngeneration.\\n",
      "abstract": "In this paper, a pitch-adaptive waveform generative model named\\nQuasi-Periodic WaveNet (QPNet) is proposed to improve the limited pitch\\ncontrollability of vanilla WaveNet (WN) using pitch-dependent dilated\\nconvolution neural networks (PDCNNs). Specifically, as a probabilistic\\nautoregressive generation model with stacked dilated convolution layers, WN\\nachieves high-fidelity audio waveform generation. However, the pure-data-driven\\nnature and the lack of prior knowledge of audio signals degrade the pitch\\ncontrollability of WN. For instance, it is difficult for WN to precisely\\ngenerate the periodic components of audio signals when the given auxiliary\\nfundamental frequency ($F_{0}$) features are outside the $F_{0}$ range observed\\nin the training data. To address this problem, QPNet with two novel designs is\\nproposed. First, the PDCNN component is applied to dynamically change the\\nnetwork architecture of WN according to the given auxiliary $F_{0}$ features.\\nSecond, a cascaded network structure is utilized to simultaneously model the\\nlong- and short-term dependencies of quasi-periodic signals such as speech. The\\nperformances of single-tone sinusoid and speech generations are evaluated. The\\nexperimental results show the effectiveness of the PDCNNs for unseen auxiliary\\n$F_{0}$ features and the effectiveness of the cascaded structure for speech\\ngeneration.\\n",
      "doi": "https://doi.org/10.1109/taslp.2021.3061245",
      "openalex_id": "https://openalex.org/W3041738652",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transferring Neural Speech Waveform Synthesizers to Musical Instrument Sounds Generation",
      "summary": "Recent neural waveform synthesizers such as WaveNet, WaveG-low, and the neural-source-filter (NSF) model have shown good performance in speech synthesis despite their different methods of waveform generation. The similarity between speech and music audio synthesis techniques suggests interesting avenues to explore in terms of the best way to apply speech synthesizers in the music domain. This work compares three neural synthesizers used for musical instrument sounds generation under three scenarios: training from scratch on music data, zero-shot learning from the speech domain, and fine-tuning-based adaptation from the speech to the music domain. The results of a large-scale perceptual test demonstrated that the performance of three synthesizers improved when they were pre-trained on speech data and fine-tuned on music data, which indicates the usefulness of knowledge from speech data for music audio generation. Among the synthesizers, WaveGlow showed the best potential in zero-shot learning while NSF performed best in the other scenarios and could generate samples that were perceptually close to natural audio.",
      "abstract": "Recent neural waveform synthesizers such as WaveNet, WaveG-low, and the neural-source-filter (NSF) model have shown good performance in speech synthesis despite their different methods of waveform generation. The similarity between speech and music audio synthesis techniques suggests interesting avenues to explore in terms of the best way to apply speech synthesizers in the music domain. This work compares three neural synthesizers used for musical instrument sounds generation under three scenarios: training from scratch on music data, zero-shot learning from the speech domain, and fine-tuning-based adaptation from the speech to the music domain. The results of a large-scale perceptual test demonstrated that the performance of three synthesizers improved when they were pre-trained on speech data and fine-tuned on music data, which indicates the usefulness of knowledge from speech data for music audio generation. Among the synthesizers, WaveGlow showed the best potential in zero-shot learning while NSF performed best in the other scenarios and could generate samples that were perceptually close to natural audio.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053047",
      "openalex_id": "https://openalex.org/W3015710813",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GACELA: A Generative Adversarial Context Encoder for Long Audio Inpainting of Music",
      "summary": "We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.",
      "abstract": "We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.",
      "doi": "https://doi.org/10.1109/jstsp.2020.3037506",
      "openalex_id": "https://openalex.org/W3022195800",
      "arxiv_id": "",
      "publication_date": "2020-11-11",
      "published": "2020-11-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training",
      "summary": "We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation—here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-voice scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music. Despite differences between the two corpora, we find that this pre-training procedure improves both quantitative and qualitative performance for our primary task.",
      "abstract": "We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation—here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-voice scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music. Despite differences between the two corpora, we find that this pre-training procedure improves both quantitative and qualitative performance for our primary task.",
      "doi": "https://doi.org/10.5281/zenodo.3527901",
      "openalex_id": "https://openalex.org/W2959020461",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised VQ-VAE for One-Shot Music Style Transfer",
      "summary": "Neural style transfer, allowing to apply the artistic style of one image to\\nanother, has become one of the most widely showcased computer vision\\napplications shortly after its introduction. In contrast, related tasks in the\\nmusic audio domain remained, until recently, largely untackled. While several\\nstyle conversion methods tailored to musical signals have been proposed, most\\nlack the 'one-shot' capability of classical image style transfer algorithms. On\\nthe other hand, the results of existing one-shot audio style transfer methods\\non musical inputs are not as compelling. In this work, we are specifically\\ninterested in the problem of one-shot timbre transfer. We present a novel\\nmethod for this task, based on an extension of the vector-quantized variational\\nautoencoder (VQ-VAE), along with a simple self-supervised learning strategy\\ndesigned to obtain disentangled representations of timbre and pitch. We\\nevaluate the method using a set of objective metrics and show that it is able\\nto outperform selected baselines.\\n",
      "abstract": "Neural style transfer, allowing to apply the artistic style of one image to\\nanother, has become one of the most widely showcased computer vision\\napplications shortly after its introduction. In contrast, related tasks in the\\nmusic audio domain remained, until recently, largely untackled. While several\\nstyle conversion methods tailored to musical signals have been proposed, most\\nlack the 'one-shot' capability of classical image style transfer algorithms. On\\nthe other hand, the results of existing one-shot audio style transfer methods\\non musical inputs are not as compelling. In this work, we are specifically\\ninterested in the problem of one-shot timbre transfer. We present a novel\\nmethod for this task, based on an extension of the vector-quantized variational\\nautoencoder (VQ-VAE), along with a simple self-supervised learning strategy\\ndesigned to obtain disentangled representations of timbre and pitch. We\\nevaluate the method using a set of objective metrics and show that it is able\\nto outperform selected baselines.\\n",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414235",
      "openalex_id": "https://openalex.org/W3127854286",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer",
      "summary": "Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.",
      "abstract": "Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.347",
      "openalex_id": "https://openalex.org/W3205226109",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Applications of Deep Learning to Audio Generation",
      "summary": "In the recent past years, deep learning based machine learning systems have demonstrated remarkable success for a wide range of learning tasks in multiple domains such as computer vision, speech recognition and other pattern recognition based applications. The purpose of this article is to contribute a timely review and introduction of state-of-the-art deep learning techniques and their effectiveness in speech/acoustic signal processing. Thorough investigations of various deep learning architectures are provided under the categories of discriminative and generative algorithms, including the up-to-date Generative Adversarial Networks (GANs) as an integrated model. A comprehensive overview of applications in audio generation is highlighted. Based on understandings from these approaches, we discuss how deep learning methods can benefit the field of speech/acoustic signal synthesis and the potential issues that need to be addressed for prospective real-world scenarios. We hope this survey provides a valuable reference for practitioners seeking to innovate in the usage of deep learning approaches for speech/acoustic signal generation.",
      "abstract": "In the recent past years, deep learning based machine learning systems have demonstrated remarkable success for a wide range of learning tasks in multiple domains such as computer vision, speech recognition and other pattern recognition based applications. The purpose of this article is to contribute a timely review and introduction of state-of-the-art deep learning techniques and their effectiveness in speech/acoustic signal processing. Thorough investigations of various deep learning architectures are provided under the categories of discriminative and generative algorithms, including the up-to-date Generative Adversarial Networks (GANs) as an integrated model. A comprehensive overview of applications in audio generation is highlighted. Based on understandings from these approaches, we discuss how deep learning methods can benefit the field of speech/acoustic signal synthesis and the potential issues that need to be addressed for prospective real-world scenarios. We hope this survey provides a valuable reference for practitioners seeking to innovate in the usage of deep learning approaches for speech/acoustic signal generation.",
      "doi": "https://doi.org/10.1109/mcas.2019.2945210",
      "openalex_id": "https://openalex.org/W2989725337",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Normalizing Flows With Multi-Scale Autoregressive Priors",
      "summary": "Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.",
      "abstract": "Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.",
      "doi": "https://doi.org/10.1109/cvpr42600.2020.00844",
      "openalex_id": "https://openalex.org/W3035481475",
      "arxiv_id": "",
      "publication_date": "2020-06-01",
      "published": "2020-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Deep Learning Approach for Low-Latency Packet Loss Concealment of Audio Signals in Networked Music Performance Applications",
      "summary": "Networked Music Performance (NMP) is envisioned as a potential game changer among Internet applications: it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic conditions for music performance, however, constitutes a significant engineering challenge due to extremely strict requirements in terms of audio quality and, most importantly, network delay. To minimize the end-to-end delay experienced by the musicians, typical implementations of NMP applications use un-compressed, bidirectional audio streams and leverage UDP as transport protocol. Being connection less and unreliable,audio packets transmitted via UDP which become lost in transit are not re-transmitted and thus cause glitches in the receiver audio playout. This article describes a technique for predicting lost packet content in real-time using a deep learning approach. The ability of concealing errors in real time can help mitigate audio impairments caused by packet losses, thus improving the quality of audio playout in real-world scenarios.",
      "abstract": "Networked Music Performance (NMP) is envisioned as a potential game changer among Internet applications: it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic conditions for music performance, however, constitutes a significant engineering challenge due to extremely strict requirements in terms of audio quality and, most importantly, network delay. To minimize the end-to-end delay experienced by the musicians, typical implementations of NMP applications use un-compressed, bidirectional audio streams and leverage UDP as transport protocol. Being connection less and unreliable,audio packets transmitted via UDP which become lost in transit are not re-transmitted and thus cause glitches in the receiver audio playout. This article describes a technique for predicting lost packet content in real-time using a deep learning approach. The ability of concealing errors in real time can help mitigate audio impairments caused by packet losses, thus improving the quality of audio playout in real-world scenarios.",
      "doi": "https://doi.org/10.23919/fruct49677.2020.9210988",
      "openalex_id": "https://openalex.org/W3042610466",
      "arxiv_id": "",
      "publication_date": "2020-09-01",
      "published": "2020-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Drum Synthesis and Rhythmic Transformation with Adversarial Autoencoders",
      "summary": "Creative rhythmic transformations of musical audio refer to automated methods for manipulation of temporally-relevant sounds in time. This paper presents a method for joint synthesis and rhythm transformation of drum sounds through the use of adversarial autoencoders (AAE). Users may navigate both the timbre and rhythm of drum patterns in audio recordings through expressive control over a low-dimensional latent space. The model is based on an AAE with Gaussian mixture latent distributions that introduce rhythmic pattern conditioning to represent a wide variety of drum performances. The AAE is trained on a dataset of bar-length segments of percussion recordings, along with their clustered rhythmic pattern labels. The decoder is conditioned during adversarial training for mixing of data-driven rhythmic and timbral properties. The system is trained with over 500000 bars from 5418 tracks in popular datasets covering various musical genres. In an evaluation using real percussion recordings, the reconstruction accuracy and latent space interpolation between drum performances are investigated for audio generation conditioned by target rhythmic patterns.",
      "abstract": "Creative rhythmic transformations of musical audio refer to automated methods for manipulation of temporally-relevant sounds in time. This paper presents a method for joint synthesis and rhythm transformation of drum sounds through the use of adversarial autoencoders (AAE). Users may navigate both the timbre and rhythm of drum patterns in audio recordings through expressive control over a low-dimensional latent space. The model is based on an AAE with Gaussian mixture latent distributions that introduce rhythmic pattern conditioning to represent a wide variety of drum performances. The AAE is trained on a dataset of bar-length segments of percussion recordings, along with their clustered rhythmic pattern labels. The decoder is conditioned during adversarial training for mixing of data-driven rhythmic and timbral properties. The system is trained with over 500000 bars from 5418 tracks in popular datasets covering various musical genres. In an evaluation using real percussion recordings, the reconstruction accuracy and latent space interpolation between drum performances are investigated for audio generation conditioned by target rhythmic patterns.",
      "doi": "https://doi.org/10.1145/3394171.3413519",
      "openalex_id": "https://openalex.org/W3093209529",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Loopnet: Musical Loop Synthesis Conditioned on Intuitive Musical Parameters",
      "summary": "Loops, seamlessly repeatable musical segments, are a cornerstone of modern music production. Contemporary artists often mix and match various sampled or pre-recorded loops based on musical criteria such as rhythm, harmony and timbral texture to create compositions. Taking such criteria into account, we present LoopNet, a feed-forward generative model for creating loops conditioned on intuitive parameters. We leverage Music Information Retrieval (MIR) models as well as a large collection of public loop samples in our study and use the Wave-U-Net architecture to map control parameters to audio. We also evaluate the quality of the generated audio and propose intuitive controls for composers to map the ideas in their minds to an audio loop.",
      "abstract": "Loops, seamlessly repeatable musical segments, are a cornerstone of modern music production. Contemporary artists often mix and match various sampled or pre-recorded loops based on musical criteria such as rhythm, harmony and timbral texture to create compositions. Taking such criteria into account, we present LoopNet, a feed-forward generative model for creating loops conditioned on intuitive parameters. We leverage Music Information Retrieval (MIR) models as well as a large collection of public loop samples in our study and use the Wave-U-Net architecture to map control parameters to audio. We also evaluate the quality of the generated audio and propose intuitive controls for composers to map the ideas in their minds to an audio loop.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9415047",
      "openalex_id": "https://openalex.org/W3160235471",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling",
      "summary": "Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. While their receptive field grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, and prohibits the use of longer receptive fields in practice. To increase efficiency, we make use of the \"slow feature\" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model (\"Seq-U-Net\") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance on real-world tasks.",
      "abstract": "Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. While their receptive field grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, and prohibits the use of longer receptive fields in practice. To increase efficiency, we make use of the \"slow feature\" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model (\"Seq-U-Net\") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance on real-world tasks.",
      "doi": "https://doi.org/10.24963/ijcai.2020/400",
      "openalex_id": "https://openalex.org/W2986830070",
      "arxiv_id": "",
      "publication_date": "2020-07-01",
      "published": "2020-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Generative Models for Synthetic Data",
      "summary": "Growing interest in synthetic data has stimulated development and advancement of a large variety of deep generative models for a wide range of applications. However, as this research has progressed, its streams have become more specialized and disconnected from each other. For example, models for synthesizing text data for natural language processing cannot readily be compared to models for synthesizing health records. To mitigate this isolation, we propose a data-driven evaluation framework for generative models for synthetic data based on five high-level criteria: representativeness, novelty, realism, diversity and coherence of a synthetic data sample relative to the original data-set regardless of the models' internal structures. The criteria reflect requirements different domains impose on synthetic data and allow model users to assess the quality of synthetic data across models. In a critical review of generative models for sequential data, we examine and compare the importance of each performance criterion in numerous domains. For example, we find that realism and coherence are more important for synthetic data for natural language, speech and audio processing, while novelty and representativeness are more important for healthcare and mobility data. We also find that measurement of representativeness is often accomplished using statistical metrics, realism by using human judgement, and novelty using privacy tests.",
      "abstract": "Growing interest in synthetic data has stimulated development and advancement of a large variety of deep generative models for a wide range of applications. However, as this research has progressed, its streams have become more specialized and disconnected from each other. For example, models for synthesizing text data for natural language processing cannot readily be compared to models for synthesizing health records. To mitigate this isolation, we propose a data-driven evaluation framework for generative models for synthetic data based on five high-level criteria: representativeness, novelty, realism, diversity and coherence of a synthetic data sample relative to the original data-set regardless of the models' internal structures. The criteria reflect requirements different domains impose on synthetic data and allow model users to assess the quality of synthetic data across models. In a critical review of generative models for sequential data, we examine and compare the importance of each performance criterion in numerous domains. For example, we find that realism and coherence are more important for synthetic data for natural language, speech and audio processing, while novelty and representativeness are more important for healthcare and mobility data. We also find that measurement of representativeness is often accomplished using statistical metrics, realism by using human judgement, and novelty using privacy tests.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3217752380",
      "arxiv_id": "",
      "publication_date": "2021-11-01",
      "published": "2021-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Long Sequences with Sparse Transformers",
      "summary": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
      "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
      "doi": "https://doi.org/10.48550/arxiv.1904.10509",
      "openalex_id": "https://openalex.org/W2940744433",
      "arxiv_id": "",
      "publication_date": "2019-04-23",
      "published": "2019-04-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MelNet: A Generative Model for Audio in the Frequency Domain",
      "summary": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.",
      "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.",
      "doi": "https://doi.org/10.48550/arxiv.1906.01083",
      "openalex_id": "https://openalex.org/W2948211236",
      "arxiv_id": "",
      "publication_date": "2019-06-04",
      "published": "2019-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2",
      "summary": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
      "abstract": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
      "doi": "https://doi.org/10.48550/arxiv.1906.00446",
      "openalex_id": "https://openalex.org/W2947590261",
      "arxiv_id": "",
      "publication_date": "2019-06-02",
      "published": "2019-06-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WaveFlow: A Compact Flow-based Model for Raw Audio",
      "summary": "In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15$\\times$ smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6$\\times$ faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.",
      "abstract": "In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15$\\times$ smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6$\\times$ faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.",
      "doi": "https://doi.org/10.48550/arxiv.1912.01219",
      "openalex_id": "https://openalex.org/W2993118648",
      "arxiv_id": "",
      "publication_date": "2019-12-03",
      "published": "2019-12-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hierarchical Autoregressive Image Models with Auxiliary Decoders",
      "summary": "Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\\times$128 and 256$\\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models.",
      "abstract": "Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\\times$128 and 256$\\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models.",
      "doi": "https://doi.org/10.48550/arxiv.1903.04933",
      "openalex_id": "https://openalex.org/W2922386270",
      "arxiv_id": "",
      "publication_date": "2019-03-06",
      "published": "2019-03-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Artificial Musical Intelligence: A Survey",
      "summary": "Computers have been used to analyze and create music since they were first introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the Internet and large scale platforms for music recommendation and retrieval have made music an increasingly prevalent domain of machine learning and artificial intelligence research. While still nascent, several different approaches have been employed to tackle what may broadly be referred to as \"musical intelligence.\" This article provides a definition of musical intelligence, introduces a taxonomy of its constituent components, and surveys the wide range of AI methods that can be, and have been, brought to bear in its pursuit, with a particular emphasis on machine learning methods.",
      "abstract": "Computers have been used to analyze and create music since they were first introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the Internet and large scale platforms for music recommendation and retrieval have made music an increasingly prevalent domain of machine learning and artificial intelligence research. While still nascent, several different approaches have been employed to tackle what may broadly be referred to as \"musical intelligence.\" This article provides a definition of musical intelligence, introduces a taxonomy of its constituent components, and surveys the wide range of AI methods that can be, and have been, brought to bear in its pursuit, with a particular emphasis on machine learning methods.",
      "doi": "https://doi.org/10.48550/arxiv.2006.10553",
      "openalex_id": "https://openalex.org/W3036013631",
      "arxiv_id": "",
      "publication_date": "2020-06-17",
      "published": "2020-06-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes",
      "summary": "Whilst diffusion probabilistic models can generate high quality image content, key limitations remain in terms of both generating high-resolution imagery and their associated high computational requirements. Recent Vector-Quantized image models have overcome this limitation of image resolution but are prohibitively slow and unidirectional as they generate tokens via element-wise autoregressive sampling from the prior. By contrast, in this paper we propose a novel discrete diffusion probabilistic model prior which enables parallel prediction of Vector-Quantized tokens by using an unconstrained Transformer architecture as the backbone. During training, tokens are randomly masked in an order-agnostic manner and the Transformer learns to predict the original tokens. This parallelism of Vector-Quantized token prediction in turn facilitates unconditional generation of globally consistent high-resolution and diverse imagery at a fraction of the computational expense. In this manner, we can generate image resolutions exceeding that of the original training set samples whilst additionally provisioning per-image likelihood estimates (in a departure from generative adversarial approaches). Our approach achieves state-of-the-art results in terms of Density (LSUN Bedroom: 1.51; LSUN Churches: 1.12; FFHQ: 1.20) and Coverage (LSUN Bedroom: 0.83; LSUN Churches: 0.73; FFHQ: 0.80), and performs competitively on FID (LSUN Bedroom: 3.64; LSUN Churches: 4.07; FFHQ: 6.11) whilst offering advantages in terms of both computation and reduced training set requirements.",
      "abstract": "Whilst diffusion probabilistic models can generate high quality image content, key limitations remain in terms of both generating high-resolution imagery and their associated high computational requirements. Recent Vector-Quantized image models have overcome this limitation of image resolution but are prohibitively slow and unidirectional as they generate tokens via element-wise autoregressive sampling from the prior. By contrast, in this paper we propose a novel discrete diffusion probabilistic model prior which enables parallel prediction of Vector-Quantized tokens by using an unconstrained Transformer architecture as the backbone. During training, tokens are randomly masked in an order-agnostic manner and the Transformer learns to predict the original tokens. This parallelism of Vector-Quantized token prediction in turn facilitates unconditional generation of globally consistent high-resolution and diverse imagery at a fraction of the computational expense. In this manner, we can generate image resolutions exceeding that of the original training set samples whilst additionally provisioning per-image likelihood estimates (in a departure from generative adversarial approaches). Our approach achieves state-of-the-art results in terms of Density (LSUN Bedroom: 1.51; LSUN Churches: 1.12; FFHQ: 1.20) and Coverage (LSUN Bedroom: 0.83; LSUN Churches: 0.73; FFHQ: 0.80), and performs competitively on FID (LSUN Bedroom: 3.64; LSUN Churches: 4.07; FFHQ: 6.11) whilst offering advantages in terms of both computation and reduced training set requirements.",
      "doi": "https://doi.org/10.48550/arxiv.2111.12701",
      "openalex_id": "https://openalex.org/W3216433667",
      "arxiv_id": "",
      "publication_date": "2021-11-24",
      "published": "2021-11-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls",
      "summary": "Performance RNN is a machine-learning system designed primarily for the generation of solo piano performances using an event-based (rather than audio) representation. More specifically, Performance RNN is a long short-term memory (LSTM) based recurrent neural network that models polyphonic music with expressive timing and dynamics (Oore et al., 2018). The neural network uses a simple language model based on the Musical Instrument Digital Interface (MIDI) file format. Performance RNN is trained on the <em>e-Piano Junior Competition Dataset </em>(International Piano e-Competition 2018), a collection of solo piano performances by expert pianists. As an artistic tool, one of the limitations of the original model has been the lack of useable controls. The standard form of Performance RNN can generate interesting pieces, but little control is provided over <em>what </em>specifically is generated. This paper explores a set of conditioning-based controls used to influence the generation process. Below, a brief description of the samples that accompany the paper are given: <strong>sample_01.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Germany in 1685. <strong>sample_02.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_03.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_04.mp3: </strong>Tempo keyword conditioning. Conditioned to begin at adagio (very slow) and end at presto (very fast). <strong>sample_05.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_06.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_07.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Debussy and the beginning of a piece. <strong>sample_08.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Bach and the ending of a piece. <strong>sample_09.mp3: </strong>Tempo keyword and velocity conditioning. Conditioned to begin slow and quiet and then become fast and loud. <strong>sample_10.mp3: </strong>Composer conditioning. Conditioned on Ravel. <strong>sample_11.mp3: </strong>Composer conditioning. Conditioned on Schumann. <strong>sample_12.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_13.mp3: </strong>Birth year conditioning. Conditioned on the 1600s. <strong>sample_14.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Spain in 1860.",
      "abstract": "Performance RNN is a machine-learning system designed primarily for the generation of solo piano performances using an event-based (rather than audio) representation. More specifically, Performance RNN is a long short-term memory (LSTM) based recurrent neural network that models polyphonic music with expressive timing and dynamics (Oore et al., 2018). The neural network uses a simple language model based on the Musical Instrument Digital Interface (MIDI) file format. Performance RNN is trained on the <em>e-Piano Junior Competition Dataset </em>(International Piano e-Competition 2018), a collection of solo piano performances by expert pianists. As an artistic tool, one of the limitations of the original model has been the lack of useable controls. The standard form of Performance RNN can generate interesting pieces, but little control is provided over <em>what </em>specifically is generated. This paper explores a set of conditioning-based controls used to influence the generation process. Below, a brief description of the samples that accompany the paper are given: <strong>sample_01.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Germany in 1685. <strong>sample_02.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_03.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_04.mp3: </strong>Tempo keyword conditioning. Conditioned to begin at adagio (very slow) and end at presto (very fast). <strong>sample_05.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_06.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_07.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Debussy and the beginning of a piece. <strong>sample_08.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Bach and the ending of a piece. <strong>sample_09.mp3: </strong>Tempo keyword and velocity conditioning. Conditioned to begin slow and quiet and then become fast and loud. <strong>sample_10.mp3: </strong>Composer conditioning. Conditioned on Ravel. <strong>sample_11.mp3: </strong>Composer conditioning. Conditioned on Schumann. <strong>sample_12.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_13.mp3: </strong>Birth year conditioning. Conditioned on the 1600s. <strong>sample_14.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Spain in 1860.",
      "doi": "https://doi.org/10.5281/zenodo.3277294",
      "openalex_id": "https://openalex.org/W2958816042",
      "arxiv_id": "",
      "publication_date": "2019-05-17",
      "published": "2019-05-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Learning for Music Composition: Generation, Recommendation and Control",
      "summary": "Technology has always helped expand the range of musical expression, from the fortepiano to synthesizers to electronic sequencers. Could machine learning further extend human creativity? We explore three ways deep learning supports the creative process: generation, recommendation, and control. Generative models can synthesize stylistic idioms, enabling artists to explore a wider palette of possibilities. Recommendation tools can assist artists in curation. Better model control helps artists stay in the creative loop. Furthermore, this control could take place at one or more musically-meaningful levels -- the score, the performance, or timbre -- or on a non-musical level, such as a subjective quality like “scary.” This dissertation posits that deep learning models designed to better match the structure of music can generate, recommend and provide control in the creative process, making music composition more accessible. I describe four projects to support this statement. AdaptiveKnobs uses Gaussian Processes to capture the nonlinear multimodal relationship between low-level sound synthesis parameters and perceived sound qualities. By using active learning, we assist sound designers in defining their own intuitive knobs by querying them on sounds that the model expects to improve the controls most. ChordRipple uses Chord2Vec to learn chord embeddings for recommending creative substitutions and a Ripple mechanism to propagate changes, allowing novices to compose more adventurous chord progressions. Music Transformer uses self-attention mechanisms to capture the self-similarity structure of music, generating coherent expressive piano music from scratch. As the model processes composition and performance as one, improvisers can play an initial motif and have the model develop it in a coherent fashion. Coconet uses convolutions to capture pitch and temporal invariance. The generative model fills in arbitrarily-partial musical scores, allowing it to perform a wide range of musical tasks. The model uses Gibbs sampling to approximate how human composers improve their music through rewriting. Recently, Coconet powered the Bach Doodle, harmonizing more than 50 million melodies composed by users. We hope machine learning can enable new ways of approaching the creative process for both novices and musicians.",
      "abstract": "Technology has always helped expand the range of musical expression, from the fortepiano to synthesizers to electronic sequencers. Could machine learning further extend human creativity? We explore three ways deep learning supports the creative process: generation, recommendation, and control. Generative models can synthesize stylistic idioms, enabling artists to explore a wider palette of possibilities. Recommendation tools can assist artists in curation. Better model control helps artists stay in the creative loop. Furthermore, this control could take place at one or more musically-meaningful levels -- the score, the performance, or timbre -- or on a non-musical level, such as a subjective quality like “scary.” This dissertation posits that deep learning models designed to better match the structure of music can generate, recommend and provide control in the creative process, making music composition more accessible. I describe four projects to support this statement. AdaptiveKnobs uses Gaussian Processes to capture the nonlinear multimodal relationship between low-level sound synthesis parameters and perceived sound qualities. By using active learning, we assist sound designers in defining their own intuitive knobs by querying them on sounds that the model expects to improve the controls most. ChordRipple uses Chord2Vec to learn chord embeddings for recommending creative substitutions and a Ripple mechanism to propagate changes, allowing novices to compose more adventurous chord progressions. Music Transformer uses self-attention mechanisms to capture the self-similarity structure of music, generating coherent expressive piano music from scratch. As the model processes composition and performance as one, improvisers can play an initial motif and have the model develop it in a coherent fashion. Coconet uses convolutions to capture pitch and temporal invariance. The generative model fills in arbitrarily-partial musical scores, allowing it to perform a wide range of musical tasks. The model uses Gibbs sampling to approximate how human composers improve their music through rewriting. Recently, Coconet powered the Bach Doodle, harmonizing more than 50 million melodies composed by users. We hope machine learning can enable new ways of approaching the creative process for both novices and musicians.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2996019936",
      "arxiv_id": "",
      "publication_date": "2019-05-21",
      "published": "2019-05-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using a Bi-directional LSTM Model with Attention Mechanism trained on MIDI Data for Generating Unique Music",
      "summary": "Generating music is an interesting and challenging problem in the field of machine learning. Mimicking human creativity has been popular in recent years, especially in the field of computer vision and image processing. With the advent of GANs, it is possible to generate new similar images, based on trained data. But this cannot be done for music similarly, as music has an extra temporal dimension. So it is necessary to understand how music is represented in digital form. When building models that perform this generative task, the learning and generation part is done in some high-level representation such as MIDI (Musical Instrument Digital Interface) or scores. This paper proposes a bi-directional LSTM (Long short-term memory) model with attention mechanism capable of generating similar type of music based on MIDI data. The music generated by the model follows the theme/style of the music the model is trained on. Also, due to the nature of MIDI, the tempo, instrument, and other parameters can be defined, and changed, post generation.",
      "abstract": "Generating music is an interesting and challenging problem in the field of machine learning. Mimicking human creativity has been popular in recent years, especially in the field of computer vision and image processing. With the advent of GANs, it is possible to generate new similar images, based on trained data. But this cannot be done for music similarly, as music has an extra temporal dimension. So it is necessary to understand how music is represented in digital form. When building models that perform this generative task, the learning and generation part is done in some high-level representation such as MIDI (Musical Instrument Digital Interface) or scores. This paper proposes a bi-directional LSTM (Long short-term memory) model with attention mechanism capable of generating similar type of music based on MIDI data. The music generated by the model follows the theme/style of the music the model is trained on. Also, due to the nature of MIDI, the tempo, instrument, and other parameters can be defined, and changed, post generation.",
      "doi": "https://doi.org/10.48550/arxiv.2011.00773",
      "openalex_id": "https://openalex.org/W3096431806",
      "arxiv_id": "",
      "publication_date": "2020-11-02",
      "published": "2020-11-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SchrödingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State",
      "summary": "We introduce SchrödingeRNN, a quantum inspired generative model for raw audio. Audio data is wave-like and is sampled from a continuous signal. Although generative modelling of raw audio has made great strides lately, relational inductive biases relevant to these two characteristics are mostly absent from models explored to date. Quantum Mechanics is a natural source of probabilistic models of wave behaviour. Our model takes the form of a stochastic Schrödinger equation describing the continuous time measurement of a quantum system, and is equivalent to the continuous Matrix Product State (cMPS) representation of wavefunctions in one dimensional many-body systems. This constitutes a deep autoregressive architecture in which the systems state is a latent representation of the past observations. We test our model on synthetic data sets of stationary and non-stationary signals. This is the first time cMPS are used in machine learning.",
      "abstract": "We introduce SchrödingeRNN, a quantum inspired generative model for raw audio. Audio data is wave-like and is sampled from a continuous signal. Although generative modelling of raw audio has made great strides lately, relational inductive biases relevant to these two characteristics are mostly absent from models explored to date. Quantum Mechanics is a natural source of probabilistic models of wave behaviour. Our model takes the form of a stochastic Schrödinger equation describing the continuous time measurement of a quantum system, and is equivalent to the continuous Matrix Product State (cMPS) representation of wavefunctions in one dimensional many-body systems. This constitutes a deep autoregressive architecture in which the systems state is a latent representation of the past observations. We test our model on synthetic data sets of stationary and non-stationary signals. This is the first time cMPS are used in machine learning.",
      "doi": "https://doi.org/10.48550/arxiv.1911.11879",
      "openalex_id": "https://openalex.org/W2990991171",
      "arxiv_id": "",
      "publication_date": "2019-11-26",
      "published": "2019-11-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PixelPyramids: Exact Inference Models from Lossless Image Pyramids",
      "summary": "Autoregressive models are a class of exact inference approaches with highly flexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these models computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose Pixel-Pyramids, a block-autoregressive approach employing a lossless pyramid decomposition with scale-specific representations to encode the joint distribution of image pixels. Crucially, it affords a sparser dependency structure compared to fully autoregressive approaches. Our PixelPyramids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. For CelebA-HQ 1024 x 1024, we observe that the density estimates (in terms of bits/dim) are improved to ~44% of the baseline despite sampling speeds superior even to easily parallelizable flow-based models.",
      "abstract": "Autoregressive models are a class of exact inference approaches with highly flexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these models computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose Pixel-Pyramids, a block-autoregressive approach employing a lossless pyramid decomposition with scale-specific representations to encode the joint distribution of image pixels. Crucially, it affords a sparser dependency structure compared to fully autoregressive approaches. Our PixelPyramids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. For CelebA-HQ 1024 x 1024, we observe that the density estimates (in terms of bits/dim) are improved to ~44% of the baseline despite sampling speeds superior even to easily parallelizable flow-based models.",
      "doi": "https://doi.org/10.48550/arxiv.2110.08787",
      "openalex_id": "https://openalex.org/W3207217026",
      "arxiv_id": "",
      "publication_date": "2021-10-17",
      "published": "2021-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations",
      "summary": "Multimodal speech recognition aims to improve the performance of automatic speech recognition (ASR) systems by leveraging additional visual information that is usually associated to the audio input. While previous approaches make crucial use of strong visual representations, e.g. by finetuning pretrained image recognition networks, significantly less attention has been paid to its counterpart: the speech component. In this work, we investigate ways of improving the base speech recognition system by following similar techniques to the ones used for the visual encoder, namely, transferring representations and data augmentation. First, we show that starting from a pretrained ASR significantly improves the state-of-the-art performance; remarkably, even when building upon a strong unimodal system, we still find gains by including the visual modality. Second, we employ speech data augmentation techniques to encourage the multimodal system to attend to the visual stimuli. This technique replaces previously used word masking and comes with the benefits of being conceptually simpler and yielding consistent improvements in the multimodal setting. We provide empirical results on three multimodal datasets, including the newly introduced Localized Narratives.",
      "abstract": "Multimodal speech recognition aims to improve the performance of automatic speech recognition (ASR) systems by leveraging additional visual information that is usually associated to the audio input. While previous approaches make crucial use of strong visual representations, e.g. by finetuning pretrained image recognition networks, significantly less attention has been paid to its counterpart: the speech component. In this work, we investigate ways of improving the base speech recognition system by following similar techniques to the ones used for the visual encoder, namely, transferring representations and data augmentation. First, we show that starting from a pretrained ASR significantly improves the state-of-the-art performance; remarkably, even when building upon a strong unimodal system, we still find gains by including the visual modality. Second, we employ speech data augmentation techniques to encourage the multimodal system to attend to the visual stimuli. This technique replaces previously used word masking and comes with the benefits of being conceptually simpler and yielding consistent improvements in the multimodal setting. We provide empirical results on three multimodal datasets, including the newly introduced Localized Narratives.",
      "doi": "https://doi.org/10.1109/cvprw56347.2022.00504",
      "openalex_id": "https://openalex.org/W4225166170",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Align or attend? Toward More Efficient and Accurate Spoken Word Discovery Using Speech-to-Image Retrieval",
      "summary": "Multimodal word discovery (MWD) is often treated as a byproduct of the speech-to-image retrieval problem. However, our theoretical analysis shows that some kind of alignment/attention mechanism is crucial for a MWD system to learn meaningful word-level representation. We verify our theory by conducting retrieval and word discovery experiments on MSCOCO and Flickr8k, and empirically demonstrate that both neural MT with self-attention and statistical MT achieve word discovery scores that are superior to those of a state-of-the-art neural retrieval system, outperforming it by 2% and 5% alignment F1 scores respectively.",
      "abstract": "Multimodal word discovery (MWD) is often treated as a byproduct of the speech-to-image retrieval problem. However, our theoretical analysis shows that some kind of alignment/attention mechanism is crucial for a MWD system to learn meaningful word-level representation. We verify our theory by conducting retrieval and word discovery experiments on MSCOCO and Flickr8k, and empirically demonstrate that both neural MT with self-attention and statistical MT achieve word discovery scores that are superior to those of a state-of-the-art neural retrieval system, outperforming it by 2% and 5% alignment F1 scores respectively.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414418",
      "openalex_id": "https://openalex.org/W3161204797",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Audio-Caption Aligning Learns Correspondences Between Individual Sound Events and Textual Phrases",
      "summary": "We investigate unsupervised learning of correspondences between sound events and textual phrases through aligning audio clips with textual captions describing the content of a whole audio clip. We align originally unaligned and unannotated audio clips and their captions by scoring the similarities between audio frames and words, as encoded by modality-specific encoders and using a ranking-loss criterion to optimize the model. After training, we obtain clip-caption similarity by averaging frame-word similarities and estimate event-phrase correspondences by calculating frame-phrase similarities. We evaluate the method with two cross-modal tasks: audio-caption retrieval, and phrase-based sound event detection (SED). Experimental results show that the proposed method can globally associate audio clips with captions as well as locally learn correspondences between individual sound events and textual phrases in an unsupervised manner.",
      "abstract": "We investigate unsupervised learning of correspondences between sound events and textual phrases through aligning audio clips with textual captions describing the content of a whole audio clip. We align originally unaligned and unannotated audio clips and their captions by scoring the similarities between audio frames and words, as encoded by modality-specific encoders and using a ranking-loss criterion to optimize the model. After training, we obtain clip-caption similarity by averaging frame-word similarities and estimate event-phrase correspondences by calculating frame-phrase similarities. We evaluate the method with two cross-modal tasks: audio-caption retrieval, and phrase-based sound event detection (SED). Experimental results show that the proposed method can globally associate audio clips with captions as well as locally learn correspondences between individual sound events and textual phrases in an unsupervised manner.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747336",
      "openalex_id": "https://openalex.org/W3204267711",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reverse engineering language acquisition with child-centered long-form recordings",
      "summary": "Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.",
      "abstract": "Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.",
      "doi": "https://doi.org/10.31234/osf.io/pt9xq",
      "openalex_id": "https://openalex.org/W4247178956",
      "arxiv_id": "",
      "publication_date": "2021-03-31",
      "published": "2021-03-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples",
      "summary": "The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.",
      "abstract": "The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095091",
      "openalex_id": "https://openalex.org/W4372266917",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks",
      "summary": "Semantically-aligned $(speech, image)$ datasets can be used to explore visually-grounded speech. In a majority of existing investigations, features of an image signal are extracted using neural networks on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without transfer learning through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \\rightarrow image$ and $image \\rightarrow speech$ queries. \r\nChoosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \\rightarrow image$ and $image \\rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \\rightarrow image$ and $image \\rightarrow speech$ queries.",
      "abstract": "Semantically-aligned $(speech, image)$ datasets can be used to explore visually-grounded speech. In a majority of existing investigations, features of an image signal are extracted using neural networks on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without transfer learning through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \\rightarrow image$ and $image \\rightarrow speech$ queries. \r\nChoosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \\rightarrow image$ and $image \\rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \\rightarrow image$ and $image \\rightarrow speech$ queries.",
      "doi": "https://doi.org/10.21437/interspeech.2020-3024",
      "openalex_id": "https://openalex.org/W3096372900",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Visually Prompted Keyword Localisation for Zero-Resource Spoken Languages",
      "summary": "Imagine being able to show a system a visual depiction of a keyword and finding spoken utterances that contain this keyword from a zero-resource speech corpus. We formalise this task and call it visually prompted keyword localisation (VPKL): given an image of a keyword, detect and predict where in an utterance the keyword occurs. To do VPKL, we propose a speech-vision model with a novel localising attention mechanism which we train with a new keyword sampling scheme. We show that these innovations give improvements in VPKL over an existing speech-vision model. We also compare to a visual bag-of-words (BoW) model where images are automatically tagged with visual labels and paired with unlabelled speech. Although this visual BoW can be queried directly with a written keyword (while our's takes image queries), our new model still outperforms the visual BoW in both detection and localisation, giving a 16% relative improvement in localisation F1.",
      "abstract": "Imagine being able to show a system a visual depiction of a keyword and finding spoken utterances that contain this keyword from a zero-resource speech corpus. We formalise this task and call it visually prompted keyword localisation (VPKL): given an image of a keyword, detect and predict where in an utterance the keyword occurs. To do VPKL, we propose a speech-vision model with a novel localising attention mechanism which we train with a new keyword sampling scheme. We show that these innovations give improvements in VPKL over an existing speech-vision model. We also compare to a visual bag-of-words (BoW) model where images are automatically tagged with visual labels and paired with unlabelled speech. Although this visual BoW can be queried directly with a written keyword (while our's takes image queries), our new model still outperforms the visual BoW in both detection and localisation, giving a 16% relative improvement in localisation F1.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023079",
      "openalex_id": "https://openalex.org/W4319862278",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Talk, Don&amp;#8217;t Write: A Study of Direct Speech-Based Image Retrieval",
      "summary": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself.As such, it is unclear how well speech-based retrieval can work in practice -both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders.In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors.Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives.Our best model configuration achieves large gains over state of the art, e.g., pushing recall-atone from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio.We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.",
      "abstract": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself.As such, it is unclear how well speech-based retrieval can work in practice -both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders.In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors.Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives.Our best model configuration achieves large gains over state of the art, e.g., pushing recall-atone from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio.We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.",
      "doi": "https://doi.org/10.21437/interspeech.2021-96",
      "openalex_id": "https://openalex.org/W3196698946",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Video-Guided Curriculum Learning for Spoken Video Grounding",
      "summary": "In this paper, we introduce a new task, spoken video grounding (SVG), which aims to localize the desired video fragments from spoken language descriptions. Compared with using text, employing audio requires the model to directly exploit the useful phonemes and syllables related to the video from raw speech. Moreover, we randomly add environmental noises to this speech audio, further increasing the difficulty of this task and better simulating real applications. To rectify the discriminative phonemes and extract video-related information from noisy audio, we develop a novel video-guided curriculum learning (VGCL) during the audio pre-training process, which can make use of the vital visual perceptions to help understand the spoken language and suppress the external noise. Considering during inference the model can not obtain ground truth video segments, we design a curriculum strategy that gradually shifts the input video from the ground truth to the entire video content during pre-training. Finally, the model can learn how to extract critical visual information from the entire video clip to help understand the spoken language. In addition, we collect the first large-scale spoken video grounding dataset based on ActivityNet, which is named as ActivityNet Speech dataset. Extensive experiments demonstrate our proposed video-guided curriculum learning can facilitate the pre-training process to obtain a mutual audio encoder, significantly promoting the performance of spoken video grounding tasks. Moreover, we prove that in the case of noisy sound, our model outperforms the method that grounding video with ASR transcripts, further demonstrating the effectiveness of our curriculum strategy.",
      "abstract": "In this paper, we introduce a new task, spoken video grounding (SVG), which aims to localize the desired video fragments from spoken language descriptions. Compared with using text, employing audio requires the model to directly exploit the useful phonemes and syllables related to the video from raw speech. Moreover, we randomly add environmental noises to this speech audio, further increasing the difficulty of this task and better simulating real applications. To rectify the discriminative phonemes and extract video-related information from noisy audio, we develop a novel video-guided curriculum learning (VGCL) during the audio pre-training process, which can make use of the vital visual perceptions to help understand the spoken language and suppress the external noise. Considering during inference the model can not obtain ground truth video segments, we design a curriculum strategy that gradually shifts the input video from the ground truth to the entire video content during pre-training. Finally, the model can learn how to extract critical visual information from the entire video clip to help understand the spoken language. In addition, we collect the first large-scale spoken video grounding dataset based on ActivityNet, which is named as ActivityNet Speech dataset. Extensive experiments demonstrate our proposed video-guided curriculum learning can facilitate the pre-training process to obtain a mutual audio encoder, significantly promoting the performance of spoken video grounding tasks. Moreover, we prove that in the case of noisy sound, our model outperforms the method that grounding video with ASR transcripts, further demonstrating the effectiveness of our curriculum strategy.",
      "doi": "https://doi.org/10.1145/3503161.3547996",
      "openalex_id": "https://openalex.org/W4294533720",
      "arxiv_id": "",
      "publication_date": "2022-10-10",
      "published": "2022-10-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Double Articulation Analyzer With Prosody for Unsupervised Word and Phone Discovery",
      "summary": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "abstract": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "doi": "https://doi.org/10.1109/tcds.2022.3210751",
      "openalex_id": "https://openalex.org/W4313053756",
      "arxiv_id": "",
      "publication_date": "2022-09-29",
      "published": "2022-09-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Fine-Grained Semantics in Spoken Language Using Visual Grounding",
      "summary": "&lt;p&gt;In the case of unwritten languages, acoustic models cannot be trained in the standard way, i.e., using speech and textual transcriptions. Recently, several methods have been proposed to learn speech representations using images, i.e., using visual grounding. Existing studies have focused on scene images. Here, we investigate whether fine-grained semantic information, reflecting the relationship between attributes and objects, can be learned from spoken language. To this end, a Fine-grained Semantic Embedding Network (FSEN) for learning semantic representations of spoken language grounded by fine-grained images is proposed. For training, we propose an efficient objective function, which includes a matching constraint, an adversarial objective, and a classification constraint. The learned speech representations are evaluated using two tasks, i.e., speech-image cross-modal retrieval and speech-to-image generation. On the retrieval task, FSEN outperforms other state-of-the-art methods on both a scene image dataset and two fine-grained datasets. The image generation task shows that the learned speech representations can be used to generate high-quality and semantic-consistent fine-grained images. Learning fine-grained semantics from spoken language via visual grounding is thus possible.&lt;/p&gt;",
      "abstract": "&lt;p&gt;In the case of unwritten languages, acoustic models cannot be trained in the standard way, i.e., using speech and textual transcriptions. Recently, several methods have been proposed to learn speech representations using images, i.e., using visual grounding. Existing studies have focused on scene images. Here, we investigate whether fine-grained semantic information, reflecting the relationship between attributes and objects, can be learned from spoken language. To this end, a Fine-grained Semantic Embedding Network (FSEN) for learning semantic representations of spoken language grounded by fine-grained images is proposed. For training, we propose an efficient objective function, which includes a matching constraint, an adversarial objective, and a classification constraint. The learned speech representations are evaluated using two tasks, i.e., speech-image cross-modal retrieval and speech-to-image generation. On the retrieval task, FSEN outperforms other state-of-the-art methods on both a scene image dataset and two fine-grained datasets. The image generation task shows that the learned speech representations can be used to generate high-quality and semantic-consistent fine-grained images. Learning fine-grained semantics from spoken language via visual grounding is thus possible.&lt;/p&gt;",
      "doi": "https://doi.org/10.1109/iscas51556.2021.9401232",
      "openalex_id": "https://openalex.org/W3158565912",
      "arxiv_id": "",
      "publication_date": "2021-04-27",
      "published": "2021-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
      "summary": "Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision.However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data.We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios.This dataset expands upon ObjectNet, which is a biascontrolled image dataset that features similar image classes to those present in ImageNet.We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks.Lastly, we show baseline results on image retrieval and audio retrieval tasks.These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned.We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting.",
      "abstract": "Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision.However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data.We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios.This dataset expands upon ObjectNet, which is a biascontrolled image dataset that features similar image classes to those present in ImageNet.We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks.Lastly, we show baseline results on image retrieval and audio retrieval tasks.These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned.We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting.",
      "doi": "https://doi.org/10.21437/interspeech.2021-245",
      "openalex_id": "https://openalex.org/W3197064454",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech",
      "summary": "The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.",
      "abstract": "The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.",
      "doi": "https://doi.org/10.18653/v1/2020.conll-1.22",
      "openalex_id": "https://openalex.org/W3035242764",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Multimodal Few-Shot Learning of Speech and Images",
      "summary": "We propose direct multimodal few-shot models that learn a shared embedding space of spoken words and images from only a few paired examples. Imagine an agent is shown an image along with a spoken word describing the object in the picture, e.g. pen, book and eraser. After observing a few paired examples of each class, the model is asked to identify the \"book\" in a set of unseen pictures. Previous work used a two-step indirect approach relying on learned unimodal representations: speech-speech and image-image comparisons are performed across the support set of given speech-image pairs. We propose two direct models which instead learn a single multimodal space where inputs from different modalities are directly comparable: a multimodal triplet network (MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these direct models, we mine speech-image pairs: the support set is used to pair up unlabelled in-domain speech and images. In a speech-to-image digit matching task, direct models outperform indirect models, with the MTriplet achieving the best multimodal five-shot accuracy. We show that the improvements are due to the combination of unsupervised and transfer learning in the direct models, and the absence of two-step compounding errors.",
      "abstract": "We propose direct multimodal few-shot models that learn a shared embedding space of spoken words and images from only a few paired examples. Imagine an agent is shown an image along with a spoken word describing the object in the picture, e.g. pen, book and eraser. After observing a few paired examples of each class, the model is asked to identify the \"book\" in a set of unseen pictures. Previous work used a two-step indirect approach relying on learned unimodal representations: speech-speech and image-image comparisons are performed across the support set of given speech-image pairs. We propose two direct models which instead learn a single multimodal space where inputs from different modalities are directly comparable: a multimodal triplet network (MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these direct models, we mine speech-image pairs: the support set is used to pair up unlabelled in-domain speech and images. In a speech-to-image digit matching task, direct models outperform indirect models, with the MTriplet achieving the best multimodal five-shot accuracy. We show that the improvements are due to the combination of unsupervised and transfer learning in the direct models, and the absence of two-step compounding errors.",
      "doi": "https://doi.org/10.21437/interspeech.2021-49",
      "openalex_id": "https://openalex.org/W3113159494",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Co-Segmentation for Athlete Movements and Live Commentaries Using Crossmodal Temporal Proximity",
      "summary": "Audio-visual co-segmentation is a task to extract segments and regions corresponding to specific events on unlabeled audio and video signals. It is particularly important to accomplish it in an unsupervised way, since it is generally very difficult to manually label all the objects and events appearing in audio-visual signals for supervised learning. Here, we propose to take advantage of the temporal proximity of corresponding audio and video entities included in the signals. For this purpose, we newly employ a guided attention scheme to this task to efficiently detect and utilize temporal co-occurrences of audio and video information. Experiments using a real TV broadcasts of sumo wrestling, a sport event, with live commentaries show that our model can automatically extract specific athlete movements and its spoken descriptions in an unsupervised manner.",
      "abstract": "Audio-visual co-segmentation is a task to extract segments and regions corresponding to specific events on unlabeled audio and video signals. It is particularly important to accomplish it in an unsupervised way, since it is generally very difficult to manually label all the objects and events appearing in audio-visual signals for supervised learning. Here, we propose to take advantage of the temporal proximity of corresponding audio and video entities included in the signals. For this purpose, we newly employ a guided attention scheme to this task to efficiently detect and utilize temporal co-occurrences of audio and video information. Experiments using a real TV broadcasts of sumo wrestling, a sport event, with live commentaries show that our model can automatically extract specific athlete movements and its spoken descriptions in an unsupervised manner.",
      "doi": "https://doi.org/10.1109/icpr48806.2021.9412233",
      "openalex_id": "https://openalex.org/W3163069788",
      "arxiv_id": "",
      "publication_date": "2021-01-10",
      "published": "2021-01-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Translation Framework for Visually Grounded Spoken Unit Discovery",
      "summary": "Multimodal acoustic unit discovery (MAUD) is a key task in self-supervised spoken language learning and low-resource speech recognition. In this paper, we proposed two models for MAUD inspired by machine translation models where we treat speech and image as source and target languages. Our word discovery model outperforms previous state-of-the-art approach by 5.3% alignment F1 on SpeechCOCO dataset and our phoneme discovery model outperforms previous state-of-the-art approach by 7% normalized mutual information on TIMIT dataset.",
      "abstract": "Multimodal acoustic unit discovery (MAUD) is a key task in self-supervised spoken language learning and low-resource speech recognition. In this paper, we proposed two models for MAUD inspired by machine translation models where we treat speech and image as source and target languages. Our word discovery model outperforms previous state-of-the-art approach by 5.3% alignment F1 on SpeechCOCO dataset and our phoneme discovery model outperforms previous state-of-the-art approach by 7% normalized mutual information on TIMIT dataset.",
      "doi": "https://doi.org/10.1109/ieeeconf53345.2021.9723367",
      "openalex_id": "https://openalex.org/W4214813806",
      "arxiv_id": "",
      "publication_date": "2021-10-31",
      "published": "2021-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Spoken Question Understanding and Speaking with Automatic Vocabulary Learning",
      "summary": "Spoken language acquisition involves automatically developing symbolic word concepts grounding their meaning to the world, recognizing the words in spoken utterances, and pronouncing them. Previous research only partly covered these aspects. One of the most comprehensive agent systems supported the word concept acquisition from pairs of raw speech and image and utterance pronunciation. However, the agent listened to nothing when interacting with the world, only pronouncing a food name to choose a favorite one among two shown images. In this work, we add a function to the agent to recognize a verbal question. Namely, we design a task where the agent must recognize a question in a sound utterance and understand the logical \"not\" concept. Experimental results show that the agent successfully learns the task. It appropriately behaves even for unseen combinations of images, correctly answering the food names it wants or the opposite one according to the question.",
      "abstract": "Spoken language acquisition involves automatically developing symbolic word concepts grounding their meaning to the world, recognizing the words in spoken utterances, and pronouncing them. Previous research only partly covered these aspects. One of the most comprehensive agent systems supported the word concept acquisition from pairs of raw speech and image and utterance pronunciation. However, the agent listened to nothing when interacting with the world, only pronouncing a food name to choose a favorite one among two shown images. In this work, we add a function to the agent to recognize a verbal question. Namely, we design a task where the agent must recognize a question in a sound utterance and understand the logical \"not\" concept. Experimental results show that the agent successfully learns the task. It appropriately behaves even for unseen combinations of images, correctly answering the food names it wants or the opposite one according to the question.",
      "doi": "https://doi.org/10.1109/o-cocosda202152914.2021.9660413",
      "openalex_id": "https://openalex.org/W4205876710",
      "arxiv_id": "",
      "publication_date": "2021-11-18",
      "published": "2021-11-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "\\nLearning to Recognise Words Using Visually Grounded Speech",
      "summary": "We investigated word recognition in a Visually Grounded Speech model. The model has been trained on pairs of images and spoken captions to create visually grounded embeddings which can be used for speech to image retrieval and vice versa. We investigate whether such a model can be used to recognise words by embedding isolated words and using them to retrieve images of their visual referents. We investigate the time- course of word recognition using a gating paradigm and perform a statistical analysis to see whether well known word competition effects in human speech processing influence word recognition. Our experiments show that the model is able to recognise words, and the gating paradigm reveals that words can be recognised from partial input as well and that recognition is negatively influenced by word competition from the word initial cohort.",
      "abstract": "We investigated word recognition in a Visually Grounded Speech model. The model has been trained on pairs of images and spoken captions to create visually grounded embeddings which can be used for speech to image retrieval and vice versa. We investigate whether such a model can be used to recognise words by embedding isolated words and using them to retrieve images of their visual referents. We investigate the time- course of word recognition using a gating paradigm and perform a statistical analysis to see whether well known word competition effects in human speech processing influence word recognition. Our experiments show that the model is able to recognise words, and the gating paradigm reveals that words can be recognised from partial input as well and that recognition is negatively influenced by word competition from the word initial cohort.",
      "doi": "https://doi.org/10.1109/iscas51556.2021.9401692",
      "openalex_id": "https://openalex.org/W3159476814",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
      "summary": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice -- both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.",
      "abstract": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice -- both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.",
      "doi": "https://doi.org/10.48550/arxiv.2104.01894",
      "openalex_id": "https://openalex.org/W3143035657",
      "arxiv_id": "",
      "publication_date": "2021-04-05",
      "published": "2021-04-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually\\n Grounded Speech",
      "summary": "The language acquisition literature shows that children do not build their\\nlexicon by segmenting the spoken input into phonemes and then building up words\\nfrom them, but rather adopt a top-down approach and start by segmenting\\nword-like units and then break them down into smaller units. This suggests that\\nthe ideal way of learning a language is by starting from full semantic units.\\nIn this paper, we investigate if this is also the case for a neural model of\\nVisually Grounded Speech trained on a speech-image retrieval task. We evaluated\\nhow well such a network is able to learn a reliable speech-to-image mapping\\nwhen provided with phone, syllable, or word boundary information. We present a\\nsimple way to introduce such information into an RNN-based model and\\ninvestigate which type of boundary is the most efficient. We also explore at\\nwhich level of the network's architecture such information should be introduced\\nso as to maximise its performances. Finally, we show that using multiple\\nboundary types at once in a hierarchical structure, by which low-level segments\\nare used to recompose high-level segments, is beneficial and yields better\\nresults than using low-level or high-level segments in isolation.\\n",
      "abstract": "The language acquisition literature shows that children do not build their\\nlexicon by segmenting the spoken input into phonemes and then building up words\\nfrom them, but rather adopt a top-down approach and start by segmenting\\nword-like units and then break them down into smaller units. This suggests that\\nthe ideal way of learning a language is by starting from full semantic units.\\nIn this paper, we investigate if this is also the case for a neural model of\\nVisually Grounded Speech trained on a speech-image retrieval task. We evaluated\\nhow well such a network is able to learn a reliable speech-to-image mapping\\nwhen provided with phone, syllable, or word boundary information. We present a\\nsimple way to introduce such information into an RNN-based model and\\ninvestigate which type of boundary is the most efficient. We also explore at\\nwhich level of the network's architecture such information should be introduced\\nso as to maximise its performances. Finally, we show that using multiple\\nboundary types at once in a hierarchical structure, by which low-level segments\\nare used to recompose high-level segments, is beneficial and yields better\\nresults than using low-level or high-level segments in isolation.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2006.08387",
      "openalex_id": "https://openalex.org/W4287757663",
      "arxiv_id": "",
      "publication_date": "2020-06-15",
      "published": "2020-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attention-Based Keyword Localisation in Speech Using Visual Grounding",
      "summary": "Visually grounded speech models learn from images paired with spoken captions. By tagging images with soft text labels using a trained visual classifier with a fixed vocabulary, previous work has shown that it is possible to train a model that can detect whether a particular text keyword occurs in speech utterances or not. Here we investigate whether visually grounded speech models can also do keyword localisation: predicting where, within an utterance, a given textual keyword occurs without any explicit text-based or alignment supervision. We specifically consider whether incorporating attention into a convolutional model is beneficial for localisation. Although absolute localisation performance with visually supervised models is still modest (compared to using unordered bag-of-word text labels for supervision), we show that attention provides a large gain in performance over previous visually grounded models. As in many other speech-image studies, we find that many of the incorrect localisations are due to semantic confusions, e.g. locating the word 'backstroke' for the query keyword 'swimming'.",
      "abstract": "Visually grounded speech models learn from images paired with spoken captions. By tagging images with soft text labels using a trained visual classifier with a fixed vocabulary, previous work has shown that it is possible to train a model that can detect whether a particular text keyword occurs in speech utterances or not. Here we investigate whether visually grounded speech models can also do keyword localisation: predicting where, within an utterance, a given textual keyword occurs without any explicit text-based or alignment supervision. We specifically consider whether incorporating attention into a convolutional model is beneficial for localisation. Although absolute localisation performance with visually supervised models is still modest (compared to using unordered bag-of-word text labels for supervision), we show that attention provides a large gain in performance over previous visually grounded models. As in many other speech-image studies, we find that many of the incorrect localisations are due to semantic confusions, e.g. locating the word 'backstroke' for the query keyword 'swimming'.",
      "doi": "https://doi.org/10.21437/interspeech.2021-435",
      "openalex_id": "https://openalex.org/W3167119498",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SAliCLIP: Generating Speech-Conditioned Images by Aligning Speech With CLIP Latents",
      "summary": "The advent of the Transformer architecture, followed by the development of contrastive models like OpenAI's CLIP, has enabled learning robust latent space representations of data in multiple modalities such as text and image. In this work, we present Speech-Aligned CLIP (SAliCLIP), developed by employing knowledge distillation to adapt an audio encoder to the embedding space of CLIP. Aligning the embedding spaces of image, speech, and text imparts additional functionality to CLIP, which can be leveraged for various downstream tasks. We utilize this framework of encoders to guide the images generated by a GAN-based network to represent any given speech signal. The proposed architecture achieves new state-of-the-art cross-modal retrieval and classification accuracies on the Spoken-COCO dataset, the Spoken ObjectNet dataset and the Flickr Audio Captions Corpus dataset which includes zero-shot learning and fine-tuning on these datasets.",
      "abstract": "The advent of the Transformer architecture, followed by the development of contrastive models like OpenAI's CLIP, has enabled learning robust latent space representations of data in multiple modalities such as text and image. In this work, we present Speech-Aligned CLIP (SAliCLIP), developed by employing knowledge distillation to adapt an audio encoder to the embedding space of CLIP. Aligning the embedding spaces of image, speech, and text imparts additional functionality to CLIP, which can be leveraged for various downstream tasks. We utilize this framework of encoders to guide the images generated by a GAN-based network to represent any given speech signal. The proposed architecture achieves new state-of-the-art cross-modal retrieval and classification accuracies on the Spoken-COCO dataset, the Spoken ObjectNet dataset and the Flickr Audio Captions Corpus dataset which includes zero-shot learning and fine-tuning on these datasets.",
      "doi": "https://doi.org/10.1109/icivc58118.2023.10270664",
      "openalex_id": "https://openalex.org/W4387445713",
      "arxiv_id": "",
      "publication_date": "2023-07-27",
      "published": "2023-07-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks",
      "summary": "This paper argues that training GANs on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Begu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world's languages. This paper also proposes (iii) how we can actively observe the network's progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network's latent space.",
      "abstract": "This paper argues that training GANs on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Begu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world's languages. This paper also proposes (iii) how we can actively observe the network's progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network's latent space.",
      "doi": "https://doi.org/10.1016/j.csl.2021.101244",
      "openalex_id": "https://openalex.org/W3162850270",
      "arxiv_id": "",
      "publication_date": "2020-09-26",
      "published": "2020-09-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales",
      "summary": "The past decade has witnessed a groundbreaking rise of machine learning for human language analysis, with current methods capable of automatically accurately recovering various aspects of syntax and semantics - including sentence structure and grounded word meaning - from large data collections. Recent research showed the promise of such tools for analyzing acoustic communication in nonhuman species. We posit that machine learning will be the cornerstone of future collection, processing, and analysis of multimodal streams of data in animal communication studies, including bioacoustic, behavioral, biological, and environmental data. Cetaceans are unique non-human model species as they possess sophisticated acoustic communications, but utilize a very different encoding system that evolved in an aquatic rather than terrestrial medium. Sperm whales, in particular, with their highly-developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent starting point for advanced machine learning tools that can be applied to other animals in the future. This paper details a roadmap toward this goal based on currently existing technology and multidisciplinary scientific community effort. We outline the key elements required for the collection and processing of massive bioacoustic data of sperm whales, detecting their basic communication units and language-like higher-level structures, and validating these models through interactive playback experiments. The technological capabilities developed by such an undertaking are likely to yield cross-applications and advancements in broader communities investigating non-human communication and animal behavioral research.",
      "abstract": "The past decade has witnessed a groundbreaking rise of machine learning for human language analysis, with current methods capable of automatically accurately recovering various aspects of syntax and semantics - including sentence structure and grounded word meaning - from large data collections. Recent research showed the promise of such tools for analyzing acoustic communication in nonhuman species. We posit that machine learning will be the cornerstone of future collection, processing, and analysis of multimodal streams of data in animal communication studies, including bioacoustic, behavioral, biological, and environmental data. Cetaceans are unique non-human model species as they possess sophisticated acoustic communications, but utilize a very different encoding system that evolved in an aquatic rather than terrestrial medium. Sperm whales, in particular, with their highly-developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent starting point for advanced machine learning tools that can be applied to other animals in the future. This paper details a roadmap toward this goal based on currently existing technology and multidisciplinary scientific community effort. We outline the key elements required for the collection and processing of massive bioacoustic data of sperm whales, detecting their basic communication units and language-like higher-level structures, and validating these models through interactive playback experiments. The technological capabilities developed by such an undertaking are likely to yield cross-applications and advancements in broader communities investigating non-human communication and animal behavioral research.",
      "doi": "https://doi.org/10.48550/arxiv.2104.08614",
      "openalex_id": "https://openalex.org/W3155477605",
      "arxiv_id": "",
      "publication_date": "2021-04-17",
      "published": "2021-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast and easy crowdsourced perceptual audio evaluation",
      "summary": "Automated objective methods of audio evaluation are fast, cheap, and require little effort by the investigator. However, objective evaluation methods do not exist for the output of all audio processing algorithms, often have output that correlates poorly with human quality assessments, and require ground truth data in their calculation. Subjective human ratings of audio quality are the gold standard for many tasks, but are expensive, slow, and require a great deal of effort to recruit subjects and run listening tests. Moving listening tests from the lab to the micro-task labor market of Amazon Mechanical Turk speeds data collection and reduces investigator effort. However, it also reduces the amount of control investigators have over the testing environment, adding new variability and potential biases to the data. In this work, we compare multiple stimulus listening tests performed in a lab environment to multiple stimulus listening tests performed in web environment on a population drawn from Mechanical Turk.",
      "abstract": "Automated objective methods of audio evaluation are fast, cheap, and require little effort by the investigator. However, objective evaluation methods do not exist for the output of all audio processing algorithms, often have output that correlates poorly with human quality assessments, and require ground truth data in their calculation. Subjective human ratings of audio quality are the gold standard for many tasks, but are expensive, slow, and require a great deal of effort to recruit subjects and run listening tests. Moving listening tests from the lab to the micro-task labor market of Amazon Mechanical Turk speeds data collection and reduces investigator effort. However, it also reduces the amount of control investigators have over the testing environment, adding new variability and potential biases to the data. In this work, we compare multiple stimulus listening tests performed in a lab environment to multiple stimulus listening tests performed in web environment on a population drawn from Mechanical Turk.",
      "doi": "https://doi.org/10.1109/icassp.2016.7471749",
      "openalex_id": "https://openalex.org/W2395718496",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Universal Phone Recognition with a Multilingual Allophone System",
      "summary": "Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE [1] large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE [1] large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054362",
      "openalex_id": "https://openalex.org/W3015877095",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Daft-Exprt: Robust Prosody Transfer Across Speakers for Expressive Speech Synthesis.",
      "summary": "This paper presents Daft-Exprt, a multi-speaker acoustic model advancing the state-of-the-art on inter-speaker and inter-text prosody transfer. This improvement is achieved using FiLM conditioning layers, alongside adversarial training that encourages disentanglement between prosodic information and speaker identity. The acoustic model inherits attractive qualities from FastSpeech 2, such as fast inference and local prosody attributes prediction for finer grained control over generation. Experimental results show that Daft-Exprt significantly outperforms strong baselines on prosody transfer tasks, while yielding naturalness comparable to state-of-the-art expressive models. Moreover, results indicate that adversarial training effectively discards speaker identity information from the prosody representation, which ensures Daft-Exprt will consistently generate speech with the desired voice. We publicly release our code and provide speech samples from our experiments.",
      "abstract": "This paper presents Daft-Exprt, a multi-speaker acoustic model advancing the state-of-the-art on inter-speaker and inter-text prosody transfer. This improvement is achieved using FiLM conditioning layers, alongside adversarial training that encourages disentanglement between prosodic information and speaker identity. The acoustic model inherits attractive qualities from FastSpeech 2, such as fast inference and local prosody attributes prediction for finer grained control over generation. Experimental results show that Daft-Exprt significantly outperforms strong baselines on prosody transfer tasks, while yielding naturalness comparable to state-of-the-art expressive models. Moreover, results indicate that adversarial training effectively discards speaker identity information from the prosody representation, which ensures Daft-Exprt will consistently generate speech with the desired voice. We publicly release our code and provide speech samples from our experiments.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3188160682",
      "arxiv_id": "",
      "publication_date": "2021-08-04",
      "published": "2021-08-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AttS2S-VC: Sequence-to-Sequence Voice Conversion with Attention and\\n Context Preservation Mechanisms",
      "summary": "This paper describes a method based on a sequence-to-sequence learning\\n(Seq2Seq) with attention and context preservation mechanism for voice\\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\\nsequence modeling such as speech synthesis and recognition, machine\\ntranslation, and image captioning. In contrast to current VC techniques, our\\nmethod 1) stabilizes and accelerates the training procedure by considering\\nguided attention and proposed context preservation losses, 2) allows not only\\nspectral envelopes but also fundamental frequency contours and durations of\\nspeech to be converted, 3) requires no context information such as phoneme\\nlabels, and 4) requires no time-aligned source and target speech data in\\nadvance. In our experiment, the proposed VC framework can be trained in only\\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\\nsynthesized speech is higher than that of speech converted by Gaussian mixture\\nmodel-based VC and is comparable to that of speech generated by recurrent\\nneural network-based text-to-speech synthesis, which can be regarded as an\\nupper limit on VC performance.\\n",
      "abstract": "This paper describes a method based on a sequence-to-sequence learning\\n(Seq2Seq) with attention and context preservation mechanism for voice\\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\\nsequence modeling such as speech synthesis and recognition, machine\\ntranslation, and image captioning. In contrast to current VC techniques, our\\nmethod 1) stabilizes and accelerates the training procedure by considering\\nguided attention and proposed context preservation losses, 2) allows not only\\nspectral envelopes but also fundamental frequency contours and durations of\\nspeech to be converted, 3) requires no context information such as phoneme\\nlabels, and 4) requires no time-aligned source and target speech data in\\nadvance. In our experiment, the proposed VC framework can be trained in only\\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\\nsynthesized speech is higher than that of speech converted by Gaussian mixture\\nmodel-based VC and is comparable to that of speech generated by recurrent\\nneural network-based text-to-speech synthesis, which can be regarded as an\\nupper limit on VC performance.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1811.04076",
      "openalex_id": "https://openalex.org/W4289299319",
      "arxiv_id": "",
      "publication_date": "2018-11-09",
      "published": "2018-11-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
      "summary": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
      "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
      "doi": "https://doi.org/10.18653/v1/p19-1285",
      "openalex_id": "https://openalex.org/W2964110616",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Acoustically Grounded Word Embeddings for Improved Acoustics-to-word Speech Recognition",
      "summary": "Direct acoustics-to-word (A2W) systems for end-to-end automatic speech recognition are simpler to train, and more efficient to decode with, than sub-word systems. However, A2W systems can have difficulties at training time when data is limited, and at decoding time when recognizing words outside the training vocabulary. To address these shortcomings, we investigate the use of recently proposed acoustic and acoustically grounded word embedding techniques in A2W systems. The idea is based on treating the final pre-softmax weight matrix of an AWE recognizer as a matrix of word embedding vectors, and using an externally trained set of word embeddings to improve the quality of this matrix. In particular we introduce two ideas: (1) Enforcing similarity at training time between the external embeddings and the recognizer weights, and (2) using the word embeddings at test time for predicting out-of-vocabulary words. Our word embedding model is acoustically grounded, that is it is learned jointly with acoustic embeddings so as to encode the words' acoustic-phonetic content; and it is parametric, so that it can embed any arbitrary (potentially out-of-vocabulary) sequence of characters. We find that both techniques improve the performance of an A2W recognizer on conversational telephone speech.",
      "abstract": "Direct acoustics-to-word (A2W) systems for end-to-end automatic speech recognition are simpler to train, and more efficient to decode with, than sub-word systems. However, A2W systems can have difficulties at training time when data is limited, and at decoding time when recognizing words outside the training vocabulary. To address these shortcomings, we investigate the use of recently proposed acoustic and acoustically grounded word embedding techniques in A2W systems. The idea is based on treating the final pre-softmax weight matrix of an AWE recognizer as a matrix of word embedding vectors, and using an externally trained set of word embeddings to improve the quality of this matrix. In particular we introduce two ideas: (1) Enforcing similarity at training time between the external embeddings and the recognizer weights, and (2) using the word embeddings at test time for predicting out-of-vocabulary words. Our word embedding model is acoustically grounded, that is it is learned jointly with acoustic embeddings so as to encode the words' acoustic-phonetic content; and it is parametric, so that it can embed any arbitrary (potentially out-of-vocabulary) sequence of characters. We find that both techniques improve the performance of an A2W recognizer on conversational telephone speech.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682903",
      "openalex_id": "https://openalex.org/W2932675979",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-supervised training of Deep Neural Networks",
      "summary": "In this paper we search for an optimal strategy for semi-supervised Deep Neural Network (DNN) training. We assume that a small part of the data is transcribed, while the majority of the data is untranscribed. We explore self-training strategies with data selection based on both the utterance-level and frame-level confidences. Further on, we study the interactions between semi-supervised frame-discriminative training and sequence-discriminative sMBR training. We found it beneficial to reduce the disproportion in amounts of transcribed and untranscribed data by including the transcribed data several times, as well as to do a frame-selection based on per-frame confidences derived from confusion in a lattice. For the experiments, we used the Limited language pack condition for the Surprise language task (Vietnamese) from the IARPA Babel program. The absolute Word Error Rate (WER) improvement for frame cross-entropy training is 2.2%, this corresponds to WER recovery of 36% when compared to the identical system, where the DNN is built on the fully transcribed data.",
      "abstract": "In this paper we search for an optimal strategy for semi-supervised Deep Neural Network (DNN) training. We assume that a small part of the data is transcribed, while the majority of the data is untranscribed. We explore self-training strategies with data selection based on both the utterance-level and frame-level confidences. Further on, we study the interactions between semi-supervised frame-discriminative training and sequence-discriminative sMBR training. We found it beneficial to reduce the disproportion in amounts of transcribed and untranscribed data by including the transcribed data several times, as well as to do a frame-selection based on per-frame confidences derived from confusion in a lattice. For the experiments, we used the Limited language pack condition for the Surprise language task (Vietnamese) from the IARPA Babel program. The absolute Word Error Rate (WER) improvement for frame cross-entropy training is 2.2%, this corresponds to WER recovery of 36% when compared to the identical system, where the DNN is built on the fully transcribed data.",
      "doi": "https://doi.org/10.1109/asru.2013.6707741",
      "openalex_id": "https://openalex.org/W2124558353",
      "arxiv_id": "",
      "publication_date": "2013-12-01",
      "published": "2013-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards spoken term discovery at scale with zero resources",
      "summary": "The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots",
      "abstract": "The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots",
      "doi": "https://doi.org/10.21437/interspeech.2010-483",
      "openalex_id": "https://openalex.org/W30845872",
      "arxiv_id": "",
      "publication_date": "2010-09-26",
      "published": "2010-09-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings",
      "summary": "Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.",
      "abstract": "Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.",
      "doi": "https://doi.org/10.1109/asru.2013.6707765",
      "openalex_id": "https://openalex.org/W2059652594",
      "arxiv_id": "",
      "publication_date": "2013-12-01",
      "published": "2013-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition",
      "summary": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639245",
      "openalex_id": "https://openalex.org/W2025482506",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder",
      "summary": "The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry.This paper proposes a parallel version, the Audio Word2Vec.It offers the vector representations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD).In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements.We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Autoencoder (SA).SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence.The two RNNs are jointly trained by minimizing the reconstruction error.Denoising Sequence-to-sequence Autoencoder (DSA) is further proposed offering more robust learning.",
      "abstract": "The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry.This paper proposes a parallel version, the Audio Word2Vec.It offers the vector representations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD).In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements.We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Autoencoder (SA).SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence.The two RNNs are jointly trained by minimizing the reconstruction error.Denoising Sequence-to-sequence Autoencoder (DSA) is further proposed offering more robust learning.",
      "doi": "https://doi.org/10.21437/interspeech.2016-82",
      "openalex_id": "https://openalex.org/W2963571336",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Pattern Discovery in Speech",
      "summary": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "abstract": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "doi": "https://doi.org/10.1109/tasl.2007.909282",
      "openalex_id": "https://openalex.org/W2114347655",
      "arxiv_id": "",
      "publication_date": "2007-12-20",
      "published": "2007-12-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Linguistic unit discovery from multimodal inputs in unwritten languages: Summary of the \"Speaking Rosetta\" JSALT 2017 Workshop",
      "summary": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2964115348",
      "arxiv_id": "",
      "publication_date": "2018-04-15",
      "published": "2018-04-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
      "summary": "We present state-of-the-art automatic speech recognition (ASR) systems\\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\\nencoder-decoder design for the LibriSpeech task. Detailed descriptions of the\\nsystem development, including model design, pretraining schemes, training\\nschedules, and optimization approaches are provided for both system\\narchitectures. Both hybrid DNN/HMM and attention-based systems employ\\nbi-directional LSTMs for acoustic modeling/encoding. For language modeling, we\\nemploy both LSTM and Transformer based architectures. All our systems are built\\nusing RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the\\nauthors, the results obtained when training on the full LibriSpeech training\\nset, are the best published currently, both for the hybrid DNN/HMM and the\\nattention-based systems. Our single hybrid system even outperforms previous\\nresults obtained from combining eight single systems. Our comparison shows that\\non the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the\\nattention-based system by 15% relative on the clean and 40% relative on the\\nother test sets in terms of word error rate. Moreover, experiments on a reduced\\n100h-subset of the LibriSpeech training corpus even show a more pronounced\\nmargin between the hybrid DNN/HMM and attention-based architectures.\\n",
      "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems\\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\\nencoder-decoder design for the LibriSpeech task. Detailed descriptions of the\\nsystem development, including model design, pretraining schemes, training\\nschedules, and optimization approaches are provided for both system\\narchitectures. Both hybrid DNN/HMM and attention-based systems employ\\nbi-directional LSTMs for acoustic modeling/encoding. For language modeling, we\\nemploy both LSTM and Transformer based architectures. All our systems are built\\nusing RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the\\nauthors, the results obtained when training on the full LibriSpeech training\\nset, are the best published currently, both for the hybrid DNN/HMM and the\\nattention-based systems. Our single hybrid system even outperforms previous\\nresults obtained from combining eight single systems. Our comparison shows that\\non the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the\\nattention-based system by 15% relative on the clean and 40% relative on the\\nother test sets in terms of word error rate. Moreover, experiments on a reduced\\n100h-subset of the LibriSpeech training corpus even show a more pronounced\\nmargin between the hybrid DNN/HMM and attention-based architectures.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-1780",
      "openalex_id": "https://openalex.org/W2944255943",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformer-Based Acoustic Modeling for Hybrid Speech Recognition",
      "summary": "We propose and evaluate transformer-based acoustic models (AMs) for hybrid\\nspeech recognition. Several modeling choices are discussed in this work,\\nincluding various positional embedding methods and an iterated loss to enable\\ntraining deep transformers. We also present a preliminary study of using\\nlimited right context in transformer models, which makes it possible for\\nstreaming applications. We demonstrate that on the widely used Librispeech\\nbenchmark, our transformer-based AM outperforms the best published hybrid\\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\\nused. Combined with neural network LM for rescoring, our proposed approach\\nachieves state-of-the-art results on Librispeech. Our findings are also\\nconfirmed on a much larger internal dataset.\\n",
      "abstract": "We propose and evaluate transformer-based acoustic models (AMs) for hybrid\\nspeech recognition. Several modeling choices are discussed in this work,\\nincluding various positional embedding methods and an iterated loss to enable\\ntraining deep transformers. We also present a preliminary study of using\\nlimited right context in transformer models, which makes it possible for\\nstreaming applications. We demonstrate that on the widely used Librispeech\\nbenchmark, our transformer-based AM outperforms the best published hybrid\\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\\nused. Combined with neural network LM for rescoring, our proposed approach\\nachieves state-of-the-art results on Librispeech. Our findings are also\\nconfirmed on a much larger internal dataset.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054345",
      "openalex_id": "https://openalex.org/W2981857663",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Word embeddings for speech recognition",
      "summary": "Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models, it is interesting to consider approaches that relax the assumption that words are made of states. We present here an alternative construction, where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary. Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate.",
      "abstract": "Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models, it is interesting to consider approaches that relax the assumption that words are made of states. We present here an alternative construction, where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary. Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate.",
      "doi": "https://doi.org/10.21437/interspeech.2014-273",
      "openalex_id": "https://openalex.org/W2296681920",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the integration of speech recognition and statistical machine translation",
      "summary": "This paper focuses on the interface between speech recognition and machine translation in a speech translation system. Based on a thorough theoretical framework, we exploit word lattices of automatic speech recognition hypotheses as input to our translation system which is based on weighted finite-state transducers. We show that acoustic recognition scores of the recognized words in the lattices positively and significantly affect the translation quality. In experiments, we have found consistent improvements on three different corpora compared with translations of single best recognized results. In addition we build and evaluate a fully integrated speech translation model.",
      "abstract": "This paper focuses on the interface between speech recognition and machine translation in a speech translation system. Based on a thorough theoretical framework, we exploit word lattices of automatic speech recognition hypotheses as input to our translation system which is based on weighted finite-state transducers. We show that acoustic recognition scores of the recognized words in the lattices positively and significantly affect the translation quality. In experiments, we have found consistent improvements on three different corpora compared with translations of single best recognized results. In addition we build and evaluate a fully integrated speech translation model.",
      "doi": "https://doi.org/10.21437/interspeech.2005-726",
      "openalex_id": "https://openalex.org/W1537859740",
      "arxiv_id": "",
      "publication_date": "2005-09-04",
      "published": "2005-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech Translation with Efficient Finetuning of Pretrained Models",
      "summary": "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation by efficient transfer learning from pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning less than 10% of the pretrained parameters. This enables effectively leveraging large pretrained models with low training cost. Using wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation, our approach advanced the new state-of-the-art for 34 translation directions (and surpassing cascaded ST for 23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average across 15 En-X directions and +5.1 BLEU on average across 19 X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.7 BLEU on average across 18 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.",
      "abstract": "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation by efficient transfer learning from pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning less than 10% of the pretrained parameters. This enables effectively leveraging large pretrained models with low training cost. Using wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation, our approach advanced the new state-of-the-art for 34 translation directions (and surpassing cascaded ST for 23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average across 15 En-X directions and +5.1 BLEU on average across 19 X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.7 BLEU on average across 18 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.",
      "doi": "https://doi.org/10.48550/arxiv.2010.12829",
      "openalex_id": "https://openalex.org/W3112092703",
      "arxiv_id": "",
      "publication_date": "2020-10-24",
      "published": "2020-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Janus-III: speech-to-speech translation in multiple languages",
      "summary": "This paper describes JANUS-III, our most recent version of the JANUS speech-to-speech translation system. We present an overview of the system and focus on how system design facilitates speech translation between multiple languages, and allows for easy adaptation to new source and target languages. We also describe our methodology for evaluation of end-to-end system performance with a variety of source and target languages. For system development and evaluation, we have experimented with both push-to-talk as well as cross-talk recording conditions. To date, our system has achieved performance levels of over 80% acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 75-90% word accuracy. Our current major research is concentrated on enhancing the capabilities of the system to deal with input in broad and general domains.",
      "abstract": "This paper describes JANUS-III, our most recent version of the JANUS speech-to-speech translation system. We present an overview of the system and focus on how system design facilitates speech translation between multiple languages, and allows for easy adaptation to new source and target languages. We also describe our methodology for evaluation of end-to-end system performance with a variety of source and target languages. For system development and evaluation, we have experimented with both push-to-talk as well as cross-talk recording conditions. To date, our system has achieved performance levels of over 80% acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 75-90% word accuracy. Our current major research is concentrated on enhancing the capabilities of the system to deal with input in broad and general domains.",
      "doi": "https://doi.org/10.1109/icassp.1997.599557",
      "openalex_id": "https://openalex.org/W2097203679",
      "arxiv_id": "",
      "publication_date": "2002-11-22",
      "published": "2002-11-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Machine Translation with Byte-Level Subwords",
      "summary": "Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice. In this paper, we investigate byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.",
      "abstract": "Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice. In this paper, we investigate byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6451",
      "openalex_id": "https://openalex.org/W2998353611",
      "arxiv_id": "",
      "publication_date": "2020-04-03",
      "published": "2020-04-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Intent transfer in speech-to-speech machine translation",
      "summary": "This paper presents an approach for transfer of speaker intent in speech-to-speech machine translation (S2SMT). Specifically, we describe techniques to retain the prominence patterns of the source language utterance through the translation pipeline and impose this information during speech synthesis in the target language. We first present an analysis of word focus across languages to motivate the problem of transfer. We then propose an approach for training an appropriate transfer function for intonation on a parallel speech corpus in the two languages within which the translation is carried out. We present our analysis and experiments on English↔Portuguese and English↔German language pairs and evaluate the proposed transformation techniques through objective measures.",
      "abstract": "This paper presents an approach for transfer of speaker intent in speech-to-speech machine translation (S2SMT). Specifically, we describe techniques to retain the prominence patterns of the source language utterance through the translation pipeline and impose this information during speech synthesis in the target language. We first present an analysis of word focus across languages to motivate the problem of transfer. We then propose an approach for training an appropriate transfer function for intonation on a parallel speech corpus in the two languages within which the translation is carried out. We present our analysis and experiments on English↔Portuguese and English↔German language pairs and evaluate the proposed transformation techniques through objective measures.",
      "doi": "https://doi.org/10.1109/slt.2012.6424214",
      "openalex_id": "https://openalex.org/W2035108931",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Prosody Generation for Speech-to-Speech Translation",
      "summary": "This paper deals with speech synthesis in the framework of speech-to-speech translation. Our current focus is to translate speeches or conversations between humans so that a third person can listen to them in its own language. In this framework the style is not written but spoken and the original speech includes a lot of non-linguistic information (as speaker emotion). In this work we propose the use of prosodic features in the original speech to produce prosody in the target language. Relevant features are found using an unsupervised clustering algorithm that finds, in a bilingual speech corpus, intonation clusters in the source speech which are relevant in the target speech. Preliminary results already show a significant improvement in the synthetic quality (from MOS=3.40 to MOS=3.65)",
      "abstract": "This paper deals with speech synthesis in the framework of speech-to-speech translation. Our current focus is to translate speeches or conversations between humans so that a third person can listen to them in its own language. In this framework the style is not written but spoken and the original speech includes a lot of non-linguistic information (as speaker emotion). In this work we propose the use of prosodic features in the original speech to produce prosody in the target language. Relevant features are found using an unsupervised clustering algorithm that finds, in a bilingual speech corpus, intonation clusters in the source speech which are relevant in the target speech. Preliminary results already show a significant improvement in the synthetic quality (from MOS=3.40 to MOS=3.65)",
      "doi": "https://doi.org/10.1109/icassp.2006.1660081",
      "openalex_id": "https://openalex.org/W2152834109",
      "arxiv_id": "",
      "publication_date": "2006-08-03",
      "published": "2006-08-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
      "summary": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).",
      "abstract": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).",
      "doi": "https://doi.org/10.21437/interspeech.2019-2858",
      "openalex_id": "https://openalex.org/W2971840980",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "summary": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
      "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.747",
      "openalex_id": "https://openalex.org/W2983040767",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion",
      "summary": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
      "abstract": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682918",
      "openalex_id": "https://openalex.org/W2963027641",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech Recognition with a Single End-to-End Model",
      "summary": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.",
      "abstract": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461972",
      "openalex_id": "https://openalex.org/W2964309797",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "summary": "In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.",
      "abstract": "In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639081",
      "openalex_id": "https://openalex.org/W2025198378",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Speech Recognition for Under-Resourced Languages: Application to Vietnamese Language",
      "summary": "This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.",
      "abstract": "This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.",
      "doi": "https://doi.org/10.1109/tasl.2009.2021723",
      "openalex_id": "https://openalex.org/W2141820854",
      "arxiv_id": "",
      "publication_date": "2009-04-28",
      "published": "2009-04-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual acoustic modeling for speech recognition based on subspace Gaussian Mixture Models",
      "summary": "Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.",
      "abstract": "Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.",
      "doi": "https://doi.org/10.1109/icassp.2010.5495646",
      "openalex_id": "https://openalex.org/W2123798005",
      "arxiv_id": "",
      "publication_date": "2010-03-01",
      "published": "2010-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-supervised training in low-resource ASR and KWS",
      "summary": "In particular for \"low resource\" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.",
      "abstract": "In particular for \"low resource\" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178862",
      "openalex_id": "https://openalex.org/W1524956127",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling",
      "summary": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
      "abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
      "doi": "https://doi.org/10.1109/slt.2018.8639655",
      "openalex_id": "https://openalex.org/W2894835365",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Confidence Estimation and Deletion Prediction Using Bidirectional Recurrent Neural Networks",
      "summary": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
      "abstract": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
      "doi": "https://doi.org/10.17863/cam.35236",
      "openalex_id": "https://openalex.org/W2898630520",
      "arxiv_id": "",
      "publication_date": "2018-12-21",
      "published": "2018-12-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An End-to-End Language-Tracking Speech Recognizer for Mixed-Language Speech",
      "summary": "End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.",
      "abstract": "End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.",
      "doi": "https://doi.org/10.1109/icassp.2018.8462180",
      "openalex_id": "https://openalex.org/W2891616026",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual training of deep neural networks",
      "summary": "We investigate multilingual modeling in the context of a deep neural network (DNN) - hidden Markov model (HMM) hybrid, where the DNN outputs are used as the HMM state likelihoods. By viewing neural networks as a cascade of feature extractors followed by a logistic regression classifier, we hypothesise that the hidden layers, which act as feature extractors, will be transferable between languages. As a corollary, we propose that training the hidden layers on multiple languages makes them more suitable for such cross-lingual transfer. We experimentally confirm these hypotheses on the GlobalPhone corpus using seven languages from three different language families: Germanic, Romance, and Slavic. The experiments demonstrate substantial improvements over a monolingual DNN-HMM hybrid baseline, and hint at avenues of further exploration.",
      "abstract": "We investigate multilingual modeling in the context of a deep neural network (DNN) - hidden Markov model (HMM) hybrid, where the DNN outputs are used as the HMM state likelihoods. By viewing neural networks as a cascade of feature extractors followed by a logistic regression classifier, we hypothesise that the hidden layers, which act as feature extractors, will be transferable between languages. As a corollary, we propose that training the hidden layers on multiple languages makes them more suitable for such cross-lingual transfer. We experimentally confirm these hypotheses on the GlobalPhone corpus using seven languages from three different language families: Germanic, Romance, and Slavic. The experiments demonstrate substantial improvements over a monolingual DNN-HMM hybrid baseline, and hint at avenues of further exploration.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639084",
      "openalex_id": "https://openalex.org/W1994606281",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The 2016 BBN Georgian telephone speech keyword spotting system",
      "summary": "In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.",
      "abstract": "In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953259",
      "openalex_id": "https://openalex.org/W2671812860",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stimulated training for automatic speech recognition and keyword search in limited resource conditions",
      "summary": "Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.",
      "abstract": "Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953074",
      "openalex_id": "https://openalex.org/W2696253854",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic discovery of a phonetic inventory for unwritten languages for statistical speech synthesis",
      "summary": "Speech synthesis systems are typically built with speech data and transcriptions. In this paper, we try to build synthesis systems when no transcriptions or knowledge about the language are available. It is usually necessary to at least possess phonetic knowledge about the language. In this paper, we propose an automated way of obtaining phones and phonetic knowledge about the corpus at hand by making use of Articulatory Features (AFs). An Articulatory Feature predictor is trained on a bootstrap corpus in an arbitrary other language using a three-hidden layer neural network. This neural network is run on the speech corpus to extract AFs. Hierarchical clustering is used to cluster the AFs into categories i.e. phones. Phonetic information about each of these inferred phones is obtained by computing the mean of the AFs in each cluster. Results of systems built with this framework in multiple languages are reported.",
      "abstract": "Speech synthesis systems are typically built with speech data and transcriptions. In this paper, we try to build synthesis systems when no transcriptions or knowledge about the language are available. It is usually necessary to at least possess phonetic knowledge about the language. In this paper, we propose an automated way of obtaining phones and phonetic knowledge about the corpus at hand by making use of Articulatory Features (AFs). An Articulatory Feature predictor is trained on a bootstrap corpus in an arbitrary other language using a three-hidden layer neural network. This neural network is run on the speech corpus to extract AFs. Hierarchical clustering is used to cluster the AFs into categories i.e. phones. Phonetic information about each of these inferred phones is obtained by computing the mean of the AFs in each cluster. Results of systems built with this framework in multiple languages are reported.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854069",
      "openalex_id": "https://openalex.org/W2134202996",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Zero Resource Speech Challenge 2015: Proposed Approaches and Results",
      "summary": "This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.",
      "abstract": "This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.031",
      "openalex_id": "https://openalex.org/W2346964103",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An auto-encoder based approach to unsupervised learning of subword units",
      "summary": "In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.",
      "abstract": "In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.",
      "doi": "https://doi.org/10.1109/icassp.2014.6855085",
      "openalex_id": "https://openalex.org/W2020607164",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Close to Human Quality TTS with Transformer",
      "summary": "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the-art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves the training efficiency. Meanwhile, any two inputs at different times are connected directly by self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).",
      "abstract": "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the-art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves the training efficiency. Meanwhile, any two inputs at different times are connected directly by self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).",
      "doi": "",
      "openalex_id": "https://openalex.org/W2892140764",
      "arxiv_id": "",
      "publication_date": "2018-09-19",
      "published": "2018-09-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model",
      "summary": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.",
      "abstract": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.",
      "doi": "https://doi.org/10.48550/arxiv.1612.07837",
      "openalex_id": "https://openalex.org/W2584032004",
      "arxiv_id": "",
      "publication_date": "2016-12-22",
      "published": "2016-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Partitioning of Posteriorgrams Using Siamese Models for Unsupervised Acoustic Modelling",
      "summary": "Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.",
      "abstract": "Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.",
      "doi": "https://doi.org/10.21437/glu.2017-6",
      "openalex_id": "https://openalex.org/W2745710152",
      "arxiv_id": "",
      "publication_date": "2017-08-25",
      "published": "2017-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Merlin: An Open Source Neural Network Speech Synthesis System",
      "summary": "We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.",
      "abstract": "We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.",
      "doi": "https://doi.org/10.21437/ssw.2016-33",
      "openalex_id": "https://openalex.org/W2598638573",
      "arxiv_id": "",
      "publication_date": "2016-09-13",
      "published": "2016-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations",
      "summary": "Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker.In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals.An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation.The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance.The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator.A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained.Conventional voice conversion metrics are reported.We also show that the speaker information has been properly reduced from the latent representations.",
      "abstract": "Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker.In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals.An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation.The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance.The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator.A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained.Conventional voice conversion metrics are reported.We also show that the speaker information has been properly reduced from the latent representations.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1830",
      "openalex_id": "https://openalex.org/W2963830550",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bayesian Models for Unit Discovery on a Very Low Resource Language",
      "summary": "Accepted to ICASSP 2018",
      "abstract": "Accepted to ICASSP 2018",
      "doi": "https://doi.org/10.1109/icassp.2018.8461545",
      "openalex_id": "https://openalex.org/W2962693497",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Blow: a single-scale hyperconditioned flow for non-parallel raw-audio\\n voice conversion",
      "summary": "End-to-end models for raw audio generation are a challenge, specially if they\\nhave to work with non-parallel data, which is a desirable setup in many\\nsituations. Voice conversion, in which a model has to impersonate a speaker in\\na recording, is one of those situations. In this paper, we propose Blow, a\\nsingle-scale normalizing flow using hypernetwork conditioning to perform\\nmany-to-many voice conversion between raw audio. Blow is trained end-to-end,\\nwith non-parallel data, on a frame-by-frame basis using a single speaker\\nidentifier. We show that Blow compares favorably to existing flow-based\\narchitectures and other competitive baselines, obtaining equal or better\\nperformance in both objective and subjective evaluations. We further assess the\\nimpact of its main components with an ablation study, and quantify a number of\\nproperties such as the necessary amount of training data or the preference for\\nsource or target speakers.\\n",
      "abstract": "End-to-end models for raw audio generation are a challenge, specially if they\\nhave to work with non-parallel data, which is a desirable setup in many\\nsituations. Voice conversion, in which a model has to impersonate a speaker in\\na recording, is one of those situations. In this paper, we propose Blow, a\\nsingle-scale normalizing flow using hypernetwork conditioning to perform\\nmany-to-many voice conversion between raw audio. Blow is trained end-to-end,\\nwith non-parallel data, on a frame-by-frame basis using a single speaker\\nidentifier. We show that Blow compares favorably to existing flow-based\\narchitectures and other competitive baselines, obtaining equal or better\\nperformance in both objective and subjective evaluations. We further assess the\\nimpact of its main components with an ablation study, and quantify a number of\\nproperties such as the necessary amount of training data or the preference for\\nsource or target speakers.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1906.00794",
      "openalex_id": "https://openalex.org/W4288337064",
      "arxiv_id": "",
      "publication_date": "2019-06-03",
      "published": "2019-06-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "F0-Consistent Many-To-Many Non-Parallel Voice Conversion Via Conditional Autoencoder",
      "summary": "Non-parallel many-to-many voice conversion remains an interesting but\\nchallenging speech processing task. Many style-transfer-inspired methods such\\nas generative adversarial networks (GANs) and variational autoencoders (VAEs)\\nhave been proposed. Recently, AutoVC, a conditional autoencoders (CAEs) based\\nmethod achieved state-of-the-art results by disentangling the speaker identity\\nand speech content using information-constraining bottlenecks, and it achieves\\nzero-shot conversion by swapping in a different speaker's identity embedding to\\nsynthesize a new voice. However, we found that while speaker identity is\\ndisentangled from speech content, a significant amount of prosodic information,\\nsuch as source F0, leaks through the bottleneck, causing target F0 to fluctuate\\nunnaturally. Furthermore, AutoVC has no control of the converted F0 and thus\\nunsuitable for many applications. In the paper, we modified and improved\\nautoencoder-based voice conversion to disentangle content, F0, and speaker\\nidentity at the same time. Therefore, we can control the F0 contour, generate\\nspeech with F0 consistent with the target speaker, and significantly improve\\nquality and similarity. We support our improvement through quantitative and\\nqualitative analysis.\\n",
      "abstract": "Non-parallel many-to-many voice conversion remains an interesting but\\nchallenging speech processing task. Many style-transfer-inspired methods such\\nas generative adversarial networks (GANs) and variational autoencoders (VAEs)\\nhave been proposed. Recently, AutoVC, a conditional autoencoders (CAEs) based\\nmethod achieved state-of-the-art results by disentangling the speaker identity\\nand speech content using information-constraining bottlenecks, and it achieves\\nzero-shot conversion by swapping in a different speaker's identity embedding to\\nsynthesize a new voice. However, we found that while speaker identity is\\ndisentangled from speech content, a significant amount of prosodic information,\\nsuch as source F0, leaks through the bottleneck, causing target F0 to fluctuate\\nunnaturally. Furthermore, AutoVC has no control of the converted F0 and thus\\nunsuitable for many applications. In the paper, we modified and improved\\nautoencoder-based voice conversion to disentangle content, F0, and speaker\\nidentity at the same time. Therefore, we can control the F0 contour, generate\\nspeech with F0 consistent with the target speaker, and significantly improve\\nquality and similarity. We support our improvement through quantitative and\\nqualitative analysis.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054734",
      "openalex_id": "https://openalex.org/W3015805741",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "summary": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",
      "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",
      "doi": "https://doi.org/10.48550/arxiv.1607.08022",
      "openalex_id": "https://openalex.org/W2502312327",
      "arxiv_id": "",
      "publication_date": "2016-07-27",
      "published": "2016-07-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS",
      "summary": "The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",
      "abstract": "The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",
      "doi": "https://doi.org/10.1111/j.1469-1809.1936.tb02137.x",
      "openalex_id": "https://openalex.org/W2001619934",
      "arxiv_id": "",
      "publication_date": "1936-09-01",
      "published": "1936-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Maximum likelihood modeling with Gaussian distributions for classification",
      "summary": "Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.",
      "abstract": "Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.",
      "doi": "https://doi.org/10.1109/icassp.1998.675351",
      "openalex_id": "https://openalex.org/W2124629003",
      "arxiv_id": "",
      "publication_date": "2002-11-27",
      "published": "2002-11-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-tied covariance matrices for hidden Markov models",
      "summary": "There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few \"full\" covariance matrices to be shared over many distributions, whilst each distribution maintains its own \"diagonal\" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.",
      "abstract": "There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few \"full\" covariance matrices to be shared over many distributions, whilst each distribution maintains its own \"diagonal\" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.",
      "doi": "https://doi.org/10.1109/89.759034",
      "openalex_id": "https://openalex.org/W2106554350",
      "arxiv_id": "",
      "publication_date": "1999-05-01",
      "published": "1999-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A compact model for speaker-adaptive training",
      "summary": "We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.",
      "abstract": "We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.",
      "doi": "https://doi.org/10.1109/icslp.1996.607807",
      "openalex_id": "https://openalex.org/W1599512239",
      "arxiv_id": "",
      "publication_date": "2002-12-24",
      "published": "2002-12-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iterative training of a DPGMM-HMM acoustic unit recognizer in a zero resource scenario",
      "summary": "In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.",
      "abstract": "In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.",
      "doi": "https://doi.org/10.1109/slt.2016.7846245",
      "openalex_id": "https://openalex.org/W2586754519",
      "arxiv_id": "",
      "publication_date": "2016-12-01",
      "published": "2016-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression",
      "summary": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/18.87000",
      "openalex_id": "https://openalex.org/W2113641473",
      "arxiv_id": "",
      "publication_date": "1991-07-01",
      "published": "1991-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge",
      "summary": "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.",
      "abstract": "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.",
      "doi": "https://doi.org/10.21437/interspeech.2015-644",
      "openalex_id": "https://openalex.org/W1796128977",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling",
      "summary": "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.",
      "abstract": "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.",
      "doi": "https://doi.org/10.21437/interspeech.2015-640",
      "openalex_id": "https://openalex.org/W2404799143",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario",
      "summary": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "abstract": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.032",
      "openalex_id": "https://openalex.org/W2345811097",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parallel Sampling of DP Mixture Models using Sub-Cluster Splits",
      "summary": "We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
      "abstract": "We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2128032727",
      "arxiv_id": "",
      "publication_date": "2013-12-05",
      "published": "2013-12-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-end Feedback Loss in Speech Chain Framework via Straight-through Estimator",
      "summary": "The speech chain mechanism integrates automatic speech recognition (ASR) and text-to-speech synthesis (TTS) modules into a single cycle during training. In our previous work, we applied a speech chain mechanism as a semi-supervised learning. It provides the ability for ASR and TTS to assist each other when they receive unpaired data and let them infer the missing pair and optimize the model with reconstruction loss. If we only have speech without transcription, ASR generates the most likely transcription from the speech data, and then TTS uses the generated transcription to reconstruct the original speech features. However, in previous papers, we just limited our back-propagation to the closest module, which is the TTS part. One reason is that back-propagating the error through the ASR is challenging due to the output of the ASR being discrete tokens, creating non-differentiability between the TTS and ASR. In this paper, we address this problem and describe how to thoroughly train a speech chain end-to-end for reconstruction loss using a straight-through estimator (ST). Experimental results revealed that, with sampling from ST-Gumbel-Softmax, we were able to update ASR parameters and improve the ASR performances by 11% relative CER reduction compared to the baseline.",
      "abstract": "The speech chain mechanism integrates automatic speech recognition (ASR) and text-to-speech synthesis (TTS) modules into a single cycle during training. In our previous work, we applied a speech chain mechanism as a semi-supervised learning. It provides the ability for ASR and TTS to assist each other when they receive unpaired data and let them infer the missing pair and optimize the model with reconstruction loss. If we only have speech without transcription, ASR generates the most likely transcription from the speech data, and then TTS uses the generated transcription to reconstruct the original speech features. However, in previous papers, we just limited our back-propagation to the closest module, which is the TTS part. One reason is that back-propagating the error through the ASR is challenging due to the output of the ASR being discrete tokens, creating non-differentiability between the TTS and ASR. In this paper, we address this problem and describe how to thoroughly train a speech chain end-to-end for reconstruction loss using a straight-through estimator (ST). Experimental results revealed that, with sampling from ST-Gumbel-Softmax, we were able to update ASR parameters and improve the ASR performances by 11% relative CER reduction compared to the baseline.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683480",
      "openalex_id": "https://openalex.org/W2963581463",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Extracting and composing robust features with denoising autoencoders",
      "summary": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",
      "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",
      "doi": "https://doi.org/10.1145/1390156.1390294",
      "openalex_id": "https://openalex.org/W2025768430",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Attentional Model for Speech Translation Without Transcription",
      "summary": "Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, Trevor Cohn. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",
      "abstract": "Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, Trevor Cohn. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",
      "doi": "https://doi.org/10.18653/v1/n16-1109",
      "openalex_id": "https://openalex.org/W2466918907",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparative study on corpora for speech translation",
      "summary": "This paper investigates issues in preparing corpora for developing speech-to-speech translation (S2ST). It is impractical to create a broad-coverage parallel corpus only from dialog speech. An alternative approach is to have bilingual experts write conversational-style texts in the target domain, with translations. There is, however, a risk of losing fidelity to the actual utterances. This paper focuses on balancing a tradeoff between these two kinds of corpora through the analysis of two newly developed corpora in the travel domain: a bilingual parallel corpus with 420 K utterances and a collection of in-domain dialogs using actual S2ST systems. We found that the first corpus is effective for covering utterances in the second corpus if complimented with a small number of utterances taken from monolingual dialogs. We also found that characteristics of in-domain utterances become closer to those of the first corpus when more restrictive conditions and instructions to speakers are given. These results suggest the possibility of a bootstrap-style of development of corpora and S2ST systems, where an initial S2ST system is developed with parallel texts, and is then gradually improved with in-domain utterances collected by the system as restrictions are relaxed",
      "abstract": "This paper investigates issues in preparing corpora for developing speech-to-speech translation (S2ST). It is impractical to create a broad-coverage parallel corpus only from dialog speech. An alternative approach is to have bilingual experts write conversational-style texts in the target domain, with translations. There is, however, a risk of losing fidelity to the actual utterances. This paper focuses on balancing a tradeoff between these two kinds of corpora through the analysis of two newly developed corpora in the travel domain: a bilingual parallel corpus with 420 K utterances and a collection of in-domain dialogs using actual S2ST systems. We found that the first corpus is effective for covering utterances in the second corpus if complimented with a small number of utterances taken from monolingual dialogs. We also found that characteristics of in-domain utterances become closer to those of the first corpus when more restrictive conditions and instructions to speakers are given. These results suggest the possibility of a bootstrap-style of development of corpora and S2ST systems, where an initial S2ST system is developed with parallel texts, and is then gradually improved with in-domain utterances collected by the system as restrictions are relaxed",
      "doi": "https://doi.org/10.1109/tasl.2006.878262",
      "openalex_id": "https://openalex.org/W2161742089",
      "arxiv_id": "",
      "publication_date": "2006-08-23",
      "published": "2006-08-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation",
      "summary": "Sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition.Recently, several works have attempted to extend the models for end-to-end speech translation task.However, the usefulness of these models were only investigated on language pairs with similar syntax and word order (e.g., English-French or English-Spanish).In this work, we focus on end-to-end speech translation tasks on syntactically distant language pairs (e.g., English-Japanese) that require distant word reordering.To guide the encoder-decoder attentional model to learn this difficult problem, we propose a structured-based curriculum learning strategy.Unlike conventional curriculum learning that gradually emphasizes difficult data examples, we formalize learning strategies from easier network structures to more difficult network structures.Here, we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task.The experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning.",
      "abstract": "Sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition.Recently, several works have attempted to extend the models for end-to-end speech translation task.However, the usefulness of these models were only investigated on language pairs with similar syntax and word order (e.g., English-French or English-Spanish).In this work, we focus on end-to-end speech translation tasks on syntactically distant language pairs (e.g., English-Japanese) that require distant word reordering.To guide the encoder-decoder attentional model to learn this difficult problem, we propose a structured-based curriculum learning strategy.Unlike conventional curriculum learning that gradually emphasizes difficult data examples, we formalize learning strategies from easier network structures to more difficult network structures.Here, we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task.The experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning.",
      "doi": "https://doi.org/10.21437/interspeech.2017-944",
      "openalex_id": "https://openalex.org/W2962680099",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "summary": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
      "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
      "doi": "https://doi.org/10.18653/v1/d15-1166",
      "openalex_id": "https://openalex.org/W1902237438",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Scale Alignment and Contextual History for Attention Mechanism in Sequence-to-Sequence Model",
      "summary": "A sequence-to-sequence model is a neural network module for mapping two sequences of different lengths. The sequence-to-sequence model has three core modules: encoder, decoder, and attention. Attention is the bridge that connects the encoder and decoder modules and improves model performance in many tasks. In this paper, we propose two ideas to improve sequence-to-sequence model performance by enhancing the attention module. First, we maintain the history of the location and the expected context from several previous time-steps. Second, we apply multiscale convolution from several previous attention vectors to the current decoder state. We utilized our proposed framework for sequence-to-sequence speech recognition and text-to-speech systems. The results reveal that our proposed extension can improve performance significantly compared to a standard attention baseline.",
      "abstract": "A sequence-to-sequence model is a neural network module for mapping two sequences of different lengths. The sequence-to-sequence model has three core modules: encoder, decoder, and attention. Attention is the bridge that connects the encoder and decoder modules and improves model performance in many tasks. In this paper, we propose two ideas to improve sequence-to-sequence model performance by enhancing the attention module. First, we maintain the history of the location and the expected context from several previous time-steps. Second, we apply multiscale convolution from several previous attention vectors to the current decoder state. We utilized our proposed framework for sequence-to-sequence speech recognition and text-to-speech systems. The results reveal that our proposed extension can improve performance significantly compared to a standard attention baseline.",
      "doi": "https://doi.org/10.1109/slt.2018.8639528",
      "openalex_id": "https://openalex.org/W2884852625",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Empirical Evaluation of Rectified Activations in Convolutional Network",
      "summary": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.",
      "abstract": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.",
      "doi": "https://doi.org/10.48550/arxiv.1505.00853",
      "openalex_id": "https://openalex.org/W1921523184",
      "arxiv_id": "",
      "publication_date": "2015-05-05",
      "published": "2015-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
      "summary": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalize concept of unigram match ing between the machine pro uce translation an h uman-pro uce reference translations. Unigrams can be match e base on their surface forms, stemme forms, an meanings; furthermore, METEOR can be easily exten e to include more advanced matching strategies. Once all generalize unigram match es between the two strings have been found, METEOR computes a score for th is matching using a combination of unigram-precision, unigram-recall, an a measure of fragmentation th at is esigne to irectly capture h ow well-or ere th e match e wor s in th e mach ine translation are in relation to th e reference. We evaluate METEOR by measuring th e correlation between th e metric scores an h uman ju gments of translation quality. We compute th e Pearson R correlation value between its scores an h uman quality assessments of th e LDC TIDES 2003 Arabic-to-English an Ch inese-to-English atasets. We perform segment-bysegment correlation, an sh ow th at METEOR gets an R correlation value of 0.347 on th e Arabic ata an 0.331 on th e Ch inese ata. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.",
      "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalize concept of unigram match ing between the machine pro uce translation an h uman-pro uce reference translations. Unigrams can be match e base on their surface forms, stemme forms, an meanings; furthermore, METEOR can be easily exten e to include more advanced matching strategies. Once all generalize unigram match es between the two strings have been found, METEOR computes a score for th is matching using a combination of unigram-precision, unigram-recall, an a measure of fragmentation th at is esigne to irectly capture h ow well-or ere th e match e wor s in th e mach ine translation are in relation to th e reference. We evaluate METEOR by measuring th e correlation between th e metric scores an h uman ju gments of translation quality. We compute th e Pearson R correlation value between its scores an h uman quality assessments of th e LDC TIDES 2003 Arabic-to-English an Ch inese-to-English atasets. We perform segment-bysegment correlation, an sh ow th at METEOR gets an R correlation value of 0.347 on th e Arabic ata an 0.331 on th e Ch inese ata. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2123301721",
      "arxiv_id": "",
      "publication_date": "2005-06-01",
      "published": "2005-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability",
      "summary": "In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately",
      "abstract": "In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately",
      "doi": "https://doi.org/10.1184/r1/6473090",
      "openalex_id": "https://openalex.org/W2144600658",
      "arxiv_id": "",
      "publication_date": "2011-01-01",
      "published": "2011-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast Decoding in Sequence Models using Discrete Latent Variables",
      "summary": "Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.",
      "abstract": "Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.",
      "doi": "https://doi.org/10.48550/arxiv.1803.03382",
      "openalex_id": "https://openalex.org/W2789543585",
      "arxiv_id": "",
      "publication_date": "2018-03-09",
      "published": "2018-03-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An embedded segmental K-means model for unsupervised segmentation and clustering of speech",
      "summary": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.",
      "abstract": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.",
      "doi": "https://doi.org/10.1109/asru.2017.8269008",
      "openalex_id": "https://openalex.org/W2599585580",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonetic learning as a pathway to language: new data and native language magnet theory expanded (NLM-e)",
      "summary": "Infants' speech perception skills show a dual change towards the end of the first year of life. Not only does non-native speech perception decline, as often shown, but native language speech perception skills show improvement, reflecting a facilitative effect of experience with native language. The mechanism underlying change at this point in development, and the relationship between the change in native and non-native speech perception, is of theoretical interest. As shown in new data presented here, at the cusp of this developmental change, infants' native and non-native phonetic perception skills predict later language ability, but in opposite directions. Better native language skill at 7.5 months of age predicts faster language advancement, whereas better non-native language skill predicts slower advancement. We suggest that native language phonetic performance is indicative of neural commitment to the native language, while non-native phonetic performance reveals un committed neural circuitry. This paper has three goals: (i) to review existing models of phonetic perception development, (ii) to present new event-related potential data showing that native and non-native phonetic perception at 7.5 months of age predicts language growth over the next 2 years, and (iii) to describe a revised version of our previous model, the native language magnet model, expanded (NLM-e). NLM-e incorporates five new principles. Specific testable predictions for future research programmes are described.",
      "abstract": "Infants' speech perception skills show a dual change towards the end of the first year of life. Not only does non-native speech perception decline, as often shown, but native language speech perception skills show improvement, reflecting a facilitative effect of experience with native language. The mechanism underlying change at this point in development, and the relationship between the change in native and non-native speech perception, is of theoretical interest. As shown in new data presented here, at the cusp of this developmental change, infants' native and non-native phonetic perception skills predict later language ability, but in opposite directions. Better native language skill at 7.5 months of age predicts faster language advancement, whereas better non-native language skill predicts slower advancement. We suggest that native language phonetic performance is indicative of neural commitment to the native language, while non-native phonetic performance reveals un committed neural circuitry. This paper has three goals: (i) to review existing models of phonetic perception development, (ii) to present new event-related potential data showing that native and non-native phonetic perception at 7.5 months of age predicts language growth over the next 2 years, and (iii) to describe a revised version of our previous model, the native language magnet model, expanded (NLM-e). NLM-e incorporates five new principles. Specific testable predictions for future research programmes are described.",
      "doi": "https://doi.org/10.1098/rstb.2007.2154",
      "openalex_id": "https://openalex.org/W2103091632",
      "arxiv_id": "",
      "publication_date": "2007-09-10",
      "published": "2007-09-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech translation: coupling of recognition and translation",
      "summary": "In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.",
      "abstract": "In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.",
      "doi": "https://doi.org/10.1109/icassp.1999.758176",
      "openalex_id": "https://openalex.org/W2113106066",
      "arxiv_id": "",
      "publication_date": "1999-01-01",
      "published": "1999-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Almost Unsupervised Text to Speech and Automatic Speech Recognition",
      "summary": "Text to speech (TTS) and automatic speech recognition (ASR) are two dual tasks in speech processing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a major practical problem for TTS and ASR on low-resource languages. In this paper, by leveraging the dual nature of the two tasks, we propose an almost unsupervised learning method that only leverages few hundreds of paired data and extra unpaired data for TTS and ASR. Our method consists of the following components: (1) a denoising auto-encoder, which reconstructs speech and text sequences respectively to develop the capability of language modeling both in speech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into speech $\\hat{x}$, and the ASR model leverages the transformed pair $(\\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which addresses error propagation especially in the long speech and text sequence when training with few paired data; (4) a unified model structure, which combines all the above components for TTS and ASR based on Transformer model. Our method achieves 99.84% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7% PER for ASR on LJSpeech dataset, by leveraging only 200 paired speech and text data (about 20 minutes audio), together with extra unpaired speech and text data.",
      "abstract": "Text to speech (TTS) and automatic speech recognition (ASR) are two dual tasks in speech processing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a major practical problem for TTS and ASR on low-resource languages. In this paper, by leveraging the dual nature of the two tasks, we propose an almost unsupervised learning method that only leverages few hundreds of paired data and extra unpaired data for TTS and ASR. Our method consists of the following components: (1) a denoising auto-encoder, which reconstructs speech and text sequences respectively to develop the capability of language modeling both in speech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into speech $\\hat{x}$, and the ASR model leverages the transformed pair $(\\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which addresses error propagation especially in the long speech and text sequence when training with few paired data; (4) a unified model structure, which combines all the above components for TTS and ASR based on Transformer model. Our method achieves 99.84% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7% PER for ASR on LJSpeech dataset, by leveraging only 200 paired speech and text data (about 20 minutes audio), together with extra unpaired speech and text data.",
      "doi": "https://doi.org/10.48550/arxiv.1905.06791",
      "openalex_id": "https://openalex.org/W2945078028",
      "arxiv_id": "",
      "publication_date": "2019-05-13",
      "published": "2019-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments",
      "summary": "Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources or stable orthography. Systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered and unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We present how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation.",
      "abstract": "Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources or stable orthography. Systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered and unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We present how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation.",
      "doi": "https://doi.org/10.48550/arxiv.1710.03501",
      "openalex_id": "https://openalex.org/W2762715843",
      "arxiv_id": "",
      "publication_date": "2017-10-10",
      "published": "2017-10-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A guide to convolution arithmetic for deep learning",
      "summary": "We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.",
      "abstract": "We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.",
      "doi": "https://doi.org/10.48550/arxiv.1603.07285",
      "openalex_id": "https://openalex.org/W2304648132",
      "arxiv_id": "",
      "publication_date": "2016-03-23",
      "published": "2016-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Technology for Unwritten Languages",
      "summary": "&lt;p&gt;Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.&lt;/p&gt;",
      "doi": "https://doi.org/10.1109/taslp.2020.2973896",
      "openalex_id": "https://openalex.org/W3005578234",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Speech Translation with the Transformer",
      "summary": "Speech Translation has been traditionally addressed with the concatenation of two tasks: Speech Recognition and Machine Translation. This approach has the main drawback that errors are concatenated. Recently, neural approaches to Speech Recognition and Machine Translation have made possible facing the task by means of an End-to-End Speech Translation architecture. In this paper, we propose to use the architecture of the Transformer which is based solely on attention-based mechanisms to address the End-to-End Speech Translation system. As a contrastive architecture, we use the same Transformer to built the Speech Recognition and Machine Translation systems to perform Speech Translation through concatenation of systems. Results on a Spanish-to-English standard task show that the end-to-end architecture is able to outperform the concatenated systems by half point BLEU.",
      "abstract": "Speech Translation has been traditionally addressed with the concatenation of two tasks: Speech Recognition and Machine Translation. This approach has the main drawback that errors are concatenated. Recently, neural approaches to Speech Recognition and Machine Translation have made possible facing the task by means of an End-to-End Speech Translation architecture. In this paper, we propose to use the architecture of the Transformer which is based solely on attention-based mechanisms to address the End-to-End Speech Translation system. As a contrastive architecture, we use the same Transformer to built the Speech Recognition and Machine Translation systems to perform Speech Translation through concatenation of systems. Results on a Spanish-to-English standard task show that the end-to-end architecture is able to outperform the concatenated systems by half point BLEU.",
      "doi": "https://doi.org/10.21437/iberspeech.2018-13",
      "openalex_id": "https://openalex.org/W2901607128",
      "arxiv_id": "",
      "publication_date": "2018-11-19",
      "published": "2018-11-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
      "summary": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.",
      "abstract": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2952468927",
      "arxiv_id": "",
      "publication_date": "2019-05-07",
      "published": "2019-05-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-To-End Named Entity And Semantic Concept Extraction From Speech",
      "summary": "Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents. Until now, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propagation, metric to tune ASR systems sub-optimal in regards to the final task, reduced space search at the ASR output level,...) and it is known that more integrated approaches outperform sequential ones, when they can be applied. In this paper, we explore an end-to-end approach that directly extracts named entities from speech, though a unique neural architecture. On a such way, a joint optimization is possible for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaigns. The results are promising since this end-to-end approach provides similar results (F-measure=0.66 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.64). Last, we also explore this approach applied to semantic concept extraction, through a slot filling task known as a spoken language understanding problem, and also observe an improvement in comparison to a pipeline approach.",
      "abstract": "Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents. Until now, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propagation, metric to tune ASR systems sub-optimal in regards to the final task, reduced space search at the ASR output level,...) and it is known that more integrated approaches outperform sequential ones, when they can be applied. In this paper, we explore an end-to-end approach that directly extracts named entities from speech, though a unique neural architecture. On a such way, a joint optimization is possible for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaigns. The results are promising since this end-to-end approach provides similar results (F-measure=0.66 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.64). Last, we also explore this approach applied to semantic concept extraction, through a slot filling task known as a spoken language understanding problem, and also observe an improvement in comparison to a pipeline approach.",
      "doi": "https://doi.org/10.1109/slt.2018.8639513",
      "openalex_id": "https://openalex.org/W2914417638",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "summary": "We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.",
      "abstract": "We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472621",
      "openalex_id": "https://openalex.org/W2327501763",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
      "summary": "This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C.",
      "abstract": "This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10368",
      "openalex_id": "https://openalex.org/W4226278833",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CoVoST 2 and Massively Multilingual Speech-to-Text Translation",
      "summary": "Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.",
      "abstract": "Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.",
      "doi": "https://doi.org/10.48550/arxiv.2007.10310",
      "openalex_id": "https://openalex.org/W3054645415",
      "arxiv_id": "",
      "publication_date": "2020-07-20",
      "published": "2020-07-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
      "summary": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
      "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
      "doi": "https://doi.org/10.48550/arxiv.1611.01462",
      "openalex_id": "https://openalex.org/W2549416390",
      "arxiv_id": "",
      "publication_date": "2016-11-04",
      "published": "2016-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "St-Bert: Cross-Modal Language Model Pre-Training for End-to-End Spoken Language Understanding",
      "summary": "Language model pre-training has shown promising results in various downstream tasks. In this context, we introduce a cross-modal pre-trained language model, called Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language understanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text as an input, ST-BERT learns a contextualized cross-modal alignment via our two proposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and Cross-modal Conditioned Language Modeling (CM-CLM). Experimental results on three benchmarks present that our approach is effective for various SLU datasets and shows a surprisingly marginal performance degradation even when 1% of the training data are available. Also, our method shows further SLU performance gain via domain-adaptive pre-training with domain-specific speech-text pair data.",
      "abstract": "Language model pre-training has shown promising results in various downstream tasks. In this context, we introduce a cross-modal pre-trained language model, called Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language understanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text as an input, ST-BERT learns a contextualized cross-modal alignment via our two proposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and Cross-modal Conditioned Language Modeling (CM-CLM). Experimental results on three benchmarks present that our approach is effective for various SLU datasets and shows a surprisingly marginal performance degradation even when 1% of the training data are available. Also, our method shows further SLU performance gain via domain-adaptive pre-training with domain-specific speech-text pair data.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414558",
      "openalex_id": "https://openalex.org/W3161302809",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech-Language Pre-Training for End-to-End Spoken Language Understanding",
      "summary": "End-to-end (E2E) spoken language understanding (SLU) can infer semantics directly from speech signal without cascading an automatic speech recognizer (ASR) with a natural language understanding (NLU) module. However, paired utterance recordings and corresponding semantics may not always be available or sufficient to train an E2E SLU model in a real production environment. In this paper, we propose to unify a well-optimized E2E ASR encoder (speech) and a pre-trained language model encoder (language) into a transformer decoder. The unified speech-language pre-trained model (SLP) is continually enhanced on limited labeled data from a target domain by using a conditional masked language model (MLM) objective, and thus can effectively generate a sequence of intent, slot type, and slot value for given input speech in the inference. The experimental results on two public corpora show that our approach to E2E SLU is superior to the conventional cascaded method. It also outperforms the present state-of-the-art approaches to E2E SLU with much less paired data.",
      "abstract": "End-to-end (E2E) spoken language understanding (SLU) can infer semantics directly from speech signal without cascading an automatic speech recognizer (ASR) with a natural language understanding (NLU) module. However, paired utterance recordings and corresponding semantics may not always be available or sufficient to train an E2E SLU model in a real production environment. In this paper, we propose to unify a well-optimized E2E ASR encoder (speech) and a pre-trained language model encoder (language) into a transformer decoder. The unified speech-language pre-trained model (SLP) is continually enhanced on limited labeled data from a target domain by using a conditional masked language model (MLM) objective, and thus can effectively generate a sequence of intent, slot type, and slot value for given input speech in the inference. The experimental results on two public corpora show that our approach to E2E SLU is superior to the conventional cascaded method. It also outperforms the present state-of-the-art approaches to E2E SLU with much less paired data.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414900",
      "openalex_id": "https://openalex.org/W3162313915",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Optimizing Alignment of Speech and Language Latent Spaces for End-To-End Speech Recognition and Understanding",
      "summary": "The advances in attention-based encoder-decoder (AED) networks have brought great progress to end-to-end (E2E) automatic speech recognition (ASR). One way to further improve the performance of AED-based E2E ASR is to introduce an extra text encoder for leveraging extensive text data and thus capture more context-aware linguistic information. However, this approach brings a mismatch problem between the speech encoder and the text encoder due to the different units used for modeling. In this paper, we propose an embedding aligner and modality switch training to better align the speech and text latent spaces. The embedding aligner is a shared linear projection between text encoder and speech encoder trained by masked language modeling (MLM) loss and connectionist temporal classification (CTC), respectively. The modality switch training randomly swaps speech and text embeddings based on the forced alignment result to learn a joint representation space. Experimental results show that our proposed approach achieves a relative 14% to 19% word error rate (WER) reduction on Librispeech ASR task. We further verify its effectiveness on spoken language understanding (SLU), i.e., an absolute 2.5% to 2.8% F1 score improvement on SNIPS slot filling task.",
      "abstract": "The advances in attention-based encoder-decoder (AED) networks have brought great progress to end-to-end (E2E) automatic speech recognition (ASR). One way to further improve the performance of AED-based E2E ASR is to introduce an extra text encoder for leveraging extensive text data and thus capture more context-aware linguistic information. However, this approach brings a mismatch problem between the speech encoder and the text encoder due to the different units used for modeling. In this paper, we propose an embedding aligner and modality switch training to better align the speech and text latent spaces. The embedding aligner is a shared linear projection between text encoder and speech encoder trained by masked language modeling (MLM) loss and connectionist temporal classification (CTC), respectively. The modality switch training randomly swaps speech and text embeddings based on the forced alignment result to learn a joint representation space. Experimental results show that our proposed approach achieves a relative 14% to 19% word error rate (WER) reduction on Librispeech ASR task. We further verify its effectiveness on spoken language understanding (SLU), i.e., an absolute 2.5% to 2.8% F1 score improvement on SNIPS slot filling task.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747760",
      "openalex_id": "https://openalex.org/W3209371554",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Networks for Pattern Recognition",
      "summary": "Abstract This book provides the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts of pattern recognition, the book describes techniques for modelling probability density functions, and discusses the properties and relative merits of the multi-layer perceptron and radial basis function network models. It also motivates the use of various forms of error functions, and reviews the principal algorithms for error function minimization. As well as providing a detailed discussion of learning and generalization in neural networks, the book also covers the important topics of data processing, feature extraction, and prior knowledge. The book concludes with an extensive treatment of Bayesian techniques and their applications to neural networks.",
      "abstract": "Abstract This book provides the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts of pattern recognition, the book describes techniques for modelling probability density functions, and discusses the properties and relative merits of the multi-layer perceptron and radial basis function network models. It also motivates the use of various forms of error functions, and reviews the principal algorithms for error function minimization. As well as providing a detailed discussion of learning and generalization in neural networks, the book also covers the important topics of data processing, feature extraction, and prior knowledge. The book concludes with an extensive treatment of Bayesian techniques and their applications to neural networks.",
      "doi": "https://doi.org/10.1093/oso/9780198538493.001.0001",
      "openalex_id": "https://openalex.org/W4388297464",
      "arxiv_id": "",
      "publication_date": "1995-11-23",
      "published": "1995-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sparse Coding of Pitch Contours with Deep Auto-Encoders",
      "summary": "This paper presents a sparse coding algorithm based on deep auto-encoders for the stylization and the clustering of pitch contours.The main objective of the proposed algorithm is to learn a set of pitch templates that can be easily interpreted by humans and whose combination can approximate efficiently the observed pitch contours.The proposed learning architecture is based on deep auto-encoders, commonly used to learn non-linear and low-dimensional latent representations that approximate the observed data.The proposed deep architecture is based on stacked auto-encoders and the sparsity of the network is investigated in order to learn a more robust and general representation of the pitch contours (dropout, denoising auto-encoder, sparsity regularization).The deep auto-encoding of the pitch contours is illustrated and discussed on the TIMIT American-English speech database † with comparison of other existing stylization and clustering algorithms.",
      "abstract": "This paper presents a sparse coding algorithm based on deep auto-encoders for the stylization and the clustering of pitch contours.The main objective of the proposed algorithm is to learn a set of pitch templates that can be easily interpreted by humans and whose combination can approximate efficiently the observed pitch contours.The proposed learning architecture is based on deep auto-encoders, commonly used to learn non-linear and low-dimensional latent representations that approximate the observed data.The proposed deep architecture is based on stacked auto-encoders and the sparsity of the network is investigated in order to learn a more robust and general representation of the pitch contours (dropout, denoising auto-encoder, sparsity regularization).The deep auto-encoding of the pitch contours is illustrated and discussed on the TIMIT American-English speech database † with comparison of other existing stylization and clustering algorithms.",
      "doi": "https://doi.org/10.21437/speechprosody.2018-161",
      "openalex_id": "https://openalex.org/W2808514527",
      "arxiv_id": "",
      "publication_date": "2018-06-11",
      "published": "2018-06-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural network based generation of fundamental frequency contours",
      "summary": "Although a number of algorithms exist for the generation of the fundamental frequency contour in automatic text-to-speech conversion systems, the absence of a general theory of intonation still prevents the correct derivation of this important feature in unrestricted text applications. A parallel distributed approach is presented in which two neural networks were designed to learn the F0 values for each phoneme and the F0 fluctuations within each phoneme for words that correspond to a small training set. The neural networks used for this task have demonstrated the ability to generalize their properties on new text, and their level of success depends on the composition and size of the training corpus.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Although a number of algorithms exist for the generation of the fundamental frequency contour in automatic text-to-speech conversion systems, the absence of a general theory of intonation still prevents the correct derivation of this important feature in unrestricted text applications. A parallel distributed approach is presented in which two neural networks were designed to learn the F0 values for each phoneme and the F0 fluctuations within each phoneme for words that correspond to a small training set. The neural networks used for this task have demonstrated the ability to generalize their properties on new text, and their level of success depends on the composition and size of the training corpus.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1989.266404",
      "openalex_id": "https://openalex.org/W1551113872",
      "arxiv_id": "",
      "publication_date": "2003-01-13",
      "published": "2003-01-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An RNN-based prosodic information synthesizer for Mandarin text-to-speech",
      "summary": "A new RNN-based prosodic information synthesizer for Mandarin Chinese text-to-speech (TTS) is proposed in this paper. Its four-layer recurrent neural network (RNN) generates prosodic information such as syllable pitch contours, syllable energy levels, syllable initial and final durations, as well as intersyllable pause durations. The input layer and first hidden layer operate with a word-synchronized clock to represent current-word phonologic states within the prosodic structure of text to be synthesized. The second hidden layer and output layer operate on a syllable-synchronized clock and use outputs from the preceding layers, along with additional syllable-level inputs fed directly to the second hidden layer, to generate desired prosodic parameters. The RNN was trained on a large set of actual utterances accompanied by associated texts, and can automatically learn many human-prosody phonologic rules, including the well-known Sandhi Tone 3 F0-change rule. Experimental results show that all synthesized prosodic parameter sequences matched quite well with their original counterparts, and a pitch-synchronous-overlap-add-based (PSOLA-based) Mandarin TTS system was also used for testing of our approach. While subjective tests are difficult to perform and remain to be done in the future, we have carried out informal listening tests by a significant number of native Chinese speakers and the results confirmed that all synthesized speech sounded quite natural.",
      "abstract": "A new RNN-based prosodic information synthesizer for Mandarin Chinese text-to-speech (TTS) is proposed in this paper. Its four-layer recurrent neural network (RNN) generates prosodic information such as syllable pitch contours, syllable energy levels, syllable initial and final durations, as well as intersyllable pause durations. The input layer and first hidden layer operate with a word-synchronized clock to represent current-word phonologic states within the prosodic structure of text to be synthesized. The second hidden layer and output layer operate on a syllable-synchronized clock and use outputs from the preceding layers, along with additional syllable-level inputs fed directly to the second hidden layer, to generate desired prosodic parameters. The RNN was trained on a large set of actual utterances accompanied by associated texts, and can automatically learn many human-prosody phonologic rules, including the well-known Sandhi Tone 3 F0-change rule. Experimental results show that all synthesized prosodic parameter sequences matched quite well with their original counterparts, and a pitch-synchronous-overlap-add-based (PSOLA-based) Mandarin TTS system was also used for testing of our approach. While subjective tests are difficult to perform and remain to be done in the future, we have carried out informal listening tests by a significant number of native Chinese speakers and the results confirmed that all synthesized speech sounded quite natural.",
      "doi": "https://doi.org/10.1109/89.668817",
      "openalex_id": "https://openalex.org/W2106564373",
      "arxiv_id": "",
      "publication_date": "1998-05-01",
      "published": "1998-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improved Prosody Generation by Maximizing Joint Probability of State and Longer Units",
      "summary": "The current state-of-the-art hidden Markov model (HMM)-based text-to-speech (TTS) can produce highly intelligible, synthesized speech with decent segmental quality. However, its prosody, especially at phrase or sentence level, still tends to be bland. This blandness is partially due to the fact that the state-based HMM is inadequate in capturing global, hierarchical suprasegmental information in speech signals. In this paper, to improve the TTS prosody, longer units are first explicitly modeled with appropriate parametric distributions. The resultant models are then integrated with the state-based baseline models in generating better prosody by maximizing the joint probability. Experimental results in both Mandarin and English show consistent improvements over our baseline system with only state-based prosody model. The improvements are both objectively measurable and subjectively perceivable.",
      "abstract": "The current state-of-the-art hidden Markov model (HMM)-based text-to-speech (TTS) can produce highly intelligible, synthesized speech with decent segmental quality. However, its prosody, especially at phrase or sentence level, still tends to be bland. This blandness is partially due to the fact that the state-based HMM is inadequate in capturing global, hierarchical suprasegmental information in speech signals. In this paper, to improve the TTS prosody, longer units are first explicitly modeled with appropriate parametric distributions. The resultant models are then integrated with the state-based baseline models in generating better prosody by maximizing the joint probability. Experimental results in both Mandarin and English show consistent improvements over our baseline system with only state-based prosody model. The improvements are both objectively measurable and subjectively perceivable.",
      "doi": "https://doi.org/10.1109/tasl.2010.2097248",
      "openalex_id": "https://openalex.org/W2109938215",
      "arxiv_id": "",
      "publication_date": "2010-12-11",
      "published": "2010-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving F0 prediction using bidirectional associative memories and syllable-level F0 features for HMM-based Mandarin speech synthesis",
      "summary": "The speech generated by hidden Markov model (HMM) based speech synthesis method always sounds monotonous compared with natural recordings. An important reason is that the predicted F0 trajectories are over-smoothed. This arises from the adoption of frame-level F0 features and the averaging effect of acoustic modeling using Gaussians in the conventional F0 modeling approach. In this paper, we propose a method to improve the F0 prediction of HMM-based Mandarin speech synthesis in a post-filtering way. Syllable-level F0 features, e.g., length-normalized logF0 vectors or quantitative target approximation (qTA) parameters, are extracted from the F0 trajectories predicted by the conventional approach. These features are mapped towards natural ones by Gaussian bidirectional associative memory (GBAM) based transformation. Our subjective experiments indicate that the GBAM-based F0 post-filtering method using either logF0 vectors or qTA parameters can significantly improve the naturalness of synthetic speech. Using raw logF0 vectors for post-filtering can achieve better performance than using derived qTA parameters.",
      "abstract": "The speech generated by hidden Markov model (HMM) based speech synthesis method always sounds monotonous compared with natural recordings. An important reason is that the predicted F0 trajectories are over-smoothed. This arises from the adoption of frame-level F0 features and the averaging effect of acoustic modeling using Gaussians in the conventional F0 modeling approach. In this paper, we propose a method to improve the F0 prediction of HMM-based Mandarin speech synthesis in a post-filtering way. Syllable-level F0 features, e.g., length-normalized logF0 vectors or quantitative target approximation (qTA) parameters, are extracted from the F0 trajectories predicted by the conventional approach. These features are mapped towards natural ones by Gaussian bidirectional associative memory (GBAM) based transformation. Our subjective experiments indicate that the GBAM-based F0 post-filtering method using either logF0 vectors or qTA parameters can significantly improve the naturalness of synthetic speech. Using raw logF0 vectors for post-filtering can achieve better performance than using derived qTA parameters.",
      "doi": "https://doi.org/10.1109/iscslp.2014.6936598",
      "openalex_id": "https://openalex.org/W2067549749",
      "arxiv_id": "",
      "publication_date": "2014-09-01",
      "published": "2014-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vae-Space: Deep Generative Model of Voice Fundamental Frequency Contours",
      "summary": "Modeling the speech generation process can provide flexible and interpretable ways to generate intended synthetic speech. In this paper, we present a deep generative model of fundamental frequency (F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> ) contours of normal speech and singing voices. The generative model we propose in this paper 1) is able to accurately decompose an F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> contour into the sum of phrase and accent components of the Fujisaki model, a mathematical model describing the control mechanism of vocal fold vibration, without an iterative algorithm, and 2) can represent/generate F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> contours of both normal speech and singing voices reasonably well.",
      "abstract": "Modeling the speech generation process can provide flexible and interpretable ways to generate intended synthetic speech. In this paper, we present a deep generative model of fundamental frequency (F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> ) contours of normal speech and singing voices. The generative model we propose in this paper 1) is able to accurately decompose an F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> contour into the sum of phrase and accent components of the Fujisaki model, a mathematical model describing the control mechanism of vocal fold vibration, without an iterative algorithm, and 2) can represent/generate F <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> contours of both normal speech and singing voices reasonably well.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461569",
      "openalex_id": "https://openalex.org/W2891535489",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Comparison of Recent Waveform Generation and Acoustic Modeling Methods for Neural-Network-Based Speech Synthesis",
      "summary": "Recent advances in speech synthesis suggest that limitations such as the lossy nature of the amplitude spectrum with minimum phase approximation and the over-smoothing effect in acoustic modeling can be overcome by using advanced machine learning approaches. In this paper, we build a framework in which we can fairly compare new vocoding and acoustic modeling techniques with conventional approaches by means of a large scale crowdsourced evaluation. Results on acoustic models showed that generative adversarial networks and an autoregressive (AR) model performed better than a normal recurrent network and the AR model performed best. Evaluation on vocoders by using the same AR acoustic model demonstrated that aWavenet vocoder outperformed classical sourcefilter-based vocoders. Particularly, generated speech waveforms from the combination of AR acoustic model and Wavenet vocoder achieved a similar score of speech quality to vocoded speech.<br/><i><b>Index Terms</b></i>— speech synthesis, deep learning, Wavenet, general adversarial network, autoregressive neural network",
      "abstract": "Recent advances in speech synthesis suggest that limitations such as the lossy nature of the amplitude spectrum with minimum phase approximation and the over-smoothing effect in acoustic modeling can be overcome by using advanced machine learning approaches. In this paper, we build a framework in which we can fairly compare new vocoding and acoustic modeling techniques with conventional approaches by means of a large scale crowdsourced evaluation. Results on acoustic models showed that generative adversarial networks and an autoregressive (AR) model performed better than a normal recurrent network and the AR model performed best. Evaluation on vocoders by using the same AR acoustic model demonstrated that aWavenet vocoder outperformed classical sourcefilter-based vocoders. Particularly, generated speech waveforms from the combination of AR acoustic model and Wavenet vocoder achieved a similar score of speech quality to vocoded speech.<br/><i><b>Index Terms</b></i>— speech synthesis, deep learning, Wavenet, general adversarial network, autoregressive neural network",
      "doi": "https://doi.org/10.1109/icassp.2018.8461452",
      "openalex_id": "https://openalex.org/W2963522141",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Continuous F0 Modeling for HMM Based Statistical Parametric Speech Synthesis",
      "summary": "The modeling of fundamental frequency, or F0, in HMM-based speech synthesis is a critical factor in delivering speech which is both natural and accurately conveys all of the many nuances of the message. However, F0 modeling is difficult because F0 values are normally considered to depend on a binary voicing decision such that they are continuous in voiced regions and undefined in unvoiced regions. F0 is therefore a discontinuous function of time. multi-space probability distribution HMM (MSDHMM) is a widely used solution to this problem. The MSDHMM essentially uses a joint distribution of discrete voicing labels and the discontinuous F0 observations. However, due to the discontinuity assumption, the MSDHMM provides a rather weak F0 trajectory model. In this paper, F0 is viewed as being a continuous function of time and this is achieved by assuming that F0 can be observed within unvoiced regions as well as voiced regions. This provides a continuous F0 data stream which can be modeled by standard HMMs. Voicing labels are modeled either implicitly or explicitly in order to perform voicing classification and a globally tied distribution (GTD) technique is used to achieve robust F0 estimation. Both objective measures and subjective listening tests demonstrate that continuous F0 modeling yields better synthesized F0 trajectories and significant improvements to the naturalness of synthesized speech compared to using the MSDHMM model.",
      "abstract": "The modeling of fundamental frequency, or F0, in HMM-based speech synthesis is a critical factor in delivering speech which is both natural and accurately conveys all of the many nuances of the message. However, F0 modeling is difficult because F0 values are normally considered to depend on a binary voicing decision such that they are continuous in voiced regions and undefined in unvoiced regions. F0 is therefore a discontinuous function of time. multi-space probability distribution HMM (MSDHMM) is a widely used solution to this problem. The MSDHMM essentially uses a joint distribution of discrete voicing labels and the discontinuous F0 observations. However, due to the discontinuity assumption, the MSDHMM provides a rather weak F0 trajectory model. In this paper, F0 is viewed as being a continuous function of time and this is achieved by assuming that F0 can be observed within unvoiced regions as well as voiced regions. This provides a continuous F0 data stream which can be modeled by standard HMMs. Voicing labels are modeled either implicitly or explicitly in order to perform voicing classification and a globally tied distribution (GTD) technique is used to achieve robust F0 estimation. Both objective measures and subjective listening tests demonstrate that continuous F0 modeling yields better synthesized F0 trajectories and significant improvements to the naturalness of synthesized speech compared to using the MSDHMM model.",
      "doi": "https://doi.org/10.1109/tasl.2010.2076805",
      "openalex_id": "https://openalex.org/W2029434926",
      "arxiv_id": "",
      "publication_date": "2010-09-20",
      "published": "2010-09-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Synthesis Based on Hidden Markov Models",
      "summary": "This paper gives a general overview of hidden Markov model (HMM)-based speech synthesis, which has recently been demonstrated to be very effective in synthesizing speech. The main advantage of this approach is its flexibility in changing speaker identities, emotions, and speaking styles. This paper also discusses the relation between the HMM-based approach and the more conventional unit-selection approach that has dominated over the last decades. Finally, advanced techniques for future developments are described.",
      "abstract": "This paper gives a general overview of hidden Markov model (HMM)-based speech synthesis, which has recently been demonstrated to be very effective in synthesizing speech. The main advantage of this approach is its flexibility in changing speaker identities, emotions, and speaking styles. This paper also discusses the relation between the HMM-based approach and the more conventional unit-selection approach that has dominated over the last decades. Finally, advanced techniques for future developments are described.",
      "doi": "https://doi.org/10.1109/jproc.2013.2251852",
      "openalex_id": "https://openalex.org/W2111284386",
      "arxiv_id": "",
      "publication_date": "2013-04-09",
      "published": "2013-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analysis and synthesis of intonation using the Tilt model",
      "summary": "This paper introduces the Tilt intonational model and describes how this model can be used to automatically analyze and synthesize intonation. In the model, intonation is represented as a linear sequence of events, which can be pitch accents or boundary tones. Each event is characterized by continuous parameters representing amplitude, duration, and tilt (a measure of the shape of the event). The paper describes an event detector, in effect an intonational recognition system, which produces a transcription of an utterance’s intonation. The features and parameters of the event detector are discussed and performance figures are shown on a variety of read and spontaneous speaker independent conversational speech databases. Given the event locations, algorithms are described which produce an automatic analysis of each event in terms of the Tilt parameters. Synthesis algorithms are also presented which generate F0 contours from Tilt representations. The accuracy of these is shown by comparing synthetic F0 contours to real F0 contours. The paper concludes with an extensive discussion on linguistic representations of intonation and gives evidence that the Tilt model goes a long way to satisfying the desired goals of such a representation in that it has the right number of degrees of freedom to be able to describe and synthesize intonation accurately.",
      "abstract": "This paper introduces the Tilt intonational model and describes how this model can be used to automatically analyze and synthesize intonation. In the model, intonation is represented as a linear sequence of events, which can be pitch accents or boundary tones. Each event is characterized by continuous parameters representing amplitude, duration, and tilt (a measure of the shape of the event). The paper describes an event detector, in effect an intonational recognition system, which produces a transcription of an utterance’s intonation. The features and parameters of the event detector are discussed and performance figures are shown on a variety of read and spontaneous speaker independent conversational speech databases. Given the event locations, algorithms are described which produce an automatic analysis of each event in terms of the Tilt parameters. Synthesis algorithms are also presented which generate F0 contours from Tilt representations. The accuracy of these is shown by comparing synthetic F0 contours to real F0 contours. The paper concludes with an extensive discussion on linguistic representations of intonation and gives evidence that the Tilt model goes a long way to satisfying the desired goals of such a representation in that it has the right number of degrees of freedom to be able to describe and synthesize intonation accurately.",
      "doi": "https://doi.org/10.1121/1.428453",
      "openalex_id": "https://openalex.org/W2093414802",
      "arxiv_id": "",
      "publication_date": "2000-03-01",
      "published": "2000-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The CSTR entry to the Blizzard Challenge 2016",
      "summary": "Similar to 2016 and 2017 Blizzard Challenge, the task for this year is to train on expressively-read children’s story-books, and to synthesise speech in the same domain. This give us an opportunity to investigate the effectiveness of several techniques we have developed when applied to expressive and prosodically varied audiobook data. This paper describes the text-to-speech system entered by The Centre for Speech Technology Research into the 2018 Blizzard Challenge. The system is a hybrid synthesis system where a halfphone unit selection synthesiser is driven by the output of a neural network based acoustic and duration model. We adopt the same neural network based models used in our last year entry with a different unit selection component. We discuss the performance of our system by reporting the results from formal listening tests provided by the challenge.",
      "abstract": "Similar to 2016 and 2017 Blizzard Challenge, the task for this year is to train on expressively-read children’s story-books, and to synthesise speech in the same domain. This give us an opportunity to investigate the effectiveness of several techniques we have developed when applied to expressive and prosodically varied audiobook data. This paper describes the text-to-speech system entered by The Centre for Speech Technology Research into the 2018 Blizzard Challenge. The system is a hybrid synthesis system where a halfphone unit selection synthesiser is driven by the output of a neural network based acoustic and duration model. We adopt the same neural network based models used in our last year entry with a different unit selection component. We discuss the performance of our system by reporting the results from formal listening tests provided by the challenge.",
      "doi": "https://doi.org/10.21437/blizzard.2016-12",
      "openalex_id": "https://openalex.org/W2795043217",
      "arxiv_id": "",
      "publication_date": "2016-09-16",
      "published": "2016-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks",
      "summary": "This paper describes bidirectional recurrent mixture density networks, which can model multi-modal distributions of the type P(xt|y1T) and P(xt|x1, x2,..., xt-1, y1T) without any explicit assumptions about the use of context. These expressions occur frequently in pattern recognition problems with sequential data, for example in speech recognition. Experiments show that the proposed generative models give a higher likelihood on test data compared to a traditional modeling approach, indicating that they can summarize the statistical properties of the data better.",
      "abstract": "This paper describes bidirectional recurrent mixture density networks, which can model multi-modal distributions of the type P(xt|y1T) and P(xt|x1, x2,..., xt-1, y1T) without any explicit assumptions about the use of context. These expressions occur frequently in pattern recognition problems with sequential data, for example in speech recognition. Experiments show that the proposed generative models give a higher likelihood on test data compared to a traditional modeling approach, indicating that they can summarize the statistical properties of the data better.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2112656927",
      "arxiv_id": "",
      "publication_date": "1999-11-29",
      "published": "1999-11-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Clockwork RNN",
      "summary": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.",
      "abstract": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2138660131",
      "arxiv_id": "",
      "publication_date": "2014-06-21",
      "published": "2014-06-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "summary": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include",
      "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include",
      "doi": "",
      "openalex_id": "https://openalex.org/W1533861849",
      "arxiv_id": "",
      "publication_date": "2010-01-01",
      "published": "2010-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introducing CURRENNT: the Munich open-source CUDA recurrent neural network toolkit",
      "summary": "In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.",
      "abstract": "In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.",
      "doi": "",
      "openalex_id": "https://openalex.org/W304834817",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilevel parametric-base F0 model for speech synthesis",
      "summary": "Abstract This paper proposes a new F0 model for speech synthesis basedon the parameterization of the logF0 contour of the syllables.This parameterization consists of the N -order discrete cosinetransform (DCT) plus some additional parameters such as thegradient of the syllable average pitch. A statistical model of thesyllable pitch contour is then created by clustering the param-eterized vectors with a decision tree. Similar statistical modelsare also created for other linguistic levels other than the syllable.For synthesis, the statistical model of each level is used to deﬁnea log-likelihood function for the input text. These functions arethen weighted and added into a global log-likelihood functionwhich is then maximized with respect to the DCT coefﬁcients ofthe syllable model. The ﬁnal logF0 contour is obtained from theinverse transformation of the syllable DCT coefﬁcients. A sub-jective test showed a clear preference for the proposed modelagainst our previous HMM-based baseline.Index Terms: speech synthesis, HMM-based synthesis,prosody, discrete cosine transform",
      "abstract": "Abstract This paper proposes a new F0 model for speech synthesis basedon the parameterization of the logF0 contour of the syllables.This parameterization consists of the N -order discrete cosinetransform (DCT) plus some additional parameters such as thegradient of the syllable average pitch. A statistical model of thesyllable pitch contour is then created by clustering the param-eterized vectors with a decision tree. Similar statistical modelsare also created for other linguistic levels other than the syllable.For synthesis, the statistical model of each level is used to deﬁnea log-likelihood function for the input text. These functions arethen weighted and added into a global log-likelihood functionwhich is then maximized with respect to the DCT coefﬁcients ofthe syllable model. The ﬁnal logF0 contour is obtained from theinverse transformation of the syllable DCT coefﬁcients. A sub-jective test showed a clear preference for the proposed modelagainst our previous HMM-based baseline.Index Terms: speech synthesis, HMM-based synthesis,prosody, discrete cosine transform",
      "doi": "https://doi.org/10.21437/interspeech.2008-558",
      "openalex_id": "https://openalex.org/W113106864",
      "arxiv_id": "",
      "publication_date": "2008-09-22",
      "published": "2008-09-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating f0 contours for speech synthesis using the tilt intonation theory.",
      "summary": "This paper presents a method for generating F 0 contours for a speech synthesis system using the Tilt intonation theory ([10], [9]). The Tilt theory offers an abstract description of natural F 0 contours which may be derived automatically from natural speech. Given a speech database labelled with Tilt events, this paper shows how that data may be used to train a model which can adequately predict Tilt parameters from features available in a text to speech system and hence produce natural sounding F 0 contours. After a short description of the Tilt theory, the database used and the necessary features used to generate the parameters are presented. For comparison, this work is contrasted with a previous similar experiment on the same database using the ToBI intonation labelling system [2]. The Tilt method not only produces better results (RMSE 32.5 and correlation 0.60) but as it offers automatic labelling of data, it promises the ability to more easily train from general speech databases...",
      "abstract": "This paper presents a method for generating F 0 contours for a speech synthesis system using the Tilt intonation theory ([10], [9]). The Tilt theory offers an abstract description of natural F 0 contours which may be derived automatically from natural speech. Given a speech database labelled with Tilt events, this paper shows how that data may be used to train a model which can adequately predict Tilt parameters from features available in a text to speech system and hence produce natural sounding F 0 contours. After a short description of the Tilt theory, the database used and the necessary features used to generate the parameters are presented. For comparison, this work is contrasted with a previous similar experiment on the same database using the ToBI intonation labelling system [2]. The Tilt method not only produces better results (RMSE 32.5 and correlation 0.60) but as it offers automatic labelling of data, it promises the ability to more easily train from general speech databases...",
      "doi": "",
      "openalex_id": "https://openalex.org/W120890421",
      "arxiv_id": "",
      "publication_date": "1997-01-01",
      "published": "1997-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis",
      "summary": "Generating versatile and appropriate synthetic speech requires control over the output expression separate from the spoken text. Important non-textual speech variation is seldom annotated, in which case output control must be learned in an unsupervised fashion. In this paper, we perform an in-depth study of methods for unsupervised learning of control in statistical speech synthesis. For example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to VQ-VAEs, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argument. The implications of these new probabilistic interpretations are discussed. We illustrate the utility of the various approaches with an application to acoustic modelling for emotional speech synthesis, where the unsupervised methods for learning expression control (without access to emotional labels) are found to give results that in many aspects match or surpass the previous best supervised approach.",
      "abstract": "Generating versatile and appropriate synthetic speech requires control over the output expression separate from the spoken text. Important non-textual speech variation is seldom annotated, in which case output control must be learned in an unsupervised fashion. In this paper, we perform an in-depth study of methods for unsupervised learning of control in statistical speech synthesis. For example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to VQ-VAEs, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argument. The implications of these new probabilistic interpretations are discussed. We illustrate the utility of the various approaches with an application to acoustic modelling for emotional speech synthesis, where the unsupervised methods for learning expression control (without access to emotional labels) are found to give results that in many aspects match or surpass the previous best supervised approach.",
      "doi": "https://doi.org/10.48550/arxiv.1807.11470",
      "openalex_id": "https://openalex.org/W2884607399",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation",
      "summary": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.",
      "abstract": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.",
      "doi": "https://doi.org/10.48550/arxiv.1308.3432",
      "openalex_id": "https://openalex.org/W2242818861",
      "arxiv_id": "",
      "publication_date": "2013-08-15",
      "published": "2013-08-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "TTS synthesis with bidirectional LSTM based recurrent neural networks",
      "summary": "Feed-forward, Deep neural networks (DNN)-based text-tospeech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems [1, 4]. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed.",
      "abstract": "Feed-forward, Deep neural networks (DNN)-based text-tospeech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems [1, 4]. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed.",
      "doi": "https://doi.org/10.21437/interspeech.2014-443",
      "openalex_id": "https://openalex.org/W2294797155",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The USTC System for Blizzard Challenge 2009",
      "summary": "This paper introduces the speech synthesis system developed by USTC for Blizzard Challenge 2012. An audiobook speech corpus is adopted as the training data for system construction this year. Similar to our previous systems, the hidden Markov model (HMM) based unit selection and waveform concatenation approach is followed to develop our speech synthesis system using this corpus. Considering the inconsistent recording conditions and the narrator’s expressiveness within the corpus, we add some channel and expressiveness related labels to each sentence besides the conventional segmental and prosodic labels for system construction. The evaluation results of Blizzard Challenge 2012 show that our system performs well in all evaluation tests, which proves the effectiveness of the HMM-based unit selection approach in coping with a non-standard speech synthesis corpus. Index Terms: Speech synthesis, unit selection, hidden Markov model",
      "abstract": "This paper introduces the speech synthesis system developed by USTC for Blizzard Challenge 2012. An audiobook speech corpus is adopted as the training data for system construction this year. Similar to our previous systems, the hidden Markov model (HMM) based unit selection and waveform concatenation approach is followed to develop our speech synthesis system using this corpus. Considering the inconsistent recording conditions and the narrator’s expressiveness within the corpus, we add some channel and expressiveness related labels to each sentence besides the conventional segmental and prosodic labels for system construction. The evaluation results of Blizzard Challenge 2012 show that our system performs well in all evaluation tests, which proves the effectiveness of the HMM-based unit selection approach in coping with a non-standard speech synthesis corpus. Index Terms: Speech synthesis, unit selection, hidden Markov model",
      "doi": "https://doi.org/10.21437/blizzard.2009-16",
      "openalex_id": "https://openalex.org/W2916083071",
      "arxiv_id": "",
      "publication_date": "2009-09-04",
      "published": "2009-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence\\n Learning",
      "summary": "We present Deep Voice 3, a fully-convolutional attention-based neural\\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\\nspeech synthesis systems in naturalness while training ten times faster. We\\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\\nthan eight hundred hours of audio from over two thousand speakers. In addition,\\nwe identify common error modes of attention-based speech synthesis networks,\\ndemonstrate how to mitigate them, and compare several different waveform\\nsynthesis methods. We also describe how to scale inference to ten million\\nqueries per day on one single-GPU server.\\n",
      "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural\\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\\nspeech synthesis systems in naturalness while training ten times faster. We\\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\\nthan eight hundred hours of audio from over two thousand speakers. In addition,\\nwe identify common error modes of attention-based speech synthesis networks,\\ndemonstrate how to mitigate them, and compare several different waveform\\nsynthesis methods. We also describe how to scale inference to ten million\\nqueries per day on one single-GPU server.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1710.07654",
      "openalex_id": "https://openalex.org/W2963691546",
      "arxiv_id": "",
      "publication_date": "2017-10-20",
      "published": "2017-10-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The NII speech synthesis entry for Blizzard Challenge 2016",
      "summary": "This paper decribes the NII speech synthesis entry for Blizzard Challenge 2016, where the task was to build a voice from audiobook data. The synthesis system is built using the NII parametric speech synthesis framework that utilizes Long Short Term Memory (LSTM) Recurrent Neural Network (RNN) for acoustic modeling. For this entry, we first built a voice using a large data set, and then used the audiobook data to adapt the acoustic model to the target speaker. Additionally, the recent fullband glottal vocoder GlottDNN was used in the system with a DNN-based excitation model for generating glottal waveforms. The vocoder estimates the vocal tract in a band-wise manner using Quasi Closed Phase (QCP) inversefiltering at the low-band. At synthesis stage, the excitation model is used to generate voiced excitation from acoustic features, after which a vocal tract filter is applied to generate synthetic speech. The Blizzard Challenge listening test results show that the proposed system achieves comparable quality with the benchmark parametric synthesis systems. Index Terms: Blizzard Challenge, parametric speech synthesis, speaker adaptation, glottal vocoding, LSTM",
      "abstract": "This paper decribes the NII speech synthesis entry for Blizzard Challenge 2016, where the task was to build a voice from audiobook data. The synthesis system is built using the NII parametric speech synthesis framework that utilizes Long Short Term Memory (LSTM) Recurrent Neural Network (RNN) for acoustic modeling. For this entry, we first built a voice using a large data set, and then used the audiobook data to adapt the acoustic model to the target speaker. Additionally, the recent fullband glottal vocoder GlottDNN was used in the system with a DNN-based excitation model for generating glottal waveforms. The vocoder estimates the vocal tract in a band-wise manner using Quasi Closed Phase (QCP) inversefiltering at the low-band. At synthesis stage, the excitation model is used to generate voiced excitation from acoustic features, after which a vocal tract filter is applied to generate synthetic speech. The Blizzard Challenge listening test results show that the proposed system achieves comparable quality with the benchmark parametric synthesis systems. Index Terms: Blizzard Challenge, parametric speech synthesis, speaker adaptation, glottal vocoding, LSTM",
      "doi": "https://doi.org/10.21437/blizzard.2016-2",
      "openalex_id": "https://openalex.org/W2549013258",
      "arxiv_id": "",
      "publication_date": "2016-09-16",
      "published": "2016-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fundamental frequency modelling: an articulatory perspective with target approximation and deep learning",
      "summary": "Current statistical parametric speech synthesis (SPSS) approaches typically aim at state/frame-level acoustic modelling, which leads to a problem of frame-by-frame independence. Besides that, whichever learning technique is used, hidden Markov model (HMM), deep neural network (DNN) or recurrent neural network (RNN), the fundamental idea is to set up a direct mapping from linguistic to acoustic features. Although progress is frequently reported, this idea is questionable in terms of biological plausibility. This thesis aims at addressing the above issues by integrating dynamic mechanisms of human speech production as a core component of F0 generation and thus developing a more human-like F0 modelling paradigm. By introducing an articulatory F0 generation model – target approximation (TA) – between text and speech that controls syllable-synchronised F0 generation, contextual F0 variations are processed in two separate yet integrated stages: linguistic to motor, and motor to acoustic. With the goal of demonstrating that human speech movement can be considered as a dynamic process of target approximation and that the TA model is a valid F0 generation model to be used at the motor-to-acoustic stage, a TA-based pitch control experiment is conducted first to simulate the subtle human behaviour of online compensation for pitch-shifted auditory feedback. Then, the TA parameters are collectively controlled by linguistic features via a deep or recurrent neural network (DNN/RNN) at the linguistic-to-motor stage. We trained the systems on a Mandarin Chinese dataset consisting of both statements and questions. The TA-based systems generally outperformed the baseline systems in both objective and subjective evaluations. Furthermore, the amount of required linguistic features were reduced first to syllable level only (with DNN) and then with all positional information removed (with RNN). Fewer linguistic features as input with limited number of TA parameters as output led to less training data and lower model complexity, which in turn led to more efficient training and faster synthesis.",
      "abstract": "Current statistical parametric speech synthesis (SPSS) approaches typically aim at state/frame-level acoustic modelling, which leads to a problem of frame-by-frame independence. Besides that, whichever learning technique is used, hidden Markov model (HMM), deep neural network (DNN) or recurrent neural network (RNN), the fundamental idea is to set up a direct mapping from linguistic to acoustic features. Although progress is frequently reported, this idea is questionable in terms of biological plausibility. This thesis aims at addressing the above issues by integrating dynamic mechanisms of human speech production as a core component of F0 generation and thus developing a more human-like F0 modelling paradigm. By introducing an articulatory F0 generation model – target approximation (TA) – between text and speech that controls syllable-synchronised F0 generation, contextual F0 variations are processed in two separate yet integrated stages: linguistic to motor, and motor to acoustic. With the goal of demonstrating that human speech movement can be considered as a dynamic process of target approximation and that the TA model is a valid F0 generation model to be used at the motor-to-acoustic stage, a TA-based pitch control experiment is conducted first to simulate the subtle human behaviour of online compensation for pitch-shifted auditory feedback. Then, the TA parameters are collectively controlled by linguistic features via a deep or recurrent neural network (DNN/RNN) at the linguistic-to-motor stage. We trained the systems on a Mandarin Chinese dataset consisting of both statements and questions. The TA-based systems generally outperformed the baseline systems in both objective and subjective evaluations. Furthermore, the amount of required linguistic features were reduced first to syllable level only (with DNN) and then with all positional information removed (with RNN). Fewer linguistic features as input with limited number of TA parameters as output led to less training data and lower model complexity, which in turn led to more efficient training and faster synthesis.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2641478948",
      "arxiv_id": "",
      "publication_date": "2017-01-28",
      "published": "2017-01-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The USTC System for Blizzard Challenge 2008",
      "summary": "This paper introduces the speech synthesis system developed by USTC for Blizzard Challenge 2008. Two synthetic voices from the released UK English database are built using the HMMbased unit selection synthesis method, which is a hybrid of statistical parametric synthesis and unit-selection techniques. In this method, the optimal sequence of phone-sized candidate units is selected from the database following the statistical criterions derived from a set of trained HMMs for different acoustic features. Then the waveforms of selected units are concatenated to generate the synthesized speech. The evaluation results of Blizzard Challenge 2008 show that our system has good performance on similarity, naturalness and intelligibility for both English voices. Index Terms: speech synthesis, Blizzard Challenge, unit selection, hidden Markov model",
      "abstract": "This paper introduces the speech synthesis system developed by USTC for Blizzard Challenge 2008. Two synthetic voices from the released UK English database are built using the HMMbased unit selection synthesis method, which is a hybrid of statistical parametric synthesis and unit-selection techniques. In this method, the optimal sequence of phone-sized candidate units is selected from the database following the statistical criterions derived from a set of trained HMMs for different acoustic features. Then the waveforms of selected units are concatenated to generate the synthesized speech. The evaluation results of Blizzard Challenge 2008 show that our system has good performance on similarity, naturalness and intelligibility for both English voices. Index Terms: speech synthesis, Blizzard Challenge, unit selection, hidden Markov model",
      "doi": "https://doi.org/10.21437/blizzard.2008-18",
      "openalex_id": "https://openalex.org/W2394761397",
      "arxiv_id": "",
      "publication_date": "2008-09-21",
      "published": "2008-09-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks",
      "summary": "Deep Neural Networks (DNNs) have been shown to provide state-of-the-art performance over other baseline models in the task of predicting prosodic targets from text in a speechsynthesis system. However, prosody prediction can be affected by an interaction of short- and long-term contextual factors that a static model that depends on a fixed-size context window can fail to properly capture. In this work, we look at a recurrent formulation of neural networks (RNNs) that are deep in time and can store state information from an arbitrarily large input history when making a prediction. We show that RNNs provide improved performance over DNNs of comparable size in terms of various objective metrics for a variety of prosodic streams (notably, a relative reduction of about 6% in F0 mean-square error accompanied by a relative increase of about 14% in F0 variance), as well as in terms of perceptual quality assessed through mean-opinion-score listening tests. Index Terms: speech synthesis,text-to-speech, prosody prediction, recurrent neural networks, deep learning",
      "abstract": "Deep Neural Networks (DNNs) have been shown to provide state-of-the-art performance over other baseline models in the task of predicting prosodic targets from text in a speechsynthesis system. However, prosody prediction can be affected by an interaction of short- and long-term contextual factors that a static model that depends on a fixed-size context window can fail to properly capture. In this work, we look at a recurrent formulation of neural networks (RNNs) that are deep in time and can store state information from an arbitrarily large input history when making a prediction. We show that RNNs provide improved performance over DNNs of comparable size in terms of various objective metrics for a variety of prosodic streams (notably, a relative reduction of about 6% in F0 mean-square error accompanied by a relative increase of about 14% in F0 variance), as well as in terms of perceptual quality assessed through mean-opinion-score listening tests. Index Terms: speech synthesis,text-to-speech, prosody prediction, recurrent neural networks, deep learning",
      "doi": "https://doi.org/10.21437/interspeech.2014-445",
      "openalex_id": "https://openalex.org/W2394662942",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "XIMERA : A new TTS from ATR based on corpus-based technologies",
      "summary": "This paper describes a new concatenative TTS system under development at ATR. The system, named XIMERA, is based on corpus-based technologies, as was the case for the preceding TTS systems from ATR, namely ν-talk and CHATR. The prominent features of XIMERA are (1) large corpora (a 110hours corpus of a Japanese male, a 60-hours corpus of a Japanese female, and a 20-hours corpus of a Chinese female), (2) HMM-based generation of prosodic parameters, and (3) a cost function for segment selection optimized based on perceptual experiments. A perception test that evaluated the naturalness of synthetic speech for XIMERA and 10 TTS products, including CHATR, showed that XIMERA outperformed the other ten. 1.",
      "abstract": "This paper describes a new concatenative TTS system under development at ATR. The system, named XIMERA, is based on corpus-based technologies, as was the case for the preceding TTS systems from ATR, namely ν-talk and CHATR. The prominent features of XIMERA are (1) large corpora (a 110hours corpus of a Japanese male, a 60-hours corpus of a Japanese female, and a 20-hours corpus of a Chinese female), (2) HMM-based generation of prosodic parameters, and (3) a cost function for segment selection optimized based on perceptual experiments. A perception test that evaluated the naturalness of synthetic speech for XIMERA and 10 TTS products, including CHATR, showed that XIMERA outperformed the other ten. 1.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2394921947",
      "arxiv_id": "",
      "publication_date": "2004-01-01",
      "published": "2004-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
      "summary": "The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model. It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task. However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization. This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions. Experiments on TIMIT and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.",
      "abstract": "The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model. It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task. However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization. This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions. Experiments on TIMIT and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.",
      "doi": "https://doi.org/10.21437/interspeech.2018-2148",
      "openalex_id": "https://openalex.org/W2888911345",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Random sampling with a reservoir",
      "summary": "We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.",
      "abstract": "We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.",
      "doi": "https://doi.org/10.1145/3147.3165",
      "openalex_id": "https://openalex.org/W2119885577",
      "arxiv_id": "",
      "publication_date": "1985-03-01",
      "published": "1985-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The language-independent bottleneck features",
      "summary": "In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.",
      "abstract": "In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.",
      "doi": "https://doi.org/10.1109/slt.2012.6424246",
      "openalex_id": "https://openalex.org/W1970890968",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Acceleration of Stochastic Approximation by Averaging",
      "summary": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
      "abstract": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
      "doi": "https://doi.org/10.1137/0330046",
      "openalex_id": "https://openalex.org/W2086161653",
      "arxiv_id": "",
      "publication_date": "1992-07-01",
      "published": "1992-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Theory and Experiments on Vector Quantized Autoencoders",
      "summary": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",
      "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",
      "doi": "https://doi.org/10.48550/arxiv.1805.11063",
      "openalex_id": "https://openalex.org/W2804145368",
      "arxiv_id": "",
      "publication_date": "2018-05-28",
      "published": "2018-05-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Variational Information Bottleneck on Vector Quantized Autoencoders",
      "summary": "In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.",
      "abstract": "In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.",
      "doi": "https://doi.org/10.48550/arxiv.1808.01048",
      "openalex_id": "https://openalex.org/W2887927938",
      "arxiv_id": "",
      "publication_date": "2018-08-02",
      "published": "2018-08-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Composing graphical models with neural networks for structured representations and fast inference",
      "summary": "We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.",
      "abstract": "We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2962695963",
      "arxiv_id": "",
      "publication_date": "2016-03-20",
      "published": "2016-03-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The information bottleneck method",
      "summary": "We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. The approach yields an exact set of self-consistent equations for the coding rules X → ˜X and ˜X → Y. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1",
      "abstract": "We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. The approach yields an exact set of self-consistent equations for the coding rules X → ˜X and ˜X → Y. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2979454998",
      "arxiv_id": "",
      "publication_date": "2000-04-24",
      "published": "2000-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
      "summary": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
      "abstract": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
      "doi": "https://doi.org/10.48550/arxiv.1703.07370",
      "openalex_id": "https://openalex.org/W2602076750",
      "arxiv_id": "",
      "publication_date": "2017-03-21",
      "published": "2017-03-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The “ScribbleLens” Dutch Historical Handwriting Corpus",
      "summary": "Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.",
      "abstract": "Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.",
      "doi": "https://doi.org/10.1109/icfhr2020.2020.00023",
      "openalex_id": "https://openalex.org/W3049315473",
      "arxiv_id": "",
      "publication_date": "2020-09-01",
      "published": "2020-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Estimating or Propagating Gradients Through Stochastic Neurons",
      "summary": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",
      "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",
      "doi": "https://doi.org/10.48550/arxiv.1305.2982",
      "openalex_id": "https://openalex.org/W1583776211",
      "arxiv_id": "",
      "publication_date": "2013-05-14",
      "published": "2013-05-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "summary": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
      "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
      "doi": "https://doi.org/10.21437/interspeech.2014-80",
      "openalex_id": "https://openalex.org/W2293634267",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "summary": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
      "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
      "doi": "https://doi.org/10.48550/arxiv.1512.02595",
      "openalex_id": "https://openalex.org/W2193413348",
      "arxiv_id": "",
      "publication_date": "2015-12-08",
      "published": "2015-12-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis",
      "summary": "Global Style Tokens (GSTs) are a recently-proposed method to learn latent disentangled representations of high-dimensional data. GSTs can be used within Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to uncover expressive factors of variation in speaking style. In this work, we introduce the Text-Predicting Global Style Token (TP-GST) architecture, which treats GST combination weights or style embeddings as \"virtual\" speaking style labels within Tacotron. TP-GST learns to predict stylistic renderings from text alone, requiring neither explicit labels during training, nor auxiliary inputs for inference. We show that, when trained on an expressive speech dataset, our system can render text with more pitch and energy variation than two state-of-the-art baseline models. We further demonstrate that TP-GSTs can synthesize speech with background noise removed, and corroborate these analyses with positive results on human-rated listener preference audiobook tasks. Finally, we demonstrate that multi-speaker TP-GST models successfully factorize speaker identity and speaking style. We provide a website with audio samples <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> for each of our findings.",
      "abstract": "Global Style Tokens (GSTs) are a recently-proposed method to learn latent disentangled representations of high-dimensional data. GSTs can be used within Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to uncover expressive factors of variation in speaking style. In this work, we introduce the Text-Predicting Global Style Token (TP-GST) architecture, which treats GST combination weights or style embeddings as \"virtual\" speaking style labels within Tacotron. TP-GST learns to predict stylistic renderings from text alone, requiring neither explicit labels during training, nor auxiliary inputs for inference. We show that, when trained on an expressive speech dataset, our system can render text with more pitch and energy variation than two state-of-the-art baseline models. We further demonstrate that TP-GSTs can synthesize speech with background noise removed, and corroborate these analyses with positive results on human-rated listener preference audiobook tasks. Finally, we demonstrate that multi-speaker TP-GST models successfully factorize speaker identity and speaking style. We provide a website with audio samples <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> for each of our findings.",
      "doi": "https://doi.org/10.1109/slt.2018.8639682",
      "openalex_id": "https://openalex.org/W2885800352",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
      "summary": "Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech recognition and text-to-speech systems.Recently, G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoderdecoder framework.However, previous works do not consider the practical issues when deploying G2P model in the production system, such as how to leverage additional unlabeled data to boost the accuracy, as well as reduce model size for online deployment.In this work, we propose token-level ensemble distillation for G2P conversion, which can (1) boost the accuracy by distilling the knowledge from additional unlabeled data, and (2) reduce the model size but maintain the high accuracy, both of which are very practical and helpful in the online production system.We use token-level knowledge distillation, which results in better accuracy than the sequence-level counterpart.What is more, we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion.Experiments on the publicly available CMU-Dict dataset and an internal English dataset demonstrate the effectiveness of our proposed method.Particularly, our method achieves 19.88% WER on CMUDict dataset, outperforming the previous works by more than 4.22% WER, and setting the new state-of-the-art results.",
      "abstract": "Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech recognition and text-to-speech systems.Recently, G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoderdecoder framework.However, previous works do not consider the practical issues when deploying G2P model in the production system, such as how to leverage additional unlabeled data to boost the accuracy, as well as reduce model size for online deployment.In this work, we propose token-level ensemble distillation for G2P conversion, which can (1) boost the accuracy by distilling the knowledge from additional unlabeled data, and (2) reduce the model size but maintain the high accuracy, both of which are very practical and helpful in the online production system.We use token-level knowledge distillation, which results in better accuracy than the sequence-level counterpart.What is more, we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion.Experiments on the publicly available CMU-Dict dataset and an internal English dataset demonstrate the effectiveness of our proposed method.Particularly, our method achieves 19.88% WER on CMUDict dataset, outperforming the previous works by more than 4.22% WER, and setting the new state-of-the-art results.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1208",
      "openalex_id": "https://openalex.org/W2972677740",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Voice: Real-time Neural Text-to-Speech",
      "summary": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
      "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
      "doi": "https://doi.org/10.48550/arxiv.1702.07825",
      "openalex_id": "https://openalex.org/W2591927543",
      "arxiv_id": "",
      "publication_date": "2017-02-25",
      "published": "2017-02-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
      "summary": "In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",
      "abstract": "In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",
      "doi": "https://doi.org/10.48550/arxiv.1803.09017",
      "openalex_id": "https://openalex.org/W2794490148",
      "arxiv_id": "",
      "publication_date": "2018-03-23",
      "published": "2018-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FastSpeech: Fast, Robust and Controllable Text to Speech",
      "summary": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.",
      "abstract": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.",
      "doi": "https://doi.org/10.48550/arxiv.1905.09263",
      "openalex_id": "https://openalex.org/W2946200149",
      "arxiv_id": "",
      "publication_date": "2019-05-22",
      "published": "2019-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mixture Density Network for Phone-Level Prosody Modelling in Speech Synthesis.",
      "summary": "Recent researches on both utterance-level and phone-level prosody modelling successfully improve the voice quality and naturalness in text-to-speech synthesis. However, most of them model the prosody with a unimodal distribution such like a single Gaussian, which is not reasonable enough. In this work, we focus on phone-level prosody modelling where we introduce a Gaussian mixture model(GMM) based mixture density network. Our experiments on the LJSpeech dataset demonstrate that GMM can better model the phone-level prosody than a single Gaussian. The subjective evaluations suggest that our method not only significantly improves the prosody diversity in synthetic speech without the need of manual control, but also achieves a better naturalness. We also find that using the additional mixture density network has only very limited influence on inference speed.",
      "abstract": "Recent researches on both utterance-level and phone-level prosody modelling successfully improve the voice quality and naturalness in text-to-speech synthesis. However, most of them model the prosody with a unimodal distribution such like a single Gaussian, which is not reasonable enough. In this work, we focus on phone-level prosody modelling where we introduce a Gaussian mixture model(GMM) based mixture density network. Our experiments on the LJSpeech dataset demonstrate that GMM can better model the phone-level prosody than a single Gaussian. The subjective evaluations suggest that our method not only significantly improves the prosody diversity in synthetic speech without the need of manual control, but also achieves a better naturalness. We also find that using the additional mixture density network has only very limited influence on inference speed.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3134835907",
      "arxiv_id": "",
      "publication_date": "2021-02-01",
      "published": "2021-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neuromodulatory Control Networks (NCNs): A Biologically Inspired Architecture for Dynamic LLM Processing",
      "summary": "Large Language Models (LLMs) based on the Transformer architecture have achieved remarkable success, yet their core processing mechanisms remain largely static after training. While powerful, this static nature limits their ability to dynamically adapt their processing strategy based on nuanced contextual cues, task demands, or desired operational modes (e.g., shifting between exploration and exploitation). We propose Neuromodulatory Control Networks (NCNs), a novel architectural modification inspired by the neuromodulatory systems in the vertebrate brain (e.g., those utilizing dopamine, acetylcholine, norepinephrine). NCNs are small, parallel networks that receive contextual input, summarizing the global state, task information, or external control signals, and compute dynamic \"modulatory signals\". These signals are distributed as layer-specific control vectors to the main LLM to influence its computational properties during a forward pass, analogous to how neuromodulators alter neuronal gain, plasticity, and network states across different cortical depths. Instead of merely routing information, NCNs aim to change how information is processed throughout the base model by modulating key components like attention mechanisms (e.g., via precision scaling), layer gains, and activation functions. Crucially, the architecture allows the model to implicitly learn to self-regulate these parameters via backpropagation, effectively becoming its own \"tuning expert.\" We further introduce formal stability mechanisms, including homeostatic regularization, to prevent control manifold collapse. This paper introduces the NCN architecture, details its components and implicit learning mechanism, discusses its conceptual advantages and potential failure modes (such as contextual stereotyping), and provides an open-source PyTorch implementation to facilitate community exploration and future empirical validation.",
      "abstract": "Large Language Models (LLMs) based on the Transformer architecture have achieved remarkable success, yet their core processing mechanisms remain largely static after training. While powerful, this static nature limits their ability to dynamically adapt their processing strategy based on nuanced contextual cues, task demands, or desired operational modes (e.g., shifting between exploration and exploitation). We propose Neuromodulatory Control Networks (NCNs), a novel architectural modification inspired by the neuromodulatory systems in the vertebrate brain (e.g., those utilizing dopamine, acetylcholine, norepinephrine). NCNs are small, parallel networks that receive contextual input, summarizing the global state, task information, or external control signals, and compute dynamic \"modulatory signals\". These signals are distributed as layer-specific control vectors to the main LLM to influence its computational properties during a forward pass, analogous to how neuromodulators alter neuronal gain, plasticity, and network states across different cortical depths. Instead of merely routing information, NCNs aim to change how information is processed throughout the base model by modulating key components like attention mechanisms (e.g., via precision scaling), layer gains, and activation functions. Crucially, the architecture allows the model to implicitly learn to self-regulate these parameters via backpropagation, effectively becoming its own \"tuning expert.\" We further introduce formal stability mechanisms, including homeostatic regularization, to prevent control manifold collapse. This paper introduces the NCN architecture, details its components and implicit learning mechanism, discusses its conceptual advantages and potential failure modes (such as contextual stereotyping), and provides an open-source PyTorch implementation to facilitate community exploration and future empirical validation.",
      "doi": "https://doi.org/10.5281/zenodo.17692856",
      "openalex_id": "https://openalex.org/W4394666973",
      "arxiv_id": "",
      "publication_date": "2025-04-25",
      "published": "2025-04-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
      "summary": "Speech coding facilitates the transmission of speech over lowbandwidth networks with minimal distortion.Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches.While this new generation of codecs is capable of synthesizing highfidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently.We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias.As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder.Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of 600 bps that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate.Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.",
      "abstract": "Speech coding facilitates the transmission of speech over lowbandwidth networks with minimal distortion.Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches.While this new generation of codecs is capable of synthesizing highfidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently.We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias.As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder.Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of 600 bps that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate.Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10988",
      "openalex_id": "https://openalex.org/W4284888249",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech coding based on a multi-layer neural network",
      "summary": "The authors present a speech-compression scheme based on a three-layer perceptron in which the number of units in the hidden layer is reduced. Input and output layers have the same number of units in order to achieve identity mapping. Speech coding is realized by scalar or vector quantization of hidden-layer outputs. By analyzing the weighting coefficients, it can be shown that speech coding based on a three-layer neural network is speaker-independent. Transform coding is automatically based on back propagation. The relation between compression ratio and SNR (signal-to-noise ratio) is investigated. The bit allocation and optimum number of hidden-layer units necessary to realize a specific bit rate are given. According to the analysis of weighting coefficients, speech coding based on a neural network is transform coding similar to Karhunen-Loeve transformation. The characteristics of a five-layer neural network are examined. It is shown that since the five-layer neural network can realize nonlinear mapping, it is more effective than the three-layer network.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The authors present a speech-compression scheme based on a three-layer perceptron in which the number of units in the hidden layer is reduced. Input and output layers have the same number of units in order to achieve identity mapping. Speech coding is realized by scalar or vector quantization of hidden-layer outputs. By analyzing the weighting coefficients, it can be shown that speech coding based on a three-layer neural network is speaker-independent. Transform coding is automatically based on back propagation. The relation between compression ratio and SNR (signal-to-noise ratio) is investigated. The bit allocation and optimum number of hidden-layer units necessary to realize a specific bit rate are given. According to the analysis of weighting coefficients, speech coding based on a neural network is transform coding similar to Karhunen-Loeve transformation. The characteristics of a five-layer neural network are examined. It is shown that since the five-layer neural network can realize nonlinear mapping, it is more effective than the three-layer network.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icc.1990.117117",
      "openalex_id": "https://openalex.org/W2129913307",
      "arxiv_id": "",
      "publication_date": "1990-01-01",
      "published": "1990-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble",
      "summary": "Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.",
      "abstract": "Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.",
      "doi": "https://doi.org/10.18653/v1/2022.findings-acl.166",
      "openalex_id": "https://openalex.org/W4285181910",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling",
      "summary": "Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-sequence models. Lingvo models are composed of modular building blocks that are flexible and easily extensible, and experiment configurations are centralized and highly customizable. Distributed training and quantized inference are supported directly within the framework, and it contains existing implementations of a large number of utilities, helper functions, and the newest research ideas. Lingvo has been used in collaboration by dozens of researchers in more than 20 papers over the last two years. This document outlines the underlying design of Lingvo and serves as an introduction to the various pieces of the framework, while also offering examples of advanced features that showcase the capabilities of the framework.",
      "abstract": "Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-sequence models. Lingvo models are composed of modular building blocks that are flexible and easily extensible, and experiment configurations are centralized and highly customizable. Distributed training and quantized inference are supported directly within the framework, and it contains existing implementations of a large number of utilities, helper functions, and the newest research ideas. Lingvo has been used in collaboration by dozens of researchers in more than 20 papers over the last two years. This document outlines the underlying design of Lingvo and serves as an introduction to the various pieces of the framework, while also offering examples of advanced features that showcase the capabilities of the framework.",
      "doi": "https://doi.org/10.48550/arxiv.1902.08295",
      "openalex_id": "https://openalex.org/W2928941594",
      "arxiv_id": "",
      "publication_date": "2019-02-21",
      "published": "2019-02-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition",
      "summary": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",
      "abstract": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",
      "doi": "https://doi.org/10.48550/arxiv.2202.01855",
      "openalex_id": "https://openalex.org/W4221161761",
      "arxiv_id": "",
      "publication_date": "2022-02-03",
      "published": "2022-02-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Back-Translation-Style Data Augmentation for end-to-end ASR",
      "summary": "In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is retrained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.",
      "abstract": "In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is retrained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.",
      "doi": "https://doi.org/10.1109/slt.2018.8639619",
      "openalex_id": "https://openalex.org/W2883586237",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks",
      "summary": "Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence. Its success heavily relies on the availability of large amounts of training data. This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition (ASR) and speech translation (ST). In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively. We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks. Our experiments show that the proposed method achieves a relative 10~15% word error rate reduction on the English LIBRISPEECH task compared with our baseline, and improves the speech translation quality on the MUST-C tasks by 3.6~9.2 BLEU.",
      "abstract": "Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence. Its success heavily relies on the availability of large amounts of training data. This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition (ASR) and speech translation (ST). In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively. We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks. Our experiments show that the proposed method achieves a relative 10~15% word error rate reduction on the English LIBRISPEECH task compared with our baseline, and improves the speech translation quality on the MUST-C tasks by 3.6~9.2 BLEU.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9415058",
      "openalex_id": "https://openalex.org/W3162037819",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Speech Recognition Using Consistent Predictions on Synthesized Speech",
      "summary": "Speech synthesis has advanced to the point of being close to indistinguishable from human speech. However, efforts to train speech recognition systems on synthesized utterances have not been able to show that synthesized data can be effectively used to augment or replace human speech. In this work, we demonstrate that promoting consistent predictions in response to real and synthesized speech enables significantly improved speech recognition performance. We also find that training on 460 hours of LibriSpeech augmented with 500 hours of transcripts (without audio) performance is within 0.2% WER of a system trained on 960 hours of transcribed audio. This suggests that with this approach, when there is sufficient text available, reliance on transcribed audio can be cut nearly in half.",
      "abstract": "Speech synthesis has advanced to the point of being close to indistinguishable from human speech. However, efforts to train speech recognition systems on synthesized utterances have not been able to show that synthesized data can be effectively used to augment or replace human speech. In this work, we demonstrate that promoting consistent predictions in response to real and synthesized speech enables significantly improved speech recognition performance. We also find that training on 460 hours of LibriSpeech augmented with 500 hours of transcripts (without audio) performance is within 0.2% WER of a system trained on 960 hours of transcribed audio. This suggests that with this approach, when there is sufficient text available, reliance on transcribed audio can be cut nearly in half.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053831",
      "openalex_id": "https://openalex.org/W3015280134",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parallel Tacotron: Non-Autoregressive and Controllable TTS",
      "summary": "Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called Parallel Tacotron, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.",
      "abstract": "Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called Parallel Tacotron, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414718",
      "openalex_id": "https://openalex.org/W3161296985",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attention-based Wav2Text with feature transfer learning",
      "summary": "Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as Attention-based Wav2Text. To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.",
      "abstract": "Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as Attention-based Wav2Text. To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.",
      "doi": "https://doi.org/10.1109/asru.2017.8268951",
      "openalex_id": "https://openalex.org/W2962759037",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
      "summary": "Integrating an external language model into a sequence-tosequence speech recognition system is non-trivial.Previous works utilize linear interpolation or a fusion network to integrate external language models.However, these approaches introduce external components, and increase decoding computation.In this paper, we instead propose a knowledge distillation based training approach to integrating external language models into a sequence-to-sequence model.A recurrent neural network language model, which is trained on large scale external text, generates soft labels to guide the sequence-to-sequence model training.Thus, the language model plays the role of the teacher.This approach does not add any external component to the sequence-to-sequence model during testing.And this approach is flexible to be combined with shallow fusion technique together for decoding.The experiments are conducted on public Chinese datasets AISHELL-1 and CLMAD.Our approach achieves a character error rate of 9.3%, which is relatively reduced by 18.42% compared with the vanilla sequenceto-sequence model.",
      "abstract": "Integrating an external language model into a sequence-tosequence speech recognition system is non-trivial.Previous works utilize linear interpolation or a fusion network to integrate external language models.However, these approaches introduce external components, and increase decoding computation.In this paper, we instead propose a knowledge distillation based training approach to integrating external language models into a sequence-to-sequence model.A recurrent neural network language model, which is trained on large scale external text, generates soft labels to guide the sequence-to-sequence model training.Thus, the language model plays the role of the teacher.This approach does not add any external component to the sequence-to-sequence model during testing.And this approach is flexible to be combined with shallow fusion technique together for decoding.The experiments are conducted on public Chinese datasets AISHELL-1 and CLMAD.Our approach achieves a character error rate of 9.3%, which is relatively reduced by 18.42% compared with the vanilla sequenceto-sequence model.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1554",
      "openalex_id": "https://openalex.org/W2972991710",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
      "summary": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
      "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
      "doi": "https://doi.org/10.21437/interspeech.2017-343",
      "openalex_id": "https://openalex.org/W2577366047",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using Synthetic Audio to Improve the Recognition of Out-of-Vocabulary Words in End-to-End Asr Systems",
      "summary": "Today, many state-of-the-art automatic speech recognition (ASR) systems apply all-neural models that map audio to word sequences trained end-to-end along one global optimisation criterion in a fully data driven fashion. These models allow high precision ASR for domains and words represented in the training material but have difficulties recognising words that are rarely or not at all represented during training, i.e. trending words and new named entities. In this paper, we use a text-to-speech (TTS) engine to provide synthetic audio for out-of-vocabulary (OOV) words. We aim to boost the recognition accuracy of a recurrent neural network transducer (RNN-T) on OOV words by using the extra audio-text pairs, while maintaining the performance on the non-OOV words. Different regularisation techniques are explored and the best performance is achieved by fine-tuning the RNN-T on both original training data and extra synthetic data with elastic weight consolidation (EWC) applied on the encoder. This yields a 57% relative word error rate (WER) reduction on utterances containing OOV words without any degradation on the whole test set.",
      "abstract": "Today, many state-of-the-art automatic speech recognition (ASR) systems apply all-neural models that map audio to word sequences trained end-to-end along one global optimisation criterion in a fully data driven fashion. These models allow high precision ASR for domains and words represented in the training material but have difficulties recognising words that are rarely or not at all represented during training, i.e. trending words and new named entities. In this paper, we use a text-to-speech (TTS) engine to provide synthetic audio for out-of-vocabulary (OOV) words. We aim to boost the recognition accuracy of a recurrent neural network transducer (RNN-T) on OOV words by using the extra audio-text pairs, while maintaining the performance on the non-OOV words. Different regularisation techniques are explored and the best performance is achieved by fine-tuning the RNN-T on both original training data and extra synthetic data with elastic weight consolidation (EWC) applied on the encoder. This yields a 57% relative word error rate (WER) reduction on utterances containing OOV words without any degradation on the whole test set.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414778",
      "openalex_id": "https://openalex.org/W3162244132",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-speaker Sequence-to-sequence Speech Synthesis for Data Augmentation in Acoustic-to-word Speech Recognition",
      "summary": "The acoustic-to-word (A2W) automatic speech recognition (ASR) realizes very fast decoding with a simple architecture and achieves state-of-the-art performance. However, the A2W model suffers from the out-of-vocabulary (OOV) word problem and cannot use text-only data to improve the language modeling capability. Meanwhile, sequence-to-sequence neural speech synthesis has also been developed and achieved naturalness comparable to human speech. We investigate leveraging sequence-to-sequence neural speech synthesis to augment training data for the ASR system in a target domain. While speech synthesis model is usually trained with single speaker data, ASR needs to cover a variety of speakers. In this work, we extend the speech synthesizer so that it can output speech of many speakers. The multi-speaker speech synthesizer is trained with a large corpus in the source domain, then used to generate acoustic features from texts of the target domain. These synthesized speech features are combined with real speech features of the source domain to train an attention-based A2W model. Experimental results show that the A2W model trained with the multi-speaker model achieved a significant improvement over the baseline and the single speaker model.",
      "abstract": "The acoustic-to-word (A2W) automatic speech recognition (ASR) realizes very fast decoding with a simple architecture and achieves state-of-the-art performance. However, the A2W model suffers from the out-of-vocabulary (OOV) word problem and cannot use text-only data to improve the language modeling capability. Meanwhile, sequence-to-sequence neural speech synthesis has also been developed and achieved naturalness comparable to human speech. We investigate leveraging sequence-to-sequence neural speech synthesis to augment training data for the ASR system in a target domain. While speech synthesis model is usually trained with single speaker data, ASR needs to cover a variety of speakers. In this work, we extend the speech synthesizer so that it can output speech of many speakers. The multi-speaker speech synthesizer is trained with a large corpus in the source domain, then used to generate acoustic features from texts of the target domain. These synthesized speech features are combined with real speech features of the source domain to train an attention-based A2W model. Experimental results show that the A2W model trained with the multi-speaker model achieved a significant improvement over the baseline and the single speaker model.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682816",
      "openalex_id": "https://openalex.org/W2940200615",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Leveraging Sequence-to-Sequence Speech Synthesis for Enhancing Acoustic-to-Word Speech Recognition",
      "summary": "Encoder-decoder models for acoustic-to-word (A2W) automatic speech recognition (ASR) are attractive for their simplicity of architecture and run-time latency while achieving state-of-the-art performances. However, word-based models commonly suffer from the-of-vocabulary (OOV) word problem. They also cannot leverage text data to improve their language modeling capability. Recently, sequence-to-sequence neural speech synthesis models trainable from corpora have been developed and shown to achieve naturalness com- parable to recorded human speech. In this paper, we explore how we can leverage the current speech synthesis technology to tailor the ASR system for a target domain by preparing only a relevant text corpus. From a set of target domain texts, we generate speech features using a sequence-to-sequence speech synthesizer. These artificial speech features together with real speech features from conventional speech corpora are used to train an attention-based A2W model. Experimental results show that the proposed approach improves the word accuracy significantly compared to the baseline trained only with the real speech, although synthetic part of the training data comes only from a single female speaker voice.",
      "abstract": "Encoder-decoder models for acoustic-to-word (A2W) automatic speech recognition (ASR) are attractive for their simplicity of architecture and run-time latency while achieving state-of-the-art performances. However, word-based models commonly suffer from the-of-vocabulary (OOV) word problem. They also cannot leverage text data to improve their language modeling capability. Recently, sequence-to-sequence neural speech synthesis models trainable from corpora have been developed and shown to achieve naturalness com- parable to recorded human speech. In this paper, we explore how we can leverage the current speech synthesis technology to tailor the ASR system for a target domain by preparing only a relevant text corpus. From a set of target domain texts, we generate speech features using a sequence-to-sequence speech synthesizer. These artificial speech features together with real speech features from conventional speech corpora are used to train an attention-based A2W model. Experimental results show that the proposed approach improves the word accuracy significantly compared to the baseline trained only with the real speech, although synthetic part of the training data comes only from a single female speaker voice.",
      "doi": "https://doi.org/10.1109/slt.2018.8639589",
      "openalex_id": "https://openalex.org/W2912937645",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Neural Transducers for End-to-End Speech Recognition",
      "summary": "In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNNTransducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.",
      "abstract": "In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNNTransducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.",
      "doi": "https://doi.org/10.48550/arxiv.1707.07413",
      "openalex_id": "https://openalex.org/W2739427748",
      "arxiv_id": "",
      "publication_date": "2017-07-24",
      "published": "2017-07-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Using Monolingual Corpora in Neural Machine Translation",
      "summary": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.",
      "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.",
      "doi": "https://doi.org/10.48550/arxiv.1503.03535",
      "openalex_id": "https://openalex.org/W1915251500",
      "arxiv_id": "",
      "publication_date": "2015-03-11",
      "published": "2015-03-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks",
      "summary": "In this paper, we present improvements made to the TED-LIUM corpus we released in 2012. These enhancements fall into two categories. First, we describe how we filtered publicly available monolingual data and used it to estimate well-suited language models (LMs), using open-source tools. Then, we describe the process of selection we applied to new acoustic data from TED talks, providing additions to our previously released corpus. Finally, we report some experiments we made around these improvements. Keywords:Corpus, Speech Recognition, Language Modeling 1.",
      "abstract": "In this paper, we present improvements made to the TED-LIUM corpus we released in 2012. These enhancements fall into two categories. First, we describe how we filtered publicly available monolingual data and used it to estimate well-suited language models (LMs), using open-source tools. Then, we describe the process of selection we applied to new acoustic data from TED talks, providing additions to our previously released corpus. Finally, we report some experiments we made around these improvements. Keywords:Corpus, Speech Recognition, Language Modeling 1.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2251321385",
      "arxiv_id": "",
      "publication_date": "2015-11-03",
      "published": "2015-11-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks",
      "summary": "Real-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion.This paper introduces HiFi-GAN, a deep learning method to transform recorded speech to sound as though it had been recorded in a studio.We use an end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain.It relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech.The proposed model generalizes well to new speakers, new speech content, and new environments.It significantly outperforms state-of-the-art baseline methods in both objective and subjective experiments.",
      "abstract": "Real-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion.This paper introduces HiFi-GAN, a deep learning method to transform recorded speech to sound as though it had been recorded in a studio.We use an end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain.It relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech.The proposed model generalizes well to new speakers, new speech content, and new environments.It significantly outperforms state-of-the-art baseline methods in both objective and subjective experiments.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2143",
      "openalex_id": "https://openalex.org/W3096159803",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pruned RNN-T for fast, memory-eﬀicient ASR training",
      "summary": "The RNN-Transducer (RNN-T) framework for speech recognition has been growing in popularity, particularly for deployed real-time ASR systems, because it combines high accuracy with naturally streaming recognition.One of the drawbacks of RNN-T is that its loss function is relatively slow to compute, and can use a lot of memory.Excessive GPU memory usage can make it impractical to use RNN-T loss in cases where the vocabulary size is large: for example, for Chinese character-based ASR.We introduce a method for faster and more memoryefficient RNN-T loss computation.We first obtain pruning bounds for the RNN-T recursion using a simple joiner network that is linear in the encoder and decoder embeddings; we can evaluate this without using much memory.We then use those pruning bounds to evaluate the full, non-linear joiner network.The code is open-sourced and publicly available.",
      "abstract": "The RNN-Transducer (RNN-T) framework for speech recognition has been growing in popularity, particularly for deployed real-time ASR systems, because it combines high accuracy with naturally streaming recognition.One of the drawbacks of RNN-T is that its loss function is relatively slow to compute, and can use a lot of memory.Excessive GPU memory usage can make it impractical to use RNN-T loss in cases where the vocabulary size is large: for example, for Chinese character-based ASR.We introduce a method for faster and more memoryefficient RNN-T loss computation.We first obtain pruning bounds for the RNN-T recursion using a simple joiner network that is linear in the encoder and decoder embeddings; we can evaluate this without using much memory.We then use those pruning bounds to evaluate the full, non-linear joiner network.The code is open-sourced and publicly available.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10340",
      "openalex_id": "https://openalex.org/W4283700324",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Rnn-Transducer with Stateless Prediction Network",
      "summary": "The RNN-Transducer (RNNT) outperforms classic Automatic Speech Recognition (ASR) systems when a large amount of supervised training data is available. For low-resource languages, the RNNT models overfit, and can not directly take advantage of additional large text corpora as in classic ASR systems.We focus on the prediction network of the RNNT, since it is believed to be analogous to the Language Model (LM) in the classic ASR systems. We pre-train the prediction network with text-only data, which is not helpful. Moreover, removing the recurrent layers from the prediction network, which makes the prediction network stateless, performs virtually as well as the original RNNT model, when using wordpieces. The stateless prediction network does not depend on the previous output symbols, except the last one. Therefore it simplifies the RNNT architectures and the inference.Our results suggest that the RNNT prediction network does not function as the LM in classical ASR. Instead, it merely helps the model align to the input audio, while the RNNT encoder and joint networks capture both the acoustic and the linguistic information.",
      "abstract": "The RNN-Transducer (RNNT) outperforms classic Automatic Speech Recognition (ASR) systems when a large amount of supervised training data is available. For low-resource languages, the RNNT models overfit, and can not directly take advantage of additional large text corpora as in classic ASR systems.We focus on the prediction network of the RNNT, since it is believed to be analogous to the Language Model (LM) in the classic ASR systems. We pre-train the prediction network with text-only data, which is not helpful. Moreover, removing the recurrent layers from the prediction network, which makes the prediction network stateless, performs virtually as well as the original RNNT model, when using wordpieces. The stateless prediction network does not depend on the previous output symbols, except the last one. Therefore it simplifies the RNNT architectures and the inference.Our results suggest that the RNNT prediction network does not function as the LM in classical ASR. Instead, it merely helps the model align to the input audio, while the RNNT encoder and joint networks capture both the acoustic and the linguistic information.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054419",
      "openalex_id": "https://openalex.org/W3015686596",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LongFNT: Long-Form Speech Recognition with Factorized Neural Transducer",
      "summary": "Traditional automatic speech recognition (ASR) systems usually focus on individual utterances, without considering long-form speech with useful historical information, which is more practical in real scenarios. Simply attending longer transcription history for a vanilla neural transducer model shows no much gain in our preliminary experiments, since the prediction network is not a pure language model. This motivates us to leverage the factorized neural transducer structure, containing a real language model, the vocabulary predictor. We propose the LongFNT-Text architecture, which fuses the sentence-level long-form features directly with the output of the vocabulary predictor and then embeds token-level long-form features inside the vocabulary predictor, with a pre-trained contextual encoder RoBERTa to further boost the performance. Moreover, we propose the LongFNT architecture by extending the long-form speech to the original speech input and achieve the best performance. The effectiveness of our LongFNT approach is validated on LibriSpeech and GigaSpeech corpora with 19% and 12% relative word error rate (WER) reduction, respectively.",
      "abstract": "Traditional automatic speech recognition (ASR) systems usually focus on individual utterances, without considering long-form speech with useful historical information, which is more practical in real scenarios. Simply attending longer transcription history for a vanilla neural transducer model shows no much gain in our preliminary experiments, since the prediction network is not a pure language model. This motivates us to leverage the factorized neural transducer structure, containing a real language model, the vocabulary predictor. We propose the LongFNT-Text architecture, which fuses the sentence-level long-form features directly with the output of the vocabulary predictor and then embeds token-level long-form features inside the vocabulary predictor, with a pre-trained contextual encoder RoBERTa to further boost the performance. Moreover, we propose the LongFNT architecture by extending the long-form speech to the original speech input and achieve the best performance. The effectiveness of our LongFNT approach is validated on LibriSpeech and GigaSpeech corpora with 19% and 12% relative word error rate (WER) reduction, respectively.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096900",
      "openalex_id": "https://openalex.org/W4372260432",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Zipformer: A faster and better encoder for automatic speech recognition",
      "summary": "The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.",
      "abstract": "The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.",
      "doi": "https://doi.org/10.48550/arxiv.2310.11230",
      "openalex_id": "https://openalex.org/W4387799863",
      "arxiv_id": "",
      "publication_date": "2023-10-17",
      "published": "2023-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The International Phonetic Association",
      "summary": "The following text is taken from the pamphlet entitled The Principles of the International Phonetic Association, published by the International Phonetics Association (1949 edition).",
      "abstract": "The following text is taken from the pamphlet entitled The Principles of the International Phonetic Association, published by the International Phonetics Association (1949 edition).",
      "doi": "https://doi.org/10.1017/cbo9780511620645.014",
      "openalex_id": "https://openalex.org/W1499360075",
      "arxiv_id": "",
      "publication_date": "1987-12-25",
      "published": "1987-12-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "summary": "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.",
      "abstract": "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1873",
      "openalex_id": "https://openalex.org/W2939710050",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semantic speech retrieval with a visually grounded model of untranscribed speech",
      "summary": "There is growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.",
      "abstract": "There is growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.",
      "doi": "https://doi.org/10.1109/taslp.2018.2872106",
      "openalex_id": "https://openalex.org/W2950133079",
      "arxiv_id": "",
      "publication_date": "2017-10-05",
      "published": "2017-10-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contrastive Learning for Image Captioning",
      "summary": "Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.",
      "abstract": "Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2963360726",
      "arxiv_id": "",
      "publication_date": "2017-10-01",
      "published": "2017-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder",
      "summary": "Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS).However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue.In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE).This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner.Experiments using the VCTK and Bliz-zard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.",
      "abstract": "Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS).However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue.In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE).This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner.Experiments using the VCTK and Bliz-zard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1113",
      "openalex_id": "https://openalex.org/W2962691331",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Mutual Information Maximization for Representation Learning",
      "summary": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",
      "abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2995480165",
      "arxiv_id": "",
      "publication_date": "2020-04-30",
      "published": "2020-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Text Generation With Unlikelihood Training",
      "summary": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",
      "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2995404354",
      "arxiv_id": "",
      "publication_date": "2020-04-30",
      "published": "2020-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Language Learning Using Speech to Image Retrieval",
      "summary": "Humans learn language by interaction with their environment and listening to\\nother humans. It should also be possible for computational models to learn\\nlanguage directly from speech but so far most approaches require text. We\\nimprove on existing neural network approaches to create visually grounded\\nembeddings for spoken utterances. Using a combination of a multi-layer GRU,\\nimportance sampling, cyclic learning rates, ensembling and vectorial\\nself-attention our results show a remarkable increase in image-caption\\nretrieval performance over previous work. Furthermore, we investigate which\\nlayers in the model learn to recognise words in the input. We find that deeper\\nnetwork layers are better at encoding word presence, although the final layer\\nhas slightly lower performance. This shows that our visually grounded sentence\\nencoder learns to recognise words from the input even though it is not\\nexplicitly trained for word recognition.\\n",
      "abstract": "Humans learn language by interaction with their environment and listening to\\nother humans. It should also be possible for computational models to learn\\nlanguage directly from speech but so far most approaches require text. We\\nimprove on existing neural network approaches to create visually grounded\\nembeddings for spoken utterances. Using a combination of a multi-layer GRU,\\nimportance sampling, cyclic learning rates, ensembling and vectorial\\nself-attention our results show a remarkable increase in image-caption\\nretrieval performance over previous work. Furthermore, we investigate which\\nlayers in the model learn to recognise words in the input. We find that deeper\\nnetwork layers are better at encoding word presence, although the final layer\\nhas slightly lower performance. This shows that our visually grounded sentence\\nencoder learns to recognise words from the input even though it is not\\nexplicitly trained for word recognition.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-3067",
      "openalex_id": "https://openalex.org/W2971709506",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Words by Drawing Images",
      "summary": "We propose a framework for learning through drawing. Our goal is to learn the correspondence between spoken words and abstract visual attributes, from a dataset of spoken descriptions of images. Building upon recent findings that GAN representations can be manipulated to edit semantic concepts in the generated output, we propose a new method to use such GAN-generated images to train a model using a triplet loss. To apply the method, we develop Audio CLEVRGAN, a new dataset of audio descriptions of GAN-generated CLEVR images, and we describe a training procedure that creates a curriculum of GAN-generated images that focuses training on image pairs that differ in a specific, informative way. Training is done without additional supervision beyond the spoken captions and the GAN. We find that training that takes advantage of GAN-generated edited examples results in improvements in the model's ability to learn attributes compared to previous results. Our proposed learning framework also results in models that can associate spoken words with some abstract visual concepts such as color and size.",
      "abstract": "We propose a framework for learning through drawing. Our goal is to learn the correspondence between spoken words and abstract visual attributes, from a dataset of spoken descriptions of images. Building upon recent findings that GAN representations can be manipulated to edit semantic concepts in the generated output, we propose a new method to use such GAN-generated images to train a model using a triplet loss. To apply the method, we develop Audio CLEVRGAN, a new dataset of audio descriptions of GAN-generated CLEVR images, and we describe a training procedure that creates a curriculum of GAN-generated images that focuses training on image pairs that differ in a specific, informative way. Training is done without additional supervision beyond the spoken captions and the GAN. We find that training that takes advantage of GAN-generated edited examples results in improvements in the model's ability to learn attributes compared to previous results. Our proposed learning framework also results in models that can associate spoken words with some abstract visual concepts such as color and size.",
      "doi": "https://doi.org/10.1109/cvpr.2019.00213",
      "openalex_id": "https://openalex.org/W2953114965",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual\\n Attention",
      "summary": "Inspired by recent work in machine translation and object detection, we\\nintroduce an attention based model that automatically learns to describe the\\ncontent of images. We describe how we can train this model in a deterministic\\nmanner using standard backpropagation techniques and stochastically by\\nmaximizing a variational lower bound. We also show through visualization how\\nthe model is able to automatically learn to fix its gaze on salient objects\\nwhile generating the corresponding words in the output sequence. We validate\\nthe use of attention with state-of-the-art performance on three benchmark\\ndatasets: Flickr8k, Flickr30k and MS COCO.\\n",
      "abstract": "Inspired by recent work in machine translation and object detection, we\\nintroduce an attention based model that automatically learns to describe the\\ncontent of images. We describe how we can train this model in a deterministic\\nmanner using standard backpropagation techniques and stochastically by\\nmaximizing a variational lower bound. We also show through visualization how\\nthe model is able to automatically learn to fix its gaze on salient objects\\nwhile generating the corresponding words in the output sequence. We validate\\nthe use of attention with state-of-the-art performance on three benchmark\\ndatasets: Flickr8k, Flickr30k and MS COCO.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1502.03044",
      "openalex_id": "https://openalex.org/W2950178297",
      "arxiv_id": "",
      "publication_date": "2015-02-10",
      "published": "2015-02-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Models of Visually Grounded Speech Signal Pay Attention to Nouns: A Bilingual Experiment on English and Japanese",
      "summary": "We investigate the behaviour of attention in neural models of visually\\ngrounded speech trained on two languages: English and Japanese. Experimental\\nresults show that attention focuses on nouns and this behaviour holds true for\\ntwo very typologically different languages. We also draw parallels between\\nartificial neural attention and human attention and show that neural attention\\nfocuses on word endings as it has been theorised for human attention. Finally,\\nwe investigate how two visually grounded monolingual models can be used to\\nperform cross-lingual speech-to-speech retrieval. For both languages, the\\nenriched bilingual (speech-image) corpora with part-of-speech tags and forced\\nalignments are distributed to the community for reproducible research.\\n",
      "abstract": "We investigate the behaviour of attention in neural models of visually\\ngrounded speech trained on two languages: English and Japanese. Experimental\\nresults show that attention focuses on nouns and this behaviour holds true for\\ntwo very typologically different languages. We also draw parallels between\\nartificial neural attention and human attention and show that neural attention\\nfocuses on word endings as it has been theorised for human attention. Finally,\\nwe investigate how two visually grounded monolingual models can be used to\\nperform cross-lingual speech-to-speech retrieval. For both languages, the\\nenriched bilingual (speech-image) corpora with part-of-speech tags and forced\\nalignments are distributed to the community for reproducible research.\\n",
      "doi": "https://doi.org/10.1109/icassp.2019.8683069",
      "openalex_id": "https://openalex.org/W2920166246",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
      "summary": "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.",
      "abstract": "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.",
      "doi": "https://doi.org/10.5555/1577069.1577078",
      "openalex_id": "https://openalex.org/W2106053110",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ImageNet: A large-scale hierarchical image database",
      "summary": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \"ImageNet\", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
      "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \"ImageNet\", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
      "doi": "https://doi.org/10.1109/cvpr.2009.5206848",
      "openalex_id": "https://openalex.org/W2108598243",
      "arxiv_id": "",
      "publication_date": "2009-06-01",
      "published": "2009-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Deep Features for Scene Recognition using Places Database",
      "summary": "Scene recognition is one of the hallmark tasks of computer vision, allowing defi-nition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level fea-tures, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competi-tive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers ’ responses al-lows us to show differences in the internal representations of object-centric and scene-centric networks. 1",
      "abstract": "Scene recognition is one of the hallmark tasks of computer vision, allowing defi-nition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level fea-tures, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competi-tive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers ’ responses al-lows us to show differences in the internal representations of object-centric and scene-centric networks. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2134670479",
      "arxiv_id": "",
      "publication_date": "2014-12-08",
      "published": "2014-12-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Show and tell: A neural image caption generator",
      "summary": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
      "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
      "doi": "https://doi.org/10.1109/cvpr.2015.7298935",
      "openalex_id": "https://openalex.org/W1895577753",
      "arxiv_id": "",
      "publication_date": "2015-06-01",
      "published": "2015-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Words from Images and Speech",
      "summary": "This paper explores the possibility to learn a semantically-relevant lexicon from images and speech only. For this, we train a multi-modal neural network working both on image fragments and on speech features, by learning an embedding in which images and content words that co-occur together are close. Making no assumption on the acoustic model, this paper shows promising results on how multi-modality could help word learning.",
      "abstract": "This paper explores the possibility to learn a semantically-relevant lexicon from images and speech only. For this, we train a multi-modal neural network working both on image fragments and on speech features, by learning an embedding in which images and content words that co-occur together are close. Making no assumption on the acoustic model, this paper shows promising results on how multi-modality could help word learning.",
      "doi": "https://doi.org/10.6084/m9.figshare.1277822.v1",
      "openalex_id": "https://openalex.org/W385555557",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised learning of spoken language with visual context",
      "summary": "Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.",
      "abstract": "Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2556930864",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods",
      "summary": "We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.",
      "abstract": "We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.",
      "doi": "https://doi.org/10.48550/arxiv.1804.04262",
      "openalex_id": "https://openalex.org/W2796495654",
      "arxiv_id": "",
      "publication_date": "2018-04-12",
      "published": "2018-04-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop",
      "summary": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461761",
      "openalex_id": "https://openalex.org/W2787779284",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set",
      "summary": "This paper presents an augmentation of MSCOCO dataset where speech is added\\nto image and text. Speech captions are generated using text-to-speech (TTS)\\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\\nimages. Disfluencies and speed perturbation are added to the signal in order to\\nsound more natural. Each speech signal (WAV) is paired with a JSON file\\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\\nspeech input or output instead of text. Investigating multimodal learning\\nschemes for unsupervised speech pattern discovery is also possible with this\\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\\ncorpus (10h, 10k spoken captions). The dataset is available on Zenodo:\\nhttps://zenodo.org/record/4282267\\n",
      "abstract": "This paper presents an augmentation of MSCOCO dataset where speech is added\\nto image and text. Speech captions are generated using text-to-speech (TTS)\\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\\nimages. Disfluencies and speed perturbation are added to the signal in order to\\nsound more natural. Each speech signal (WAV) is paired with a JSON file\\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\\nspeech input or output instead of text. Investigating multimodal learning\\nschemes for unsupervised speech pattern discovery is also possible with this\\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\\ncorpus (10h, 10k spoken captions). The dataset is available on Zenodo:\\nhttps://zenodo.org/record/4282267\\n",
      "doi": "https://doi.org/10.21437/glu.2017-9",
      "openalex_id": "https://openalex.org/W2736876693",
      "arxiv_id": "",
      "publication_date": "2017-08-25",
      "published": "2017-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diverse Beam Search for Improved Description of Complex Scenes",
      "summary": "A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse---with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks---image captioning and visual question generation---particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.",
      "abstract": "A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse---with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks---image captioning and visual question generation---particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.",
      "doi": "https://doi.org/10.1609/aaai.v32i1.12340",
      "openalex_id": "https://openalex.org/W2788277448",
      "arxiv_id": "",
      "publication_date": "2018-04-27",
      "published": "2018-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "summary": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
      "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
      "doi": "https://doi.org/10.1109/cvpr.2018.00636",
      "openalex_id": "https://openalex.org/W2745461083",
      "arxiv_id": "",
      "publication_date": "2018-06-01",
      "published": "2018-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice Synthesis for in-the-Wild Speakers via a Phonological Loop.",
      "summary": "We present a new neural text to speech method that is able to transform text to speech in voices that are sampled in the wild. Unlike other text to speech systems, our solution is able to deal with unconstrained samples obtained from public speeches. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. Lastly, the speakers are similarly represented by a short vector that can also be fitted to new speakers and variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on two datasets demonstrate convincing multi-speaker and in-the-wild capabilities. In order to promote reproducibility, we release our source code and models: PyTorch code and sample audio files are available at ytaigman.github.io/loop.",
      "abstract": "We present a new neural text to speech method that is able to transform text to speech in voices that are sampled in the wild. Unlike other text to speech systems, our solution is able to deal with unconstrained samples obtained from public speeches. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. Lastly, the speakers are similarly represented by a short vector that can also be fitted to new speakers and variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on two datasets demonstrate convincing multi-speaker and in-the-wild capabilities. In order to promote reproducibility, we release our source code and models: PyTorch code and sample audio files are available at ytaigman.github.io/loop.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2736900972",
      "arxiv_id": "",
      "publication_date": "2017-07-20",
      "published": "2017-07-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning",
      "summary": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
      "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
      "doi": "https://doi.org/10.1109/cvpr.2017.345",
      "openalex_id": "https://openalex.org/W2575842049",
      "arxiv_id": "",
      "publication_date": "2017-07-01",
      "published": "2017-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Show and Speak: Directly Synthesize Spoken Description of Images",
      "summary": "This database is for the image-to-speech task. (Flickr8k)",
      "abstract": "This database is for the image-to-speech task. (Flickr8k)",
      "doi": "https://doi.org/10.5281/zenodo.4126934",
      "openalex_id": "https://openalex.org/W3093845497",
      "arxiv_id": "",
      "publication_date": "2020-10-24",
      "published": "2020-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck",
      "summary": "Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem \"skip-modal generation\" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.",
      "abstract": "Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem \"skip-modal generation\" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.",
      "doi": "https://doi.org/10.1109/iccv.2019.00769",
      "openalex_id": "https://openalex.org/W3009205145",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Lattice-Free Boosted MMI Training of HMM and CTC-Based Full-Context ASR Models",
      "summary": "Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.",
      "abstract": "Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688056",
      "openalex_id": "https://openalex.org/W4225741214",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep neural network features and semi-supervised training for low resource speech recognition",
      "summary": "We propose a new technique for training deep neural networks (DNNs) as data-driven feature front-ends for large vocabulary continuous speech recognition (LVCSR) in low resource settings. To circumvent the lack of sufficient training data for acoustic modeling in these scenarios, we use transcribed multilingual data and semi-supervised training to build the proposed feature front-ends. In our experiments, the proposed features provide an absolute improvement of 16% in a low-resource LVCSR setting with only one hour of in-domain training data. While close to three-fourths of these gains come from DNN-based features, the remaining are from semi-supervised training.",
      "abstract": "We propose a new technique for training deep neural networks (DNNs) as data-driven feature front-ends for large vocabulary continuous speech recognition (LVCSR) in low resource settings. To circumvent the lack of sufficient training data for acoustic modeling in these scenarios, we use transcribed multilingual data and semi-supervised training to build the proposed feature front-ends. In our experiments, the proposed features provide an absolute improvement of 16% in a low-resource LVCSR setting with only one hour of in-domain training data. While close to three-fourths of these gains come from DNN-based features, the remaining are from semi-supervised training.",
      "doi": "https://doi.org/10.1109/icassp.2013.6638959",
      "openalex_id": "https://openalex.org/W1993660824",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Minimum Phone Error and I-smoothing for improved discriminative training",
      "summary": "In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with up to 265 hours of training data. It is shown that for the maximum mutual information estimation (MMIE) criterion, I-smoothing reduces the word error rate (WER) by 0.4% absolute over the MMIE baseline. The combination of MPE and I-smoothing gives an improvement of 1 % over MMIE and a total reduction in WER of 4.8% absolute over the original MLE system.",
      "abstract": "In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with up to 265 hours of training data. It is shown that for the maximum mutual information estimation (MMIE) criterion, I-smoothing reduces the word error rate (WER) by 0.4% absolute over the MMIE baseline. The combination of MPE and I-smoothing gives an improvement of 1 % over MMIE and a total reduction in WER of 4.8% absolute over the original MLE system.",
      "doi": "https://doi.org/10.1109/icassp.2002.5743665",
      "openalex_id": "https://openalex.org/W2150907703",
      "arxiv_id": "",
      "publication_date": "2002-05-01",
      "published": "2002-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Front-End Factor Analysis for Speaker Verification",
      "summary": "This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.",
      "abstract": "This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.",
      "doi": "https://doi.org/10.1109/tasl.2010.2064307",
      "openalex_id": "https://openalex.org/W2150769028",
      "arxiv_id": "",
      "publication_date": "2010-08-10",
      "published": "2010-08-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition",
      "summary": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation.",
      "abstract": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation.",
      "doi": "https://doi.org/10.1109/icassp.1986.1169179",
      "openalex_id": "https://openalex.org/W1877570817",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker adaptation of neural network acoustic models using i-vectors",
      "summary": "We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.",
      "abstract": "We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.",
      "doi": "https://doi.org/10.1109/asru.2013.6707705",
      "openalex_id": "https://openalex.org/W2079623482",
      "arxiv_id": "",
      "publication_date": "2013-12-01",
      "published": "2013-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers",
      "summary": "Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.",
      "abstract": "Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.",
      "doi": "https://doi.org/10.1109/taslp.2016.2585878",
      "openalex_id": "https://openalex.org/W2516001803",
      "arxiv_id": "",
      "publication_date": "2016-08-10",
      "published": "2016-08-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis",
      "summary": "Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose an approach to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is almost twice as intelligible as previous works in this space.",
      "abstract": "Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose an approach to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is almost twice as intelligible as previous works in this space.",
      "doi": "https://doi.org/10.1109/cvpr42600.2020.01381",
      "openalex_id": "https://openalex.org/W3035626590",
      "arxiv_id": "",
      "publication_date": "2020-06-01",
      "published": "2020-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker disentanglement in video-to-speech conversion",
      "summary": "The task of video-to-speech aims to translate silent video of lip movement to its corresponding audio signal. Previous approaches to this task are generally limited to the case of a single speaker, but a method that accounts for multiple speakers is desirable as it allows to (i) leverage datasets with multiple speakers or few samples per speaker; and (ii) control speaker identity at inference time. In this paper, we introduce a new video-to-speech architecture and explore ways of extending it to the multi-speaker scenario: we augment the network with an additional speaker-related input, through which we feed either a discrete identity or a speaker embedding. Interestingly, we observe that the visual encoder of the network is capable of learning the speaker identity from the lip region of the face alone. To better disentangle the two inputs-linguistic content and speaker identity-we add adversarial losses that dispel the identity from the video embeddings. To the best of our knowledge, the proposed method is the first to provide important functionalities such as (i) control of the target voice and (ii) speech synthesis for unseen identities over the state-of-the-art, while still maintaining the intelligibility of the spoken output.",
      "abstract": "The task of video-to-speech aims to translate silent video of lip movement to its corresponding audio signal. Previous approaches to this task are generally limited to the case of a single speaker, but a method that accounts for multiple speakers is desirable as it allows to (i) leverage datasets with multiple speakers or few samples per speaker; and (ii) control speaker identity at inference time. In this paper, we introduce a new video-to-speech architecture and explore ways of extending it to the multi-speaker scenario: we augment the network with an additional speaker-related input, through which we feed either a discrete identity or a speaker embedding. Interestingly, we observe that the visual encoder of the network is capable of learning the speaker identity from the lip region of the face alone. To better disentangle the two inputs-linguistic content and speaker identity-we add adversarial losses that dispel the identity from the video embeddings. To the best of our knowledge, the proposed method is the first to provide important functionalities such as (i) control of the target voice and (ii) speech synthesis for unseen identities over the state-of-the-art, while still maintaining the intelligibility of the spoken output.",
      "doi": "https://doi.org/10.23919/eusipco54536.2021.9616266",
      "openalex_id": "https://openalex.org/W4206204999",
      "arxiv_id": "",
      "publication_date": "2021-08-23",
      "published": "2021-08-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An audio-visual corpus for speech perception and automatic speech recognition",
      "summary": "An audio-visual corpus has been collected to support the use of common material in speech perception and automatic speech recognition studies. The corpus consists of high-quality audio and video recordings of 1000 sentences spoken by each of 34 talkers. Sentences are simple, syntactically identical phrases such as “place green at B 4 now.” Intelligibility tests using the audio signals suggest that the material is easily identifiable in quiet and low levels of stationary noise. The annotated corpus is available on the web for research use.",
      "abstract": "An audio-visual corpus has been collected to support the use of common material in speech perception and automatic speech recognition studies. The corpus consists of high-quality audio and video recordings of 1000 sentences spoken by each of 34 talkers. Sentences are simple, syntactically identical phrases such as “place green at B 4 now.” Intelligibility tests using the audio signals suggest that the material is easily identifiable in quiet and low levels of stationary noise. The annotated corpus is available on the web for research use.",
      "doi": "https://doi.org/10.1121/1.2229005",
      "openalex_id": "https://openalex.org/W2015143272",
      "arxiv_id": "",
      "publication_date": "2006-11-01",
      "published": "2006-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Visual-to-speech conversion based on maximum likelihood estimation",
      "summary": "This paper proposes a visual-to-speech conversion method that converts voiceless lip movements into voiced utterances without recognizing text information. Inspired by a Gaussian Mixture Model (GMM)-based voice conversion method, GMM is estimated from jointed visual and audio features and input visual features are converted to audio features using maximum likelihood estimation. In order to capture lip movements whose frame rate data is smaller than the audio data, we construct long-term image features. The proposed method has been evaluated using large-vocabulary continuous speech and experimental results show that our proposed method effectively estimates spectral envelopes and fundamental frequencies of audio speech from voiceless lip movements.",
      "abstract": "This paper proposes a visual-to-speech conversion method that converts voiceless lip movements into voiced utterances without recognizing text information. Inspired by a Gaussian Mixture Model (GMM)-based voice conversion method, GMM is estimated from jointed visual and audio features and input visual features are converted to audio features using maximum likelihood estimation. In order to capture lip movements whose frame rate data is smaller than the audio data, we construct long-term image features. The proposed method has been evaluated using large-vocabulary continuous speech and experimental results show that our proposed method effectively estimates spectral envelopes and fundamental frequencies of audio speech from voiceless lip movements.",
      "doi": "https://doi.org/10.23919/mva.2017.7986914",
      "openalex_id": "https://openalex.org/W2739009240",
      "arxiv_id": "",
      "publication_date": "2017-05-01",
      "published": "2017-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vid2speech: Speech reconstruction from silent video",
      "summary": "Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.",
      "abstract": "Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953127",
      "openalex_id": "https://openalex.org/W2585824449",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Intelligible Audio Speech From Visual Speech",
      "summary": "This work is concerned with generating intelligible audio speech from a video of a person talking. Regression and classification methods are proposed first to estimate static spectral envelope features from active appearance model (AAM) visual features. Two further methods are then developed to incorporate temporal information into the prediction - a feature-level method using multiple frames and a model-level method based on recurrent neural networks. Speech excitation information is not available from the visual signal, so methods to artificially generate aperiodicity and fundamental frequency are developed. These are combined within the STRAIGHT vocoder to produce a speech signal. The various systems are optimised through objective tests before applying subjective intelligibility tests that determine a word accuracy of 85% from a set of human listeners on the GRID audio-visual speech database. This compares favourably with a previous regression-based system that serves as a baseline which achieved a word accuracy of 33%.",
      "abstract": "This work is concerned with generating intelligible audio speech from a video of a person talking. Regression and classification methods are proposed first to estimate static spectral envelope features from active appearance model (AAM) visual features. Two further methods are then developed to incorporate temporal information into the prediction - a feature-level method using multiple frames and a model-level method based on recurrent neural networks. Speech excitation information is not available from the visual signal, so methods to artificially generate aperiodicity and fundamental frequency are developed. These are combined within the STRAIGHT vocoder to produce a speech signal. The various systems are optimised through objective tests before applying subjective intelligibility tests that determine a word accuracy of 85% from a set of human listeners on the GRID audio-visual speech database. This compares favourably with a previous regression-based system that serves as a baseline which achieved a word accuracy of 33%.",
      "doi": "https://doi.org/10.1109/taslp.2017.2716178",
      "openalex_id": "https://openalex.org/W2625027024",
      "arxiv_id": "",
      "publication_date": "2017-06-15",
      "published": "2017-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed",
      "summary": "Speechreading or lipreading is the technique of understanding and getting\\nphonetic features from a speaker's visual features such as movement of lips,\\nface, teeth and tongue. It has a wide range of multimedia applications such as\\nin surveillance, Internet telephony, and as an aid to a person with hearing\\nimpairments. However, most of the work in speechreading has been limited to\\ntext generation from silent videos. Recently, research has started venturing\\ninto generating (audio) speech from silent video sequences but there have been\\nno developments thus far in dealing with divergent views and poses of a\\nspeaker. Thus although, we have multiple camera feeds for the speech of a user,\\nbut we have failed in using these multiple video feeds for dealing with the\\ndifferent poses. To this end, this paper presents the world's first ever\\nmulti-view speech reading and reconstruction system. This work encompasses the\\nboundaries of multimedia research by putting forth a model which leverages\\nsilent video feeds from multiple cameras recording the same subject to generate\\nintelligent speech for a speaker. Initial results confirm the usefulness of\\nexploiting multiple camera views in building an efficient speech reading and\\nreconstruction system. It further shows the optimal placement of cameras which\\nwould lead to the maximum intelligibility of speech. Next, it lays out various\\ninnovative applications for the proposed system focusing on its potential\\nprodigious impact in not just security arena but in many other multimedia\\nanalytics problems.\\n",
      "abstract": "Speechreading or lipreading is the technique of understanding and getting\\nphonetic features from a speaker's visual features such as movement of lips,\\nface, teeth and tongue. It has a wide range of multimedia applications such as\\nin surveillance, Internet telephony, and as an aid to a person with hearing\\nimpairments. However, most of the work in speechreading has been limited to\\ntext generation from silent videos. Recently, research has started venturing\\ninto generating (audio) speech from silent video sequences but there have been\\nno developments thus far in dealing with divergent views and poses of a\\nspeaker. Thus although, we have multiple camera feeds for the speech of a user,\\nbut we have failed in using these multiple video feeds for dealing with the\\ndifferent poses. To this end, this paper presents the world's first ever\\nmulti-view speech reading and reconstruction system. This work encompasses the\\nboundaries of multimedia research by putting forth a model which leverages\\nsilent video feeds from multiple cameras recording the same subject to generate\\nintelligent speech for a speaker. Initial results confirm the usefulness of\\nexploiting multiple camera views in building an efficient speech reading and\\nreconstruction system. It further shows the optimal placement of cameras which\\nwould lead to the maximum intelligibility of speech. Next, it lays out various\\ninnovative applications for the proposed system focusing on its potential\\nprodigious impact in not just security arena but in many other multimedia\\nanalytics problems.\\n",
      "doi": "https://doi.org/10.1145/3240508.3241911",
      "openalex_id": "https://openalex.org/W2887437849",
      "arxiv_id": "",
      "publication_date": "2018-10-15",
      "published": "2018-10-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks",
      "summary": "Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on generative adversarial networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of the raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for Lip Reading in the Wild (LRW), featuring hundreds of speakers recorded entirely \"in the wild.\" We evaluate the generated samples in two different scenarios-seen and unseen speakers-using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW.",
      "abstract": "Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on generative adversarial networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of the raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for Lip Reading in the Wild (LRW), featuring hundreds of speakers recorded entirely \"in the wild.\" We evaluate the generated samples in two different scenarios-seen and unseen speakers-using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW.",
      "doi": "https://doi.org/10.1109/tcyb.2022.3162495",
      "openalex_id": "https://openalex.org/W3157840621",
      "arxiv_id": "",
      "publication_date": "2022-04-19",
      "published": "2022-04-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reconstructing intelligible audio speech from visual speech features",
      "summary": "This work describes an investigation into the feasibility of producing intelligible audio speech from only visual speech fea- tures. The proposed method aims to estimate a spectral enve- lope from visual features which is then combined with an arti- ficial excitation signal and used within a model of speech pro- duction to reconstruct an audio signal. Different combinations of audio and visual features are considered, along with both a statistical method of estimation and a deep neural network. The intelligibility of the reconstructed audio speech is measured by human listeners, and then compared to the intelligibility of the video signal only and when combined with the reconstructed audio.",
      "abstract": "This work describes an investigation into the feasibility of producing intelligible audio speech from only visual speech fea- tures. The proposed method aims to estimate a spectral enve- lope from visual features which is then combined with an arti- ficial excitation signal and used within a model of speech pro- duction to reconstruct an audio signal. Different combinations of audio and visual features are considered, along with both a statistical method of estimation and a deep neural network. The intelligibility of the reconstructed audio speech is measured by human listeners, and then compared to the intelligibility of the video signal only and when combined with the reconstructed audio.",
      "doi": "https://doi.org/10.21437/interspeech.2015-139",
      "openalex_id": "https://openalex.org/W2293856338",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dlib-ml: A Machine Learning Toolkit",
      "summary": "There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.",
      "abstract": "There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.",
      "doi": "https://doi.org/10.5555/1577069.1755843",
      "openalex_id": "https://openalex.org/W2115252128",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations",
      "summary": "In this paper, a novel two-branch neural network model structure is proposed\\nfor multimodal emotion recognition, which consists of a time synchronous branch\\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\\neach word and its acoustic realisation, the TSB combines speech and text\\nmodalities at each input window frame and then does pooling across time to form\\na single embedding vector. The TAB, by contrast, provides cross-utterance\\ninformation by integrating sentence text embeddings from a number of context\\nutterances into another embedding vector. The final emotion classification uses\\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\\ndataset demonstrate that the two-branch structure achieves state-of-the-art\\nresults in 4-way classification with all common test setups. When using\\nautomatic speech recognition (ASR) output instead of manually transcribed\\nreference text, it is shown that the cross-utterance information considerably\\nimproves the robustness against ASR errors. Furthermore, by incorporating an\\nextra class for all the other emotions, the final 5-way classification system\\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\\nrecognition systems.\\n",
      "abstract": "In this paper, a novel two-branch neural network model structure is proposed\\nfor multimodal emotion recognition, which consists of a time synchronous branch\\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\\neach word and its acoustic realisation, the TSB combines speech and text\\nmodalities at each input window frame and then does pooling across time to form\\na single embedding vector. The TAB, by contrast, provides cross-utterance\\ninformation by integrating sentence text embeddings from a number of context\\nutterances into another embedding vector. The final emotion classification uses\\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\\ndataset demonstrate that the two-branch structure achieves state-of-the-art\\nresults in 4-way classification with all common test setups. When using\\nautomatic speech recognition (ASR) output instead of manually transcribed\\nreference text, it is shown that the cross-utterance information considerably\\nimproves the robustness against ASR errors. Furthermore, by incorporating an\\nextra class for all the other emotions, the final 5-way classification system\\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\\nrecognition systems.\\n",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414880",
      "openalex_id": "https://openalex.org/W3096723250",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotional speech processing: Disentangling the effects of prosody and semantic cues",
      "summary": "To inform how emotions in speech are implicitly processed and registered in memory, we compared how emotional prosody, emotional semantics, and both cues in tandem prime decisions about conjoined emotional faces. Fifty-two participants rendered facial affect decisions (Pell, 2005a), indicating whether a target face represented an emotion (happiness or sadness) or not (a facial grimace), after passively listening to happy, sad, or neutral prime utterances. Emotional information from primes was conveyed by: (1) prosody only; (2) semantic cues only; or (3) combined prosody and semantic cues. Results indicated that prosody, semantics, and combined prosody-semantic cues facilitate emotional decisions about target faces in an emotion-congruent manner. However, the magnitude of priming did not vary across tasks. Our findings highlight that emotional meanings of prosody and semantic cues are systematically registered during speech processing, but with similar effects on associative knowledge about emotions, which is presumably shared by prosody, semantics, and faces.",
      "abstract": "To inform how emotions in speech are implicitly processed and registered in memory, we compared how emotional prosody, emotional semantics, and both cues in tandem prime decisions about conjoined emotional faces. Fifty-two participants rendered facial affect decisions (Pell, 2005a), indicating whether a target face represented an emotion (happiness or sadness) or not (a facial grimace), after passively listening to happy, sad, or neutral prime utterances. Emotional information from primes was conveyed by: (1) prosody only; (2) semantic cues only; or (3) combined prosody and semantic cues. Results indicated that prosody, semantics, and combined prosody-semantic cues facilitate emotional decisions about target faces in an emotion-congruent manner. However, the magnitude of priming did not vary across tasks. Our findings highlight that emotional meanings of prosody and semantic cues are systematically registered during speech processing, but with similar effects on associative knowledge about emotions, which is presumably shared by prosody, semantics, and faces.",
      "doi": "https://doi.org/10.1080/02699931.2010.516915",
      "openalex_id": "https://openalex.org/W2130821326",
      "arxiv_id": "",
      "publication_date": "2010-11-12",
      "published": "2010-11-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vocal Expression and Perception of Emotion",
      "summary": "Speech is an acoustically rich signal that provides considerable personal information about talkers. The expression of emotions in speech sounds and corresponding abilities to perceive such emotions are both fundamental aspects of human communication. Findings from studies seeking to characterize the acoustic properties of emotional speech indicate that speech acoustics provide an external cue to the level of nonspecific arousal associated with emotionalprocesses and to a lesser extent, the relative pleasantness of experienced emotions. Outcomes from perceptual tests show that listeners are able to accurately judge emotions from speech at rates far greater than expected by chance. More detailed characterizations of these production and perception aspects of vocal communication will necessarily involve knowledge aboutdifferences among talkers, such as those components of speech that provide comparatively stable cues to individual talkers identities.",
      "abstract": "Speech is an acoustically rich signal that provides considerable personal information about talkers. The expression of emotions in speech sounds and corresponding abilities to perceive such emotions are both fundamental aspects of human communication. Findings from studies seeking to characterize the acoustic properties of emotional speech indicate that speech acoustics provide an external cue to the level of nonspecific arousal associated with emotionalprocesses and to a lesser extent, the relative pleasantness of experienced emotions. Outcomes from perceptual tests show that listeners are able to accurately judge emotions from speech at rates far greater than expected by chance. More detailed characterizations of these production and perception aspects of vocal communication will necessarily involve knowledge aboutdifferences among talkers, such as those components of speech that provide comparatively stable cues to individual talkers identities.",
      "doi": "https://doi.org/10.1111/1467-8721.00013",
      "openalex_id": "https://openalex.org/W2069924379",
      "arxiv_id": "",
      "publication_date": "1999-04-01",
      "published": "1999-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis",
      "summary": "In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.",
      "abstract": "In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683623",
      "openalex_id": "https://openalex.org/W2904459034",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading",
      "summary": "The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 that consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is first pre-trained on ∼ 2400 -h multilingual (e.g., English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on domain-specific datasets (GRID and TCD-TIMIT) for English speech reconstruction and achieve a significant improvement on speech quality and intelligibility compared to previous approaches in speaker-dependent and speaker-independent settings. In addition to English, we conduct Chinese speech reconstruction on the Chinese Mandarin Lip Reading (CMLR) dataset to verify the impact on transferability. Finally, we train the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve the state-of-the-art performance on both English and Chinese benchmark datasets.",
      "abstract": "The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 that consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is first pre-trained on ∼ 2400 -h multilingual (e.g., English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on domain-specific datasets (GRID and TCD-TIMIT) for English speech reconstruction and achieve a significant improvement on speech quality and intelligibility compared to previous approaches in speaker-dependent and speaker-independent settings. In addition to English, we conduct Chinese speech reconstruction on the Chinese Mandarin Lip Reading (CMLR) dataset to verify the impact on transferability. Finally, we train the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve the state-of-the-art performance on both English and Chinese benchmark datasets.",
      "doi": "https://doi.org/10.1109/tnnls.2022.3191677",
      "openalex_id": "https://openalex.org/W4200635083",
      "arxiv_id": "",
      "publication_date": "2022-07-22",
      "published": "2022-07-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "summary": "The lack of a large, natural emotional database is one of the key barriers to translate results on speech emotion recognition in controlled conditions into real-life applications. Collecting emotional databases is expensive and time demanding, which limits the size of existing corpora. Current approaches used to collect spontaneous databases tend to provide unbalanced emotional content, which is dictated by the given recording protocol (e.g., positive for colloquial conversations, negative for discussion or debates). The size and speaker diversity are also limited. This paper proposes a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. It relies on existing spontaneous recordings obtained from audio-sharing websites. The proposed approach combines machine learning algorithms to retrieve recordings conveying balanced emotional content with a cost effective annotation process using crowdsourcing, which make it possible to build a large scale speech emotional database. This approach provides natural emotional renditions from multiple speakers, with different channel conditions and conveying balanced emotional content that are difficult to obtain with alternative data collection protocols.",
      "abstract": "The lack of a large, natural emotional database is one of the key barriers to translate results on speech emotion recognition in controlled conditions into real-life applications. Collecting emotional databases is expensive and time demanding, which limits the size of existing corpora. Current approaches used to collect spontaneous databases tend to provide unbalanced emotional content, which is dictated by the given recording protocol (e.g., positive for colloquial conversations, negative for discussion or debates). The size and speaker diversity are also limited. This paper proposes a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. It relies on existing spontaneous recordings obtained from audio-sharing websites. The proposed approach combines machine learning algorithms to retrieve recordings conveying balanced emotional content with a cost effective annotation process using crowdsourcing, which make it possible to build a large scale speech emotional database. This approach provides natural emotional renditions from multiple speakers, with different channel conditions and conveying balanced emotional content that are difficult to obtain with alternative data collection protocols.",
      "doi": "https://doi.org/10.1109/taffc.2017.2736999",
      "openalex_id": "https://openalex.org/W2742542661",
      "arxiv_id": "",
      "publication_date": "2017-08-07",
      "published": "2017-08-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Res2Net: A New Multi-Scale Backbone Architecture",
      "summary": "Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.",
      "abstract": "Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.",
      "doi": "https://doi.org/10.1109/tpami.2019.2938758",
      "openalex_id": "https://openalex.org/W2928165649",
      "arxiv_id": "",
      "publication_date": "2019-08-30",
      "published": "2019-08-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "summary": "Recurrent neural network architectures have been shown to efficiently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward DNNs. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 6% over the baseline DNN model. We present results on several LVCSR tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the TDNN architecture in learning wider temporal dependencies in both small and large data scenarios.",
      "abstract": "Recurrent neural network architectures have been shown to efficiently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward DNNs. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 6% over the baseline DNN model. We present results on several LVCSR tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the TDNN architecture in learning wider temporal dependencies in both small and large data scenarios.",
      "doi": "https://doi.org/10.21437/interspeech.2015-647",
      "openalex_id": "https://openalex.org/W2402146185",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Squeeze-and-Excitation Networks",
      "summary": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
      "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
      "doi": "https://doi.org/10.1109/cvpr.2018.00745",
      "openalex_id": "https://openalex.org/W2752782242",
      "arxiv_id": "",
      "publication_date": "2018-06-01",
      "published": "2018-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "summary": "We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.",
      "abstract": "We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.",
      "doi": "https://doi.org/10.1109/taffc.2016.2515617",
      "openalex_id": "https://openalex.org/W2342475039",
      "arxiv_id": "",
      "publication_date": "2016-01-07",
      "published": "2016-01-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Estimating Mutual Information in Prosody Representation for Emotional Prosody Transfer in Speech Synthesis",
      "summary": "An end-to-end prosody transfer system aims to transfer the speech prosody from one speaker to another speaker. One major application is the generation of emotional speech with a new speaker's voice. The end-to-end system uses an intermediate representation of prosody, which encompasses both speaker and emotion related information. The present study tackles the problem of estimating the mutual information between emotion and speaker-related factors in the prosody representation. A mutual information neural estimator (MINE) which could measure the mutual information between high-dimensional continuous prosody embedding and discrete speaker/emotion label is applied. The experimental results show that: 1) the prosody representation generated by the end-to-end system indeed contains both emotion and speaker information; 2) The mutual information would be determined by the type of input acoustic features to the reference encoder; 3) normalization for the log F0 feature is very effective in increasing emotion-related information in the prosody representation; 4) adversarial learning can be applied to reduce speaker information in the prosody representation. These results are useful to the further development of an optimal and practical emotional prosody transfer systems.",
      "abstract": "An end-to-end prosody transfer system aims to transfer the speech prosody from one speaker to another speaker. One major application is the generation of emotional speech with a new speaker's voice. The end-to-end system uses an intermediate representation of prosody, which encompasses both speaker and emotion related information. The present study tackles the problem of estimating the mutual information between emotion and speaker-related factors in the prosody representation. A mutual information neural estimator (MINE) which could measure the mutual information between high-dimensional continuous prosody embedding and discrete speaker/emotion label is applied. The experimental results show that: 1) the prosody representation generated by the end-to-end system indeed contains both emotion and speaker information; 2) The mutual information would be determined by the type of input acoustic features to the reference encoder; 3) normalization for the log F0 feature is very effective in increasing emotion-related information in the prosody representation; 4) adversarial learning can be applied to reduce speaker information in the prosody representation. These results are useful to the further development of an optimal and practical emotional prosody transfer systems.",
      "doi": "https://doi.org/10.1109/iscslp49672.2021.9362098",
      "openalex_id": "https://openalex.org/W3135547455",
      "arxiv_id": "",
      "publication_date": "2021-01-24",
      "published": "2021-01-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition",
      "summary": "This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram. The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps. We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8% weighted accuracy (WA) and 68% unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3% absolute for WA and 4% for UA.",
      "abstract": "This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram. The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps. We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8% weighted accuracy (WA) and 68% unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3% absolute for WA and 4% for UA.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1242",
      "openalex_id": "https://openalex.org/W2889374687",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
      "summary": "&amp;lt;p&amp;gt;Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors&amp;rsquo; knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches.&amp;lt;/p&amp;gt;",
      "abstract": "&amp;lt;p&amp;gt;Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors&amp;rsquo; knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches.&amp;lt;/p&amp;gt;",
      "doi": "https://doi.org/10.21437/interspeech.2019-1649",
      "openalex_id": "https://openalex.org/W2973181312",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotional voice conversion using deep neural networks with MCC and F0 features",
      "summary": "An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.",
      "abstract": "An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.",
      "doi": "https://doi.org/10.1109/icis.2016.7550889",
      "openalex_id": "https://openalex.org/W2511640485",
      "arxiv_id": "",
      "publication_date": "2016-06-01",
      "published": "2016-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
      "summary": "Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation.",
      "abstract": "Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation.",
      "doi": "https://doi.org/10.21437/interspeech.2021-781",
      "openalex_id": "https://openalex.org/W3197993066",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stargan for Emotional Speech Conversion: Validated by Data Augmentation of End-To-End Emotion Recognition",
      "summary": "In this paper, we propose an adversarial network implementation for speech emotion conversion as a data augmentation method, validated by a multi-class speech affect recognition task. In our setting, we do not assume the availability of parallel data, and we additionally make it a priority to exploit as much as possible the available training data by adopting a cycle-consistent, class-conditional generative adversarial network with an auxiliary domain classifier. Our generated samples are valuable for data augmentation, achieving a corresponding 2% and 6% absolute increase in Micro- and MacroF1 compared to the baseline in a 3-class classification paradigm using a deep, end-to-end network. We finally perform a human perception evaluation of the samples, through which we conclude that our samples are indicative of their target emotion, albeit showing a tendency for confusion in cases where the emotional attribute of valence and arousal are inconsistent.",
      "abstract": "In this paper, we propose an adversarial network implementation for speech emotion conversion as a data augmentation method, validated by a multi-class speech affect recognition task. In our setting, we do not assume the availability of parallel data, and we additionally make it a priority to exploit as much as possible the available training data by adopting a cycle-consistent, class-conditional generative adversarial network with an auxiliary domain classifier. Our generated samples are valuable for data augmentation, achieving a corresponding 2% and 6% absolute increase in Micro- and MacroF1 compared to the baseline in a 3-class classification paradigm using a deep, end-to-end network. We finally perform a human perception evaluation of the samples, through which we conclude that our samples are indicative of their target emotion, albeit showing a tendency for confusion in cases where the emotional attribute of valence and arousal are inconsistent.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054579",
      "openalex_id": "https://openalex.org/W3015241559",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Decoupling Speaker-Independent Emotions for Voice Conversion via Source-Filter Networks",
      "summary": "Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.",
      "abstract": "Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.",
      "doi": "https://doi.org/10.1109/taslp.2022.3190715",
      "openalex_id": "https://openalex.org/W3204457821",
      "arxiv_id": "",
      "publication_date": "2022-07-14",
      "published": "2022-07-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploiting Annotators’ Typed Description of Emotion Perception to Maximize Utilization of Ratings for Speech Emotion Recognition",
      "summary": "The decision of ground truth for speech emotion recognition (SER) is still a critical issue in affective computing tasks. Previous studies on emotion recognition often rely on consensus labels after aggregating the classes selected by multiple annotators. It is common for a perceptual evaluation conducted to annotate emotional corpora to include the class \"other,\" allowing the annotators the opportunity to describe the emotion with their own words. This practice provides valuable emotional information, which, however, is ignored in most emotion recognition studies. This paper utilizes easy-accessed natural language processing toolkits to mine the sentiment of these typed descriptions, enriching and maximizing the information obtained from the annotators. The polarity information is combined with primary and secondary annotations provided by individual evaluators under a label distribution framework, creating a complete representation of the emotional content of the spoken sentences. Finally, we train multitask learning SER models with existing learning methods (soft-label, multi-label, and distribution-label) to show the performance of the novel ground truth in the MSP-Podcast corpus.",
      "abstract": "The decision of ground truth for speech emotion recognition (SER) is still a critical issue in affective computing tasks. Previous studies on emotion recognition often rely on consensus labels after aggregating the classes selected by multiple annotators. It is common for a perceptual evaluation conducted to annotate emotional corpora to include the class \"other,\" allowing the annotators the opportunity to describe the emotion with their own words. This practice provides valuable emotional information, which, however, is ignored in most emotion recognition studies. This paper utilizes easy-accessed natural language processing toolkits to mine the sentiment of these typed descriptions, enriching and maximizing the information obtained from the annotators. The polarity information is combined with primary and secondary annotations provided by individual evaluators under a label distribution framework, creating a complete representation of the emotional content of the spoken sentences. Finally, we train multitask learning SER models with existing learning methods (soft-label, multi-label, and distribution-label) to show the performance of the novel ground truth in the MSP-Podcast corpus.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746990",
      "openalex_id": "https://openalex.org/W4224918091",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
      "summary": "Emotional voice conversion aims to convert the spectrum and prosody to change the emotional patterns of speech, while preserving the speaker identity and linguistic content.Many studies require parallel speech data between different emotional patterns, which is not practical in real life.Moreover, they often model the conversion of fundamental frequency (F0) with a simple linear transform.As F0 is a key aspect of intonation that is hierarchical in nature, we believe that it is more adequate to model F0 in different temporal scales by using wavelet transform.We propose a CycleGAN network to find an optimal pseudo pair from non-parallel training data by learning forward and inverse mappings simultaneously using adversarial and cycle-consistency losses.We also study the use of continuous wavelet transform (CWT) to decompose F0 into ten temporal scales that describes speech prosody at different time resolution, for effective F0 conversion.Experimental results show that our proposed framework outperforms the baselines both in objective and subjective evaluations.",
      "abstract": "Emotional voice conversion aims to convert the spectrum and prosody to change the emotional patterns of speech, while preserving the speaker identity and linguistic content.Many studies require parallel speech data between different emotional patterns, which is not practical in real life.Moreover, they often model the conversion of fundamental frequency (F0) with a simple linear transform.As F0 is a key aspect of intonation that is hierarchical in nature, we believe that it is more adequate to model F0 in different temporal scales by using wavelet transform.We propose a CycleGAN network to find an optimal pseudo pair from non-parallel training data by learning forward and inverse mappings simultaneously using adversarial and cycle-consistency losses.We also study the use of continuous wavelet transform (CWT) to decompose F0 into ten temporal scales that describes speech prosody at different time resolution, for effective F0 conversion.Experimental results show that our proposed framework outperforms the baselines both in objective and subjective evaluations.",
      "doi": "https://doi.org/10.21437/odyssey.2020-33",
      "openalex_id": "https://openalex.org/W3025680351",
      "arxiv_id": "",
      "publication_date": "2020-05-15",
      "published": "2020-05-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using corpus methodology for semantic and pragmatic analyses: What can corpora tell us about the linguistic expression of emotions?",
      "summary": "Abstract The aim of this paper is to explore some of the possibilities, advantages and difficulties of corpus-based analyses of semantic and pragmatic aspects of language in one particular field, namely the linguistic expression of emotion concepts. For this purpose, a methodological procedure is proposed and an exemplary analysis of the emotion concept “fear” in English is performed. The procedure combines Kövecses' lexical approach and Stefanowitsch's metaphorical pattern analysis with additional concepts from corpus linguistics such as semantic preference and semantic prosody. The results of the study show that such a corpus-based analysis of emotion words offers several advantages. Firstly, by exploring the surroundings of the search word in a vast amount of text, we are not only able to find evidence of conceptual metaphor and metonymy that structure the emotion concept and of related emotion concepts, but also we can enrich the description of the emotion concept with information from a series of dimensions and add a pragmatic viewpoint by revealing an explicit or implicit evaluation of the emotion. The second advantage offered by a corpus-based approach lies in the possibility of quantifying results, i.e., comparing the frequency, productivity and creative use of individual metaphors and metonymies, which is especially interesting in view of contrastive studies.",
      "abstract": "Abstract The aim of this paper is to explore some of the possibilities, advantages and difficulties of corpus-based analyses of semantic and pragmatic aspects of language in one particular field, namely the linguistic expression of emotion concepts. For this purpose, a methodological procedure is proposed and an exemplary analysis of the emotion concept “fear” in English is performed. The procedure combines Kövecses' lexical approach and Stefanowitsch's metaphorical pattern analysis with additional concepts from corpus linguistics such as semantic preference and semantic prosody. The results of the study show that such a corpus-based analysis of emotion words offers several advantages. Firstly, by exploring the surroundings of the search word in a vast amount of text, we are not only able to find evidence of conceptual metaphor and metonymy that structure the emotion concept and of related emotion concepts, but also we can enrich the description of the emotion concept with information from a series of dimensions and add a pragmatic viewpoint by revealing an explicit or implicit evaluation of the emotion. The second advantage offered by a corpus-based approach lies in the possibility of quantifying results, i.e., comparing the frequency, productivity and creative use of individual metaphors and metonymies, which is especially interesting in view of contrastive studies.",
      "doi": "https://doi.org/10.1515/cogl.2010.023",
      "openalex_id": "https://openalex.org/W2050681655",
      "arxiv_id": "",
      "publication_date": "2010-11-01",
      "published": "2010-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformation of prosody in voice conversion",
      "summary": "Voice Conversion (VC) aims to convert one's voice to sound like that of another. So far, most of the voice conversion frameworks mainly focus only on the conversion of spectrum. We note that speaker identity is also characterized by the prosody features such as fundamental frequency (F0), energy contour and duration. Motivated by this, we propose a framework that can perform F0, energy contour and duration conversion. In the traditional exemplar-based sparse representation approach to voice conversion, a general source-target dictionary of exemplars is constructed to establish the correspondence between source and target speakers. In this work, we propose a Phonetically Aware Sparse Representation of fundamental frequency and energy contour by using Continuous Wavelet Transform (CWT). Our idea is motivated by the facts that CWT decompositions of F0 and energy contours describe prosody patterns in different temporal scales and allow for effective prosody manipulation in speech synthesis. Furthermore, phonetically aware exemplars lead to better estimation of activation matrix, therefore, possibly better conversion of prosody. We also propose a phonetically aware duration conversion framework which takes into account both phone-level and sentence-level speaking rates. We report that the proposed prosody conversion outperforms the traditional prosody conversion techniques in both objective and subjective evaluations.",
      "abstract": "Voice Conversion (VC) aims to convert one's voice to sound like that of another. So far, most of the voice conversion frameworks mainly focus only on the conversion of spectrum. We note that speaker identity is also characterized by the prosody features such as fundamental frequency (F0), energy contour and duration. Motivated by this, we propose a framework that can perform F0, energy contour and duration conversion. In the traditional exemplar-based sparse representation approach to voice conversion, a general source-target dictionary of exemplars is constructed to establish the correspondence between source and target speakers. In this work, we propose a Phonetically Aware Sparse Representation of fundamental frequency and energy contour by using Continuous Wavelet Transform (CWT). Our idea is motivated by the facts that CWT decompositions of F0 and energy contours describe prosody patterns in different temporal scales and allow for effective prosody manipulation in speech synthesis. Furthermore, phonetically aware exemplars lead to better estimation of activation matrix, therefore, possibly better conversion of prosody. We also propose a phonetically aware duration conversion framework which takes into account both phone-level and sentence-level speaking rates. We report that the proposed prosody conversion outperforms the traditional prosody conversion techniques in both objective and subjective evaluations.",
      "doi": "https://doi.org/10.1109/apsipa.2017.8282288",
      "openalex_id": "https://openalex.org/W2785978752",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Development of Emotion Reasoning in Infancy and Early Childhood",
      "summary": "Historically, research characterizing the development of emotion recognition has focused on identifying specific skills and the age periods, or milestones, at which these abilities emerge. However, advances in emotion research raise questions about whether this conceptualization accurately reflects how children learn about, understand, and respond to others’ emotions in everyday life. In this review, we propose a developmental framework for the emergence of emotion reasoning—that is, how children develop the ability to make reasonably accurate inferences and predictions about the emotion states of other people. We describe how this framework holds promise for building upon extant research. Our review suggests that use of the term emotion recognition can be misleading and imprecise, with the developmental processes of interest better characterized by the term emotion reasoning. We also highlight how the age at which children succeed on many tasks reflects myriad developmental processes. This new framing of emotional development can open new lines of inquiry about how humans learn to navigate their social worlds.",
      "abstract": "Historically, research characterizing the development of emotion recognition has focused on identifying specific skills and the age periods, or milestones, at which these abilities emerge. However, advances in emotion research raise questions about whether this conceptualization accurately reflects how children learn about, understand, and respond to others’ emotions in everyday life. In this review, we propose a developmental framework for the emergence of emotion reasoning—that is, how children develop the ability to make reasonably accurate inferences and predictions about the emotion states of other people. We describe how this framework holds promise for building upon extant research. Our review suggests that use of the term emotion recognition can be misleading and imprecise, with the developmental processes of interest better characterized by the term emotion reasoning. We also highlight how the age at which children succeed on many tasks reflects myriad developmental processes. This new framing of emotional development can open new lines of inquiry about how humans learn to navigate their social worlds.",
      "doi": "https://doi.org/10.1146/annurev-devpsych-060320-102556",
      "openalex_id": "https://openalex.org/W3112594642",
      "arxiv_id": "",
      "publication_date": "2020-12-15",
      "published": "2020-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The expression of the emotions in man and animals.",
      "summary": "Acknowledgments List of Illustrations Figures Plates Preface to the Anniversary Edition by Paul Ekman Preface to the Third Edition by Paul Ekman Preface to the Second Edition by Francis Darwin Introduction to the Third Edition by Paul Ekman The Expression of the Emotions in Man and Animals Introduction to the First Edition 1. General Principles of Expression 2. General Principles of Expression -- continued 3. General Principles of Expression -- continued 4. Means of Expression in Animals 5. Special Expressions of Animals 6. Special Expressions of Man: Suffering and Weeping 7. Low Spirits, Anxiety, Grief, Dejection, Despair 8. Joy, High Spirits, Love, Tender Feelings, Devotion 9. Reflection - Meditation - Ill-temper - Sulkiness - Determination 10. Hatred and Anger 11. Disdain - Contempt - Disgust - Guilt - Pride, Etc. - Helplessness - Patience - Affirmation and Negation 12. Surprise - Astonishment - Fear - Horror 13. Self-attention - Shame - Shyness - Modesty: Blushing 14. Concluding Remarks and Summary Afterword, by Paul Ekman APPENDIX I: Charles Darwin's Obituary, by T. H. Huxley APPENDIX II: Changes to the Text, by Paul Ekman APPENDIX III: Photography and The Expression of the Emotions, by Phillip Prodger APPENDIX IV: A Note on the Orientation of the Plates, by Phillip Prodger and Paul Ekman APPENDIX V: Concordance of Illustrations, by Phillip Prodger APPENDIX VI: List of Head Words from the Index to the First Edition NOTES NOTES TO THE COMMENTARIES INDEX",
      "abstract": "Acknowledgments List of Illustrations Figures Plates Preface to the Anniversary Edition by Paul Ekman Preface to the Third Edition by Paul Ekman Preface to the Second Edition by Francis Darwin Introduction to the Third Edition by Paul Ekman The Expression of the Emotions in Man and Animals Introduction to the First Edition 1. General Principles of Expression 2. General Principles of Expression -- continued 3. General Principles of Expression -- continued 4. Means of Expression in Animals 5. Special Expressions of Animals 6. Special Expressions of Man: Suffering and Weeping 7. Low Spirits, Anxiety, Grief, Dejection, Despair 8. Joy, High Spirits, Love, Tender Feelings, Devotion 9. Reflection - Meditation - Ill-temper - Sulkiness - Determination 10. Hatred and Anger 11. Disdain - Contempt - Disgust - Guilt - Pride, Etc. - Helplessness - Patience - Affirmation and Negation 12. Surprise - Astonishment - Fear - Horror 13. Self-attention - Shame - Shyness - Modesty: Blushing 14. Concluding Remarks and Summary Afterword, by Paul Ekman APPENDIX I: Charles Darwin's Obituary, by T. H. Huxley APPENDIX II: Changes to the Text, by Paul Ekman APPENDIX III: Photography and The Expression of the Emotions, by Phillip Prodger APPENDIX IV: A Note on the Orientation of the Plates, by Phillip Prodger and Paul Ekman APPENDIX V: Concordance of Illustrations, by Phillip Prodger APPENDIX VI: List of Head Words from the Index to the First Edition NOTES NOTES TO THE COMMENTARIES INDEX",
      "doi": "https://doi.org/10.1037/10001-000",
      "openalex_id": "https://openalex.org/W2009375902",
      "arxiv_id": "",
      "publication_date": "1872-01-01",
      "published": "1872-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "summary": "Current speaker verification techniques rely on a neural network to extract\\nspeaker representations. The successful x-vector architecture is a Time Delay\\nNeural Network (TDNN) that applies statistics pooling to project\\nvariable-length utterances into fixed-length speaker characterizing embeddings.\\nIn this paper, we propose multiple enhancements to this architecture based on\\nrecent trends in the related fields of face verification and computer vision.\\nFirstly, the initial frame layers can be restructured into 1-dimensional\\nRes2Net modules with impactful skip connections. Similarly to SE-ResNet, we\\nintroduce Squeeze-and-Excitation blocks in these modules to explicitly model\\nchannel interdependencies. The SE block expands the temporal context of the\\nframe layer by rescaling the channels according to global properties of the\\nrecording. Secondly, neural networks are known to learn hierarchical features,\\nwith each layer operating on a different level of complexity. To leverage this\\ncomplementary information, we aggregate and propagate features of different\\nhierarchical levels. Finally, we improve the statistics pooling module with\\nchannel-dependent frame attention. This enables the network to focus on\\ndifferent subsets of frames during each of the channel's statistics estimation.\\nThe proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art\\nTDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker\\nRecognition Challenge.\\n",
      "abstract": "Current speaker verification techniques rely on a neural network to extract\\nspeaker representations. The successful x-vector architecture is a Time Delay\\nNeural Network (TDNN) that applies statistics pooling to project\\nvariable-length utterances into fixed-length speaker characterizing embeddings.\\nIn this paper, we propose multiple enhancements to this architecture based on\\nrecent trends in the related fields of face verification and computer vision.\\nFirstly, the initial frame layers can be restructured into 1-dimensional\\nRes2Net modules with impactful skip connections. Similarly to SE-ResNet, we\\nintroduce Squeeze-and-Excitation blocks in these modules to explicitly model\\nchannel interdependencies. The SE block expands the temporal context of the\\nframe layer by rescaling the channels according to global properties of the\\nrecording. Secondly, neural networks are known to learn hierarchical features,\\nwith each layer operating on a different level of complexity. To leverage this\\ncomplementary information, we aggregate and propagate features of different\\nhierarchical levels. Finally, we improve the statistics pooling module with\\nchannel-dependent frame attention. This enables the network to focus on\\ndifferent subsets of frames during each of the channel's statistics estimation.\\nThe proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art\\nTDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker\\nRecognition Challenge.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-2650",
      "openalex_id": "https://openalex.org/W3024869864",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Emotion Recognition with Local-Global Aware Deep Representation Learning",
      "summary": "Convolutional neural network (CNN) based deep representation learning methods for speech emotion recognition (SER) have demonstrated great success. The basic design of CNN restricts the ability to model only local information well. Capsule network (CapsNet) can overcome the shortages of CNNs to capture the shallow global features from the spectrogram, although CapsNet cannot learn the local and deep global information. In this paper, we propose a local-global aware deep representation learning system that mainly includes two modules. One module contains a multi-scale CNN, time- frequency CNN (TFCNN) to learn the local representation. In the other module, we introduce a structure with dense connections of multiple blocks to learn shallow and deep global information. Every block in this structure is a complete CapsNet improved by a new routing algorithm. The local and global representations are fed to the classifier and achieve an absolute increase of at least 4.25% than benchmarks on IEMOCAP.",
      "abstract": "Convolutional neural network (CNN) based deep representation learning methods for speech emotion recognition (SER) have demonstrated great success. The basic design of CNN restricts the ability to model only local information well. Capsule network (CapsNet) can overcome the shortages of CNNs to capture the shallow global features from the spectrogram, although CapsNet cannot learn the local and deep global information. In this paper, we propose a local-global aware deep representation learning system that mainly includes two modules. One module contains a multi-scale CNN, time- frequency CNN (TFCNN) to learn the local representation. In the other module, we introduce a structure with dense connections of multiple blocks to learn shallow and deep global information. Every block in this structure is a complete CapsNet improved by a new routing algorithm. The local and global representations are fed to the classifier and achieve an absolute increase of at least 4.25% than benchmarks on IEMOCAP.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053192",
      "openalex_id": "https://openalex.org/W3015884429",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition",
      "summary": "Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.",
      "abstract": "Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414540",
      "openalex_id": "https://openalex.org/W3160039712",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition",
      "summary": "Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET",
      "abstract": "Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746679",
      "openalex_id": "https://openalex.org/W3204087964",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-task Learning for Speech Emotion and Emotion Intensity Recognition",
      "summary": "Speech emotion recognition (SER) helps achieve better human-computer interaction and thus has attracted extensive attention from industry and academia. Speech emotion intensity plays an important role in the emotional description, but its effect on emotion recognition still has been rarely studied in the area of SER to the best of our knowledge. Previous studies have shown that there is a certain relationship between speech emotion intensity and emotion category, so each recognition task of multi-task learning is supposed to be beneficial to each other. We propose a multi-task learning framework with a self-supervised speech representation extractor based on Wav2Vec 2.0 to detect speech emotion and intensity at the same time in downstream networks. Experiment results show that the multi-task learning framework outperforms SOTA SER models and achieves 5% and 7% SER performance improvement on IEMOCAP and RAVDESS thanks to the auxiliary task of emotion intensity recognition.",
      "abstract": "Speech emotion recognition (SER) helps achieve better human-computer interaction and thus has attracted extensive attention from industry and academia. Speech emotion intensity plays an important role in the emotional description, but its effect on emotion recognition still has been rarely studied in the area of SER to the best of our knowledge. Previous studies have shown that there is a certain relationship between speech emotion intensity and emotion category, so each recognition task of multi-task learning is supposed to be beneficial to each other. We propose a multi-task learning framework with a self-supervised speech representation extractor based on Wav2Vec 2.0 to detect speech emotion and intensity at the same time in downstream networks. Experiment results show that the multi-task learning framework outperforms SOTA SER models and achieves 5% and 7% SER performance improvement on IEMOCAP and RAVDESS thanks to the auxiliary task of emotion intensity recognition.",
      "doi": "https://doi.org/10.23919/apsipaasc55919.2022.9979844",
      "openalex_id": "https://openalex.org/W4312120641",
      "arxiv_id": "",
      "publication_date": "2022-11-07",
      "published": "2022-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron",
      "summary": "We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.",
      "abstract": "We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.",
      "doi": "https://doi.org/10.48550/arxiv.1803.09047",
      "openalex_id": "https://openalex.org/W2795109282",
      "arxiv_id": "",
      "publication_date": "2018-03-24",
      "published": "2018-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Emotion and Naturalness Recognitions With Multitask and Single-Task Learnings",
      "summary": "This paper evaluates speech emotion and naturalness recognitions by utilizing deep learning models with multitask learning and single-task learning approaches. The emotion model accommodates valence, arousal, and dominance attributes known as dimensional emotion. The naturalness ratings are labeled on a five-point scale as dimensional emotion. Multitask learning predicts both dimensional emotion (as the main task) and naturalness scores (as an auxiliary task) simultaneously. The single-task learning predicts either dimensional emotion (valence, arousal, and dominance) or naturalness score independently. The results with multitask learning show improvement from previous studies on single-task learning for both dimensional emotion recognition and naturalness predictions. Within this study, single-task learning still shows superiority over multitask learning for naturalness recognition. The scatter plots of emotion and naturalness prediction scores against the true labels in multitask learning exhibit the lack of the model; it fails to predict the low and extremely high scores. The low score of naturalness prediction in this study is possibly due to a low number of samples of unnatural speech samples since the MSP-IMPROV dataset promotes the naturalness of speech. The finding that jointly predicting naturalness with emotion helps improve the performance of emotion recognition may be embodied in the emotion recognition model in future work.",
      "abstract": "This paper evaluates speech emotion and naturalness recognitions by utilizing deep learning models with multitask learning and single-task learning approaches. The emotion model accommodates valence, arousal, and dominance attributes known as dimensional emotion. The naturalness ratings are labeled on a five-point scale as dimensional emotion. Multitask learning predicts both dimensional emotion (as the main task) and naturalness scores (as an auxiliary task) simultaneously. The single-task learning predicts either dimensional emotion (valence, arousal, and dominance) or naturalness score independently. The results with multitask learning show improvement from previous studies on single-task learning for both dimensional emotion recognition and naturalness predictions. Within this study, single-task learning still shows superiority over multitask learning for naturalness recognition. The scatter plots of emotion and naturalness prediction scores against the true labels in multitask learning exhibit the lack of the model; it fails to predict the low and extremely high scores. The low score of naturalness prediction in this study is possibly due to a low number of samples of unnatural speech samples since the MSP-IMPROV dataset promotes the naturalness of speech. The finding that jointly predicting naturalness with emotion helps improve the performance of emotion recognition may be embodied in the emotion recognition model in future work.",
      "doi": "https://doi.org/10.1109/access.2022.3189481",
      "openalex_id": "https://openalex.org/W4285251897",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Attention for Speech Emotion Recognition",
      "summary": "Speech Emotion Recognition (SER) has been shown to benefit from many of the recent advances in deep learning, including recurrent based and attention based neural network architectures as well. Nevertheless, performance still falls short of that of humans. In this work, we investigate whether SER could benefit from the self-attention and global windowing of the transformer model. We show on the IEMOCAP database that this is indeed the case. Finally, we investigate whether using the distribution of, possibly conflicting, annotations in the training data, as soft targets could outperform a majority voting. We prove that this performance increases with the agreement level of the annotators.",
      "abstract": "Speech Emotion Recognition (SER) has been shown to benefit from many of the recent advances in deep learning, including recurrent based and attention based neural network architectures as well. Nevertheless, performance still falls short of that of humans. In this work, we investigate whether SER could benefit from the self-attention and global windowing of the transformer model. We show on the IEMOCAP database that this is indeed the case. Finally, we investigate whether using the distribution of, possibly conflicting, annotations in the training data, as soft targets could outperform a majority voting. We prove that this performance increases with the agreement level of the annotators.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2822",
      "openalex_id": "https://openalex.org/W2972498864",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechSplit2.0: Unsupervised Speech Disentanglement for Voice Conversion without Tuning Autoencoder Bottlenecks",
      "summary": "SpeechSplit can perform aspect-specific voice conversion by disentangling speech into content, rhythm, pitch, and timbre using multiple autoencoders in an unsupervised manner. However, SpeechSplit requires careful tuning of the autoencoder bottlenecks, which can be time-consuming and less robust. This paper proposes SpeechSplit2.0, which constrains the information flow of the speech component to be disentangled on the autoencoder input using efficient signal processing methods instead of bottleneck tuning. Evaluation results show that SpeechSplit2.0 achieves comparable performance to SpeechSplit in speech disentanglement and superior robustness to the bottleneck size variations. Our code is available at https://github.com/biggytruck/SpeechSplit2.",
      "abstract": "SpeechSplit can perform aspect-specific voice conversion by disentangling speech into content, rhythm, pitch, and timbre using multiple autoencoders in an unsupervised manner. However, SpeechSplit requires careful tuning of the autoencoder bottlenecks, which can be time-consuming and less robust. This paper proposes SpeechSplit2.0, which constrains the information flow of the speech component to be disentangled on the autoencoder input using efficient signal processing methods instead of bottleneck tuning. Evaluation results show that SpeechSplit2.0 achieves comparable performance to SpeechSplit in speech disentanglement and superior robustness to the bottleneck size variations. Our code is available at https://github.com/biggytruck/SpeechSplit2.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747763",
      "openalex_id": "https://openalex.org/W4225939199",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LRS3-TED: a large-scale dataset for visual speech recognition",
      "summary": "This paper introduces a new multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research.",
      "abstract": "This paper introduces a new multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research.",
      "doi": "https://doi.org/10.48550/arxiv.1809.00496",
      "openalex_id": "https://openalex.org/W2891205112",
      "arxiv_id": "",
      "publication_date": "2018-09-03",
      "published": "2018-09-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "summary": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.",
      "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.",
      "doi": "https://doi.org/10.48550/arxiv.1506.03099",
      "openalex_id": "https://openalex.org/W648786980",
      "arxiv_id": "",
      "publication_date": "2015-06-09",
      "published": "2015-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reusing Neural Speech Representations for Auditory Emotion Recognition",
      "summary": "Acoustic emotion recognition aims to categorize the affective state of the speaker and is still a difficult task for machine learning models. The difficulties come from the scarcity of training data, general subjectivity in emotion perception resulting in low annotator agreement, and the uncertainty about which features are the most relevant and robust ones for classification. In this paper, we will tackle the latter problem. Inspired by the recent success of transfer learning methods we propose a set of architectures which utilize neural representations inferred by training on large speech databases for the acoustic emotion recognition task. Our experiments on the IEMOCAP dataset show ~10% relative improvements in the accuracy and F1-score over the baseline recurrent neural network which is trained end-to-end for emotion recognition.",
      "abstract": "Acoustic emotion recognition aims to categorize the affective state of the speaker and is still a difficult task for machine learning models. The difficulties come from the scarcity of training data, general subjectivity in emotion perception resulting in low annotator agreement, and the uncertainty about which features are the most relevant and robust ones for classification. In this paper, we will tackle the latter problem. Inspired by the recent success of transfer learning methods we propose a set of architectures which utilize neural representations inferred by training on large speech databases for the acoustic emotion recognition task. Our experiments on the IEMOCAP dataset show ~10% relative improvements in the accuracy and F1-score over the baseline recurrent neural network which is trained end-to-end for emotion recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1803.11508",
      "openalex_id": "https://openalex.org/W2774085128",
      "arxiv_id": "",
      "publication_date": "2018-03-30",
      "published": "2018-03-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Study and Performance of AMR Codecs for GSM",
      "summary": "In wireless communication system, limited bandwidth and power is the primary restriction.The existing wireless systems involved in transmission of speech visualized that efficient and effective methods be developed to transmit and receive the same while maintaining quality of speech, especially at the receiving end.Speech coding technique is a material of research for the scientific and academic community since the era of digitization (digital).Amongst all elements of the communication systems (transmitter, channel and receiver), transmission channel is the most critical and plays a key role in the transmission and reception of information.The quality of speech at receiver end decides by channel conditions.Modelling a channel is a multifarious task.A number of techniques are adopted to alleviate the effect of the channel.Adaptive Multi Rate is one of the techniques that neutralize the deleterious effect of the channel on speech.This technique utilizes variable bit rate that dynamically switches to specific modes of operation depending upon the channel conditions.For example, Low bit rate mode of operation is selected in adverse channel conditions, this helps to provide more error protection bits for channel coding and vice versa.Therefore, in this paper, application of Code Excited Linear Prediction (CELP) source codec on speech followed by AMR codec is studied.Further, higher the bit rate used, the better is the quality of speech.In this paper apart from speech codec about AMR is also studied that why the AMR is proposed for the GSM, how the bits rates are reduced in AMR, operation of AMR and other applications of AMR.",
      "abstract": "In wireless communication system, limited bandwidth and power is the primary restriction.The existing wireless systems involved in transmission of speech visualized that efficient and effective methods be developed to transmit and receive the same while maintaining quality of speech, especially at the receiving end.Speech coding technique is a material of research for the scientific and academic community since the era of digitization (digital).Amongst all elements of the communication systems (transmitter, channel and receiver), transmission channel is the most critical and plays a key role in the transmission and reception of information.The quality of speech at receiver end decides by channel conditions.Modelling a channel is a multifarious task.A number of techniques are adopted to alleviate the effect of the channel.Adaptive Multi Rate is one of the techniques that neutralize the deleterious effect of the channel on speech.This technique utilizes variable bit rate that dynamically switches to specific modes of operation depending upon the channel conditions.For example, Low bit rate mode of operation is selected in adverse channel conditions, this helps to provide more error protection bits for channel coding and vice versa.Therefore, in this paper, application of Code Excited Linear Prediction (CELP) source codec on speech followed by AMR codec is studied.Further, higher the bit rate used, the better is the quality of speech.In this paper apart from speech codec about AMR is also studied that why the AMR is proposed for the GSM, how the bits rates are reduced in AMR, operation of AMR and other applications of AMR.",
      "doi": "https://doi.org/10.17148/ijarcce.2014.31006",
      "openalex_id": "https://openalex.org/W2138675764",
      "arxiv_id": "",
      "publication_date": "2014-10-30",
      "published": "2014-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Rate Distribution Between Model and Signal",
      "summary": "Knowledge of a statistical model of the signal can be used to increase coding efficiency. A common approach is to use a fixed model structure with parameters that adapt to the signal. The model parameters and a signal representation that depends on the model are encoded. We show that, if the signal is divided into segments of a particular duration, and the model structure is fixed, then the optimal bit allocation for the model parameters does not vary with the overall rate. We discuss in detail the parameter rate for the autoregressive (AR) model. Our approach shows that the square error criterion in the signal domain is consistent with the commonly used root mean square log spectral error for the model parameters. Without using perceptual knowledge, we obtain a rate allocation for the model that is consistent with what is commonly used. This model rate is independent of overall coding rate. We provide experimental results for the application of the autoregressive model to speech that confirm the theory.",
      "abstract": "Knowledge of a statistical model of the signal can be used to increase coding efficiency. A common approach is to use a fixed model structure with parameters that adapt to the signal. The model parameters and a signal representation that depends on the model are encoded. We show that, if the signal is divided into segments of a particular duration, and the model structure is fixed, then the optimal bit allocation for the model parameters does not vary with the overall rate. We discuss in detail the parameter rate for the autoregressive (AR) model. Our approach shows that the square error criterion in the signal domain is consistent with the commonly used root mean square log spectral error for the model parameters. Without using perceptual knowledge, we obtain a rate allocation for the model that is consistent with what is commonly used. This model rate is independent of overall coding rate. We provide experimental results for the application of the autoregressive model to speech that confirm the theory.",
      "doi": "https://doi.org/10.1109/aspaa.2007.4393033",
      "openalex_id": "https://openalex.org/W2149288684",
      "arxiv_id": "",
      "publication_date": "2007-10-01",
      "published": "2007-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech coding methods, standards, and applications",
      "summary": "Voice is the preferred method of human communication. Although there have been times when it seemed that the voice communications problem was solved, such as when the PSTN was our primary network or later when digital cellular networks reached maturity, such is not the case today. This paper addresses the challenges and opportunities starting from the basic issues in speech coder design, developing the important speech coding techniques and standards, discussing current and future applications, outlining techniques for evaluating speech coder performance, and identifying research directions. The most prominent speech coding standards are presented and their properties, such as performance, complexity, and coding delay, analyzed. Particular networks and applications for each standard are included. Further, reflecting upon the issues and developments highlighted in this paper, it becomes evident that there is a diverse set of challenges and opportunities for research and innovation in speech coding and voice communications.",
      "abstract": "Voice is the preferred method of human communication. Although there have been times when it seemed that the voice communications problem was solved, such as when the PSTN was our primary network or later when digital cellular networks reached maturity, such is not the case today. This paper addresses the challenges and opportunities starting from the basic issues in speech coder design, developing the important speech coding techniques and standards, discussing current and future applications, outlining techniques for evaluating speech coder performance, and identifying research directions. The most prominent speech coding standards are presented and their properties, such as performance, complexity, and coding delay, analyzed. Particular networks and applications for each standard are included. Further, reflecting upon the issues and developments highlighted in this paper, it becomes evident that there is a diverse set of challenges and opportunities for research and innovation in speech coding and voice communications.",
      "doi": "https://doi.org/10.1109/mcas.2005.1550167",
      "openalex_id": "https://openalex.org/W2125126590",
      "arxiv_id": "",
      "publication_date": "2005-01-01",
      "published": "2005-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech coding: a tutorial review",
      "summary": "The past decade has witnessed substantial progress towards the application of low-rate speech coders to civilian and military communications as well as computer-related voice applications. Central to this progress has been the development of new speech coders capable of producing high-quality speech at low data rates. Most of these coders incorporate mechanisms to: represent the spectral properties of speech, provide for speech waveform matching, and \"optimize\" the coder's performance for the human ear. A number of these coders have already been adopted in national and international cellular telephony standards. The objective of this paper is to provide a tutorial overview of speech coding methodologies with emphasis on those algorithms that are part of the recent low-rate standards for cellular communications. Although the emphasis is on the new low-rate coders, we attempt to provide a comprehensive survey by covering some of the traditional methodologies as well. We feel that this approach will not only point out key references but will also provide valuable background to the beginner. The paper starts with a historical perspective and continues with a brief discussion on the speech properties and performance measures. We then proceed with descriptions of waveform coders, sinusoidal transform coders, linear predictive vocoders, and analysis-by-synthesis linear predictive coders. Finally, we present concluding remarks followed by a discussion of opportunities for future research.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The past decade has witnessed substantial progress towards the application of low-rate speech coders to civilian and military communications as well as computer-related voice applications. Central to this progress has been the development of new speech coders capable of producing high-quality speech at low data rates. Most of these coders incorporate mechanisms to: represent the spectral properties of speech, provide for speech waveform matching, and \"optimize\" the coder's performance for the human ear. A number of these coders have already been adopted in national and international cellular telephony standards. The objective of this paper is to provide a tutorial overview of speech coding methodologies with emphasis on those algorithms that are part of the recent low-rate standards for cellular communications. Although the emphasis is on the new low-rate coders, we attempt to provide a comprehensive survey by covering some of the traditional methodologies as well. We feel that this approach will not only point out key references but will also provide valuable background to the beginner. The paper starts with a historical perspective and continues with a brief discussion on the speech properties and performance measures. We then proceed with descriptions of waveform coders, sinusoidal transform coders, linear predictive vocoders, and analysis-by-synthesis linear predictive coders. Finally, we present concluding remarks followed by a discussion of opportunities for future research.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/5.326413",
      "openalex_id": "https://openalex.org/W2148371116",
      "arxiv_id": "",
      "publication_date": "1994-01-01",
      "published": "1994-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Progress in LPC-based frequency-domain audio coding",
      "summary": "This paper describes the progress in frequency-domain linear prediction coding (LPC)-based audio coding schemes.Although LPC was originally used only for time-domain speech coders, it has been applied to frequency-domain coders since the late 1980s.With the progress in associated technologies, the frequency-domain LPC-based audio coding scheme has become more promising, and it has been used in speech/audio coding standards, such as MPEG-D unified speech and audio coding and 3GPP enhanced voice services since 2010.Three of the latest investigations on the representations of LPC envelopes in frequency-domain coders are shown.These are the harmonic model, frequency-resolution warping and the Powered All-Pole Spectral Envelope, all of which are aiming at further enhancement of the coding efficiency.",
      "abstract": "This paper describes the progress in frequency-domain linear prediction coding (LPC)-based audio coding schemes.Although LPC was originally used only for time-domain speech coders, it has been applied to frequency-domain coders since the late 1980s.With the progress in associated technologies, the frequency-domain LPC-based audio coding scheme has become more promising, and it has been used in speech/audio coding standards, such as MPEG-D unified speech and audio coding and 3GPP enhanced voice services since 2010.Three of the latest investigations on the representations of LPC envelopes in frequency-domain coders are shown.These are the harmonic model, frequency-resolution warping and the Powered All-Pole Spectral Envelope, all of which are aiming at further enhancement of the coding efficiency.",
      "doi": "https://doi.org/10.1017/atsip.2016.11",
      "openalex_id": "https://openalex.org/W2606703671",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Coding: Fundamentals and Applications",
      "summary": "Abstract In this chapter, we present an overview of the most widely used algorithms, standards, and applications of wideband and narrowband speech coding. Algorithms for speech coding are classified into four broad headings: (1) waveform coding techniques (including PCM, companded PCM, and DPCM), which are typically used for landline telephony, internet telephony, and secure military communications; (2) subband coding, including perceptually transparent multi‐rate and embedded coding which is mainly used for internet and digital audio applications; (3) linear predictive analysis by synthesis coding (LPC‐AS) algorithms, including multipulse LPC, CELP, SELP, VSELP, and low‐delay CELP, which are typically used for digital cellular and telephony; and (4) LPC vocoders, including advanced vocoder algorithms (e.g., MELP, MBE, and PWI) are used for applications such as secure telephony and satellite telephony. Applications in areas such as voiceover IP (VoIP) and digital cellular are emerging and require a speech coder to gracefully adapt to rapidly changing channel conditions—a need that is met by embedded and multirate speech coders associated with joint source‐channel coding algorithms. Measures of speech coder perceptual quality include subjective measures of intelligibility (DRT and DALT) and naturalness (MOS and DAM), as well as objective measures such as segmental SNR, Bark spectral distortion, PSQM, and PESQ. Speech coding standards are set by organizations including the ITU (for landline telephony), MPEG (for multimedia applications), ETSI (for European digital cellular), TIA (for U.S. digital cellular), and DDVPC (for United States military applications).",
      "abstract": "Abstract In this chapter, we present an overview of the most widely used algorithms, standards, and applications of wideband and narrowband speech coding. Algorithms for speech coding are classified into four broad headings: (1) waveform coding techniques (including PCM, companded PCM, and DPCM), which are typically used for landline telephony, internet telephony, and secure military communications; (2) subband coding, including perceptually transparent multi‐rate and embedded coding which is mainly used for internet and digital audio applications; (3) linear predictive analysis by synthesis coding (LPC‐AS) algorithms, including multipulse LPC, CELP, SELP, VSELP, and low‐delay CELP, which are typically used for digital cellular and telephony; and (4) LPC vocoders, including advanced vocoder algorithms (e.g., MELP, MBE, and PWI) are used for applications such as secure telephony and satellite telephony. Applications in areas such as voiceover IP (VoIP) and digital cellular are emerging and require a speech coder to gracefully adapt to rapidly changing channel conditions—a need that is met by embedded and multirate speech coders associated with joint source‐channel coding algorithms. Measures of speech coder perceptual quality include subjective measures of intelligibility (DRT and DALT) and naturalness (MOS and DAM), as well as objective measures such as segmental SNR, Bark spectral distortion, PSQM, and PESQ. Speech coding standards are set by organizations including the ITU (for landline telephony), MPEG (for multimedia applications), ETSI (for European digital cellular), TIA (for U.S. digital cellular), and DDVPC (for United States military applications).",
      "doi": "https://doi.org/10.1002/0471219282.eot156",
      "openalex_id": "https://openalex.org/W1653357422",
      "arxiv_id": "",
      "publication_date": "2003-04-15",
      "published": "2003-04-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Line spectrum pair (LSP) and speech data compression",
      "summary": "Line Spectrum Pair (LSP) was first introduced by Itakura [1,2] as an alternative LPC spectral representations. It was found that this new representation has such interesting properties as (1) all zeros of LSP polynomials are on the unit circle, (2) the corresponding zeros of the symmetric and anti-symmetric LSP polynomials are interlaced, and (3) the reconstructed LPC all-pole filter preserves its minimum phase property if (1) and (2) are kept intact through a quantization procedure. In this paper we prove all these properties via a \"phase function.\" The statistical characteristics of LSP frequencies are investigated by analyzing a speech data base. In addition, we derive an expression for spectral sensitivity with respect to single LSP frequency deviation such that some insight on their quantization effects can be obtained. Results on multi-pulse LPC using LSP for spectral information compression are finally presented.",
      "abstract": "Line Spectrum Pair (LSP) was first introduced by Itakura [1,2] as an alternative LPC spectral representations. It was found that this new representation has such interesting properties as (1) all zeros of LSP polynomials are on the unit circle, (2) the corresponding zeros of the symmetric and anti-symmetric LSP polynomials are interlaced, and (3) the reconstructed LPC all-pole filter preserves its minimum phase property if (1) and (2) are kept intact through a quantization procedure. In this paper we prove all these properties via a \"phase function.\" The statistical characteristics of LSP frequencies are investigated by analyzing a speech data base. In addition, we derive an expression for spectral sensitivity with respect to single LSP frequency deviation such that some insight on their quantization effects can be obtained. Results on multi-pulse LPC using LSP for spectral information compression are finally presented.",
      "doi": "https://doi.org/10.1109/icassp.1984.1172448",
      "openalex_id": "https://openalex.org/W1519698338",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adaptive Quantization in Differential PCM Coding of Speech",
      "summary": "We describe an adaptive differential PCM (ADPCM) coder which makes instantaneous exponential changes of quantizer step-size. The coder includes a simple first-order predictor and a time-invariant, minimally complex adaptation strategy. Step-size multipliers depend only on the most recent quantizer output, and input signals of unknown variance can be accommodated. We derive appropriate multiplier values from computer simulations with speech signals and with Gauss-Markov inputs. We compare performance of the ADPCM coder with conventional log-PCM, using both objective and subjective criteria. Finally, we describe an economical integrated hardware implementation of the ADPCM coder. We believe that at bit rates of 24 to 32 kb/s, ADPCM provides a robust and efficient technique for speech communication and for digital storage of speech.",
      "abstract": "We describe an adaptive differential PCM (ADPCM) coder which makes instantaneous exponential changes of quantizer step-size. The coder includes a simple first-order predictor and a time-invariant, minimally complex adaptation strategy. Step-size multipliers depend only on the most recent quantizer output, and input signals of unknown variance can be accommodated. We derive appropriate multiplier values from computer simulations with speech signals and with Gauss-Markov inputs. We compare performance of the ADPCM coder with conventional log-PCM, using both objective and subjective criteria. Finally, we describe an economical integrated hardware implementation of the ADPCM coder. We believe that at bit rates of 24 to 32 kb/s, ADPCM provides a robust and efficient technique for speech communication and for digital storage of speech.",
      "doi": "https://doi.org/10.1002/j.1538-7305.1973.tb02007.x",
      "openalex_id": "https://openalex.org/W2055815603",
      "arxiv_id": "",
      "publication_date": "1973-09-01",
      "published": "1973-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations",
      "summary": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.",
      "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.",
      "doi": "https://doi.org/10.48550/arxiv.1704.00648",
      "openalex_id": "https://openalex.org/W2732044853",
      "arxiv_id": "",
      "publication_date": "2017-04-03",
      "published": "2017-04-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perceptual Objective Listening Quality Assessment (POLQA), The Third Generation ITU-T Standard for End-to-End Speech Quality Measurement Part I-Temporal Alignment",
      "summary": "In this and the companion paper Part II, the authors present the Perceptual Objective Listening Quality Assessment (POLQA), the third-generation speech quality measurement algorithm, standardized by the International Telecommunication Union in 2011 as Recommendation P.863. In contrast to the previous standard (P.862 Perceptual Evaluation of Speech Quality), a more complex temporal alignment was developed allowing for the alignment of a wide variety of complex distortions for which P.862 was known to fail, such as multiple delay variations within utterances as well as temporal stretching and compression of the degraded signal. When this new algorithm is used in combination with the advanced perceptual model described in Part II, it provides a new measurement standard for predicting Mean Opinion Scores that outperforms the older PESQ standard, especially for wideband and super wideband speech signals (7 and 14 kHz audio bandwidth). Part I provides the basics of the POLQA approach and outlines the core elements of the temporal alignment.",
      "abstract": "In this and the companion paper Part II, the authors present the Perceptual Objective Listening Quality Assessment (POLQA), the third-generation speech quality measurement algorithm, standardized by the International Telecommunication Union in 2011 as Recommendation P.863. In contrast to the previous standard (P.862 Perceptual Evaluation of Speech Quality), a more complex temporal alignment was developed allowing for the alignment of a wide variety of complex distortions for which P.862 was known to fail, such as multiple delay variations within utterances as well as temporal stretching and compression of the degraded signal. When this new algorithm is used in combination with the advanced perceptual model described in Part II, it provides a new measurement standard for predicting Mean Opinion Scores that outperforms the older PESQ standard, especially for wideband and super wideband speech signals (7 and 14 kHz audio bandwidth). Part I provides the basics of the POLQA approach and outlines the core elements of the temporal alignment.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2103934944",
      "arxiv_id": "",
      "publication_date": "2013-07-08",
      "published": "2013-07-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ESPnet-ST: All-in-One Speech Translation Toolkit",
      "summary": "Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",
      "abstract": "Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-demos.34",
      "openalex_id": "https://openalex.org/W3037217258",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System",
      "summary": "In this paper, we explore the encoding/pooling layer and loss function in the end-to-end speaker and language recognition system.First, a unified and interpretable end-to-end system for both speaker and language recognition is developed.It accepts variable-length input and produces an utterance level result.In the end-to-end system, the encoding layer plays a role in aggregating the variable-length input sequence into an utterance level representation.Besides the basic temporal average pooling, we introduce a self-attentive pooling layer and a learnable dictionary encoding layer to get the utterance level representation.In terms of loss function for open-set speaker verification, to get more discriminative speaker embedding, center loss and angular softmax loss is introduced in the end-to-end system.Experimental results on Voxceleb and NIST LRE 07 datasets show that the performance of end-to-end learning system could be significantly improved by the proposed encoding layer and loss function.",
      "abstract": "In this paper, we explore the encoding/pooling layer and loss function in the end-to-end speaker and language recognition system.First, a unified and interpretable end-to-end system for both speaker and language recognition is developed.It accepts variable-length input and produces an utterance level result.In the end-to-end system, the encoding layer plays a role in aggregating the variable-length input sequence into an utterance level representation.Besides the basic temporal average pooling, we introduce a self-attentive pooling layer and a learnable dictionary encoding layer to get the utterance level representation.In terms of loss function for open-set speaker verification, to get more discriminative speaker embedding, center loss and angular softmax loss is introduced in the end-to-end system.Experimental results on Voxceleb and NIST LRE 07 datasets show that the performance of end-to-end learning system could be significantly improved by the proposed encoding layer and loss function.",
      "doi": "https://doi.org/10.21437/odyssey.2018-11",
      "openalex_id": "https://openalex.org/W2963371159",
      "arxiv_id": "",
      "publication_date": "2018-06-06",
      "published": "2018-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentangling Style Factors from Speaker Representations",
      "summary": "Our goal is to separate out speaking style from speaker identity in utterance-level representations of speech such as i-vectors and x-vectors. We first show that both i-vectors and x-vectors contain information not only about speaker but also about speaking style (for one data set) or emotion (for another data set), even when projected into a low-dimensional space. To disentangle these factors, we use an autoencoder in which the latent space is split into two subspaces. The entangled information about speaker and style/emotion is pushed apart by the use of auxiliary classifiers that take one of the two latent subspaces as input and that are jointly learned with the autoencoder. We evaluate how well the latent subspaces separate the factors by using them as input to separate style/emotion classification tasks. In traditional speaker identification tasks, speaker-invariant characteristics are factorized from channel and then the channel information is ignored. Our results suggest that this so-called channel may contain exploitable information, which we refer to as style factors. Finally, we propose future work to use information theory to formalize style factors in the context of speaker identity.",
      "abstract": "Our goal is to separate out speaking style from speaker identity in utterance-level representations of speech such as i-vectors and x-vectors. We first show that both i-vectors and x-vectors contain information not only about speaker but also about speaking style (for one data set) or emotion (for another data set), even when projected into a low-dimensional space. To disentangle these factors, we use an autoencoder in which the latent space is split into two subspaces. The entangled information about speaker and style/emotion is pushed apart by the use of auxiliary classifiers that take one of the two latent subspaces as input and that are jointly learned with the autoencoder. We evaluate how well the latent subspaces separate the factors by using them as input to separate style/emotion classification tasks. In traditional speaker identification tasks, speaker-invariant characteristics are factorized from channel and then the channel information is ignored. Our results suggest that this so-called channel may contain exploitable information, which we refer to as style factors. Finally, we propose future work to use information theory to formalize style factors in the context of speaker identity.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1769",
      "openalex_id": "https://openalex.org/W2972921407",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Probing the Information Encoded in X-Vectors",
      "summary": "Deep neural network based speaker embeddings, such as x-vectors, have been\\nshown to perform well in text-independent speaker recognition/verification\\ntasks. In this paper, we use simple classifiers to investigate the contents\\nencoded by x-vector embeddings. We probe these embeddings for information\\nrelated to the speaker, channel, transcription (sentence, words, phones), and\\nmeta information about the utterance (duration and augmentation type), and\\ncompare these with the information encoded by i-vectors across a varying number\\nof dimensions. We also study the effect of data augmentation during extractor\\ntraining on the information captured by x-vectors. Experiments on the RedDots\\ndata set show that x-vectors capture spoken content and channel-related\\ninformation, while performing well on speaker verification tasks.\\n",
      "abstract": "Deep neural network based speaker embeddings, such as x-vectors, have been\\nshown to perform well in text-independent speaker recognition/verification\\ntasks. In this paper, we use simple classifiers to investigate the contents\\nencoded by x-vector embeddings. We probe these embeddings for information\\nrelated to the speaker, channel, transcription (sentence, words, phones), and\\nmeta information about the utterance (duration and augmentation type), and\\ncompare these with the information encoded by i-vectors across a varying number\\nof dimensions. We also study the effect of data augmentation during extractor\\ntraining on the information captured by x-vectors. Experiments on the RedDots\\ndata set show that x-vectors capture spoken content and channel-related\\ninformation, while performing well on speaker verification tasks.\\n",
      "doi": "https://doi.org/10.1109/asru46091.2019.9003979",
      "openalex_id": "https://openalex.org/W2972403660",
      "arxiv_id": "",
      "publication_date": "2019-12-01",
      "published": "2019-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition",
      "summary": "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.",
      "abstract": "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.",
      "doi": "https://doi.org/10.21437/interspeech.2004-668",
      "openalex_id": "https://openalex.org/W38194800",
      "arxiv_id": "",
      "publication_date": "2004-10-04",
      "published": "2004-10-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adversarial Contrastive Predictive Coding for Unsupervised Learning of Disentangled Representations.",
      "summary": "In this work we tackle disentanglement of speaker and content related variations in speech signals. We propose a fully convolutional variational autoencoder employing two encoders: a content encoder and a speaker encoder. To foster disentanglement we propose adversarial contrastive predictive coding. This new disentanglement method does neither need parallel data nor any supervision, not even speaker labels. With successful disentanglement the model is able to perform voice conversion by recombining content and speaker attributes. Due to the speaker encoder which learns to extract speaker traits from an audio signal, the proposed model not only provides meaningful speaker embeddings but is also able to perform zero-shot voice conversion, i.e. with previously unseen source and target speakers. Compared to state-of-the-art disentanglement approaches we show competitive disentanglement and voice conversion performance for speakers seen during training and superior performance for unseen speakers.",
      "abstract": "In this work we tackle disentanglement of speaker and content related variations in speech signals. We propose a fully convolutional variational autoencoder employing two encoders: a content encoder and a speaker encoder. To foster disentanglement we propose adversarial contrastive predictive coding. This new disentanglement method does neither need parallel data nor any supervision, not even speaker labels. With successful disentanglement the model is able to perform voice conversion by recombining content and speaker attributes. Due to the speaker encoder which learns to extract speaker traits from an audio signal, the proposed model not only provides meaningful speaker embeddings but is also able to perform zero-shot voice conversion, i.e. with previously unseen source and target speakers. Compared to state-of-the-art disentanglement approaches we show competitive disentanglement and voice conversion performance for speakers seen during training and superior performance for unseen speakers.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3030987249",
      "arxiv_id": "",
      "publication_date": "2020-05-26",
      "published": "2020-05-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning",
      "summary": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6% relative improvements in Character Error Rate (CER).",
      "abstract": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6% relative improvements in Character Error Rate (CER).",
      "doi": "https://doi.org/10.1109/icassp.2017.7953075",
      "openalex_id": "https://openalex.org/W2526425061",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised learning of acoustic sub-word units",
      "summary": "Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.",
      "abstract": "Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.",
      "doi": "https://doi.org/10.3115/1557690.1557736",
      "openalex_id": "https://openalex.org/W2117041980",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche",
      "summary": "Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.",
      "abstract": "Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.",
      "doi": "https://doi.org/10.1126/sciadv.aaw2594",
      "openalex_id": "https://openalex.org/W2971775690",
      "arxiv_id": "",
      "publication_date": "2019-09-04",
      "published": "2019-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Lexicon Discovery from Acoustic Input",
      "summary": "We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.",
      "abstract": "We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.",
      "doi": "https://doi.org/10.1162/tacl_a_00146",
      "openalex_id": "https://openalex.org/W1778492285",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Supervised Feature Transformations on Zero Resources for Improved Acoustic Unit Discovery",
      "summary": "In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.",
      "abstract": "In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.",
      "doi": "https://doi.org/10.1587/transinf.2017edp7175",
      "openalex_id": "https://openalex.org/W2780786457",
      "arxiv_id": "",
      "publication_date": "2017-12-31",
      "published": "2017-12-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Speech Recognition Sequence Training With Reinforcement Learning",
      "summary": "End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.",
      "abstract": "End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.",
      "doi": "https://doi.org/10.1109/access.2019.2922617",
      "openalex_id": "https://openalex.org/W2951444698",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
      "summary": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "abstract": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "doi": "https://doi.org/10.1109/icassp.2011.5947338",
      "openalex_id": "https://openalex.org/W2170659185",
      "arxiv_id": "",
      "publication_date": "2011-05-01",
      "published": "2011-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "TCNN: Temporal Convolutional Neural Network for Real-time Speech Enhancement in the Time Domain",
      "summary": "This work proposes a fully convolutional neural network (CNN) for real-time speech enhancement in the time domain. The proposed CNN is an encoder-decoder based architecture with an additional temporal convolutional module (TCM) inserted between the encoder and the decoder. We call this architecture a Temporal Convolutional Neural Network (TCNN). The encoder in the TCNN creates a low dimensional representation of a noisy input frame. The TCM uses causal and dilated convolutional layers to utilize the encoder output of the current and previous frames. The decoder uses the TCM output to reconstruct the enhanced frame. The proposed model is trained in a speaker- and noise-independent way. Experimental results demonstrate that the proposed model gives consistently better enhancement results than a state-of-the-art real-time convolutional recurrent model. Moreover, since the model is fully convolutional, it has much fewer trainable parameters than earlier models.",
      "abstract": "This work proposes a fully convolutional neural network (CNN) for real-time speech enhancement in the time domain. The proposed CNN is an encoder-decoder based architecture with an additional temporal convolutional module (TCM) inserted between the encoder and the decoder. We call this architecture a Temporal Convolutional Neural Network (TCNN). The encoder in the TCNN creates a low dimensional representation of a noisy input frame. The TCM uses causal and dilated convolutional layers to utilize the encoder output of the current and previous frames. The decoder uses the TCM output to reconstruct the enhanced frame. The proposed model is trained in a speaker- and noise-independent way. Experimental results demonstrate that the proposed model gives consistently better enhancement results than a state-of-the-art real-time convolutional recurrent model. Moreover, since the model is fully convolutional, it has much fewer trainable parameters than earlier models.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683634",
      "openalex_id": "https://openalex.org/W2937484199",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A review of vector quantization techniques",
      "summary": "The fundamental principles of quantization and the two basic types of quantization techniques-scalar and vector-have been introduced. The concept of VQ, its salient features, design of code book, and advantages/disadvantages has been dealt with in detail. VQ is a data compression technique, producing a reconstruction with as small a distortion as possible. The quality of the reconstruction depends on the amount of data that is discarded. The performance of different classes of VQ techniques like structured and unstructured VQ, memory and memoryless VQ, the types of VQ under each of these categories have been discussed. This article has surveyed these to a certain extent, and much more remains if a detailed analysis is required",
      "abstract": "The fundamental principles of quantization and the two basic types of quantization techniques-scalar and vector-have been introduced. The concept of VQ, its salient features, design of code book, and advantages/disadvantages has been dealt with in detail. VQ is a data compression technique, producing a reconstruction with as small a distortion as possible. The quality of the reconstruction depends on the amount of data that is discarded. The performance of different classes of VQ techniques like structured and unstructured VQ, memory and memoryless VQ, the types of VQ under each of these categories have been discussed. This article has surveyed these to a certain extent, and much more remains if a detailed analysis is required",
      "doi": "https://doi.org/10.1109/mp.2006.1664069",
      "openalex_id": "https://openalex.org/W2131738223",
      "arxiv_id": "",
      "publication_date": "2006-07-01",
      "published": "2006-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams",
      "summary": "In this paper, we present an unsupervised learning framework to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posteriorgram. Given one or more spoken examples of a keyword, we use segmental dynamic time warping to compare the Gaussian posteriorgrams between keyword samples and test utterances. The keyword detection result is then obtained by ranking the distortion scores of all the test utterances. We examine the TIMIT corpus as a development set to tune the parameters in our system, and the MIT Lecture corpus for more substantial evaluation. The results demonstrate the viability and effectiveness of our unsupervised learning framework on the keyword spotting task.",
      "abstract": "In this paper, we present an unsupervised learning framework to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posteriorgram. Given one or more spoken examples of a keyword, we use segmental dynamic time warping to compare the Gaussian posteriorgrams between keyword samples and test utterances. The keyword detection result is then obtained by ranking the distortion scores of all the test utterances. We examine the TIMIT corpus as a development set to tune the parameters in our system, and the MIT Lecture corpus for more substantial evaluation. The results demonstrate the viability and effectiveness of our unsupervised learning framework on the keyword spotting task.",
      "doi": "https://doi.org/10.1109/asru.2009.5372931",
      "openalex_id": "https://openalex.org/W2126203737",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "summary": "Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.",
      "abstract": "Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.",
      "doi": "https://doi.org/10.1109/tassp.1980.1163420",
      "openalex_id": "https://openalex.org/W2148154194",
      "arxiv_id": "",
      "publication_date": "1980-08-01",
      "published": "1980-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Invariant Feature Extraction for Zero-Resource Languages with Adversarial Learning",
      "summary": "We introduce a novel type of representation learning to obtain a speaker invariant feature for zero-resource languages. Speaker adaptation is an important technique to build a robust acoustic model. For a zero-resource language, however, conventional model-dependent speaker adaptation methods such as constrained maximum likelihood linear regression are insufficient because the acoustic model of the target language is not accessible. Therefore, we introduce a model-independent feature extraction based on a neural network. Specifically, we introduce a multi-task learning to a bottleneck feature-based approach to make bottleneck feature invariant to a change of speakers. The proposed network simultaneously tackles two tasks: phoneme and speaker classifications. This network trains a feature extractor in an adversarial manner to allow it to map input data into a discriminative representation to predict phonemes, whereas it is difficult to predict speakers. We conduct phone discriminant experiments in Zero Resource Speech Challenge 2017. Experimental results showed that our multi-task network yielded more discriminative features eliminating the variety in speakers.",
      "abstract": "We introduce a novel type of representation learning to obtain a speaker invariant feature for zero-resource languages. Speaker adaptation is an important technique to build a robust acoustic model. For a zero-resource language, however, conventional model-dependent speaker adaptation methods such as constrained maximum likelihood linear regression are insufficient because the acoustic model of the target language is not accessible. Therefore, we introduce a model-independent feature extraction based on a neural network. Specifically, we introduce a multi-task learning to a bottleneck feature-based approach to make bottleneck feature invariant to a change of speakers. The proposed network simultaneously tackles two tasks: phoneme and speaker classifications. This network trains a feature extractor in an adversarial manner to allow it to map input data into a discriminative representation to predict phonemes, whereas it is difficult to predict speakers. We conduct phone discriminant experiments in Zero Resource Speech Challenge 2017. Experimental results showed that our multi-task network yielded more discriminative features eliminating the variety in speakers.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461648",
      "openalex_id": "https://openalex.org/W2826003142",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
      "summary": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",
      "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",
      "doi": "https://doi.org/10.48550/arxiv.1611.00712",
      "openalex_id": "https://openalex.org/W2548228487",
      "arxiv_id": "",
      "publication_date": "2016-11-02",
      "published": "2016-11-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
      "summary": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
      "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
      "doi": "https://doi.org/10.1109/cvpr.2016.207",
      "openalex_id": "https://openalex.org/W2476548250",
      "arxiv_id": "",
      "publication_date": "2016-06-01",
      "published": "2016-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks",
      "summary": "Abstract: Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.",
      "abstract": "Abstract: Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2963619462",
      "arxiv_id": "",
      "publication_date": "2015-05-07",
      "published": "2015-05-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Segmental acoustic indexing for zero resource keyword search",
      "summary": "The task of zero resource query-by-example keyword search has received much attention in recent years as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear in the size of the search collection. As a result, their scalability substantially lags that of their supervised counterparts, which take advantage of efficient word-based indices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By indexing word-scale segments directly, we avoid higher cost frame-based processing of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversational telephone speech benchmark, we demonstrate major improvements in both speed and accuracy over the original RAILS system.",
      "abstract": "The task of zero resource query-by-example keyword search has received much attention in recent years as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear in the size of the search collection. As a result, their scalability substantially lags that of their supervised counterparts, which take advantage of efficient word-based indices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By indexing word-scale segments directly, we avoid higher cost frame-based processing of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversational telephone speech benchmark, we demonstrate major improvements in both speed and accuracy over the original RAILS system.",
      "doi": "https://doi.org/10.1109/icassp.2015.7179089",
      "openalex_id": "https://openalex.org/W1577418252",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weak top-down constraints for unsupervised acoustic model training",
      "summary": "Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.",
      "abstract": "Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639241",
      "openalex_id": "https://openalex.org/W1967924372",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning phonetic categories by learning a lexicon",
      "summary": "Learning Phonetic Categories by Learning a Lexicon Naomi H. Feldman (naomi feldman@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Thomas L. Griffiths (tom griffiths@berkeley.edu) Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA James L. Morgan (james morgan@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Abstract Infants learn to segment words from fluent speech during the same period as they learn native language phonetic cate- gories, yet accounts of phonetic category acquisition typically ignore information about the words in which speech sounds appear. We use a Bayesian model to illustrate how feed- back from segmented words might constrain phonetic category learning, helping a learner disambiguate overlapping phonetic categories. Simulations show that information from an artifi- cial lexicon can successfully disambiguate English vowel cat- egories, leading to more robust category learning than distri- butional information alone. Keywords: language acquisition; Bayesian inference phonetic categories; Infants learning their native language need to extract sev- eral levels of structure, including the locations of phonetic categories in perceptual space and the identities of words they segment from fluent speech. It is often implicitly as- sumed that these steps occur sequentially, with infants first learning about the phonetic categories in their language and subsequently using those categories to help them map word tokens onto lexical items. However, infants begin to segment words from fluent speech as early as 6 months (Bortfeld, Mor- gan, Golinkoff, & Rathbun, 2005) and this skill continues to develop over the next several months (Jusczyk & Aslin, 1995; Jusczyk, Houston, & Newsome, 1999). Discrimina- tion of non-native speech sound contrasts declines during the same time period, between 6 and 12 months (Werker & Tees, 1984). This suggests an alternative learning trajectory in which infants simultaneously learn to categorize both speech sounds and words, potentially allowing the two learning pro- cesses to interact. In this paper we explore the hypothesis that the words in- fants segment from fluent speech can provide a useful source of information for phonetic category acquisition. We use a Bayesian approach to explore the nature of the phonetic cat- egory learning problem in an interactive system, where infor- mation from segmented words can feed back and constrain phonetic category learning. Our interactive model learns a rudimentary lexicon and a phoneme inventory 1 simulta- neously, deciding whether acoustic representations of seg- mented tokens correspond to the same or different lexical items (e.g. bed vs. bad) and whether lexical items contain 1 We make the simplifying assumption that phonemes are equiv- alent to phonetic categories, and use the terms interchangeably. the same or different vowels (e.g. send vs. act). Simulations demonstrate that using information from segmented words to constrain phonetic category acquisition allows more robust category learning from fewer data points, due to the inter- active learner’s ability to use information about which words contain particular speech sounds to disambiguate overlapping categories. The paper is organized as follows. We begin with an intro- duction to the mathematical framework for our model, then present toy simulations to demonstrate its qualitative proper- ties. Next, simulations show that information from an artifi- cial lexicon can disambiguate formant values associated with English vowel categories. The last section discusses potential implications for language acquisition, revisits the model’s as- sumptions, and suggests directions for future research. Bayesian Model of Phonetic Category Learning Recent research on phonetic category acquisition has focused on the importance of distributional learning. Maye, Werker, and Gerken (2002) found that the specific frequency dis- tribution (bimodal or unimodal) of speech sounds along a continuum could affect infants’ discrimination of the contin- uum endpoints, with infants showing better discrimination of the endpoints when familiarized with the bimodal distribu- tion. This work has inspired computational models that use a Mixture of Gaussians approach, assuming that phonetic cate- gories are represented as Gaussian, or normal, distributions of speech sounds and that learners find the set of Gaussian cat- egories that best represents the distribution of speech sounds they hear. Boer and Kuhl (2003) used the Expectation Max- imization (EM) algorithm (Dempster, Laird, & Rubin, 1977) to learn the locations of three such vowel categories from for- mant data. McMurray, Aslin, and Toscano (2009) introduced a gradient descent algorithm similar to EM to learn a stop consonant voicing contrast, and this algorithm has been ex- tended to multiple dimensions for both consonant and vowel data (Toscano & McMurray, 2008; Vallabha, McClelland, Pons, Werker, & Amano, 2007). Our model adopts the Mixture of Gaussians approach from these previous models but uses a non-parametric Bayesian framework that allows extension of the model to the word level, making it possible to investigate the learning outcome when multiple levels of structure interact. As in previous models, speech sounds in our model are represented using",
      "abstract": "Learning Phonetic Categories by Learning a Lexicon Naomi H. Feldman (naomi feldman@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Thomas L. Griffiths (tom griffiths@berkeley.edu) Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA James L. Morgan (james morgan@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Abstract Infants learn to segment words from fluent speech during the same period as they learn native language phonetic cate- gories, yet accounts of phonetic category acquisition typically ignore information about the words in which speech sounds appear. We use a Bayesian model to illustrate how feed- back from segmented words might constrain phonetic category learning, helping a learner disambiguate overlapping phonetic categories. Simulations show that information from an artifi- cial lexicon can successfully disambiguate English vowel cat- egories, leading to more robust category learning than distri- butional information alone. Keywords: language acquisition; Bayesian inference phonetic categories; Infants learning their native language need to extract sev- eral levels of structure, including the locations of phonetic categories in perceptual space and the identities of words they segment from fluent speech. It is often implicitly as- sumed that these steps occur sequentially, with infants first learning about the phonetic categories in their language and subsequently using those categories to help them map word tokens onto lexical items. However, infants begin to segment words from fluent speech as early as 6 months (Bortfeld, Mor- gan, Golinkoff, & Rathbun, 2005) and this skill continues to develop over the next several months (Jusczyk & Aslin, 1995; Jusczyk, Houston, & Newsome, 1999). Discrimina- tion of non-native speech sound contrasts declines during the same time period, between 6 and 12 months (Werker & Tees, 1984). This suggests an alternative learning trajectory in which infants simultaneously learn to categorize both speech sounds and words, potentially allowing the two learning pro- cesses to interact. In this paper we explore the hypothesis that the words in- fants segment from fluent speech can provide a useful source of information for phonetic category acquisition. We use a Bayesian approach to explore the nature of the phonetic cat- egory learning problem in an interactive system, where infor- mation from segmented words can feed back and constrain phonetic category learning. Our interactive model learns a rudimentary lexicon and a phoneme inventory 1 simulta- neously, deciding whether acoustic representations of seg- mented tokens correspond to the same or different lexical items (e.g. bed vs. bad) and whether lexical items contain 1 We make the simplifying assumption that phonemes are equiv- alent to phonetic categories, and use the terms interchangeably. the same or different vowels (e.g. send vs. act). Simulations demonstrate that using information from segmented words to constrain phonetic category acquisition allows more robust category learning from fewer data points, due to the inter- active learner’s ability to use information about which words contain particular speech sounds to disambiguate overlapping categories. The paper is organized as follows. We begin with an intro- duction to the mathematical framework for our model, then present toy simulations to demonstrate its qualitative proper- ties. Next, simulations show that information from an artifi- cial lexicon can disambiguate formant values associated with English vowel categories. The last section discusses potential implications for language acquisition, revisits the model’s as- sumptions, and suggests directions for future research. Bayesian Model of Phonetic Category Learning Recent research on phonetic category acquisition has focused on the importance of distributional learning. Maye, Werker, and Gerken (2002) found that the specific frequency dis- tribution (bimodal or unimodal) of speech sounds along a continuum could affect infants’ discrimination of the contin- uum endpoints, with infants showing better discrimination of the endpoints when familiarized with the bimodal distribu- tion. This work has inspired computational models that use a Mixture of Gaussians approach, assuming that phonetic cate- gories are represented as Gaussian, or normal, distributions of speech sounds and that learners find the set of Gaussian cat- egories that best represents the distribution of speech sounds they hear. Boer and Kuhl (2003) used the Expectation Max- imization (EM) algorithm (Dempster, Laird, & Rubin, 1977) to learn the locations of three such vowel categories from for- mant data. McMurray, Aslin, and Toscano (2009) introduced a gradient descent algorithm similar to EM to learn a stop consonant voicing contrast, and this algorithm has been ex- tended to multiple dimensions for both consonant and vowel data (Toscano & McMurray, 2008; Vallabha, McClelland, Pons, Werker, & Amano, 2007). Our model adopts the Mixture of Gaussians approach from these previous models but uses a non-parametric Bayesian framework that allows extension of the model to the word level, making it possible to investigate the learning outcome when multiple levels of structure interact. As in previous models, speech sounds in our model are represented using",
      "doi": "",
      "openalex_id": "https://openalex.org/W52412328",
      "arxiv_id": "",
      "publication_date": "2009-01-01",
      "published": "2009-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On rectified linear units for speech processing",
      "summary": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",
      "abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",
      "doi": "https://doi.org/10.1109/icassp.2013.6638312",
      "openalex_id": "https://openalex.org/W2035424729",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Text Transformations in Contrastive Self-Supervised Learning: A Review",
      "summary": "Contrastive self-supervised learning has become a prominent technique in representation learning. The main step in these methods is to contrast semantically similar and dissimilar pairs of samples. However, in the domain of Natural Language Processing (NLP), the augmentation methods used in creating similar pairs with regard to contrastive learning (CL) assumptions are challenging. This is because, even simply modifying a word in the input might change the semantic meaning of the sentence, and hence, would violate the distributional hypothesis. In this review paper, we formalize the contrastive learning framework, emphasize the considerations that need to be addressed in the data transformation step, and review the state-of-the-art methods and evaluations for contrastive representation learning in NLP. Finally, we describe some challenges and potential directions for learning better text representations using contrastive methods.",
      "abstract": "Contrastive self-supervised learning has become a prominent technique in representation learning. The main step in these methods is to contrast semantically similar and dissimilar pairs of samples. However, in the domain of Natural Language Processing (NLP), the augmentation methods used in creating similar pairs with regard to contrastive learning (CL) assumptions are challenging. This is because, even simply modifying a word in the input might change the semantic meaning of the sentence, and hence, would violate the distributional hypothesis. In this review paper, we formalize the contrastive learning framework, emphasize the considerations that need to be addressed in the data transformation step, and review the state-of-the-art methods and evaluations for contrastive representation learning in NLP. Finally, we describe some challenges and potential directions for learning better text representations using contrastive methods.",
      "doi": "https://doi.org/10.24963/ijcai.2022/757",
      "openalex_id": "https://openalex.org/W4221163898",
      "arxiv_id": "",
      "publication_date": "2022-07-01",
      "published": "2022-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Solving Inefficiency of Self-supervised Representation Learning",
      "summary": "Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model’s superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model’s superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/iccv48922.2021.00937",
      "openalex_id": "https://openalex.org/W3154677174",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The design for the wall street journal-based CSR corpus",
      "summary": "The DARPA Spoken Language System (SLS) community has long taken a leadership position in designing, implementing, and globally distributing significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here is the newest addition to this valuable set of resources. In contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby providing a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.",
      "abstract": "The DARPA Spoken Language System (SLS) community has long taken a leadership position in designing, implementing, and globally distributing significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here is the newest addition to this valuable set of resources. In contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby providing a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.",
      "doi": "https://doi.org/10.3115/1075527.1075614",
      "openalex_id": "https://openalex.org/W2024490156",
      "arxiv_id": "",
      "publication_date": "1992-01-01",
      "published": "1992-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FaceNet: A unified embedding for face recognition and clustering",
      "summary": "Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.",
      "abstract": "Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.",
      "doi": "https://doi.org/10.1109/cvpr.2015.7298682",
      "openalex_id": "https://openalex.org/W2096733369",
      "arxiv_id": "",
      "publication_date": "2015-06-01",
      "published": "2015-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stochastic Class-Based Hard Example Mining for Deep Metric Learning",
      "summary": "Performance of deep metric learning depends heavily on the capability of mining hard negative examples during training. However, many metric learning algorithms often require intractable computational cost due to frequent feature computations and nearest neighbor searches in a large-scale dataset. As a result, existing approaches often suffer from trade-off between training speed and prediction accuracy. To alleviate this limitation, we propose a stochastic hard negative mining method. Our key idea is to adopt class signatures that keep track of feature embedding online with minor additional cost during training, and identify hard negative example candidates using the signatures. Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes. As most of the classes are discarded at the first step, it is much more efficient than exhaustive search while effectively mining a large number of hard examples. Our experiment shows that the proposed technique improves image retrieval accuracy substantially; it achieves the state-of-the-art performance on the several standard benchmark datasets.",
      "abstract": "Performance of deep metric learning depends heavily on the capability of mining hard negative examples during training. However, many metric learning algorithms often require intractable computational cost due to frequent feature computations and nearest neighbor searches in a large-scale dataset. As a result, existing approaches often suffer from trade-off between training speed and prediction accuracy. To alleviate this limitation, we propose a stochastic hard negative mining method. Our key idea is to adopt class signatures that keep track of feature embedding online with minor additional cost during training, and identify hard negative example candidates using the signatures. Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes. As most of the classes are discarded at the first step, it is much more efficient than exhaustive search while effectively mining a large number of hard examples. Our experiment shows that the proposed technique improves image retrieval accuracy substantially; it achieves the state-of-the-art performance on the several standard benchmark datasets.",
      "doi": "https://doi.org/10.1109/cvpr.2019.00742",
      "openalex_id": "https://openalex.org/W2948638722",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MixSpeech: Data Augmentation for Low-Resource Automatic Speech Recognition",
      "summary": "In this paper, we propose MixSpeech, a simple yet effective data augmentation method based on mixup for automatic speech recognition (ASR). MixSpeech trains an ASR model by taking a weighted combination of two different speech features (e.g., mel-spectrograms or MFCC) as the input, and recognizing both text sequences, where the two recognition losses use the same combination weight. We apply MixSpeech on two popular end-to-end speech recognition models including LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental results show that MixSpeech achieves better accuracy than the baseline models without data augmentation, and outperforms a strong data augmentation method SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms SpecAugment with a relative PER improvement of 10.6% on TIMIT dataset, and achieves a strong WER of 4.7% on WSJ dataset.",
      "abstract": "In this paper, we propose MixSpeech, a simple yet effective data augmentation method based on mixup for automatic speech recognition (ASR). MixSpeech trains an ASR model by taking a weighted combination of two different speech features (e.g., mel-spectrograms or MFCC) as the input, and recognizing both text sequences, where the two recognition losses use the same combination weight. We apply MixSpeech on two popular end-to-end speech recognition models including LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental results show that MixSpeech achieves better accuracy than the baseline models without data augmentation, and outperforms a strong data augmentation method SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms SpecAugment with a relative PER improvement of 10.6% on TIMIT dataset, and achieves a strong WER of 4.7% on WSJ dataset.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414483",
      "openalex_id": "https://openalex.org/W3163839574",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "summary": "The environmental robustness of DNN-based acoustic models can be significantly improved by using multi-condition training data. However, as data collection is a costly proposition, simulation of the desired conditions is a frequently adopted strategy. In this paper we detail a data augmentation approach for far-field ASR. We examine the impact of using simulated room impulse responses (RIRs), as real RIRs can be difficult to acquire, and also the effect of adding point-source noises. We find that the performance gap between using simulated and real RIRs can be eliminated when point-source noises are added. Further we show that the trained acoustic models not only perform well in the distant-talking scenario but also provide better results in the close-talking scenario. We evaluate our approach on several LVCSR tasks which can adequately represent both scenarios.",
      "abstract": "The environmental robustness of DNN-based acoustic models can be significantly improved by using multi-condition training data. However, as data collection is a costly proposition, simulation of the desired conditions is a frequently adopted strategy. In this paper we detail a data augmentation approach for far-field ASR. We examine the impact of using simulated room impulse responses (RIRs), as real RIRs can be difficult to acquire, and also the effect of adding point-source noises. We find that the performance gap between using simulated and real RIRs can be eliminated when point-source noises are added. Further we show that the trained acoustic models not only perform well in the distant-talking scenario but also provide better results in the close-talking scenario. We evaluate our approach on several LVCSR tasks which can adequately represent both scenarios.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953152",
      "openalex_id": "https://openalex.org/W2696967604",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios",
      "summary": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great performance in tasks that have numerous paired data, it is still challenging to make E2E ASR robust against noisy and low-resource conditions.In this study, we investigated data augmentation methods for E2E ASR in distanttalk scenarios.E2E ASR models are trained on the series of CHiME challenge datasets, which are suitable tasks for studying robustness against noisy and spontaneous speech.We propose to use three augmentation methods and thier combinations: 1) data augmentation using text-to-speech (TTS) data, 2) cycleconsistent generative adversarial network (Cycle-GAN) augmentation trained to map two different audio characteristics, the one of clean speech and of noisy recordings, to match the testing condition, and 3) pseudo-label augmentation provided by the pretrained ASR module for smoothing label distributions.Experimental results using the CHiME-6/CHiME-4 datasets show that each augmentation method individually improves the accuracy on top of the conventional SpecAugment; further improvements are obtained by combining these approaches.We achieved 4.3% word error rate (WER) reduction, which was more significant than that of the SpecAugment, when we combine all three augmentations for the CHiME-6 task.",
      "abstract": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great performance in tasks that have numerous paired data, it is still challenging to make E2E ASR robust against noisy and low-resource conditions.In this study, we investigated data augmentation methods for E2E ASR in distanttalk scenarios.E2E ASR models are trained on the series of CHiME challenge datasets, which are suitable tasks for studying robustness against noisy and spontaneous speech.We propose to use three augmentation methods and thier combinations: 1) data augmentation using text-to-speech (TTS) data, 2) cycleconsistent generative adversarial network (Cycle-GAN) augmentation trained to map two different audio characteristics, the one of clean speech and of noisy recordings, to match the testing condition, and 3) pseudo-label augmentation provided by the pretrained ASR module for smoothing label distributions.Experimental results using the CHiME-6/CHiME-4 datasets show that each augmentation method individually improves the accuracy on top of the conventional SpecAugment; further improvements are obtained by combining these approaches.We achieved 4.3% word error rate (WER) reduction, which was more significant than that of the SpecAugment, when we combine all three augmentations for the CHiME-6 task.",
      "doi": "https://doi.org/10.21437/interspeech.2021-958",
      "openalex_id": "https://openalex.org/W3196843354",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MUSAN: A Music, Speech, and Noise Corpus",
      "summary": "This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.",
      "abstract": "This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.",
      "doi": "https://doi.org/10.48550/arxiv.1510.08484",
      "openalex_id": "https://openalex.org/W2219249508",
      "arxiv_id": "",
      "publication_date": "2015-10-28",
      "published": "2015-10-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition",
      "summary": "We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR) model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions, especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.",
      "abstract": "We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR) model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions, especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.",
      "doi": "https://doi.org/10.48550/arxiv.2010.12715",
      "openalex_id": "https://openalex.org/W3094225009",
      "arxiv_id": "",
      "publication_date": "2020-10-23",
      "published": "2020-10-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The History of Speech Recognition to the Year 2030",
      "summary": "The decade from 2010 to 2020 saw remarkable improvements in automatic speech recognition. Many people now use speech recognition on a daily basis, for example to perform voice search queries, send text messages, and interact with voice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people rarely used speech recognition. Given the remarkable changes in the state of speech recognition over the previous decade, what can we expect over the coming decade? I attempt to forecast the state of speech recognition research and applications by the year 2030. While the changes to general speech recognition accuracy will not be as dramatic as in the previous decade, I suggest we have an exciting decade of progress in speech technology ahead of us.",
      "abstract": "The decade from 2010 to 2020 saw remarkable improvements in automatic speech recognition. Many people now use speech recognition on a daily basis, for example to perform voice search queries, send text messages, and interact with voice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people rarely used speech recognition. Given the remarkable changes in the state of speech recognition over the previous decade, what can we expect over the coming decade? I attempt to forecast the state of speech recognition research and applications by the year 2030. While the changes to general speech recognition accuracy will not be as dramatic as in the previous decade, I suggest we have an exciting decade of progress in speech technology ahead of us.",
      "doi": "https://doi.org/10.48550/arxiv.2108.00084",
      "openalex_id": "https://openalex.org/W3187822143",
      "arxiv_id": "",
      "publication_date": "2021-07-30",
      "published": "2021-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Negative Selection by Clustering for Contrastive Learning in Human Activity Recognition",
      "summary": "Contrastive learning is an emerging and important self-supervised learning paradigm that has been successfully applied to sensor-based human activity recognition (HAR) because it can achieve competitive performance relative to supervised learning. Contrastive learning methods generally involve instance discrimination, which means that the instances are regarded as negatives of each other, and thus their representations are pulled away from each other during the training process. However, instance discrimination could cause overclustering, meaning that the representations of instances from the same class could be overly separated. To alleviate this overclustering phenomenon, we propose a new contrastive learning framework to select negatives by clustering in HAR, which is named clustering for contrastive learning in human activity recognition (ClusterCLHAR). First, ClusterCLHAR clusters the instance representations, and for each instance, only those from different clusters are regarded as negatives. Second, a new contrastive loss function is proposed to mask the same-cluster instances from the negative pairs. We evaluate ClusterCLHAR on three popular benchmark data sets: 1) USC-HAD; 2) MotionSense; and 3) UCI-HAR, using the mean F1-score as an evaluation metric for downstream tasks. The experimental results show that ClusterCLHAR outperforms all the state-of-the-art methods applied to HAR in self-supervised learning and semi-supervised learning.",
      "abstract": "Contrastive learning is an emerging and important self-supervised learning paradigm that has been successfully applied to sensor-based human activity recognition (HAR) because it can achieve competitive performance relative to supervised learning. Contrastive learning methods generally involve instance discrimination, which means that the instances are regarded as negatives of each other, and thus their representations are pulled away from each other during the training process. However, instance discrimination could cause overclustering, meaning that the representations of instances from the same class could be overly separated. To alleviate this overclustering phenomenon, we propose a new contrastive learning framework to select negatives by clustering in HAR, which is named clustering for contrastive learning in human activity recognition (ClusterCLHAR). First, ClusterCLHAR clusters the instance representations, and for each instance, only those from different clusters are regarded as negatives. Second, a new contrastive loss function is proposed to mask the same-cluster instances from the negative pairs. We evaluate ClusterCLHAR on three popular benchmark data sets: 1) USC-HAD; 2) MotionSense; and 3) UCI-HAR, using the mean F1-score as an evaluation metric for downstream tasks. The experimental results show that ClusterCLHAR outperforms all the state-of-the-art methods applied to HAR in self-supervised learning and semi-supervised learning.",
      "doi": "https://doi.org/10.1109/jiot.2023.3239945",
      "openalex_id": "https://openalex.org/W4318148707",
      "arxiv_id": "",
      "publication_date": "2023-01-26",
      "published": "2023-01-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition",
      "summary": "Automatic speech recognition (ASR) has shown rapid advances in recent years but still degrades significantly in far-field and noisy environments. The recent development of self-supervised learning (SSL) technology can improve the ASR performance by pre-training the model with additional unlabeled speech and the SSL pre-trained model has achieved the state-of-the-art result on several speech benchmarks. Nevertheless, most of the previous SSL methods ignore the influence of the background noise or reverberation, which is crucial to deploying ASR systems in real-world speech applications. This study addresses the robust ASR by introducing a multi-variant consistency (MVC) based SSL method that adapts to different environments. The MVC-SSL is a robust SSL pre-training method designed for noisy and distant-talking speech in real-world applications. Compared to the previous SSL method, the MVC-SSL can calculate the contrastive loss among audios from different acoustic conditions or channels and can learn invariant representations with the change in the environment or the recording equipment. We also explore different SSL training pipelines to balance the noisy distant-talking speech and extra high resource clean speech. We evaluate the proposed method on the commercially-motivated dataset, CHiME-4, and the meeting dataset, AMI. With the help of the MVC-SSL and appropriate training pipeline, we can achieve up to 30% relative word error rate reductions over the baseline wav2vec2.0, one of the most successful SSL methods for ASR.",
      "abstract": "Automatic speech recognition (ASR) has shown rapid advances in recent years but still degrades significantly in far-field and noisy environments. The recent development of self-supervised learning (SSL) technology can improve the ASR performance by pre-training the model with additional unlabeled speech and the SSL pre-trained model has achieved the state-of-the-art result on several speech benchmarks. Nevertheless, most of the previous SSL methods ignore the influence of the background noise or reverberation, which is crucial to deploying ASR systems in real-world speech applications. This study addresses the robust ASR by introducing a multi-variant consistency (MVC) based SSL method that adapts to different environments. The MVC-SSL is a robust SSL pre-training method designed for noisy and distant-talking speech in real-world applications. Compared to the previous SSL method, the MVC-SSL can calculate the contrastive loss among audios from different acoustic conditions or channels and can learn invariant representations with the change in the environment or the recording equipment. We also explore different SSL training pipelines to balance the noisy distant-talking speech and extra high resource clean speech. We evaluate the proposed method on the commercially-motivated dataset, CHiME-4, and the meeting dataset, AMI. With the help of the MVC-SSL and appropriate training pipeline, we can achieve up to 30% relative word error rate reductions over the baseline wav2vec2.0, one of the most successful SSL methods for ASR.",
      "doi": "https://doi.org/10.48550/arxiv.2112.12522",
      "openalex_id": "https://openalex.org/W4306672449",
      "arxiv_id": "",
      "publication_date": "2021-12-23",
      "published": "2021-12-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers",
      "summary": "Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations.",
      "abstract": "Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations.",
      "doi": "https://doi.org/10.48550/arxiv.2204.09224",
      "openalex_id": "https://openalex.org/W4283659485",
      "arxiv_id": "",
      "publication_date": "2022-04-20",
      "published": "2022-04-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Prosody in the Comprehension of Spoken Language: A Literature Review",
      "summary": "Research on the exploitation of prosodic information in the comprehension of spoken language is reviewed. The research falls into three main areas: the use of prosody in the recognition of spoken words, in which most attention has been paid to the question of whether the prosodic structure of a word plays a role in initial activation of stored lexical representations; the use of prosody in the computation of syntactic structure, in which the resolution of global and local ambiguities has formed the central focus; and the role of prosody in the processing of discourse structure, in which there has been a preponderance of work on the contribution of accentuation and deaccentuation to integration of concepts with an existing discourse model. The review reveals that in each area progress has been made towards new conceptions of prosody's role in processing, and in particular this has involved abandonment of previously held deterministic views of the relationship between prosodic structure and other aspects of linguistic structure.",
      "abstract": "Research on the exploitation of prosodic information in the comprehension of spoken language is reviewed. The research falls into three main areas: the use of prosody in the recognition of spoken words, in which most attention has been paid to the question of whether the prosodic structure of a word plays a role in initial activation of stored lexical representations; the use of prosody in the computation of syntactic structure, in which the resolution of global and local ambiguities has formed the central focus; and the role of prosody in the processing of discourse structure, in which there has been a preponderance of work on the contribution of accentuation and deaccentuation to integration of concepts with an existing discourse model. The review reveals that in each area progress has been made towards new conceptions of prosody's role in processing, and in particular this has involved abandonment of previously held deterministic views of the relationship between prosodic structure and other aspects of linguistic structure.",
      "doi": "https://doi.org/10.1177/002383099704000203",
      "openalex_id": "https://openalex.org/W2143827132",
      "arxiv_id": "",
      "publication_date": "1997-04-01",
      "published": "1997-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Streaming End-to-end Speech Recognition for Mobile Devices",
      "summary": "End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",
      "abstract": "End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682336",
      "openalex_id": "https://openalex.org/W2962760690",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
      "summary": "Automatic speech recognition (ASR) is a key technology in many services and\\napplications. This typically requires user devices to send their speech data to\\nthe cloud for ASR decoding. As the speech signal carries a lot of information\\nabout the speaker, this raises serious privacy concerns. As a solution, an\\nencoder may reside on each user device which performs local computations to\\nanonymize the representation. In this paper, we focus on the protection of\\nspeaker identity and study the extent to which users can be recognized based on\\nthe encoded representation of their speech as obtained by a deep\\nencoder-decoder architecture trained for ASR. Through speaker identification\\nand verification experiments on the Librispeech corpus with open and closed\\nsets of speakers, we show that the representations obtained from a standard\\narchitecture still carry a lot of information about speaker identity. We then\\npropose to use adversarial training to learn representations that perform well\\nin ASR while hiding speaker identity. Our results demonstrate that adversarial\\ntraining dramatically reduces the closed-set classification accuracy, but this\\ndoes not translate into increased open-set verification error hence into\\nincreased protection of the speaker identity in practice. We suggest several\\npossible reasons behind this negative result.\\n",
      "abstract": "Automatic speech recognition (ASR) is a key technology in many services and\\napplications. This typically requires user devices to send their speech data to\\nthe cloud for ASR decoding. As the speech signal carries a lot of information\\nabout the speaker, this raises serious privacy concerns. As a solution, an\\nencoder may reside on each user device which performs local computations to\\nanonymize the representation. In this paper, we focus on the protection of\\nspeaker identity and study the extent to which users can be recognized based on\\nthe encoded representation of their speech as obtained by a deep\\nencoder-decoder architecture trained for ASR. Through speaker identification\\nand verification experiments on the Librispeech corpus with open and closed\\nsets of speakers, we show that the representations obtained from a standard\\narchitecture still carry a lot of information about speaker identity. We then\\npropose to use adversarial training to learn representations that perform well\\nin ASR while hiding speaker identity. Our results demonstrate that adversarial\\ntraining dramatically reduces the closed-set classification accuracy, but this\\ndoes not translate into increased open-set verification error hence into\\nincreased protection of the speaker identity in practice. We suggest several\\npossible reasons behind this negative result.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-2415",
      "openalex_id": "https://openalex.org/W2953900859",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores",
      "summary": "The performance of automatic speech recognition based on coded-decoded speech heavily depends on the quality of the transmitted signals, determined by channel impairments. This paper examines relationships between speech recognition performance and measurements of speech quality and intelligibility over transmission channels. Different to previous studies, the effects of super-wideband transmissions are analyzed and compared to those of wideband and narrowband channels. Furthermore, intelligibility scores, gathered by conducting a listening test based on logatomes, are also considered for the prediction of automatic speech recognition results. The modern instrumental measurement techniques POLQA and POLQA-based intelligibility have been respectively applied to estimate the quality and the intelligibility of transmitted speech. Based on our results, polynomial models are proposed that permit the prediction of speech recognition accuracy from the subjective and instrumental measures, involving a number of channel distortions in the three bandwidths. This approach can save the costs of performing automatic speech recognition experiments and can be seen as a first step towards a useful tool for communication channel designers. Copyright © 2017 ISCA. Amazon Alexa; Apple; DiDi; et al.; Furhat Robotics; Microsoft",
      "abstract": "The performance of automatic speech recognition based on coded-decoded speech heavily depends on the quality of the transmitted signals, determined by channel impairments. This paper examines relationships between speech recognition performance and measurements of speech quality and intelligibility over transmission channels. Different to previous studies, the effects of super-wideband transmissions are analyzed and compared to those of wideband and narrowband channels. Furthermore, intelligibility scores, gathered by conducting a listening test based on logatomes, are also considered for the prediction of automatic speech recognition results. The modern instrumental measurement techniques POLQA and POLQA-based intelligibility have been respectively applied to estimate the quality and the intelligibility of transmitted speech. Based on our results, polynomial models are proposed that permit the prediction of speech recognition accuracy from the subjective and instrumental measures, involving a number of channel distortions in the three bandwidths. This approach can save the costs of performing automatic speech recognition experiments and can be seen as a first step towards a useful tool for communication channel designers. Copyright © 2017 ISCA. Amazon Alexa; Apple; DiDi; et al.; Furhat Robotics; Microsoft",
      "doi": "https://doi.org/10.21437/interspeech.2017-36",
      "openalex_id": "https://openalex.org/W2747368236",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ETEH: Unified Attention-Based End-to-End ASR and KWS Architecture",
      "summary": "Even though attention-based end-to-end (E2E) automatic speech recognition (ASR) models have been yielding state-of-the-art recognition accuracy, they still fall behind many of the ASR models deployed in the industry in some crucial functionalities such as online processing and precise timestamps generating. This weakness prevents attention-based E2E ASR models from being applied in several essential speech tasks, such as online speech recognition and keyword searching (KWS). In this paper, we describe our proposed unified attention-based E2E ASR and KWS architecture–ETEH, which supports, in one model, both online and offline ASR decoding modes, thus allowing for precise and reliable KWS. \"ETE\" stands for attention-based E2E modeling, whereas \"H\" represents the hybrid gaussian mixture model and hidden Markov model (GMM-HMM). As a combination of both, ETEH is an attention-based E2E ASR architecture which utilizes the frame-wise time alignment (FTA) generated by GMM-HMM ASR models. This FTA is used to better the model in two ways: first, it helps the monotonic attentions of ETEH models to capture more accurate word time stamps, thus resulting in lower latency for online decoding; second, it helps ETEH models to provide accurate and reliable KWS results. Furthermore, we are able to combine both offline and online modes in one ETEH model and establish a concise system by adopt the universal training strategy. ETEH is functional and unique, and to the best of our knowledge, we can hardly find a comparable single attention-based E2E ASR system as the baseline. To evaluate ASR accuracy and latency for ETEH, we use our previously proposed monotonic truncated attention (MTA) based online CTC/attention (OCA) ASR models as baselines. Experimental results show that ETEH ASR models gain significant improvement in ASR latency compared to the baseline. To evaluate KWS performance, we compare ETEH models with CTC-based KWS models. Results demonstrate that our ETEH models achieve significantly better KWS performance compared to the CTC baselines.",
      "abstract": "Even though attention-based end-to-end (E2E) automatic speech recognition (ASR) models have been yielding state-of-the-art recognition accuracy, they still fall behind many of the ASR models deployed in the industry in some crucial functionalities such as online processing and precise timestamps generating. This weakness prevents attention-based E2E ASR models from being applied in several essential speech tasks, such as online speech recognition and keyword searching (KWS). In this paper, we describe our proposed unified attention-based E2E ASR and KWS architecture–ETEH, which supports, in one model, both online and offline ASR decoding modes, thus allowing for precise and reliable KWS. \"ETE\" stands for attention-based E2E modeling, whereas \"H\" represents the hybrid gaussian mixture model and hidden Markov model (GMM-HMM). As a combination of both, ETEH is an attention-based E2E ASR architecture which utilizes the frame-wise time alignment (FTA) generated by GMM-HMM ASR models. This FTA is used to better the model in two ways: first, it helps the monotonic attentions of ETEH models to capture more accurate word time stamps, thus resulting in lower latency for online decoding; second, it helps ETEH models to provide accurate and reliable KWS results. Furthermore, we are able to combine both offline and online modes in one ETEH model and establish a concise system by adopt the universal training strategy. ETEH is functional and unique, and to the best of our knowledge, we can hardly find a comparable single attention-based E2E ASR system as the baseline. To evaluate ASR accuracy and latency for ETEH, we use our previously proposed monotonic truncated attention (MTA) based online CTC/attention (OCA) ASR models as baselines. Experimental results show that ETEH ASR models gain significant improvement in ASR latency compared to the baseline. To evaluate KWS performance, we compare ETEH models with CTC-based KWS models. Results demonstrate that our ETEH models achieve significantly better KWS performance compared to the CTC baselines.",
      "doi": "https://doi.org/10.1109/taslp.2022.3161159",
      "openalex_id": "https://openalex.org/W4221040649",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
      "summary": "Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.",
      "abstract": "Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.",
      "doi": "https://doi.org/10.1109/jstsp.2017.2763455",
      "openalex_id": "https://openalex.org/W2766219058",
      "arxiv_id": "",
      "publication_date": "2017-10-25",
      "published": "2017-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MELP: the new Federal Standard at 2400 bps",
      "summary": "This paper describes the new U.S. Federal Standard at 2400 bps. The mixed excitation linear prediction (MELP) coder was chosen by the DoD Digital Voice Processing Consortium to replace the existing 2400 bps Federal Standard FS 1015 (LPC-10). This new standard provides equal or improved performance over the 4800 bps Federal Standard FS 1016 (CELP) at a rate equivalent to LPC-10. The MELP coder is based on the traditional LPC model, but includes additional features to improve its performance.",
      "abstract": "This paper describes the new U.S. Federal Standard at 2400 bps. The mixed excitation linear prediction (MELP) coder was chosen by the DoD Digital Voice Processing Consortium to replace the existing 2400 bps Federal Standard FS 1015 (LPC-10). This new standard provides equal or improved performance over the 4800 bps Federal Standard FS 1016 (CELP) at a rate equivalent to LPC-10. The MELP coder is based on the traditional LPC model, but includes additional features to improve its performance.",
      "doi": "https://doi.org/10.1109/icassp.1997.596257",
      "openalex_id": "https://openalex.org/W1591492847",
      "arxiv_id": "",
      "publication_date": "2002-11-22",
      "published": "2002-11-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates",
      "summary": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.",
      "abstract": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.",
      "doi": "https://doi.org/10.1109/icassp.1985.1168147",
      "openalex_id": "https://openalex.org/W2151626637",
      "arxiv_id": "",
      "publication_date": "2005-03-23",
      "published": "2005-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Efficient vector quantization of LPC parameters at 24 bits/frame",
      "summary": "For low bit rate speech coding applications, it is important to quantize the LPC parameters accurately using as few bits as possible. Though vector quantizers are more efficient than scalar quantizers, their use for accurate quantization of linear predictive coding (LPC) information (using 24-26 bits/frame) is impeded by their prohibitively high complexity. A split vector quantization approach is used here to overcome the complexity problem. An LPC vector consisting of 10 line spectral frequencies (LSFs) is divided into two parts, and each part is quantized separately using vector quantization. Using the localized spectral sensitivity property of the LSF parameters, a weighted LSF distance measure is proposed. With this distance measure, it is shown that the split vector quantizer can quantize LPC information in 24 bits/frame with an average spectral distortion of 1 dB and less than 2% of the frames having spectral distortion greater than 2 dB. The effect of channel errors on the performance of this quantizer is also investigated and results are reported.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "For low bit rate speech coding applications, it is important to quantize the LPC parameters accurately using as few bits as possible. Though vector quantizers are more efficient than scalar quantizers, their use for accurate quantization of linear predictive coding (LPC) information (using 24-26 bits/frame) is impeded by their prohibitively high complexity. A split vector quantization approach is used here to overcome the complexity problem. An LPC vector consisting of 10 line spectral frequencies (LSFs) is divided into two parts, and each part is quantized separately using vector quantization. Using the localized spectral sensitivity property of the LSF parameters, a weighted LSF distance measure is proposed. With this distance measure, it is shown that the split vector quantizer can quantize LPC information in 24 bits/frame with an average spectral distortion of 1 dB and less than 2% of the frames having spectral distortion greater than 2 dB. The effect of channel errors on the performance of this quantizer is also investigated and results are reported.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/89.221363",
      "openalex_id": "https://openalex.org/W2079337129",
      "arxiv_id": "",
      "publication_date": "1993-01-01",
      "published": "1993-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector quantization: a review",
      "summary": "Vector quantization (VQ) is a very effective way to save bandwidth and storage for speech coding and image coding. Traditional vector quantization methods can be divided into mainly seven types, tree-structured VQ, direct sum VQ, Cartesian product VQ, lattice VQ, classified VQ, feedback VQ, and fuzzy VQ, according to their codebook generation procedures. Over the past decade, quantization-based approximate nearest neighbor (ANN) search has been developing very fast and many methods have emerged for searching images with binary codes in the memory for large-scale datasets. Their most impressive characteristics are the use of multiple codebooks. This leads to the appearance of two kinds of codebook: the linear combination codebook and the joint codebook. This may be a trend for the future. However, these methods are just finding a balance among speed, accuracy, and memory consumption for ANN search, and sometimes one of these three suffers. So, finding a vector quantization method that can strike a balance between speed and accuracy and consume moderately sized memory, is still a problem requiring study.",
      "abstract": "Vector quantization (VQ) is a very effective way to save bandwidth and storage for speech coding and image coding. Traditional vector quantization methods can be divided into mainly seven types, tree-structured VQ, direct sum VQ, Cartesian product VQ, lattice VQ, classified VQ, feedback VQ, and fuzzy VQ, according to their codebook generation procedures. Over the past decade, quantization-based approximate nearest neighbor (ANN) search has been developing very fast and many methods have emerged for searching images with binary codes in the memory for large-scale datasets. Their most impressive characteristics are the use of multiple codebooks. This leads to the appearance of two kinds of codebook: the linear combination codebook and the joint codebook. This may be a trend for the future. However, these methods are just finding a balance among speed, accuracy, and memory consumption for ANN search, and sometimes one of these three suffers. So, finding a vector quantization method that can strike a balance between speed and accuracy and consume moderately sized memory, is still a problem requiring study.",
      "doi": "https://doi.org/10.1631/fitee.1700833",
      "openalex_id": "https://openalex.org/W2946822178",
      "arxiv_id": "",
      "publication_date": "2019-04-01",
      "published": "2019-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "All Information is Necessary: Integrating Speech Positive and Negative Information by Contrastive Learning for Speech Enhancement",
      "summary": "Monaural speech enhancement (SE) is an ill-posed problem due to the irreversible degradation process. Recent methods to achieve SE tasks rely solely on positive information, e.g., ground-truth speech and speech-relevant features. Different from the above, we observe that the negative information, such as original speech mixture and speech-irrelevant features, are valuable to guide the SE model training procedure. In this study, we propose a SE model that integrates both speech positive and negative information for improving SE performance by adopting contrastive learning, in which two innovations have consisted. (1) We design a collaboration module (CM), which contains two parts, contrastive attention for separating relevant and irrelevant features via contrastive learning and interactive attention for establishing the correlation between both speech features in a learnable and self-adaptive manner. (2) We propose a contrastive regularization (CR) built upon contrastive learning to ensure that the estimated speech is pulled closer to the clean speech and pushed far away from the noisy speech in the representation space by integrating self-supervised models. We term the proposed SE network with CM and CR as CMCR-Net. Experimental results demonstrate that our CMCR-Net achieves comparable and superior performance to recent approaches.",
      "abstract": "Monaural speech enhancement (SE) is an ill-posed problem due to the irreversible degradation process. Recent methods to achieve SE tasks rely solely on positive information, e.g., ground-truth speech and speech-relevant features. Different from the above, we observe that the negative information, such as original speech mixture and speech-irrelevant features, are valuable to guide the SE model training procedure. In this study, we propose a SE model that integrates both speech positive and negative information for improving SE performance by adopting contrastive learning, in which two innovations have consisted. (1) We design a collaboration module (CM), which contains two parts, contrastive attention for separating relevant and irrelevant features via contrastive learning and interactive attention for establishing the correlation between both speech features in a learnable and self-adaptive manner. (2) We propose a contrastive regularization (CR) built upon contrastive learning to ensure that the estimated speech is pulled closer to the clean speech and pushed far away from the noisy speech in the representation space by integrating self-supervised models. We term the proposed SE network with CM and CR as CMCR-Net. Experimental results demonstrate that our CMCR-Net achieves comparable and superior performance to recent approaches.",
      "doi": "https://doi.org/10.48550/arxiv.2304.13439",
      "openalex_id": "https://openalex.org/W4367190594",
      "arxiv_id": "",
      "publication_date": "2023-04-26",
      "published": "2023-04-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contrastive Predictive Coding Based Feature for Automatic Speaker Verification",
      "summary": "This thesis describes our ongoing work on Contrastive Predictive Coding (CPC) features for speaker verification. CPC is a recently proposed representation learning framework based on predictive coding and noise contrastive estimation. We focus on incorporating CPC features into the standard automatic speaker verification systems, and we present our methods, experiments, and analysis. This thesis also details necessary background knowledge in past and recent work on automatic speaker verification systems, conventional speech features, and the motivation and techniques behind CPC.",
      "abstract": "This thesis describes our ongoing work on Contrastive Predictive Coding (CPC) features for speaker verification. CPC is a recently proposed representation learning framework based on predictive coding and noise contrastive estimation. We focus on incorporating CPC features into the standard automatic speaker verification systems, and we present our methods, experiments, and analysis. This thesis also details necessary background knowledge in past and recent work on automatic speaker verification systems, conventional speech features, and the motivation and techniques behind CPC.",
      "doi": "https://doi.org/10.48550/arxiv.1904.01575",
      "openalex_id": "https://openalex.org/W2930682606",
      "arxiv_id": "",
      "publication_date": "2019-04-01",
      "published": "2019-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings",
      "summary": "Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.",
      "abstract": "Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.",
      "doi": "https://doi.org/10.21437/chime.2020-1",
      "openalex_id": "https://openalex.org/W3020336359",
      "arxiv_id": "",
      "publication_date": "2020-05-04",
      "published": "2020-05-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Domain Adaptation via Pseudo In-Domain Data Selection",
      "summary": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora – 1 % the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. 1",
      "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora – 1 % the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W1905522558",
      "arxiv_id": "",
      "publication_date": "2011-07-27",
      "published": "2011-07-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Confidence-measure-driven unsupervised incremental adaptation for HMM-based speech recognition",
      "summary": "In this work, we first review the usual ways to take into account confidence measures in unsupervised adaptation and then propose a new unsupervised incremental adaptation based on a ranking of the adaptation data according to their confidence measures. A semi-supervised adaptation process is also proposed: the confidence measure is used to select the main part of the data for unsupervised adaptation and the remaining small part of the data is handled in a supervised mode. Experiments are conducted on a field database. Generic context-dependent phoneme HMMs are adapted to task- and field-specific conditions. These experiments show a significant improvement for unsupervised adaptation when confidence measures are used. In this work, we also show that the adaptation rate (that measures how important adaptation data are considered with respect to prior data) influences a lot the efficiency of the confidence measure in unsupervised adaptation.",
      "abstract": "In this work, we first review the usual ways to take into account confidence measures in unsupervised adaptation and then propose a new unsupervised incremental adaptation based on a ranking of the adaptation data according to their confidence measures. A semi-supervised adaptation process is also proposed: the confidence measure is used to select the main part of the data for unsupervised adaptation and the remaining small part of the data is handled in a supervised mode. Experiments are conducted on a field database. Generic context-dependent phoneme HMMs are adapted to task- and field-specific conditions. These experiments show a significant improvement for unsupervised adaptation when confidence measures are used. In this work, we also show that the adaptation rate (that measures how important adaptation data are considered with respect to prior data) influences a lot the efficiency of the confidence measure in unsupervised adaptation.",
      "doi": "https://doi.org/10.1109/icassp.2001.940841",
      "openalex_id": "https://openalex.org/W1588359339",
      "arxiv_id": "",
      "publication_date": "2002-11-13",
      "published": "2002-11-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised training of acoustic models for large vocabulary continuous speech recognition",
      "summary": "For speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were recorded and transcribed manually for training. Since untranscribed speech is available in various forms these days, the unsupervised training of a speech recognizer on recognized transcriptions is studied. A low-cost recognizer trained with only one hour of manually transcribed speech is used to recognize 72 hours of untranscribed acoustic data. These transcriptions are then used in combination with confidence measures to train an improved recognizer. The effect of confidence measures which are used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Using this method, the recognizer is trained with very little manual effort while losing only 14.3% relative on the Broadcast News '96 and 18.6% relative on the Broadcast News '98 evaluation test sets.",
      "abstract": "For speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were recorded and transcribed manually for training. Since untranscribed speech is available in various forms these days, the unsupervised training of a speech recognizer on recognized transcriptions is studied. A low-cost recognizer trained with only one hour of manually transcribed speech is used to recognize 72 hours of untranscribed acoustic data. These transcriptions are then used in combination with confidence measures to train an improved recognizer. The effect of confidence measures which are used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Using this method, the recognizer is trained with very little manual effort while losing only 14.3% relative on the Broadcast News '96 and 18.6% relative on the Broadcast News '98 evaluation test sets.",
      "doi": "https://doi.org/10.1109/asru.2001.1034648",
      "openalex_id": "https://openalex.org/W2171761326",
      "arxiv_id": "",
      "publication_date": "2005-08-25",
      "published": "2005-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An investigation of deep neural networks for noise robust speech recognition",
      "summary": "Recently, a new acoustic model based on deep neural networks (DNN) has been introduced. While the DNN has generated significant improvements over GMM-based systems on several tasks, there has been no evaluation of the robustness of such systems to environmental distortion. In this paper, we investigate the noise robustness of DNN-based acoustic models and find that they can match state-of-the-art performance on the Aurora 4 task without any explicit noise compensation. This performance can be further improved by incorporating information about the environment into DNN training using a new method called noise-aware training. When combined with the recently proposed dropout training technique, a 7.5% relative improvement over the previously best published result on this task is achieved using only a single decoding pass and no additional decoding complexity compared to a standard DNN.",
      "abstract": "Recently, a new acoustic model based on deep neural networks (DNN) has been introduced. While the DNN has generated significant improvements over GMM-based systems on several tasks, there has been no evaluation of the robustness of such systems to environmental distortion. In this paper, we investigate the noise robustness of DNN-based acoustic models and find that they can match state-of-the-art performance on the Aurora 4 task without any explicit noise compensation. This performance can be further improved by incorporating information about the environment into DNN training using a new method called noise-aware training. When combined with the recently proposed dropout training technique, a 7.5% relative improvement over the previously best published result on this task is achieved using only a single decoding pass and no additional decoding complexity compared to a standard DNN.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639100",
      "openalex_id": "https://openalex.org/W2062164080",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection",
      "summary": "Voice Activity Detection (VAD) is an important preprocessing step in any state-of-the-art speech recognition system.Choosing the right set of features and model architecture can be challenging and is an active area of research.In this paper we propose a novel approach to VAD to tackle both feature and model selection jointly.The proposed method is based on a CLDNN (Convolutional, Long Short-Term Memory, Deep Neural Networks) architecture fed directly with the raw waveform.We show that using the raw waveform allows the neural network to learn features directly for the task at hand, which is more powerful than using log-mel features, specially for noisy environments.In addition, using a CLDNN, which takes advantage of both frequency modeling with the CNN and temporal modeling with LSTM, is a much better model for VAD compared to the DNN.The proposed system achieves over 78% relative improvement in False Alarms (FA) at the operating point of 2% False Rejects (FR) on both clean and noisy conditions compared to a DNN of comparable size trained with log-mel features.In addition, we study the impact of the model size and the learned features to provide a better understanding of the proposed architecture.",
      "abstract": "Voice Activity Detection (VAD) is an important preprocessing step in any state-of-the-art speech recognition system.Choosing the right set of features and model architecture can be challenging and is an active area of research.In this paper we propose a novel approach to VAD to tackle both feature and model selection jointly.The proposed method is based on a CLDNN (Convolutional, Long Short-Term Memory, Deep Neural Networks) architecture fed directly with the raw waveform.We show that using the raw waveform allows the neural network to learn features directly for the task at hand, which is more powerful than using log-mel features, specially for noisy environments.In addition, using a CLDNN, which takes advantage of both frequency modeling with the CNN and temporal modeling with LSTM, is a much better model for VAD compared to the DNN.The proposed system achieves over 78% relative improvement in False Alarms (FA) at the operating point of 2% False Rejects (FR) on both clean and noisy conditions compared to a DNN of comparable size trained with log-mel features.In addition, we study the impact of the model size and the learned features to provide a better understanding of the proposed architecture.",
      "doi": "https://doi.org/10.21437/interspeech.2016-268",
      "openalex_id": "https://openalex.org/W2513345070",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The People's Speech: A Large-Scale Diverse English Speech Recognition\\n Dataset for Commercial Usage",
      "summary": "The People's Speech is a free-to-download 30,000-hour and growing supervised\\nconversational English speech recognition dataset licensed for academic and\\ncommercial usage under CC-BY-SA (with a CC-BY subset). The data is collected\\nvia searching the Internet for appropriately licensed audio data with existing\\ntranscriptions. We describe our data collection methodology and release our\\ndata collection system under the Apache 2.0 license. We show that a model\\ntrained on this dataset achieves a 9.98% word error rate on Librispeech's\\ntest-clean test set.Finally, we discuss the legal and ethical issues\\nsurrounding the creation of a sizable machine learning corpora and plans for\\ncontinued maintenance of the project under MLCommons's sponsorship.\\n",
      "abstract": "The People's Speech is a free-to-download 30,000-hour and growing supervised\\nconversational English speech recognition dataset licensed for academic and\\ncommercial usage under CC-BY-SA (with a CC-BY subset). The data is collected\\nvia searching the Internet for appropriately licensed audio data with existing\\ntranscriptions. We describe our data collection methodology and release our\\ndata collection system under the Apache 2.0 license. We show that a model\\ntrained on this dataset achieves a 9.98% word error rate on Librispeech's\\ntest-clean test set.Finally, we discuss the legal and ethical issues\\nsurrounding the creation of a sizable machine learning corpora and plans for\\ncontinued maintenance of the project under MLCommons's sponsorship.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2111.09344",
      "openalex_id": "https://openalex.org/W4307106469",
      "arxiv_id": "",
      "publication_date": "2021-11-17",
      "published": "2021-11-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Streaming Automatic Speech Recognition with Non-Streaming Model Distillation on Unsupervised Data",
      "summary": "Streaming end-to-end automatic speech recognition (ASR) models are widely used on smart speakers and on-device applications. Since these models are expected to transcribe speech with minimal latency, they are constrained to be causal with no future context, compared to their non-streaming counterparts. Consequently, streaming models usually perform worse than non-streaming models. We propose a novel and effective learning method by leveraging a non-streaming ASR model as a teacher to generate transcripts on an arbitrarily large data set, which is then used to distill knowledge into streaming ASR models. This way, we scale the training of streaming models to up to 3 million hours of YouTube audio. Experiments show that our approach can significantly reduce the word error rate (WER) of RNN-T models not only on LibriSpeech but also on YouTube data in four languages. For example, in French, we are able to reduce the WER by 16.4% relatively to a baseline streaming model by leveraging a non-streaming teacher model trained on the same amount of labeled data as the baseline.",
      "abstract": "Streaming end-to-end automatic speech recognition (ASR) models are widely used on smart speakers and on-device applications. Since these models are expected to transcribe speech with minimal latency, they are constrained to be causal with no future context, compared to their non-streaming counterparts. Consequently, streaming models usually perform worse than non-streaming models. We propose a novel and effective learning method by leveraging a non-streaming ASR model as a teacher to generate transcripts on an arbitrarily large data set, which is then used to distill knowledge into streaming ASR models. This way, we scale the training of streaming models to up to 3 million hours of YouTube audio. Experiments show that our approach can significantly reduce the word error rate (WER) of RNN-T models not only on LibriSpeech but also on YouTube data in four languages. For example, in French, we are able to reduce the WER by 16.4% relatively to a baseline streaming model by leveraging a non-streaming teacher model trained on the same amount of labeled data as the baseline.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9413692",
      "openalex_id": "https://openalex.org/W3160628828",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Intelligent Selection of Language Model Training Data",
      "summary": "We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. 1",
      "abstract": "We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2117278770",
      "arxiv_id": "",
      "publication_date": "2010-07-11",
      "published": "2010-07-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving broadcast news transcription by lightly supervised discriminative training",
      "summary": "We present our experiments on lightly supervised discriminative training with large amounts of broadcast news data for which only closed caption transcriptions are available (TDT data). In particular, we use language models biased to the closed-caption transcripts to recognise the audio data, and the recognised transcripts are then used as the training transcriptions for acoustic model training. A range of experiments that use maximum likelihood (ML) training as well as discriminative training based on either maximum mutual information (MMI) or minimum phone error (MPE) are presented. In a 5xRT broadcast news transcription system that includes adaptation, it is shown that reductions in word error rate (WER) in the range of 1% absolute can be achieved. Finally, some experiments on training data selection are presented to compare different methods of \"filtering\" the transcripts.",
      "abstract": "We present our experiments on lightly supervised discriminative training with large amounts of broadcast news data for which only closed caption transcriptions are available (TDT data). In particular, we use language models biased to the closed-caption transcripts to recognise the audio data, and the recognised transcripts are then used as the training transcriptions for acoustic model training. A range of experiments that use maximum likelihood (ML) training as well as discriminative training based on either maximum mutual information (MMI) or minimum phone error (MPE) are presented. In a 5xRT broadcast news transcription system that includes adaptation, it is shown that reductions in word error rate (WER) in the range of 1% absolute can be achieved. Finally, some experiments on training data selection are presented to compare different methods of \"filtering\" the transcripts.",
      "doi": "https://doi.org/10.1109/icassp.2004.1326091",
      "openalex_id": "https://openalex.org/W2143577772",
      "arxiv_id": "",
      "publication_date": "2004-09-28",
      "published": "2004-09-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil",
      "summary": "Speech evaluation is an essential component in computerassisted language learning (CALL).While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging.Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English.Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm.In this work, we examine three different languages that possess distinct rhythm patterns: English (stresstimed), Malay (syllable-timed), and Tamil (mora-timed).We exploit robust feature representations inspired by music processing and vector representation learning.Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance.",
      "abstract": "Speech evaluation is an essential component in computerassisted language learning (CALL).While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging.Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English.Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm.In this work, we examine three different languages that possess distinct rhythm patterns: English (stresstimed), Malay (syllable-timed), and Tamil (mora-timed).We exploit robust feature representations inspired by music processing and vector representation learning.Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1258",
      "openalex_id": "https://openalex.org/W3196891430",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NN-based Ordinal Regression for Assessing Fluency of ESL Speech",
      "summary": "Automatic assessment of a language learner's speech fluency is highly desirable for language education, e.g. for English as a Second Language (ESL) learning. In this paper, we formulate the fluency assessment as a problem of Ordinal Regression with Anchored Reference Samples (ORARS), where the fluency of a speech utterance is predicted by an ordinal regression neural network (NN) trained with anchored reference samples. The ORARS is trained and tested by: picking human expert labeled samples in each mean opinion score (MOS) bucket as the anchored reference samples and pairing them with input speech samples as training couplets; training an NN-based binary classifier to determine which sample in a pair is better in fluency; predicting the rank (MOS) of a test sample based upon the posteriors of all binary comparisons between the test sample and all anchored reference samples. Experimentally, our proposed approach outperforms the traditional NN-based methods and reaches a performance of \"human parity\", i.e. as comparable as human experts, in its fluency assessment of collected ESL speech. To the best of our knowledge, this is the first attempt to assess speech fluency with an ordinal regression framework where a test input is paired with bucketed and anchored reference samples.",
      "abstract": "Automatic assessment of a language learner's speech fluency is highly desirable for language education, e.g. for English as a Second Language (ESL) learning. In this paper, we formulate the fluency assessment as a problem of Ordinal Regression with Anchored Reference Samples (ORARS), where the fluency of a speech utterance is predicted by an ordinal regression neural network (NN) trained with anchored reference samples. The ORARS is trained and tested by: picking human expert labeled samples in each mean opinion score (MOS) bucket as the anchored reference samples and pairing them with input speech samples as training couplets; training an NN-based binary classifier to determine which sample in a pair is better in fluency; predicting the rank (MOS) of a test sample based upon the posteriors of all binary comparisons between the test sample and all anchored reference samples. Experimentally, our proposed approach outperforms the traditional NN-based methods and reaches a performance of \"human parity\", i.e. as comparable as human experts, in its fluency assessment of collected ESL speech. To the best of our knowledge, this is the first attempt to assess speech fluency with an ordinal regression framework where a test input is paired with bucketed and anchored reference samples.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682187",
      "openalex_id": "https://openalex.org/W2935807810",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Complexity, Accuracy, and Fluency in Second Language Acquisition",
      "summary": "Journal Article Complexity, Accuracy, and Fluency in Second Language Acquisition Get access Alex Housen, Alex Housen 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Folkert Kuiken Folkert Kuiken 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Applied Linguistics, Volume 30, Issue 4, December 2009, Pages 461–473, https://doi.org/10.1093/applin/amp048 Published: 02 December 2009",
      "abstract": "Journal Article Complexity, Accuracy, and Fluency in Second Language Acquisition Get access Alex Housen, Alex Housen 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Folkert Kuiken Folkert Kuiken 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Applied Linguistics, Volume 30, Issue 4, December 2009, Pages 461–473, https://doi.org/10.1093/applin/amp048 Published: 02 December 2009",
      "doi": "https://doi.org/10.1093/applin/amp048",
      "openalex_id": "https://openalex.org/W2121884006",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A new statistical approach for the automatic segmentation of continuous speech signals",
      "summary": "A statistical approach for the segmentation of a continuous speech signal to detect acoustic events is presented. Experiments are carried out to test the segmentation algorithms. Reasonable results are obtained with speech signals, although these are not exactly piecewise stationary. A comparison between the experimental results of automatic and handmade segmentations, demonstrates the potential acoustic-phonetic classification capability of the proposed algorithms.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "A statistical approach for the segmentation of a continuous speech signal to detect acoustic events is presented. Experiments are carried out to test the segmentation algorithms. Reasonable results are obtained with speech signals, although these are not exactly piecewise stationary. A comparison between the experimental results of automatic and handmade segmentations, demonstrates the potential acoustic-phonetic classification capability of the proposed algorithms.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/29.1486",
      "openalex_id": "https://openalex.org/W2112446559",
      "arxiv_id": "",
      "publication_date": "1988-01-01",
      "published": "1988-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep-FSMN for Large Vocabulary Continuous Speech Recognition",
      "summary": "In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers. These skip connections enable the information flow across different layers and thus alleviate the gradient vanishing problem when building very deep structure. As a result, DFSMN significantly benefits from these skip connections and deep structure. We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin. Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units. In the 20000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5% absolute improvement compared to the BLSTM. In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20% relative improvement compared to the LFR trained BLSTM. Moreover, we can easily design the lookahead filter order of the memory blocks in DFSMN to control the latency for real-time applications.",
      "abstract": "In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers. These skip connections enable the information flow across different layers and thus alleviate the gradient vanishing problem when building very deep structure. As a result, DFSMN significantly benefits from these skip connections and deep structure. We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin. Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units. In the 20000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5% absolute improvement compared to the BLSTM. In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20% relative improvement compared to the LFR trained BLSTM. Moreover, we can easily design the lookahead filter order of the memory blocks in DFSMN to control the latency for real-time applications.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461404",
      "openalex_id": "https://openalex.org/W2963308316",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment",
      "summary": "This paper introduces a new open-source speech corpus named \"speechocean762\" designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children.Five experts annotated each of the utterances at sentence-level, wordlevel and phoneme-level.A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus.This corpus is allowed to be used freely for commercial and non-commercial purposes.It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit.",
      "abstract": "This paper introduces a new open-source speech corpus named \"speechocean762\" designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children.Five experts annotated each of the utterances at sentence-level, wordlevel and phoneme-level.A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus.This corpus is allowed to be used freely for commercial and non-commercial purposes.It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1259",
      "openalex_id": "https://openalex.org/W3197742413",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Fluency Evaluation of Spontaneous Speech Using Disfluency-Based Features",
      "summary": "This paper describes an automatic fluency evaluation of spontaneous speech. Although we regularly observe a variety of different disfluencies in spontaneous speech, we focus on two types of phenomena, i.e., filled pauses and word fragments. This paper aims to reveal that these two types of disfluencies have effects on speech fluency evaluation differently. To this end, we conduct a series of SVM classification experiments on the Japanese spontaneous speech corpus. The experimental results show that the features derived from word fragments are effective in evaluating disfluent speech especially when combined with prosodic features such as speech rate and pauses/silence, while the features from filled pauses are not effective in evaluating fluency.",
      "abstract": "This paper describes an automatic fluency evaluation of spontaneous speech. Although we regularly observe a variety of different disfluencies in spontaneous speech, we focus on two types of phenomena, i.e., filled pauses and word fragments. This paper aims to reveal that these two types of disfluencies have effects on speech fluency evaluation differently. To this end, we conduct a series of SVM classification experiments on the Japanese spontaneous speech corpus. The experimental results show that the features derived from word fragments are effective in evaluating disfluent speech especially when combined with prosodic features such as speech rate and pauses/silence, while the features from filled pauses are not effective in evaluating fluency.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053452",
      "openalex_id": "https://openalex.org/W3016114816",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech",
      "summary": "This paper describes two experiments aimed at exploring the relationship between objective properties of speech and perceived fluency in read and spontaneous speech. The aim is to determine whether such quantitative measures can be used to develop objective fluency tests. Fragments of read speech (Experiment 1) of 60 non-native speakers of Dutch and of spontaneous speech (Experiment 2) of another group of 57 non-native speakers of Dutch were scored for fluency by human raters and were analyzed by means of a continuous speech recognizer to calculate a number of objective measures of speech quality known to be related to perceived fluency. The results show that the objective measures investigated in this study can be employed to predict fluency ratings, but the predictive power of such measures is stronger for read speech than for spontaneous speech. Moreover, the adequacy of the variables to be employed appears to be dependent on the specific type of speech material investigated and the specific task performed by the speaker.",
      "abstract": "This paper describes two experiments aimed at exploring the relationship between objective properties of speech and perceived fluency in read and spontaneous speech. The aim is to determine whether such quantitative measures can be used to develop objective fluency tests. Fragments of read speech (Experiment 1) of 60 non-native speakers of Dutch and of spontaneous speech (Experiment 2) of another group of 57 non-native speakers of Dutch were scored for fluency by human raters and were analyzed by means of a continuous speech recognizer to calculate a number of objective measures of speech quality known to be related to perceived fluency. The results show that the objective measures investigated in this study can be employed to predict fluency ratings, but the predictive power of such measures is stronger for read speech than for spontaneous speech. Moreover, the adequacy of the variables to be employed appears to be dependent on the specific type of speech material investigated and the specific task performed by the speaker.",
      "doi": "https://doi.org/10.1121/1.1471894",
      "openalex_id": "https://openalex.org/W2069029003",
      "arxiv_id": "",
      "publication_date": "2002-06-01",
      "published": "2002-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Developing Speech Recognition and Synthesis Technologies to Support Computer-Aided Pronunciation Training for Chinese Learners of English",
      "summary": "Abstract. We describe ongoing research in the development of speech technologies that strives to raise the efficacy of computer-aided pronunciation training, especially for Chinese learners of English. Our approach is grounded on the theory of language transfer and involves a systematic phonological comparison between the primary language (L1 being Chinese) and secondary language (L2 being English) to predict possible segmental and suprasegmental realizations that constitute mispronunciations in L2 English. The predictions are validated based on a specially designed corpus that consists of several hundred hours of L2 English speech. The speech data supports the development of automatic speech recognition technologies that can detect and diagnose mispronunciations. The diagnosis aims to support the design of pedagogical and remedial instructions, which involves text-tospeech synthesis technologies in audiovisual forms. 1",
      "abstract": "Abstract. We describe ongoing research in the development of speech technologies that strives to raise the efficacy of computer-aided pronunciation training, especially for Chinese learners of English. Our approach is grounded on the theory of language transfer and involves a systematic phonological comparison between the primary language (L1 being Chinese) and secondary language (L2 being English) to predict possible segmental and suprasegmental realizations that constitute mispronunciations in L2 English. The predictions are validated based on a specially designed corpus that consists of several hundred hours of L2 English speech. The speech data supports the development of automatic speech recognition technologies that can detect and diagnose mispronunciations. The diagnosis aims to support the design of pedagogical and remedial instructions, which involves text-tospeech synthesis technologies in audiovisual forms. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2155774313",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Statistical Significance Tests for Machine Translation Evaluation.",
      "summary": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",
      "abstract": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",
      "doi": "",
      "openalex_id": "https://openalex.org/W222053410",
      "arxiv_id": "",
      "publication_date": "2004-07-01",
      "published": "2004-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tagged Back-translation Revisited: Why Does It Really Work?",
      "summary": "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.",
      "abstract": "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.532",
      "openalex_id": "https://openalex.org/W3034474651",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Curriculum Pre-training for End-to-End Speech Translation",
      "summary": "End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",
      "abstract": "End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.344",
      "openalex_id": "https://openalex.org/W3034571331",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation",
      "summary": "End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.",
      "abstract": "End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6452",
      "openalex_id": "https://openalex.org/W2997436923",
      "arxiv_id": "",
      "publication_date": "2020-04-03",
      "published": "2020-04-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Consecutive Decoding for Speech-to-text Translation",
      "summary": "Speech-to-text translation (ST), which directly translates the source language speech to the target language text, has attracted intensive attention recently. However, the combination of speech recognition and machine translation in a single model poses a heavy burden on the direct cross-modal cross-lingual mapping. To reduce the learning difficulty, we propose COnSecutive Transcription and Translation (COSTT), an integral approach for speech-to-text translation. The key idea is to generate source transcript and target translation text with a single decoder. It benefits the model training so that additional large parallel text corpus can be fully exploited to enhance the speech translation training. Our method is verified on three mainstream datasets, including Augmented LibriSpeech English-French dataset, TED English-German dataset, and TED English-Chinese dataset. Experiments show that our proposed COSTT outperforms the previous state-of-the-art methods. The code is available at https://github.com/dqqcasia/st.",
      "abstract": "Speech-to-text translation (ST), which directly translates the source language speech to the target language text, has attracted intensive attention recently. However, the combination of speech recognition and machine translation in a single model poses a heavy burden on the direct cross-modal cross-lingual mapping. To reduce the learning difficulty, we propose COnSecutive Transcription and Translation (COSTT), an integral approach for speech-to-text translation. The key idea is to generate source transcript and target translation text with a single decoder. It benefits the model training so that additional large parallel text corpus can be fully exploited to enhance the speech translation training. Our method is verified on three mainstream datasets, including Augmented LibriSpeech English-French dataset, TED English-German dataset, and TED English-Chinese dataset. Experiments show that our proposed COSTT outperforms the previous state-of-the-art methods. The code is available at https://github.com/dqqcasia/st.",
      "doi": "https://doi.org/10.1609/aaai.v35i14.17508",
      "openalex_id": "https://openalex.org/W3113676066",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effectively pretraining a speech translation decoder with Machine Translation data",
      "summary": "Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",
      "abstract": "Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.644",
      "openalex_id": "https://openalex.org/W3105825505",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Findings of the 2019 Conference on Machine Translation (WMT19)",
      "summary": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.",
      "abstract": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.",
      "doi": "https://doi.org/10.5167/uzh-176407",
      "openalex_id": "https://openalex.org/W3120929527",
      "arxiv_id": "",
      "publication_date": "2019-08-02",
      "published": "2019-08-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Listen, Understand and Translate: Triple Supervision Decouples End-to-end Speech-to-text Translation",
      "summary": "An end-to-end speech-to-text translation (ST) takes audio in a source language and outputs the text in a target language. Existing methods are limited by the amount of parallel corpus. Can we build a system to fully utilize signals in a parallel ST corpus? We are inspired by human understanding system which is composed of auditory perception and cognitive processing. In this paper, we propose Listen-Understand-Translate, (LUT), a unified framework with triple supervision signals to decouple the end-to-end speech-to-text translation task. LUT is able to guide the acoustic encoder to extract as much information from the auditory input. In addition, LUT utilizes a pre-trained BERT model to enforce the upper encoder to produce as much semantic information as possible, without extra data. We perform experiments on a diverse set of speech translation benchmarks, including Librispeech English-French, IWSLT English-German and TED English-Chinese. Our results demonstrate LUT achieves the state-of-the-art performance, outperforming previous methods. The code is available at https://github.com/dqqcasia/st.",
      "abstract": "An end-to-end speech-to-text translation (ST) takes audio in a source language and outputs the text in a target language. Existing methods are limited by the amount of parallel corpus. Can we build a system to fully utilize signals in a parallel ST corpus? We are inspired by human understanding system which is composed of auditory perception and cognitive processing. In this paper, we propose Listen-Understand-Translate, (LUT), a unified framework with triple supervision signals to decouple the end-to-end speech-to-text translation task. LUT is able to guide the acoustic encoder to extract as much information from the auditory input. In addition, LUT utilizes a pre-trained BERT model to enforce the upper encoder to produce as much semantic information as possible, without extra data. We perform experiments on a diverse set of speech translation benchmarks, including Librispeech English-French, IWSLT English-German and TED English-Chinese. Our results demonstrate LUT achieves the state-of-the-art performance, outperforming previous methods. The code is available at https://github.com/dqqcasia/st.",
      "doi": "https://doi.org/10.1609/aaai.v35i14.17509",
      "openalex_id": "https://openalex.org/W3113908264",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dynamic Data Selection and Weighting for Iterative Back-Translation",
      "summary": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",
      "abstract": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.475",
      "openalex_id": "https://openalex.org/W3103169714",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation",
      "summary": "We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.",
      "abstract": "We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.",
      "doi": "https://doi.org/10.18653/v1/2020.coling-main.314",
      "openalex_id": "https://openalex.org/W3102811925",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Revisiting End-to-End Speech-to-Text Translation From Scratch",
      "summary": "End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speech-translation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.",
      "abstract": "End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speech-translation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.",
      "doi": "https://doi.org/10.48550/arxiv.2206.04571",
      "openalex_id": "https://openalex.org/W4281982771",
      "arxiv_id": "",
      "publication_date": "2022-06-09",
      "published": "2022-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Synthetic Data for Back Translation",
      "summary": "Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: {\\em what kind of synthetic data contributes to BT performance?} Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.",
      "abstract": "Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: {\\em what kind of synthetic data contributes to BT performance?} Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.32",
      "openalex_id": "https://openalex.org/W4287854398",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Findings of the WMT 2016 Bilingual Document Alignment Shared Task",
      "summary": "This paper presents the results of the WMT16 Bilingual Document Alignment Shared Task.Given crawls of web sites, we asked participants to align documents that are translations of each other.11 research groups submitted 19 systems, with a top performance of 95.0%.",
      "abstract": "This paper presents the results of the WMT16 Bilingual Document Alignment Shared Task.Given crawls of web sites, we asked participants to align documents that are translations of each other.11 research groups submitted 19 systems, with a top performance of 95.0%.",
      "doi": "https://doi.org/10.18653/v1/w16-2347",
      "openalex_id": "https://openalex.org/W2508809683",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iterative Back-Translation for Neural Machine Translation",
      "summary": "We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 German↔English tasks.<br/>",
      "abstract": "We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 German↔English tasks.<br/>",
      "doi": "https://doi.org/10.18653/v1/w18-2703",
      "openalex_id": "https://openalex.org/W2886095922",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding Back-Translation at Scale",
      "summary": "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.",
      "abstract": "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.",
      "doi": "https://doi.org/10.18653/v1/d18-1045",
      "openalex_id": "https://openalex.org/W2889326796",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generalized End-to-End Loss for Speaker Verification",
      "summary": "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., \"OK Google\" and \"Hey Google\") as well as multiple dialects.",
      "abstract": "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., \"OK Google\" and \"Hey Google\") as well as multiple dialects.",
      "doi": "https://doi.org/10.1109/icassp.2018.8462665",
      "openalex_id": "https://openalex.org/W2962788625",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Comparison between Pre-training and Large-scale Back-translation for Neural Machine Translation",
      "summary": "BERT has been studied as a promising technique to improve NMT.Given that BERT is based on the similar Transformer architecture to NMT and the current datasets for most MT tasks are rather large, how pre-training has managed to outperform standard Transformer NMT models is underestimated.We compare MT engines trained with pre-trained BERT and back-translation with incrementally larger amounts of data, implementing the two most widely-used monolingual paradigms.We analyze their strengths and weaknesses based on both standard automatic metrics and intrinsic test suites that comprise a large range of linguistic phenomena.Primarily, we find that 1) BERT has limited advantages compared with large-scale back-translation in accuracy and consistency on morphology and syntax; 2) BERT can boost the Transformer baseline in semantic and pragmatic tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness.",
      "abstract": "BERT has been studied as a promising technique to improve NMT.Given that BERT is based on the similar Transformer architecture to NMT and the current datasets for most MT tasks are rather large, how pre-training has managed to outperform standard Transformer NMT models is underestimated.We compare MT engines trained with pre-trained BERT and back-translation with incrementally larger amounts of data, implementing the two most widely-used monolingual paradigms.We analyze their strengths and weaknesses based on both standard automatic metrics and intrinsic test suites that comprise a large range of linguistic phenomena.Primarily, we find that 1) BERT has limited advantages compared with large-scale back-translation in accuracy and consistency on morphology and syntax; 2) BERT can boost the Transformer baseline in semantic and pragmatic tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness.",
      "doi": "https://doi.org/10.18653/v1/2021.findings-acl.150",
      "openalex_id": "https://openalex.org/W3173666333",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders",
      "summary": "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "abstract": "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-long.204",
      "openalex_id": "https://openalex.org/W3176382501",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation",
      "summary": "Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT.",
      "abstract": "Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT.",
      "doi": "https://doi.org/10.18653/v1/2021.findings-emnlp.247",
      "openalex_id": "https://openalex.org/W3202201199",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints",
      "summary": "Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don’t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.",
      "abstract": "Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don’t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.129",
      "openalex_id": "https://openalex.org/W3196292088",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Task Aware Multi-Task Learning for Speech to Text Tasks",
      "summary": "In general, the direct Speech-to-text translation (ST) is jointly trained with Automatic Speech Recognition (ASR), and Machine Translation (MT) tasks. However, the issues with the current joint learning strategies inhibit the knowledge transfer across these tasks. We propose a task modulation network which allows the model to learn task specific features, while learning the shared features simultaneously. This proposed approach removes the need for separate finetuning step resulting in a single model which performs all these tasks. This single model achieves a performance of 28.64 BLEU score on ST MuST-C English-German, WER of 11.61% on ASR TEDLium v3, 23.35 BLEU score on MT WMT'15 English-German task. This sets a new state-of-the-art performance (SOTA) on the ST task while outperforming the existing end-to-end ASR systems.",
      "abstract": "In general, the direct Speech-to-text translation (ST) is jointly trained with Automatic Speech Recognition (ASR), and Machine Translation (MT) tasks. However, the issues with the current joint learning strategies inhibit the knowledge transfer across these tasks. We propose a task modulation network which allows the model to learn task specific features, while learning the shared features simultaneously. This proposed approach removes the need for separate finetuning step resulting in a single model which performs all these tasks. This single model achieves a performance of 28.64 BLEU score on ST MuST-C English-German, WER of 11.61% on ASR TEDLium v3, 23.35 BLEU score on MT WMT'15 English-German task. This sets a new state-of-the-art performance (SOTA) on the ST task while outperforming the existing end-to-end ASR systems.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414703",
      "openalex_id": "https://openalex.org/W3162471442",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
      "summary": "Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data.This is important but challenging owing to the requirement of learning multiple mappings and the nonavailability of explicit supervision.Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator.However, there is still a gap between real and converted speech.To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2.Particularly, we rethink conditional methods in two aspects: training objectives and network architectures.For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data.For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner.We evaluated our methods on non-parallel multi-speaker VC.An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures.Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity.",
      "abstract": "Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data.This is important but challenging owing to the requirement of learning multiple mappings and the nonavailability of explicit supervision.Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator.However, there is still a gap between real and converted speech.To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2.Particularly, we rethink conditional methods in two aspects: training objectives and network architectures.For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data.For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner.We evaluated our methods on non-parallel multi-speaker VC.An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures.Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2236",
      "openalex_id": "https://openalex.org/W2972667718",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What Makes for Good Views for Contrastive Learning?",
      "summary": "Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",
      "abstract": "Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",
      "doi": "https://doi.org/10.48550/arxiv.2005.10243",
      "openalex_id": "https://openalex.org/W3026092005",
      "arxiv_id": "",
      "publication_date": "2020-05-20",
      "published": "2020-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Pre-training with Acoustic Piece",
      "summary": "Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called \"acoustic piece\"s, are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method \"HuBERT-AP\" significantly outperforms strong baselines on the LibriSpeech ASR task.",
      "abstract": "Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called \"acoustic piece\"s, are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method \"HuBERT-AP\" significantly outperforms strong baselines on the LibriSpeech ASR task.",
      "doi": "https://doi.org/10.21437/interspeech.2022-981",
      "openalex_id": "https://openalex.org/W4226507725",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
      "summary": "Speech embeddings are fixed-size acoustic representations of variable-length\\nspeech sequences. They are increasingly used for a variety of tasks ranging\\nfrom information retrieval to unsupervised term discovery and speech\\nsegmentation. However, there is currently no clear methodology to compare or\\noptimise the quality of these embeddings in a task-neutral way. Here, we\\nsystematically compare two popular metrics, ABX discrimination and Mean Average\\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\\nsupervised to fully unsupervised, and using different loss functions\\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\\nMAP to predict performances on a new downstream task: the unsupervised\\nestimation of the frequencies of speech segments in a given corpus. We find\\nthat overall, ABX and MAP correlate with one another and with frequency\\nestimation. However, substantial discrepancies appear in the fine-grained\\ndistinctions across languages and/or embedding methods. This makes it\\nunrealistic at present to propose a task-independent silver bullet method for\\ncomputing the intrinsic quality of speech embeddings. There is a need for more\\ndetailed analysis of the metrics currently used to evaluate such embeddings.\\n",
      "abstract": "Speech embeddings are fixed-size acoustic representations of variable-length\\nspeech sequences. They are increasingly used for a variety of tasks ranging\\nfrom information retrieval to unsupervised term discovery and speech\\nsegmentation. However, there is currently no clear methodology to compare or\\noptimise the quality of these embeddings in a task-neutral way. Here, we\\nsystematically compare two popular metrics, ABX discrimination and Mean Average\\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\\nsupervised to fully unsupervised, and using different loss functions\\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\\nMAP to predict performances on a new downstream task: the unsupervised\\nestimation of the frequencies of speech segments in a given corpus. We find\\nthat overall, ABX and MAP correlate with one another and with frequency\\nestimation. However, substantial discrepancies appear in the fine-grained\\ndistinctions across languages and/or embedding methods. This makes it\\nunrealistic at present to propose a task-independent silver bullet method for\\ncomputing the intrinsic quality of speech embeddings. There is a need for more\\ndetailed analysis of the metrics currently used to evaluate such embeddings.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-2362",
      "openalex_id": "https://openalex.org/W3044967013",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analyzing analytical methods: The case of phonology in neural models of\\n spoken language",
      "summary": "Given the fast development of analysis techniques for NLP and speech\\nprocessing systems, few systematic studies have been conducted to compare the\\nstrengths and weaknesses of each method. As a step in this direction we study\\nthe case of representations of phonology in neural network models of spoken\\nlanguage. We use two commonly applied analytical techniques, diagnostic\\nclassifiers and representational similarity analysis, to quantify to what\\nextent neural activation patterns encode phonemes and phoneme sequences. We\\nmanipulate two factors that can affect the outcome of analysis. First, we\\ninvestigate the role of learning by comparing neural activations extracted from\\ntrained versus randomly-initialized models. Second, we examine the temporal\\nscope of the activations by probing both local activations corresponding to a\\nfew milliseconds of the speech signal, and global activations pooled over the\\nwhole utterance. We conclude that reporting analysis results with randomly\\ninitialized models is crucial, and that global-scope methods tend to yield more\\nconsistent results and we recommend their use as a complement to local-scope\\ndiagnostic methods.\\n",
      "abstract": "Given the fast development of analysis techniques for NLP and speech\\nprocessing systems, few systematic studies have been conducted to compare the\\nstrengths and weaknesses of each method. As a step in this direction we study\\nthe case of representations of phonology in neural network models of spoken\\nlanguage. We use two commonly applied analytical techniques, diagnostic\\nclassifiers and representational similarity analysis, to quantify to what\\nextent neural activation patterns encode phonemes and phoneme sequences. We\\nmanipulate two factors that can affect the outcome of analysis. First, we\\ninvestigate the role of learning by comparing neural activations extracted from\\ntrained versus randomly-initialized models. Second, we examine the temporal\\nscope of the activations by probing both local activations corresponding to a\\nfew milliseconds of the speech signal, and global activations pooled over the\\nwhole utterance. We conclude that reporting analysis results with randomly\\ninitialized models is crucial, and that global-scope methods tend to yield more\\nconsistent results and we recommend their use as a complement to local-scope\\ndiagnostic methods.\\n",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.381",
      "openalex_id": "https://openalex.org/W3035750922",
      "arxiv_id": "",
      "publication_date": "2020-04-15",
      "published": "2020-04-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding and Improving Word Embeddings through a Neuroscientific Lens",
      "summary": "Abstract Despite the success of models making use of word embeddings on many natural language tasks, these models often perform significantly worse than humans on several natural language understanding tasks. This difference in performance motivates us to ask: (1) if existing word vector representations have any basis in the brain’s representational structure for individual words, and (2) whether features from the brain can be used to improve word embedding model performance, defined as their correlation with human semantic judgements. To answer the first question, we compare the representational spaces of existing word embedding models with that of brain imaging data through representational similarity analysis. We answer the second question by using regression-based learning to constrain word vectors to the features of the brain imaging data, thereby determining if these modified word vectors exhibit increased performance over their unmodified counterparts. To collect semantic judgements as a measure of performance, we employed a novel multi-arrangement method. Our results show that there is variance in the representational space of the brain imaging data that remains uncaptured by word embedding models, and that brain imaging data can be used to increase their coherence with human performance.",
      "abstract": "Abstract Despite the success of models making use of word embeddings on many natural language tasks, these models often perform significantly worse than humans on several natural language understanding tasks. This difference in performance motivates us to ask: (1) if existing word vector representations have any basis in the brain’s representational structure for individual words, and (2) whether features from the brain can be used to improve word embedding model performance, defined as their correlation with human semantic judgements. To answer the first question, we compare the representational spaces of existing word embedding models with that of brain imaging data through representational similarity analysis. We answer the second question by using regression-based learning to constrain word vectors to the features of the brain imaging data, thereby determining if these modified word vectors exhibit increased performance over their unmodified counterparts. To collect semantic judgements as a measure of performance, we employed a novel multi-arrangement method. Our results show that there is variance in the representational space of the brain imaging data that remains uncaptured by word embedding models, and that brain imaging data can be used to increase their coherence with human performance.",
      "doi": "https://doi.org/10.1101/2020.09.18.304436",
      "openalex_id": "https://openalex.org/W3087357110",
      "arxiv_id": "",
      "publication_date": "2020-09-20",
      "published": "2020-09-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How agents see things: On visual representations in an emergent language game",
      "summary": "There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents’ symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.",
      "abstract": "There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents’ symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.",
      "doi": "https://doi.org/10.18653/v1/d18-1119",
      "openalex_id": "https://openalex.org/W2888912391",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "From Audio to Semantics: Approaches to End-to-End Spoken Language Understanding",
      "summary": "Conventional spoken language understanding systems consist of two main components: an automatic speech recognition module that converts audio to a transcript, and a natural language understanding module that transforms the resulting text (or top N hypotheses) into a set of domains, intents, and arguments. These modules are typically optimized independently. In this paper, we formulate audio to semantic understanding as a sequence-to-sequence problem [1]. We propose and compare various encoder-decoder based approaches that optimize both modules jointly, in an end-to-end manner. Evaluations on a real-world task show that 1) having an intermediate text representation is crucial for the quality of the predicted semantics, especially the intent arguments and 2) jointly optimizing the full system improves overall accuracy of prediction. Compared to independently trained models, our best jointly trained model achieves similar domain and intent prediction F1 scores, but improves argument word error rate by 18% relative.",
      "abstract": "Conventional spoken language understanding systems consist of two main components: an automatic speech recognition module that converts audio to a transcript, and a natural language understanding module that transforms the resulting text (or top N hypotheses) into a set of domains, intents, and arguments. These modules are typically optimized independently. In this paper, we formulate audio to semantic understanding as a sequence-to-sequence problem [1]. We propose and compare various encoder-decoder based approaches that optimize both modules jointly, in an end-to-end manner. Evaluations on a real-world task show that 1) having an intermediate text representation is crucial for the quality of the predicted semantics, especially the intent arguments and 2) jointly optimizing the full system improves overall accuracy of prediction. Compared to independently trained models, our best jointly trained model achieves similar domain and intent prediction F1 scores, but improves argument word error rate by 18% relative.",
      "doi": "https://doi.org/10.1109/slt.2018.8639043",
      "openalex_id": "https://openalex.org/W2894164357",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust Locally Weighted Regression and Smoothing Scatterplots",
      "summary": "Abstract The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (x i , y i ), i = 1, …, n, in which the fitted value at z k is the value of a polynomial fit to the data using weighted least squares, where the weight for (x i , y i ) is large if x i is close to x k and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.",
      "abstract": "Abstract The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (x i , y i ), i = 1, …, n, in which the fitted value at z k is the value of a polynomial fit to the data using weighted least squares, where the weight for (x i , y i ) is large if x i is close to x k and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.",
      "doi": "https://doi.org/10.1080/01621459.1979.10481038",
      "openalex_id": "https://openalex.org/W2024081693",
      "arxiv_id": "",
      "publication_date": "1979-12-01",
      "published": "1979-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep multimodal semantic embeddings for speech and images",
      "summary": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.",
      "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.",
      "doi": "https://doi.org/10.1109/asru.2015.7404800",
      "openalex_id": "https://openalex.org/W2137010615",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representational similarity analysis – connecting the branches of systems neuroscience",
      "summary": "A FUNDAMENTAL CHALLENGE FOR SYSTEMS NEUROSCIENCE IS TO QUANTITATIVELY RELATE ITS THREE MAJOR BRANCHES OF RESEARCH: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.",
      "abstract": "A FUNDAMENTAL CHALLENGE FOR SYSTEMS NEUROSCIENCE IS TO QUANTITATIVELY RELATE ITS THREE MAJOR BRANCHES OF RESEARCH: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.",
      "doi": "https://doi.org/10.3389/neuro.06.004.2008",
      "openalex_id": "https://openalex.org/W2160654481",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
      "summary": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.",
      "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.",
      "doi": "https://doi.org/10.48550/arxiv.1608.04207",
      "openalex_id": "https://openalex.org/W2515741950",
      "arxiv_id": "",
      "publication_date": "2016-08-15",
      "published": "2016-08-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Correlating Neural and Symbolic Representations of Language",
      "summary": "Analysis methods which enable us to better understand the representations and\\nfunctioning of neural models of language are increasingly needed as deep\\nlearning becomes the dominant approach in NLP. Here we present two methods\\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\\nallow us to directly quantify how strongly the information encoded in neural\\nactivation patterns corresponds to information represented by symbolic\\nstructures such as syntax trees. We first validate our methods on the case of a\\nsimple synthetic language for arithmetic expressions with clearly defined\\nsyntax and semantics, and show that they exhibit the expected pattern of\\nresults. We then apply our methods to correlate neural representations of\\nEnglish sentences with their constituency parse trees.\\n",
      "abstract": "Analysis methods which enable us to better understand the representations and\\nfunctioning of neural models of language are increasingly needed as deep\\nlearning becomes the dominant approach in NLP. Here we present two methods\\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\\nallow us to directly quantify how strongly the information encoded in neural\\nactivation patterns corresponds to information represented by symbolic\\nstructures such as syntax trees. We first validate our methods on the case of a\\nsimple synthetic language for arithmetic expressions with clearly defined\\nsyntax and semantics, and show that they exhibit the expected pattern of\\nresults. We then apply our methods to correlate neural representations of\\nEnglish sentences with their constituency parse trees.\\n",
      "doi": "https://doi.org/10.18653/v1/p19-1283",
      "openalex_id": "https://openalex.org/W2946296745",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties",
      "summary": "Comunicació presentada a: 56th Annual Meeting of the Association for Computational Linguistics celebrat del 15 al 20 de juliol de 2018 a Melbourne, Australia.",
      "abstract": "Comunicació presentada a: 56th Annual Meeting of the Association for Computational Linguistics celebrat del 15 al 20 de juliol de 2018 a Melbourne, Australia.",
      "doi": "https://doi.org/10.18653/v1/p18-1198",
      "openalex_id": "https://openalex.org/W2964204621",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Blackbox Meets Blackbox: Representational Similarity &amp; Stability Analysis of Neural Language Models and Brains",
      "summary": "In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al./ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.",
      "abstract": "In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al./ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.",
      "doi": "https://doi.org/10.18653/v1/w19-4820",
      "openalex_id": "https://openalex.org/W2973047874",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice conversion in high-order eigen space using deep belief nets",
      "summary": "This paper presents a voice conversion technique using Deep Belief Nets (DBNs) to build high-order eigen spaces of the source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. DBNs have a deep architecture that automatically discovers abstractions to maximally express the original input features. If we train the DBNs using only the speech of an individual speaker, it can be considered that there is less phonological information and relatively more speaker individuality in the output features at the highest layer. Training the DBNs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NNs). The converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the DBNs of the target speaker. We conducted speakervoice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model-based method.",
      "abstract": "This paper presents a voice conversion technique using Deep Belief Nets (DBNs) to build high-order eigen spaces of the source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. DBNs have a deep architecture that automatically discovers abstractions to maximally express the original input features. If we train the DBNs using only the speech of an individual speaker, it can be considered that there is less phonological information and relatively more speaker individuality in the output features at the highest layer. Training the DBNs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NNs). The converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the DBNs of the target speaker. We conducted speakervoice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model-based method.",
      "doi": "https://doi.org/10.21437/interspeech.2013-102",
      "openalex_id": "https://openalex.org/W2294351487",
      "arxiv_id": "",
      "publication_date": "2013-08-25",
      "published": "2013-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scaling Speech Technology to 1,000+ Languages",
      "summary": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
      "abstract": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2305.13516",
      "openalex_id": "https://openalex.org/W4378105483",
      "arxiv_id": "",
      "publication_date": "2023-05-22",
      "published": "2023-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechBrain: A General-Purpose Speech Toolkit",
      "summary": "SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.",
      "abstract": "SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.",
      "doi": "https://doi.org/10.48550/arxiv.2106.04624",
      "openalex_id": "https://openalex.org/W3167533889",
      "arxiv_id": "",
      "publication_date": "2021-06-08",
      "published": "2021-06-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques",
      "summary": "Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training. Code and audio samples are available at https://github.com/mindslab-ai/assem-vc.",
      "abstract": "Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training. Code and audio samples are available at https://github.com/mindslab-ai/assem-vc.",
      "doi": "https://doi.org/10.48550/arxiv.2104.00931",
      "openalex_id": "https://openalex.org/W4306169273",
      "arxiv_id": "",
      "publication_date": "2021-04-02",
      "published": "2021-04-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A metric for distributions with applications to image databases",
      "summary": "We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving \"distribution mass\" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.",
      "abstract": "We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving \"distribution mass\" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.",
      "doi": "https://doi.org/10.1109/iccv.1998.710701",
      "openalex_id": "https://openalex.org/W2125101937",
      "arxiv_id": "",
      "publication_date": "2002-11-27",
      "published": "2002-11-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
      "summary": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P ( y|x ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂ , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.",
      "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P ( y|x ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂ , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.",
      "doi": "https://doi.org/10.1145/3560815",
      "openalex_id": "https://openalex.org/W3185341429",
      "arxiv_id": "",
      "publication_date": "2022-09-14",
      "published": "2022-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "summary": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
      "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.346",
      "openalex_id": "https://openalex.org/W3098267758",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
      "summary": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.158",
      "openalex_id": "https://openalex.org/W3188542058",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "summary": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
      "abstract": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
      "doi": "https://doi.org/10.1038/s42256-023-00626-4",
      "openalex_id": "https://openalex.org/W4322766882",
      "arxiv_id": "",
      "publication_date": "2023-03-02",
      "published": "2023-03-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
      "summary": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",
      "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10610",
      "openalex_id": "https://openalex.org/W4226162428",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "summary": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
      "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.759",
      "openalex_id": "https://openalex.org/W4385567149",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
      "summary": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",
      "abstract": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1160",
      "openalex_id": "https://openalex.org/W3096251052",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Pre-Training for Voice Activation",
      "summary": "The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.",
      "abstract": "The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.",
      "doi": "https://doi.org/10.3390/app10238643",
      "openalex_id": "https://openalex.org/W3108231750",
      "arxiv_id": "",
      "publication_date": "2020-12-03",
      "published": "2020-12-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Speech Command Control-Based Recognition System for Dysarthric Patients Based on Deep Learning Technology",
      "summary": "Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.",
      "abstract": "Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.",
      "doi": "https://doi.org/10.3390/app11062477",
      "openalex_id": "https://openalex.org/W3134187040",
      "arxiv_id": "",
      "publication_date": "2021-03-10",
      "published": "2021-03-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection",
      "summary": "We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.",
      "abstract": "We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414470",
      "openalex_id": "https://openalex.org/W3160747466",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Joint Audio and Speech Understanding",
      "summary": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.",
      "abstract": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389742",
      "openalex_id": "https://openalex.org/W4391021627",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio Codec Enhancement with Generative Adversarial Networks",
      "summary": "Audio codecs are typically transform-domain based and efficiently code stationary audio signals, but they struggle with speech and signals containing dense transient events such as applause. Specifically, with these two classes of signals as examples, we demonstrate a technique for restoring audio from coding noise based on generative adversarial networks (GAN). A primary advantage of the proposed GAN-based coded audio enhancer is that the method operates end-to-end directly on decoded audio samples, eliminating the need to design any manually-crafted frontend. Furthermore, the enhancement approach described in this paper can improve the sound quality of low-bit rate coded audio without any modifications to the existent standard-compliant encoders. Subjective tests illustrate that the proposed enhancer improves the quality of speech and difficult to code applause excerpts significantly.",
      "abstract": "Audio codecs are typically transform-domain based and efficiently code stationary audio signals, but they struggle with speech and signals containing dense transient events such as applause. Specifically, with these two classes of signals as examples, we demonstrate a technique for restoring audio from coding noise based on generative adversarial networks (GAN). A primary advantage of the proposed GAN-based coded audio enhancer is that the method operates end-to-end directly on decoded audio samples, eliminating the need to design any manually-crafted frontend. Furthermore, the enhancement approach described in this paper can improve the sound quality of low-bit rate coded audio without any modifications to the existent standard-compliant encoders. Subjective tests illustrate that the proposed enhancer improves the quality of speech and difficult to code applause excerpts significantly.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053113",
      "openalex_id": "https://openalex.org/W3015780049",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WaveNetEQ — Packet Loss Concealment with WaveRNN",
      "summary": "We present WaveNetEQ, a novel packet loss concealment method based on a WaveRNN architecture. The model is conditioned on a log-mel spectrogram of the past signal to extract slow moving features, like voice characteristics and prosody and achieves significantly better quality than pattern based methods for medium and long term packet loss. Through aggressive sparsification the model is efficient enough to run on a phone.",
      "abstract": "We present WaveNetEQ, a novel packet loss concealment method based on a WaveRNN architecture. The model is conditioned on a log-mel spectrogram of the past signal to extract slow moving features, like voice characteristics and prosody and achieves significantly better quality than pattern based methods for medium and long term packet loss. Through aggressive sparsification the model is efficient enough to run on a phone.",
      "doi": "https://doi.org/10.1109/ieeeconf51394.2020.9443419",
      "openalex_id": "https://openalex.org/W3169418678",
      "arxiv_id": "",
      "publication_date": "2020-11-01",
      "published": "2020-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition",
      "summary": "Denoising autoencoders (DAs) have shown success in generating robust features for images, but there has been limited work in applying DAs for speech. In this paper we present a deep denoising autoencoder (DDA) framework that can produce robust speech features for noisy reverberant speech recognition. The DDA is first pre-trained as restricted Boltzmann machines (RBMs) in an unsupervised fashion. Then it is unrolled to autoencoders, and fine-tuned by corresponding clean speech features to learn a nonlinear mapping from noisy to clean features. Acoustic models are re-trained using the reconstructed features from the DDA, and speech recognition is performed. The proposed approach is evaluated on the CHiME-WSJ0 corpus, and shows a 16-25% absolute improvement on the recognition accuracy under various SNRs.",
      "abstract": "Denoising autoencoders (DAs) have shown success in generating robust features for images, but there has been limited work in applying DAs for speech. In this paper we present a deep denoising autoencoder (DDA) framework that can produce robust speech features for noisy reverberant speech recognition. The DDA is first pre-trained as restricted Boltzmann machines (RBMs) in an unsupervised fashion. Then it is unrolled to autoencoders, and fine-tuned by corresponding clean speech features to learn a nonlinear mapping from noisy to clean features. Acoustic models are re-trained using the reconstructed features from the DDA, and speech recognition is performed. The proposed approach is evaluated on the CHiME-WSJ0 corpus, and shows a 16-25% absolute improvement on the recognition accuracy under various SNRs.",
      "doi": "https://doi.org/10.1109/icassp.2014.6853900",
      "openalex_id": "https://openalex.org/W1973681148",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reverberant speech recognition based on denoising autoencoder",
      "summary": "Denoising autoencoder is applied to reverberant speech recognition as a noise robust front-end to reconstruct clean speech spectrum from noisy input. In order to capture context effects of speech sounds, a window of multiple short-windowed spectral frames are concatenated to form a single input vector. Additionally, a combination of short and long-term spectra is investigated to properly handle long impulse response of reverberation while keeping necessary time resolution for speech recognition. Experiments are performed using the CENSREC-4dataset that is designed as an evaluation framework for distant-talking speech recognition. Experimental results show that the proposed denoising autoencoder based front-end using the shortwindowed spectra gives better results than conventional methods. By combining the long-term spectra, further improvement is obtained. The recognition accuracy by the proposed method using the short and long-term spectra is 97.0% for the open condition test set of the dataset, whereas it is 87.8% when a multicondition training based baseline is used. As a supplemental experiment, large vocabulary speech recognition is also performed and the effectiveness of the proposed method has been confirmed. Index Terms: Denoising autoencoder, reverberant speech recognition, restricted Boltzmann machine, distant-talking speech recognition, CENSREC-4",
      "abstract": "Denoising autoencoder is applied to reverberant speech recognition as a noise robust front-end to reconstruct clean speech spectrum from noisy input. In order to capture context effects of speech sounds, a window of multiple short-windowed spectral frames are concatenated to form a single input vector. Additionally, a combination of short and long-term spectra is investigated to properly handle long impulse response of reverberation while keeping necessary time resolution for speech recognition. Experiments are performed using the CENSREC-4dataset that is designed as an evaluation framework for distant-talking speech recognition. Experimental results show that the proposed denoising autoencoder based front-end using the shortwindowed spectra gives better results than conventional methods. By combining the long-term spectra, further improvement is obtained. The recognition accuracy by the proposed method using the short and long-term spectra is 97.0% for the open condition test set of the dataset, whereas it is 87.8% when a multicondition training based baseline is used. As a supplemental experiment, large vocabulary speech recognition is also performed and the effectiveness of the proposed method has been confirmed. Index Terms: Denoising autoencoder, reverberant speech recognition, restricted Boltzmann machine, distant-talking speech recognition, CENSREC-4",
      "doi": "https://doi.org/10.21437/interspeech.2013-267",
      "openalex_id": "https://openalex.org/W2296581541",
      "arxiv_id": "",
      "publication_date": "2013-08-25",
      "published": "2013-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising",
      "summary": "In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments.",
      "abstract": "In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments.",
      "doi": "https://doi.org/10.1109/taslp.2017.2696307",
      "openalex_id": "https://openalex.org/W2609317876",
      "arxiv_id": "",
      "publication_date": "2017-04-24",
      "published": "2017-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Time-Frequency Networks for Audio Super-Resolution",
      "summary": "Audio super-resolution (a.k.a. bandwidth extension) is the challenging task of increasing the temporal resolution of audio signals. Recent deep networks approaches achieved promising results by modeling the task as a regression problem in either time or frequency domain. In this paper, we introduced Time-Frequency Network (TFNet), a deep network that utilizes supervision in both the time and frequency domain. We proposed a novel model architecture which allows the two domains to be jointly optimized. Results demonstrate that our method outperforms the state-of-the-art both quantitatively and qualitatively.",
      "abstract": "Audio super-resolution (a.k.a. bandwidth extension) is the challenging task of increasing the temporal resolution of audio signals. Recent deep networks approaches achieved promising results by modeling the task as a regression problem in either time or frequency domain. In this paper, we introduced Time-Frequency Network (TFNet), a deep network that utilizes supervision in both the time and frequency domain. We proposed a novel model architecture which allows the two domains to be jointly optimized. Results demonstrate that our method outperforms the state-of-the-art both quantitatively and qualitatively.",
      "doi": "https://doi.org/10.1109/icassp.2018.8462049",
      "openalex_id": "https://openalex.org/W2802034954",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Algorithm for Vector Quantizer Design",
      "summary": "An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",
      "abstract": "An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",
      "doi": "https://doi.org/10.1109/tcom.1980.1094577",
      "openalex_id": "https://openalex.org/W2134383396",
      "arxiv_id": "",
      "publication_date": "1980-01-01",
      "published": "1980-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector quantization in speech coding",
      "summary": "Quantization, the process of approximating continuous-amplitude signals by digital (discrete-amplitude) signals, is an important aspect of data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar quantization. Vector quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementation, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters.",
      "abstract": "Quantization, the process of approximating continuous-amplitude signals by digital (discrete-amplitude) signals, is an important aspect of data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar quantization. Vector quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementation, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters.",
      "doi": "https://doi.org/10.1109/proc.1985.13340",
      "openalex_id": "https://openalex.org/W2002182716",
      "arxiv_id": "",
      "publication_date": "1985-01-01",
      "published": "1985-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Perception-Distortion Tradeoff",
      "summary": "Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.",
      "abstract": "Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.",
      "doi": "https://doi.org/10.1109/cvpr.2018.00652",
      "openalex_id": "https://openalex.org/W2768814045",
      "arxiv_id": "",
      "publication_date": "2018-06-01",
      "published": "2018-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
      "summary": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.",
      "abstract": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.",
      "doi": "https://doi.org/10.1609/aaai.v32i1.11671",
      "openalex_id": "https://openalex.org/W2760103357",
      "arxiv_id": "",
      "publication_date": "2018-04-29",
      "published": "2018-04-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Web Real-Time Communication Use Cases and Requirements",
      "summary": "This document describes web-based real-time communication use cases.Requirements on the browser functionality are derived from the use cases.This document was developed in an initial phase of the work with rather minor updates at later stages.It has not really served as a tool in deciding features or scope for the WG's efforts so far.It is being published to record the early conclusions of the WG.It will not be used as a set of rigid guidelines that specifications and implementations will be held to in the future.",
      "abstract": "This document describes web-based real-time communication use cases.Requirements on the browser functionality are derived from the use cases.This document was developed in an initial phase of the work with rather minor updates at later stages.It has not really served as a tool in deciding features or scope for the WG's efforts so far.It is being published to record the early conclusions of the WG.It will not be used as a set of rigid guidelines that specifications and implementations will be held to in the future.",
      "doi": "https://doi.org/10.17487/rfc7478",
      "openalex_id": "https://openalex.org/W2286601668",
      "arxiv_id": "",
      "publication_date": "2015-03-01",
      "published": "2015-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Some methods for classification and analysis of multivariate observations",
      "summary": "This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated. (Author)",
      "abstract": "This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated. (Author)",
      "doi": "",
      "openalex_id": "https://openalex.org/W2127218421",
      "arxiv_id": "",
      "publication_date": "1967-01-01",
      "published": "1967-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Denoising with Deep Feature Losses",
      "summary": "We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.",
      "abstract": "We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1924",
      "openalex_id": "https://openalex.org/W2810843531",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VISQOL: The Virtual Speech Quality Objective Listener",
      "summary": "A model of human speech quality perception has been developed to provide an objective measure for predicting subjective quality assessments. The Virtual Speech Quality Objective Listener (ViSQOL) model is a signal based full reference metric that uses a spectro-temporal measure of similarity between a reference and a test speech signal. This paper describes the algorithm and compares the results with PESQ for common problems in VoIP: clock drift, associated time warping and jitter. The results indicate that ViSQOL is less prone to underestimation of speech quality in both scenarios than the ITU standard.",
      "abstract": "A model of human speech quality perception has been developed to provide an objective measure for predicting subjective quality assessments. The Virtual Speech Quality Objective Listener (ViSQOL) model is a signal based full reference metric that uses a spectro-temporal measure of similarity between a reference and a test speech signal. This paper describes the algorithm and compares the results with PESQ for common problems in VoIP: clock drift, associated time warping and jitter. The results indicate that ViSQOL is less prone to underestimation of speech quality in both scenarios than the ITU standard.",
      "doi": "https://doi.org/10.21427/8dcc-ba52",
      "openalex_id": "https://openalex.org/W1607435270",
      "arxiv_id": "",
      "publication_date": "2021-02-25",
      "published": "2021-02-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "summary": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
      "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
      "doi": "https://doi.org/10.48550/arxiv.1511.07289",
      "openalex_id": "https://openalex.org/W2176412452",
      "arxiv_id": "",
      "publication_date": "2015-11-23",
      "published": "2015-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A linear prediction vocoder simulation based upon the autocorrelation method",
      "summary": "A detailed discussion of the computer simulation of a linear prediction vocoder system is presented. The basic technique used for analysis is the autocorrelation method of linear prediction. New results include modifications to the simplified inverse filter tracking (SIFT) algorithm for more efficient pitch extraction, coding algorithms for low-bit rate transmission, a simplified synthesizer gain calculation, and a bias correction for the synthesizer driving function. Experimental results are presented which illustrate both the capabilities and limitations of linear prediction vocoders.",
      "abstract": "A detailed discussion of the computer simulation of a linear prediction vocoder system is presented. The basic technique used for analysis is the autocorrelation method of linear prediction. New results include modifications to the simplified inverse filter tracking (SIFT) algorithm for more efficient pitch extraction, coding algorithms for low-bit rate transmission, a simplified synthesizer gain calculation, and a bias correction for the synthesizer driving function. Experimental results are presented which illustrate both the capabilities and limitations of linear prediction vocoders.",
      "doi": "https://doi.org/10.1109/tassp.1974.1162554",
      "openalex_id": "https://openalex.org/W2151409182",
      "arxiv_id": "",
      "publication_date": "1974-04-01",
      "published": "1974-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hierarchical disentangled representation learning for singing voice conversion",
      "summary": "Conventional singing voice conversion (SVC) methods often suffer from operating in high-resolution audio owing to a high dimensionality of data. In this paper, we propose a hierarchical representation learning that enables the learning of disentangled representations with multiple resolutions independently. With the learned disentangled representations, the proposed method progressively performs SVC from low to high resolutions. Experimental results show that the proposed method outperforms baselines that operate with a single resolution in terms of mean opinion score (MOS), similarity score, and pitch accuracy.",
      "abstract": "Conventional singing voice conversion (SVC) methods often suffer from operating in high-resolution audio owing to a high dimensionality of data. In this paper, we propose a hierarchical representation learning that enables the learning of disentangled representations with multiple resolutions independently. With the learned disentangled representations, the proposed method progressively performs SVC from low to high resolutions. Experimental results show that the proposed method outperforms baselines that operate with a single resolution in terms of mean opinion score (MOS), similarity score, and pitch accuracy.",
      "doi": "https://doi.org/10.1109/ijcnn52387.2021.9533583",
      "openalex_id": "https://openalex.org/W3199364993",
      "arxiv_id": "",
      "publication_date": "2021-07-18",
      "published": "2021-07-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Band Melgan: Faster Waveform Generation For High-Quality Text-To-Speech",
      "summary": "In this paper, we propose multi-band MelGAN, a much faster waveform generation model targeting to high-quality text-to-speech. Specifically, we improve the original MelGAN by the following aspects. First, we increase the receptive field of the generator, which is proven to be beneficial to speech generation. Second, we substitute the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Together with pre-training, this improvement leads to both better quality and better training stability. More importantly, we extend MelGAN with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. The proposed multi-band MelGAN has achieved high MOS of 4.34 and 4.22 in waveform generation and TTS, respectively. With only 1.91M parameters, our model effectively reduces the total computational complexity of the original MelGAN from 5.85 to 0.95 GFLOPS. Our Pytorch implementation can achieve a real-time factor of 0.03 on CPU without hardware specific optimization.",
      "abstract": "In this paper, we propose multi-band MelGAN, a much faster waveform generation model targeting to high-quality text-to-speech. Specifically, we improve the original MelGAN by the following aspects. First, we increase the receptive field of the generator, which is proven to be beneficial to speech generation. Second, we substitute the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Together with pre-training, this improvement leads to both better quality and better training stability. More importantly, we extend MelGAN with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. The proposed multi-band MelGAN has achieved high MOS of 4.34 and 4.22 in waveform generation and TTS, respectively. With only 1.91M parameters, our model effectively reduces the total computational complexity of the original MelGAN from 5.85 to 0.95 GFLOPS. Our Pytorch implementation can achieve a real-time factor of 0.03 on CPU without hardware specific optimization.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383551",
      "openalex_id": "https://openalex.org/W3144035034",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
      "summary": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.",
      "abstract": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1465",
      "openalex_id": "https://openalex.org/W3197349023",
      "arxiv_id": "",
      "publication_date": "2021-06-22",
      "published": "2021-06-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Decision tree state tying based on penalized Bayesian information criterion",
      "summary": "In this paper, an approach of the penalized Bayesian information criterion (pBIC) for decision tree state tying is described. The pBIC is applied to two important applications. First, it is used as a decision tree growing criterion in place of the conventional approach of using a heuristic constant threshold. It is found that original BIC penalty is too low and will not lead to a compact decision tree state tying model. Based on Wolfe's modification to the asymptotic null distribution, it is derived that two times BIC penalty should be used for decision tree state tying based on pBIC. Secondly, pBIC is studied as a model compression criterion for decision tree state tying based acoustic modeling. Experimental results on a large vocabulary (Wall Street Journal) speech recognition task indicate that a compact decision tree could be achieved with almost no loss of the speech recognition performance.",
      "abstract": "In this paper, an approach of the penalized Bayesian information criterion (pBIC) for decision tree state tying is described. The pBIC is applied to two important applications. First, it is used as a decision tree growing criterion in place of the conventional approach of using a heuristic constant threshold. It is found that original BIC penalty is too low and will not lead to a compact decision tree state tying model. Based on Wolfe's modification to the asymptotic null distribution, it is derived that two times BIC penalty should be used for decision tree state tying based on pBIC. Secondly, pBIC is studied as a model compression criterion for decision tree state tying based acoustic modeling. Experimental results on a large vocabulary (Wall Street Journal) speech recognition task indicate that a compact decision tree could be achieved with almost no loss of the speech recognition performance.",
      "doi": "https://doi.org/10.1109/icassp.1999.758133",
      "openalex_id": "https://openalex.org/W2123799894",
      "arxiv_id": "",
      "publication_date": "1999-01-01",
      "published": "1999-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bayesian Speech and Language Processing",
      "summary": "With this comprehensive guide you will learn how to apply Bayesian machine learning techniques systematically to solve various problems in speech and language processing. A range of statistical models is detailed, from hidden Markov models to Gaussian mixture models, n-gram models and latent topic models, along with applications including automatic speech recognition, speaker verification, and information retrieval. Approximate Bayesian inferences based on MAP, Evidence, Asymptotic, VB, and MCMC approximations are provided as well as full derivations of calculations, useful notations, formulas, and rules. The authors address the difficulties of straightforward applications and provide detailed examples and case studies to demonstrate how you can successfully use practical Bayesian inference methods to improve the performance of information systems. This is an invaluable resource for students, researchers, and industry practitioners working in machine learning, signal processing, and speech and language processing.",
      "abstract": "With this comprehensive guide you will learn how to apply Bayesian machine learning techniques systematically to solve various problems in speech and language processing. A range of statistical models is detailed, from hidden Markov models to Gaussian mixture models, n-gram models and latent topic models, along with applications including automatic speech recognition, speaker verification, and information retrieval. Approximate Bayesian inferences based on MAP, Evidence, Asymptotic, VB, and MCMC approximations are provided as well as full derivations of calculations, useful notations, formulas, and rules. The authors address the difficulties of straightforward applications and provide detailed examples and case studies to demonstrate how you can successfully use practical Bayesian inference methods to improve the performance of information systems. This is an invaluable resource for students, researchers, and industry practitioners working in machine learning, signal processing, and speech and language processing.",
      "doi": "https://doi.org/10.1017/cbo9781107295360",
      "openalex_id": "https://openalex.org/W2405331948",
      "arxiv_id": "",
      "publication_date": "2015-07-15",
      "published": "2015-07-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MDL-based context-dependent subword modeling for speech recognition.",
      "summary": "Context-dependent phone units, such as triphones, have recently come to be used to model subword units in speech recognition systems that are based on the use of hidden Markov models(HMMs).While most such systems employ clustering of the HMM parameters(e.g., subword clustering and state clustering)to control the HMM size, so as to avoid poor recognition accuracy due to a lack of training data, none of them provide any effective criteria for determining the optimal number of clusters.This paper proposes a method in which state clustering is accomplished by way of phonetic decision trees and in which the minimum description length(MDL)criterion is used to optimize the number of clusters.Large-vocabulary Japanese-language recognition experiments show that this method achieves higher accuracy than the maximum-likelihood approach.",
      "abstract": "Context-dependent phone units, such as triphones, have recently come to be used to model subword units in speech recognition systems that are based on the use of hidden Markov models(HMMs).While most such systems employ clustering of the HMM parameters(e.g., subword clustering and state clustering)to control the HMM size, so as to avoid poor recognition accuracy due to a lack of training data, none of them provide any effective criteria for determining the optimal number of clusters.This paper proposes a method in which state clustering is accomplished by way of phonetic decision trees and in which the minimum description length(MDL)criterion is used to optimize the number of clusters.Large-vocabulary Japanese-language recognition experiments show that this method achieves higher accuracy than the maximum-likelihood approach.",
      "doi": "https://doi.org/10.1250/ast.21.79",
      "openalex_id": "https://openalex.org/W1963627370",
      "arxiv_id": "",
      "publication_date": "2000-01-01",
      "published": "2000-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources)",
      "summary": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
      "abstract": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
      "doi": "https://doi.org/10.1162/tacl_a_00321",
      "openalex_id": "https://openalex.org/W2996728628",
      "arxiv_id": "",
      "publication_date": "2020-07-29",
      "published": "2020-07-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sequence Distillation for Purely Sequence Trained Acoustic Models",
      "summary": "This paper presents our exploration into teacher-student (TS) training for acoustic models (AMs) based on the lattice-free maximum mutual information technique. Whereas most previous studies of TS training used a frame-level distance between teacher and student models' distributions, we propose using the sequence-level temper-atured Kullback-Leibler divergence as a metric for TS training. In our experiment on the AMI meeting corpus, we prepared a strong teacher model consisting of a convolutional neural network, time delay neural network, and long short-term memory, which had 47.7M parameters and achieved a state-of-the-art word error rate (WER) of 18.05%. Whereas the small student AM (10.8M params. and 19.72% WER) trained by a frame-level TS training was able to fill only 43% of the WER gap between teacher and student AMs, the student AM trained by the proposed method achieved a 18.23% WER, filling 89% of the WER gap from the teacher AM. We also show that the frame-level TS training sometimes even degrades the performance of the student model whereas the proposed method consistently improved the accuracy.",
      "abstract": "This paper presents our exploration into teacher-student (TS) training for acoustic models (AMs) based on the lattice-free maximum mutual information technique. Whereas most previous studies of TS training used a frame-level distance between teacher and student models' distributions, we propose using the sequence-level temper-atured Kullback-Leibler divergence as a metric for TS training. In our experiment on the AMI meeting corpus, we prepared a strong teacher model consisting of a convolutional neural network, time delay neural network, and long short-term memory, which had 47.7M parameters and achieved a state-of-the-art word error rate (WER) of 18.05%. Whereas the small student AM (10.8M params. and 19.72% WER) trained by a frame-level TS training was able to fill only 43% of the WER gap between teacher and student AMs, the student AM trained by the proposed method achieved a 18.23% WER, filling 89% of the WER gap from the teacher AM. We also show that the frame-level TS training sometimes even degrades the performance of the student model whereas the proposed method consistently improved the accuracy.",
      "doi": "https://doi.org/10.1109/icassp.2018.8462619",
      "openalex_id": "https://openalex.org/W2889871534",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Teacher-Student Learning Approach for Unsupervised Domain Adaptation of Sequence-Trained ASR Models",
      "summary": "Teacher-student (T-S) learning is a transfer learning approach, where a teacher network is used to \"teach\" a student network to make the same predictions as the teacher. Originally formulated for model compression, this approach has also been used for domain adaptation, and is particularly effective when parallel data is available in source and target domains. The standard approach uses a frame-level objective of minimizing the KL divergence between the frame-level posteriors of the teacher and student networks. However, for sequence-trained models for speech recognition, it is more appropriate to train the student to mimic the sequence-level posterior of the teacher network. In this work, we compare this sequence-level KL divergence objective with another semi-supervised sequence-training method, namely the lattice-free MMI, for unsupervised domain adaptation. We investigate the approaches in multiple scenarios including adapting from clean to noisy speech, bandwidth mismatch and channel mismatch.",
      "abstract": "Teacher-student (T-S) learning is a transfer learning approach, where a teacher network is used to \"teach\" a student network to make the same predictions as the teacher. Originally formulated for model compression, this approach has also been used for domain adaptation, and is particularly effective when parallel data is available in source and target domains. The standard approach uses a frame-level objective of minimizing the KL divergence between the frame-level posteriors of the teacher and student networks. However, for sequence-trained models for speech recognition, it is more appropriate to train the student to mimic the sequence-level posterior of the teacher network. In this work, we compare this sequence-level KL divergence objective with another semi-supervised sequence-training method, namely the lattice-free MMI, for unsupervised domain adaptation. We investigate the approaches in multiple scenarios including adapting from clean to noisy speech, bandwidth mismatch and channel mismatch.",
      "doi": "https://doi.org/10.1109/slt.2018.8639635",
      "openalex_id": "https://openalex.org/W2911629330",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Domain Adaptation via Teacher-Student Learning for End-to-End Speech Recognition",
      "summary": "Teacher-student (T/S) has shown to be effective for domain adaptation of deep neural network acoustic models in hybrid speech recognition systems. In this work, we extend the T/S learning to large-scale unsupervised domain adaptation of an attention-based end-to-end (E2E) model through two levels of knowledge transfer: teacher's token posteriors as soft labels and one-best predictions as decoder guidance. To further improve T/S learning with the help of ground-truth labels, we propose adaptive T/S (AT/S) learning. Instead of conditionally choosing from either the teacher's soft token posteriors or the one-hot ground-truth label, in AT/S, the student always learns from both the teacher and the ground truth with a pair of adaptive weights assigned to the soft and one-hot labels quantifying the confidence on each of the knowledge sources. The confidence scores are dynamically estimated at each decoder step as a function of the soft and one-hot labels. With 3400 hours parallel close-talk and far-field Microsoft Cortana data for domain adaptation, T/S and AT/S achieves 6.3% and 10.3% relative word error rate improvement over a strong E2E model trained with the same amount of far-field data.",
      "abstract": "Teacher-student (T/S) has shown to be effective for domain adaptation of deep neural network acoustic models in hybrid speech recognition systems. In this work, we extend the T/S learning to large-scale unsupervised domain adaptation of an attention-based end-to-end (E2E) model through two levels of knowledge transfer: teacher's token posteriors as soft labels and one-best predictions as decoder guidance. To further improve T/S learning with the help of ground-truth labels, we propose adaptive T/S (AT/S) learning. Instead of conditionally choosing from either the teacher's soft token posteriors or the one-hot ground-truth label, in AT/S, the student always learns from both the teacher and the ground truth with a pair of adaptive weights assigned to the soft and one-hot labels quantifying the confidence on each of the knowledge sources. The confidence scores are dynamically estimated at each decoder step as a function of the soft and one-hot labels. With 3400 hours parallel close-talk and far-field Microsoft Cortana data for domain adaptation, T/S and AT/S achieves 6.3% and 10.3% relative word error rate improvement over a strong E2E model trained with the same amount of far-field data.",
      "doi": "https://doi.org/10.1109/asru46091.2019.9003776",
      "openalex_id": "https://openalex.org/W3008008574",
      "arxiv_id": "",
      "publication_date": "2019-12-01",
      "published": "2019-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Advances in residual vector quantization: a review",
      "summary": "Advances in residual vector quantization (RVQ) are surveyed. Definitions of joint encoder optimality and joint decoder optimality are discussed. Design techniques for RVQs with large numbers of stages and generally different encoder and decoder codebooks are elaborated and extended. Fixed-rate RVQs, and variable-rate RVQs that employ entropy coding are examined. Predictive and finite state RVQs designed and integrated into neural-network based source coding structures are revisited. Successive approximation RVQs that achieve embedded and refinable coding are reviewed. A new type of successive approximation RVQ that varies the instantaneous block rate by using different numbers of stages on different blocks is introduced and applied to image waveforms, and a scalar version of the new residual quantizer is applied to image subbands in an embedded wavelet transform coding system.",
      "abstract": "Advances in residual vector quantization (RVQ) are surveyed. Definitions of joint encoder optimality and joint decoder optimality are discussed. Design techniques for RVQs with large numbers of stages and generally different encoder and decoder codebooks are elaborated and extended. Fixed-rate RVQs, and variable-rate RVQs that employ entropy coding are examined. Predictive and finite state RVQs designed and integrated into neural-network based source coding structures are revisited. Successive approximation RVQs that achieve embedded and refinable coding are reviewed. A new type of successive approximation RVQ that varies the instantaneous block rate by using different numbers of stages on different blocks is introduced and applied to image waveforms, and a scalar version of the new residual quantizer is applied to image subbands in an embedded wavelet transform coding system.",
      "doi": "https://doi.org/10.1109/83.480761",
      "openalex_id": "https://openalex.org/W1970491336",
      "arxiv_id": "",
      "publication_date": "1996-01-01",
      "published": "1996-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector quantizers with direct sum codebooks",
      "summary": "The use of direct sum codebooks to minimize the memory requirements of vector quantizers is investigated. Assuming arbitrary fixed partitions, necessary conditions for minimum distortion codebooks are derived, first for scalar codebooks, assuming mean-squared error distortion, and then for vector codebooks and a broader class of distortion measures. An iterative procedure is described for designing locally optimal direct sum codebooks. Both optimal and computationally efficient suboptimal encoding schemes are considered. It is shown that although an optimal encoding can be implemented by a sequential encoder, the complexity of implementing optimal stagewise partitions generally exceeds the complexity of an exhaustive search of the direct sum codebook. It is also shown that sequential nearest-neighbor encoders can be extremely inefficient. The M-search method is explored as one method of improving the effectiveness of suboptimal sequential encoders. Representative results for simulated direct sum quantizers are presented.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The use of direct sum codebooks to minimize the memory requirements of vector quantizers is investigated. Assuming arbitrary fixed partitions, necessary conditions for minimum distortion codebooks are derived, first for scalar codebooks, assuming mean-squared error distortion, and then for vector codebooks and a broader class of distortion measures. An iterative procedure is described for designing locally optimal direct sum codebooks. Both optimal and computationally efficient suboptimal encoding schemes are considered. It is shown that although an optimal encoding can be implemented by a sequential encoder, the complexity of implementing optimal stagewise partitions generally exceeds the complexity of an exhaustive search of the direct sum codebook. It is also shown that sequential nearest-neighbor encoders can be extremely inefficient. The M-search method is explored as one method of improving the effectiveness of suboptimal sequential encoders. Representative results for simulated direct sum quantizers are presented.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/18.212286",
      "openalex_id": "https://openalex.org/W2129935012",
      "arxiv_id": "",
      "publication_date": "1993-03-01",
      "published": "1993-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Embedded wavelet zerotree coding with direct sum quantization structures",
      "summary": "One of the more effective data compression systems that has been recently proposed is the relatively simple embedded wavelet image coder developed by J.M. Shapiro (1994). Two key components of Shapiro's system are the use of zerotrees to keep track of insignificant subband coefficients and progressive transmission of successive bit planes of significant coefficients. Shapiro's quantization mechanism is the use of scaled successive approximation uniform scalar quantizers. This paper investigates ways of improving the performance of embedded wavelet coders with the use of optimized successive approximation direct sum quantization structures.",
      "abstract": "One of the more effective data compression systems that has been recently proposed is the relatively simple embedded wavelet image coder developed by J.M. Shapiro (1994). Two key components of Shapiro's system are the use of zerotrees to keep track of insignificant subband coefficients and progressive transmission of successive bit planes of significant coefficients. Shapiro's quantization mechanism is the use of scaled successive approximation uniform scalar quantizers. This paper investigates ways of improving the performance of embedded wavelet coders with the use of optimized successive approximation direct sum quantization structures.",
      "doi": "https://doi.org/10.1109/dcc.1995.515515",
      "openalex_id": "https://openalex.org/W2156333391",
      "arxiv_id": "",
      "publication_date": "2002-11-19",
      "published": "2002-11-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition",
      "summary": "In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system.",
      "abstract": "In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system.",
      "doi": "https://doi.org/10.48550/arxiv.2012.05481",
      "openalex_id": "https://openalex.org/W3111562797",
      "arxiv_id": "",
      "publication_date": "2020-12-10",
      "published": "2020-12-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Distilling the Knowledge in a Neural Network",
      "summary": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "doi": "https://doi.org/10.48550/arxiv.1503.02531",
      "openalex_id": "https://openalex.org/W1821462560",
      "arxiv_id": "",
      "publication_date": "2015-03-09",
      "published": "2015-03-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "summary": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",
      "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",
      "doi": "https://doi.org/10.48550/arxiv.2106.08254",
      "openalex_id": "https://openalex.org/W3170863103",
      "arxiv_id": "",
      "publication_date": "2021-06-15",
      "published": "2021-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contextual correlates of synonymy",
      "summary": "article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "abstract": "article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "doi": "https://doi.org/10.1145/365628.365657",
      "openalex_id": "https://openalex.org/W2080100102",
      "arxiv_id": "",
      "publication_date": "1965-10-01",
      "published": "1965-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets",
      "summary": "With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.",
      "abstract": "With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.",
      "doi": "https://doi.org/10.1162/tacl_a_00074",
      "openalex_id": "https://openalex.org/W2963583956",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A study on similarity and relatedness using distributional and WordNet-based approaches",
      "summary": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.",
      "abstract": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.",
      "doi": "https://doi.org/10.3115/1620754.1620758",
      "openalex_id": "https://openalex.org/W2170682101",
      "arxiv_id": "",
      "publication_date": "2009-01-01",
      "published": "2009-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Unsupervised Model for Instance Level Subcategorization Acquisition",
      "summary": "Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .",
      "abstract": "Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .",
      "doi": "https://doi.org/10.3115/v1/d14-1034",
      "openalex_id": "https://openalex.org/W2176085882",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Better Word Representations with Recursive Neural Networks for Morphology",
      "summary": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.",
      "abstract": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2251012068",
      "arxiv_id": "",
      "publication_date": "2013-08-01",
      "published": "2013-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity",
      "summary": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.",
      "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.",
      "doi": "https://doi.org/10.18653/v1/d16-1235",
      "openalex_id": "https://openalex.org/W2510413766",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A word at a time",
      "summary": "Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.",
      "abstract": "Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.",
      "doi": "https://doi.org/10.1145/1963405.1963455",
      "openalex_id": "https://openalex.org/W2026487812",
      "arxiv_id": "",
      "publication_date": "2011-03-28",
      "published": "2011-03-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
      "summary": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
      "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
      "doi": "https://doi.org/10.1162/coli_a_00237",
      "openalex_id": "https://openalex.org/W1854884267",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Verb similarity on the taxonomy of WordNet",
      "summary": "In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity",
      "abstract": "In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity",
      "doi": "",
      "openalex_id": "https://openalex.org/W2132631284",
      "arxiv_id": "",
      "publication_date": "2006-01-01",
      "published": "2006-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contextual correlates of semantic similarity",
      "summary": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
      "abstract": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
      "doi": "https://doi.org/10.1080/01690969108406936",
      "openalex_id": "https://openalex.org/W2103318667",
      "arxiv_id": "",
      "publication_date": "1991-01-01",
      "published": "1991-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Distributional Semantics in Technicolor",
      "summary": "Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",
      "abstract": "Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2137735870",
      "arxiv_id": "",
      "publication_date": "2012-07-08",
      "published": "2012-07-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large-scale learning of word relatedness with constraints",
      "summary": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
      "abstract": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
      "doi": "https://doi.org/10.1145/2339530.2339751",
      "openalex_id": "https://openalex.org/W2142625445",
      "arxiv_id": "",
      "publication_date": "2012-08-12",
      "published": "2012-08-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "summary": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
      "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
      "doi": "https://doi.org/10.18653/v1/p17-1099",
      "openalex_id": "https://openalex.org/W2606974598",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Modal Data Augmentation for End-to-end ASR",
      "summary": "We present a new end-to-end architecture for automatic speech recognition (ASR) that can be trained using \\emph{symbolic} input in addition to the traditional acoustic input. This architecture utilizes two separate encoders: one for acoustic input and another for symbolic input, both sharing the attention and decoder parameters. We call this architecture a multi-modal data augmentation network (MMDA), as it can support multi-modal (acoustic and symbolic) input and enables seamless mixing of large text datasets with significantly smaller transcribed speech corpora during training. We study different ways of transforming large text corpora into a symbolic form suitable for training our MMDA network. Our best MMDA setup obtains small improvements on character error rate (CER), and as much as 7-10\\% relative word error rate (WER) improvement over a baseline both with and without an external language model.",
      "abstract": "We present a new end-to-end architecture for automatic speech recognition (ASR) that can be trained using \\emph{symbolic} input in addition to the traditional acoustic input. This architecture utilizes two separate encoders: one for acoustic input and another for symbolic input, both sharing the attention and decoder parameters. We call this architecture a multi-modal data augmentation network (MMDA), as it can support multi-modal (acoustic and symbolic) input and enables seamless mixing of large text datasets with significantly smaller transcribed speech corpora during training. We study different ways of transforming large text corpora into a symbolic form suitable for training our MMDA network. Our best MMDA setup obtains small improvements on character error rate (CER), and as much as 7-10\\% relative word error rate (WER) improvement over a baseline both with and without an external language model.",
      "doi": "https://doi.org/10.21437/interspeech.2018-2456",
      "openalex_id": "https://openalex.org/W2964012862",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Applying a Speaker-Dependent Speech Compression Technique to Concatenative TTS Synthesizers",
      "summary": "This paper proposes a new speaker-dependent coding algorithm to efficiently compress a large speech database for corpus-based concatenative text-to-speech (TTS) engines while maintaining high fidelity. To achieve a high compression ratio and meet the fundamental requirements of concatenative TTS synthesizers, such as partial segment decoding and random access capability, we adopt a nonpredictive analysis-by-synthesis scheme for speaker-dependent parameter estimation and quantization. The spectral coefficients are quantized by using a memoryless split vector quantization (VQ) approach that does not use frame correlation. Considering that excitation signals of a specific speaker show low intra-variation especially in the voiced regions, the conventional adaptive codebook for pitch prediction is replaced by a speaker-dependent pitch-pulse codebook trained by a corpus of single-speaker speech signals. To further improve the coding efficiency, the proposed coder flexibly combines nonpredictive and predictive type method considering the structure of the TTS system. By applying the proposed algorithm to a Korean TTS system, we could obtain comparable quality to the G.729 speech coder and satisfy all the requirements that TTS system needs. The results are verified by both objective and subjective quality measurements. In addition, the decoding complexity of the proposed coder is around 55% lower than that of G.729 annex A.",
      "abstract": "This paper proposes a new speaker-dependent coding algorithm to efficiently compress a large speech database for corpus-based concatenative text-to-speech (TTS) engines while maintaining high fidelity. To achieve a high compression ratio and meet the fundamental requirements of concatenative TTS synthesizers, such as partial segment decoding and random access capability, we adopt a nonpredictive analysis-by-synthesis scheme for speaker-dependent parameter estimation and quantization. The spectral coefficients are quantized by using a memoryless split vector quantization (VQ) approach that does not use frame correlation. Considering that excitation signals of a specific speaker show low intra-variation especially in the voiced regions, the conventional adaptive codebook for pitch prediction is replaced by a speaker-dependent pitch-pulse codebook trained by a corpus of single-speaker speech signals. To further improve the coding efficiency, the proposed coder flexibly combines nonpredictive and predictive type method considering the structure of the TTS system. By applying the proposed algorithm to a Korean TTS system, we could obtain comparable quality to the G.729 speech coder and satisfy all the requirements that TTS system needs. The results are verified by both objective and subjective quality measurements. In addition, the decoding complexity of the proposed coder is around 55% lower than that of G.729 annex A.",
      "doi": "https://doi.org/10.1109/tasl.2006.876762",
      "openalex_id": "https://openalex.org/W2101409664",
      "arxiv_id": "",
      "publication_date": "2007-01-23",
      "published": "2007-01-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust Speaker-Adaptive HMM-Based Text-to-Speech Synthesis",
      "summary": "This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called ldquoHTS-2007,rdquo employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.",
      "abstract": "This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called ldquoHTS-2007,rdquo employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.",
      "doi": "https://doi.org/10.1109/tasl.2009.2016394",
      "openalex_id": "https://openalex.org/W2117418893",
      "arxiv_id": "",
      "publication_date": "2009-07-01",
      "published": "2009-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi Speaker Speech Synthesis System for Indonesian Language",
      "summary": "Generally, text-to-speech models only produce voice from a single speaker. The most straightforward method to produce another speaker's voice, is to build a standalone synthesis model for each desired speaker's voice. But such approach needs large amount of training data and computational resource. To overcome the problem, several architectures has been successful in producing synthesized speech from various speakers efficiently in terms of data and computation. One of the architectures is Deep Voice 3. In this work, a multi speaker speech synthesis system is built for Indonesian language. The system is using Deep Voice 3 architecture, with several additional components for preprocessing dan post-processing. Some of the components are specifically implemented for Indonesian language. The system is built using a multi speaker dataset, consists of speech data from 145 Indonesian speaker. This system is evaluated subjectively to assess naturalness, similarity to original speaker, and intelligibility of the produced speech. The result shows that the system has MOS (mean opinion score) of 3.39 for speech naturalness dan 3.11 for speech similarity. In assessing speech intelligibility using SUS (semantically unpredictable sentence), the test gives 73.88% for sentence accuracy and 93.48% for word accuracy.",
      "abstract": "Generally, text-to-speech models only produce voice from a single speaker. The most straightforward method to produce another speaker's voice, is to build a standalone synthesis model for each desired speaker's voice. But such approach needs large amount of training data and computational resource. To overcome the problem, several architectures has been successful in producing synthesized speech from various speakers efficiently in terms of data and computation. One of the architectures is Deep Voice 3. In this work, a multi speaker speech synthesis system is built for Indonesian language. The system is using Deep Voice 3 architecture, with several additional components for preprocessing dan post-processing. Some of the components are specifically implemented for Indonesian language. The system is built using a multi speaker dataset, consists of speech data from 145 Indonesian speaker. This system is evaluated subjectively to assess naturalness, similarity to original speaker, and intelligibility of the produced speech. The result shows that the system has MOS (mean opinion score) of 3.39 for speech naturalness dan 3.11 for speech similarity. In assessing speech intelligibility using SUS (semantically unpredictable sentence), the test gives 73.88% for sentence accuracy and 93.48% for word accuracy.",
      "doi": "https://doi.org/10.1109/icaicta49861.2020.9429050",
      "openalex_id": "https://openalex.org/W3164843644",
      "arxiv_id": "",
      "publication_date": "2020-09-08",
      "published": "2020-09-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hierarchical Transfer Learning for Text-to-Speech in Indonesian, Javanese, and Sundanese Languages",
      "summary": "This research develops end-to-end deep learning-based text-to-speech (TTS) in Indonesian, Javanese, and Sundanese. While end-to-end neural TTS, such as Tacotron-2, has made remarkable progress recently, it still suffers from a data scarcity problem for low-resource languages such as Javanese and Sundanese. Our preliminary study shows that Tacotron-2-based TTS needs a large amount of training data; a minimum of 10 hours of training data is required for the model to be able to synthesize acceptable quality and intelligible speech. To solve this low-resource problem, our work proposes a hierarchical transfer learning to train TTS for Javanese and Sundanese, by taking advantage of a dissimilar high-resource language of English domain and a similar intermediate-resource language of Indonesian domain. We report that the evaluation of synthesized speech using the mean opinion score (MOS) reaches 4.27 for Indonesian, and 4.08 for Javanese, and 3.92 for Sundanese. The word accuracy (WAcc) evaluation on semantically unpredicted sentences (SUS) reaches 98.26% for Indonesian, 95.02% for Javanese, and 95.43% for Sundanese. The subjective evaluations of the synthetic speech quality demonstrate that our transfer learning scheme is successfully applied to TTS model for low-resource target domain. Using less than one hour of training data, 38 minutes for Indonesian, 16 minutes for Javanese, and 19 minutes for Sundanese, TTS models can learn fast and achieve adequate performance.",
      "abstract": "This research develops end-to-end deep learning-based text-to-speech (TTS) in Indonesian, Javanese, and Sundanese. While end-to-end neural TTS, such as Tacotron-2, has made remarkable progress recently, it still suffers from a data scarcity problem for low-resource languages such as Javanese and Sundanese. Our preliminary study shows that Tacotron-2-based TTS needs a large amount of training data; a minimum of 10 hours of training data is required for the model to be able to synthesize acceptable quality and intelligible speech. To solve this low-resource problem, our work proposes a hierarchical transfer learning to train TTS for Javanese and Sundanese, by taking advantage of a dissimilar high-resource language of English domain and a similar intermediate-resource language of Indonesian domain. We report that the evaluation of synthesized speech using the mean opinion score (MOS) reaches 4.27 for Indonesian, and 4.08 for Javanese, and 3.92 for Sundanese. The word accuracy (WAcc) evaluation on semantically unpredicted sentences (SUS) reaches 98.26% for Indonesian, 95.02% for Javanese, and 95.43% for Sundanese. The subjective evaluations of the synthetic speech quality demonstrate that our transfer learning scheme is successfully applied to TTS model for low-resource target domain. Using less than one hour of training data, 38 minutes for Indonesian, 16 minutes for Javanese, and 19 minutes for Sundanese, TTS models can learn fast and achieve adequate performance.",
      "doi": "https://doi.org/10.1109/icacsis51025.2020.9263086",
      "openalex_id": "https://openalex.org/W3109182305",
      "arxiv_id": "",
      "publication_date": "2020-10-17",
      "published": "2020-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transfer Learning, Style Control, and Speaker Reconstruction Loss for Zero-Shot Multilingual Multi-Speaker Text-to-Speech on Low-Resource Languages",
      "summary": "Deep neural network (DNN)-based systems generally require large amounts of training data, so they have data scarcity problems in low-resource languages. Recent studies have succeeded in building zero-shot multi-speaker DNN-based TTS on high-resource languages, but they still have unsatisfactory performance on unseen speakers. This study addresses two main problems: overcoming the problem of data scarcity in the DNN-based TTS on low-resource languages and improving the performance of zero-shot speaker adaptation for unseen speakers. We propose a novel multi-stage transfer learning strategy using a partial network-based deep transfer learning to overcome the low-resource problem by utilizing pre-trained monolingual single-speaker TTS and d-vector speaker encoder on a high-resource language as the source domain. Meanwhile, to improve the performance of zero-shot speaker adaptation, we propose a new TTS model that incorporates an explicit style control from the target speaker for TTS conditioning and an utterance-level speaker reconstruction loss during TTS training. We use publicly available speech datasets for experiments. We show that our proposed training strategy is able to effectively train the TTS models using a limited amount of training data of low-resource target languages. The models trained using the proposed transfer learning successfully produce intelligible natural speech sounds, while in contrast using standard training fails to make the models synthesize understandable speech. We also demonstrate that our proposed style encoder network and speaker reconstruction loss significantly improves speaker similarity in zero-shot speaker adaptation task compared to the baseline model. Overall, our proposed TTS model and training strategy has succeeded in increasing the speaker cosine similarity of the synthesized speech on the unseen speakers test set by 0.468 and 0.279 in native and foreign languages respectively.",
      "abstract": "Deep neural network (DNN)-based systems generally require large amounts of training data, so they have data scarcity problems in low-resource languages. Recent studies have succeeded in building zero-shot multi-speaker DNN-based TTS on high-resource languages, but they still have unsatisfactory performance on unseen speakers. This study addresses two main problems: overcoming the problem of data scarcity in the DNN-based TTS on low-resource languages and improving the performance of zero-shot speaker adaptation for unseen speakers. We propose a novel multi-stage transfer learning strategy using a partial network-based deep transfer learning to overcome the low-resource problem by utilizing pre-trained monolingual single-speaker TTS and d-vector speaker encoder on a high-resource language as the source domain. Meanwhile, to improve the performance of zero-shot speaker adaptation, we propose a new TTS model that incorporates an explicit style control from the target speaker for TTS conditioning and an utterance-level speaker reconstruction loss during TTS training. We use publicly available speech datasets for experiments. We show that our proposed training strategy is able to effectively train the TTS models using a limited amount of training data of low-resource target languages. The models trained using the proposed transfer learning successfully produce intelligible natural speech sounds, while in contrast using standard training fails to make the models synthesize understandable speech. We also demonstrate that our proposed style encoder network and speaker reconstruction loss significantly improves speaker similarity in zero-shot speaker adaptation task compared to the baseline model. Overall, our proposed TTS model and training strategy has succeeded in increasing the speaker cosine similarity of the synthesized speech on the unseen speakers test set by 0.468 and 0.279 in native and foreign languages respectively.",
      "doi": "https://doi.org/10.1109/access.2022.3141200",
      "openalex_id": "https://openalex.org/W4206596421",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Robust Indonesian Speech Recognition with Spontaneous-Speech Adapted Acoustic Models",
      "summary": "This paper presents our work in building an Indonesian speech recognizer to handle both spontaneous and dictated speech. The recognizer is based on the Gaussian Mixture and Hidden Markov Models (GMM-HMM). The model is first trained on 73 hours of dictated speech and 43.5 minutes of spontaneous speech. The dictated speech is read from prepared transcripts by a diverse group of 244 Indonesian speakers. The spontaneous speech is manually labelled from recordings of an Indonesian parliamentary meeting, and is interspersed with noises and fillers. The resulting triphone model is then adapted only to the spontaneous speech using the Maximum A-posteriori Probability (MAP) method. We evaluate the adapted model using separate dictated and spontaneous evaluation sets. The dictated set consists of speech from 20 speakers totaling 14.5 hours. The spontaneous set is derived from the recording of a regional government meeting, consisting of 1085 utterances totaling 48.5 minutes. Evaluation of a MAP-adapted spontaneous set yields a 2.60% absolute increase in Word Accuracy Rate (WAR) over the un-adapted model, outperforming MMI adaptation. Conversely, MMI adaption of the dictated set outperforms the MAP adaptation by achieving an absolute increase of 1.48% in WAR over the un-adapted model. We also demonstrate that fMLLR speaker adaptation is unsuitable for our task due to limited adaptation data.",
      "abstract": "This paper presents our work in building an Indonesian speech recognizer to handle both spontaneous and dictated speech. The recognizer is based on the Gaussian Mixture and Hidden Markov Models (GMM-HMM). The model is first trained on 73 hours of dictated speech and 43.5 minutes of spontaneous speech. The dictated speech is read from prepared transcripts by a diverse group of 244 Indonesian speakers. The spontaneous speech is manually labelled from recordings of an Indonesian parliamentary meeting, and is interspersed with noises and fillers. The resulting triphone model is then adapted only to the spontaneous speech using the Maximum A-posteriori Probability (MAP) method. We evaluate the adapted model using separate dictated and spontaneous evaluation sets. The dictated set consists of speech from 20 speakers totaling 14.5 hours. The spontaneous set is derived from the recording of a regional government meeting, consisting of 1085 utterances totaling 48.5 minutes. Evaluation of a MAP-adapted spontaneous set yields a 2.60% absolute increase in Word Accuracy Rate (WAR) over the un-adapted model, outperforming MMI adaptation. Conversely, MMI adaption of the dictated set outperforms the MAP adaptation by achieving an absolute increase of 1.48% in WAR over the un-adapted model. We also demonstrate that fMLLR speaker adaptation is unsuitable for our task due to limited adaptation data.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.045",
      "openalex_id": "https://openalex.org/W2345671047",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EarSpeech: Exploring In-Ear Occlusion Effect on Earphones for Data-efficient Airborne Speech Enhancement",
      "summary": "Earphones have become a popular voice input and interaction device. However, airborne speech is susceptible to ambient noise, making it necessary to improve the quality and intelligibility of speech on earphones in noisy conditions. As the dual-microphone structure (i.e., outer and in-ear microphones) has been widely adopted in earphones (especially ANC earphones), we design EarSpeech which exploits in-ear acoustic sensory as the complementary modality to enable airborne speech enhancement. The key idea of EarSpeech is that in-ear speech is less sensitive to ambient noise and exhibits a correlation with airborne speech. However, due to the occlusion effect, in-ear speech has limited bandwidth, making it challenging to directly correlate with full-band airborne speech. Therefore, we exploit the occlusion effect to carry out theoretical modeling and quantitative analysis of this cross-channel correlation and study how to leverage such cross-channel correlation for speech enhancement. Specifically, we design a series of methodologies including data augmentation, deep learning-based fusion, and noise mixture scheme, to improve the generalization, effectiveness, and robustness of EarSpeech, respectively. Lastly, we conduct real-world experiments to evaluate the performance of our system. Specifically, EarSpeech achieves an average improvement ratio of 27.23% and 13.92% in terms of PESQ and STOI, respectively, and significantly improves SI-SDR by 8.91 dB. Benefiting from data augmentation, EarSpeech can achieve comparable performance with a small-scale dataset that is 40 times less than the original dataset. In addition, we validate the generalization of different users, speech content, and language types, respectively, as well as robustness in the real world via comprehensive experiments. The audio demo of EarSpeech is available on https://github.com/EarSpeech/earspeech.github.io/.",
      "abstract": "Earphones have become a popular voice input and interaction device. However, airborne speech is susceptible to ambient noise, making it necessary to improve the quality and intelligibility of speech on earphones in noisy conditions. As the dual-microphone structure (i.e., outer and in-ear microphones) has been widely adopted in earphones (especially ANC earphones), we design EarSpeech which exploits in-ear acoustic sensory as the complementary modality to enable airborne speech enhancement. The key idea of EarSpeech is that in-ear speech is less sensitive to ambient noise and exhibits a correlation with airborne speech. However, due to the occlusion effect, in-ear speech has limited bandwidth, making it challenging to directly correlate with full-band airborne speech. Therefore, we exploit the occlusion effect to carry out theoretical modeling and quantitative analysis of this cross-channel correlation and study how to leverage such cross-channel correlation for speech enhancement. Specifically, we design a series of methodologies including data augmentation, deep learning-based fusion, and noise mixture scheme, to improve the generalization, effectiveness, and robustness of EarSpeech, respectively. Lastly, we conduct real-world experiments to evaluate the performance of our system. Specifically, EarSpeech achieves an average improvement ratio of 27.23% and 13.92% in terms of PESQ and STOI, respectively, and significantly improves SI-SDR by 8.91 dB. Benefiting from data augmentation, EarSpeech can achieve comparable performance with a small-scale dataset that is 40 times less than the original dataset. In addition, we validate the generalization of different users, speech content, and language types, respectively, as well as robustness in the real world via comprehensive experiments. The audio demo of EarSpeech is available on https://github.com/EarSpeech/earspeech.github.io/.",
      "doi": "https://doi.org/10.1145/3678594",
      "openalex_id": "https://openalex.org/W4402349786",
      "arxiv_id": "",
      "publication_date": "2024-08-22",
      "published": "2024-08-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Knowledge Distillation-Based Training of Speech Enhancement for Noise-Robust Automatic Speech Recognition",
      "summary": "This paper addresses the training issues associated with neural network-based automatic speech recognition (ASR) under noise conditions. In particular, conventional joint training approaches for a pipeline comprising speech enhancement (SE) and end-to-end ASR model surfer from a conflicting problem and a frame mismatched alignment problem because of different goals and different frame structures for ASR and SE. To mitigate such problems, a knowledge distillation (KD)-based training approach is proposed by interpreting the ASR and SE models in the pipeline as teacher and student models, respectively. In the proposed KD-based training approach, the ASR model is first trained using a training dataset, and then, acoustic tokens are generated via K-means clustering using the latent vectors of the ASR encoder. Thereafter, KD-based training of the SE model is performed using the generated acoustic tokens. The performance of the SE and ASR models is evaluated on two different databases, noisy LibriSpeech and CHiME-4, which correspond to simulated and real-world noise conditions, respectively. The experimental results show that the proposed KD-based training approach yields a lower character error rate (CER) and word error rate (WER) on the two datasets than conventional joint training approaches, including multi-condition training. The results also show that the speech quality scores of the SE model trained using the proposed training approach are higher than those of SE models trained using conventional training approaches. Moreover, the noise reduction scores of the proposed training approach are higher than those of conventional joint training approaches but slightly lower than those of the standalone-SE training approach. Finally, an ablation study is conducted to examine the contribution of different combinations of loss functions in the proposed training approach to SE and ASR performance. The results show that the combination of all loss functions yields the lowest CER and WER and that tokenizer loss contributes more to SE and ASR performance improvement than ASR encoder loss.",
      "abstract": "This paper addresses the training issues associated with neural network-based automatic speech recognition (ASR) under noise conditions. In particular, conventional joint training approaches for a pipeline comprising speech enhancement (SE) and end-to-end ASR model surfer from a conflicting problem and a frame mismatched alignment problem because of different goals and different frame structures for ASR and SE. To mitigate such problems, a knowledge distillation (KD)-based training approach is proposed by interpreting the ASR and SE models in the pipeline as teacher and student models, respectively. In the proposed KD-based training approach, the ASR model is first trained using a training dataset, and then, acoustic tokens are generated via K-means clustering using the latent vectors of the ASR encoder. Thereafter, KD-based training of the SE model is performed using the generated acoustic tokens. The performance of the SE and ASR models is evaluated on two different databases, noisy LibriSpeech and CHiME-4, which correspond to simulated and real-world noise conditions, respectively. The experimental results show that the proposed KD-based training approach yields a lower character error rate (CER) and word error rate (WER) on the two datasets than conventional joint training approaches, including multi-condition training. The results also show that the speech quality scores of the SE model trained using the proposed training approach are higher than those of SE models trained using conventional training approaches. Moreover, the noise reduction scores of the proposed training approach are higher than those of conventional joint training approaches but slightly lower than those of the standalone-SE training approach. Finally, an ablation study is conducted to examine the contribution of different combinations of loss functions in the proposed training approach to SE and ASR performance. The results show that the combination of all loss functions yields the lowest CER and WER and that tokenizer loss contributes more to SE and ASR performance improvement than ASR encoder loss.",
      "doi": "https://doi.org/10.1109/access.2024.3403761",
      "openalex_id": "https://openalex.org/W4398162650",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spatio-Temporal Features Representation Using Recurrent Capsules for Monaural Speech Enhancement",
      "summary": "Single-channel speech enhancement is important for modern communication systems and has received a lot of attention. A convolutional neural network (CNN) successfully learns feature representations from speech spectrograms but loses spatial information due to distortion, which is important for humans to understand speech. Speech feature learning is an important ongoing research to capture higher-level representations of speech that go beyond conventional techniques. By considering the hierarchical structure and temporal relationships within speech signals, capsule networks (CapsNets) have the potential to provide more expressive and context-aware feature representations. By considering the advantages of CapNets over CNN, this study presents a model for monaural speech enhancement that keeps spatial information in a capsule and uses dynamic routing to pass it to higher layers. Dynamic routing replaces the pooling recurrent hidden states to get speech features from the outputs of the capsule. Leveraging long-term contexts provides identification of the target speaker. Therefore, a gated recurrent layer, gated recurrent unit (GRU), or long-short-term memory (LSTM), is placed above the CNN module and next to the capsule module in the architecture. This makes it viable to extract spatial features and long-term temporal dynamics. The suggested convolutional recurrent CapNet performs better compared to the models based on CNNs and recurrent neural networks. The suggested speech enhancement produces considerably better speech quality and intelligibility. With the LibriSpeech and VoiceBank&#x002B;DEMAND databases, the suggested speech enhancement improves the intelligibility and quality by 18.33&#x0025; and (0.94) 36.82&#x0025; over the noisy mixtures.",
      "abstract": "Single-channel speech enhancement is important for modern communication systems and has received a lot of attention. A convolutional neural network (CNN) successfully learns feature representations from speech spectrograms but loses spatial information due to distortion, which is important for humans to understand speech. Speech feature learning is an important ongoing research to capture higher-level representations of speech that go beyond conventional techniques. By considering the hierarchical structure and temporal relationships within speech signals, capsule networks (CapsNets) have the potential to provide more expressive and context-aware feature representations. By considering the advantages of CapNets over CNN, this study presents a model for monaural speech enhancement that keeps spatial information in a capsule and uses dynamic routing to pass it to higher layers. Dynamic routing replaces the pooling recurrent hidden states to get speech features from the outputs of the capsule. Leveraging long-term contexts provides identification of the target speaker. Therefore, a gated recurrent layer, gated recurrent unit (GRU), or long-short-term memory (LSTM), is placed above the CNN module and next to the capsule module in the architecture. This makes it viable to extract spatial features and long-term temporal dynamics. The suggested convolutional recurrent CapNet performs better compared to the models based on CNNs and recurrent neural networks. The suggested speech enhancement produces considerably better speech quality and intelligibility. With the LibriSpeech and VoiceBank&#x002B;DEMAND databases, the suggested speech enhancement improves the intelligibility and quality by 18.33&#x0025; and (0.94) 36.82&#x0025; over the noisy mixtures.",
      "doi": "https://doi.org/10.1109/access.2024.3361286",
      "openalex_id": "https://openalex.org/W4391454503",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Powerful and Extensible WFST Framework for Rnn-Transducer Losses",
      "summary": "This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) \"Compose-Transducer\", based on a composition of the WFST graphs from acoustic and textual schema – computationally competitive and easy to modify; (2) \"Grid-Transducer\", which constructs the lattice directly for further computations – most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss – the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.",
      "abstract": "This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) \"Compose-Transducer\", based on a composition of the WFST graphs from acoustic and textual schema – computationally competitive and easy to modify; (2) \"Grid-Transducer\", which constructs the lattice directly for further computations – most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss – the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096679",
      "openalex_id": "https://openalex.org/W4372267411",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech",
      "summary": "Prosody Transfer (PT) is a technique that aims to use the prosody from a\\nsource audio as a reference while synthesising speech. Fine-grained PT aims at\\ncapturing prosodic aspects like rhythm, emphasis, melody, duration, and\\nloudness, from a source audio at a very granular level and transferring them\\nwhen synthesising speech in a different target speaker's voice. Current\\napproaches for fine-grained PT suffer from source speaker leakage, where the\\nsynthesised speech has the voice identity of the source speaker as opposed to\\nthe target speaker. In order to mitigate this issue, they compromise on the\\nquality of PT. In this paper, we propose CopyCat, a novel, many-to-many PT\\nsystem that is robust to source speaker leakage, without using parallel data.\\nWe achieve this through a novel reference encoder architecture capable of\\ncapturing temporal prosodic representations which are robust to source speaker\\nleakage. We compare CopyCat against a state-of-the-art fine-grained PT model\\nthrough various subjective evaluations, where we show a relative improvement of\\n$47\\\\%$ in the quality of prosody transfer and $14\\\\%$ in preserving the target\\nspeaker identity, while still maintaining the same naturalness.\\n",
      "abstract": "Prosody Transfer (PT) is a technique that aims to use the prosody from a\\nsource audio as a reference while synthesising speech. Fine-grained PT aims at\\ncapturing prosodic aspects like rhythm, emphasis, melody, duration, and\\nloudness, from a source audio at a very granular level and transferring them\\nwhen synthesising speech in a different target speaker's voice. Current\\napproaches for fine-grained PT suffer from source speaker leakage, where the\\nsynthesised speech has the voice identity of the source speaker as opposed to\\nthe target speaker. In order to mitigate this issue, they compromise on the\\nquality of PT. In this paper, we propose CopyCat, a novel, many-to-many PT\\nsystem that is robust to source speaker leakage, without using parallel data.\\nWe achieve this through a novel reference encoder architecture capable of\\ncapturing temporal prosodic representations which are robust to source speaker\\nleakage. We compare CopyCat against a state-of-the-art fine-grained PT model\\nthrough various subjective evaluations, where we show a relative improvement of\\n$47\\\\%$ in the quality of prosody transfer and $14\\\\%$ in preserving the target\\nspeaker identity, while still maintaining the same naturalness.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-1251",
      "openalex_id": "https://openalex.org/W3022876224",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust and Fine-grained Prosody Control of End-to-end Speech Synthesis",
      "summary": "We propose prosody embeddings for emotional and expressive speech synthesis networks. The proposed methods introduce temporal structures in the embedding networks, thus enabling fine-grained control of the speaking style of the synthesized speech. The temporal structures can be designed either on the speech side or the text side, leading to different control resolutions in time. The prosody embedding networks are plugged into end-to-end speech synthesis networks and trained without any other supervision except for the target speech for synthesizing. It is demonstrated that the prosody embedding networks learned to extract prosodic features. By adjusting the learned prosody features, we could change the pitch and amplitude of the synthesized speech both at the frame level and the phoneme level. We also introduce the temporal normalization of prosody embeddings, which shows better robustness against speaker perturbations during prosody transfer tasks.",
      "abstract": "We propose prosody embeddings for emotional and expressive speech synthesis networks. The proposed methods introduce temporal structures in the embedding networks, thus enabling fine-grained control of the speaking style of the synthesized speech. The temporal structures can be designed either on the speech side or the text side, leading to different control resolutions in time. The prosody embedding networks are plugged into end-to-end speech synthesis networks and trained without any other supervision except for the target speech for synthesizing. It is demonstrated that the prosody embedding networks learned to extract prosodic features. By adjusting the learned prosody features, we could change the pitch and amplitude of the synthesized speech both at the frame level and the phoneme level. We also introduce the temporal normalization of prosody embeddings, which shows better robustness against speaker perturbations during prosody transfer tasks.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683501",
      "openalex_id": "https://openalex.org/W2964138190",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Information Bottleneck for Gaussian Variables",
      "summary": "The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information a...",
      "abstract": "The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information a...",
      "doi": "https://doi.org/10.5555/1046920.1046926",
      "openalex_id": "https://openalex.org/W3013104126",
      "arxiv_id": "",
      "publication_date": "2005-12-01",
      "published": "2005-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding disentangling in $\\beta$-VAE",
      "summary": "We present new intuitions and theoretical assessments of the emergence of\\ndisentangled representation in variational autoencoders. Taking a\\nrate-distortion theory perspective, we show the circumstances under which\\nrepresentations aligned with the underlying generative factors of variation of\\ndata emerge when optimising the modified ELBO bound in $\\\\beta$-VAE, as training\\nprogresses. From these insights, we propose a modification to the training\\nregime of $\\\\beta$-VAE, that progressively increases the information capacity of\\nthe latent code during training. This modification facilitates the robust\\nlearning of disentangled representations in $\\\\beta$-VAE, without the previous\\ntrade-off in reconstruction accuracy.\\n",
      "abstract": "We present new intuitions and theoretical assessments of the emergence of\\ndisentangled representation in variational autoencoders. Taking a\\nrate-distortion theory perspective, we show the circumstances under which\\nrepresentations aligned with the underlying generative factors of variation of\\ndata emerge when optimising the modified ELBO bound in $\\\\beta$-VAE, as training\\nprogresses. From these insights, we propose a modification to the training\\nregime of $\\\\beta$-VAE, that progressively increases the information capacity of\\nthe latent code during training. This modification facilitates the robust\\nlearning of disentangled representations in $\\\\beta$-VAE, without the previous\\ntrade-off in reconstruction accuracy.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1804.03599",
      "openalex_id": "https://openalex.org/W2796704765",
      "arxiv_id": "",
      "publication_date": "2018-04-10",
      "published": "2018-04-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What Does BERT Learn about the Structure of Language?",
      "summary": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
      "abstract": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
      "doi": "https://doi.org/10.18653/v1/p19-1356",
      "openalex_id": "https://openalex.org/W2948947170",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization",
      "summary": "To leverage crowd-sourced data to train multi-speaker text-to-speech (TTS) models that can synthesize clean speech for all speakers, it is essential to learn disentangled representations which can independently control the speaker identity and background noise in generated signals. However, learning such representations can be challenging, due to the lack of labels describing the recording conditions of each training example, and the fact that speakers and recording conditions are often correlated, e.g. since users often make many recordings using the same equipment. This paper proposes three components to address this problem by: (1) formulating a conditional generative model with factorized latent variables, (2) using data augmentation to add noise that is not correlated with speaker identity and whose label is known during training, and (3) using adversarial factorization to improve disentanglement. Experimental results demonstrate that the proposed method can disentangle speaker and noise attributes even if they are correlated in the training data, and can be used to consistently synthesize clean speech for all speakers. Ablation studies verify the importance of each proposed component.",
      "abstract": "To leverage crowd-sourced data to train multi-speaker text-to-speech (TTS) models that can synthesize clean speech for all speakers, it is essential to learn disentangled representations which can independently control the speaker identity and background noise in generated signals. However, learning such representations can be challenging, due to the lack of labels describing the recording conditions of each training example, and the fact that speakers and recording conditions are often correlated, e.g. since users often make many recordings using the same equipment. This paper proposes three components to address this problem by: (1) formulating a conditional generative model with factorized latent variables, (2) using data augmentation to add noise that is not correlated with speaker identity and whose label is known during training, and (3) using adversarial factorization to improve disentanglement. Experimental results demonstrate that the proposed method can disentangle speaker and noise attributes even if they are correlated in the training data, and can be used to consistently synthesize clean speech for all speakers. Ablation studies verify the importance of each proposed component.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683561",
      "openalex_id": "https://openalex.org/W2907262790",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network",
      "summary": "The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational autoencoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.",
      "abstract": "The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational autoencoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.",
      "doi": "https://doi.org/10.48550/arxiv.1905.07195",
      "openalex_id": "https://openalex.org/W2952269766",
      "arxiv_id": "",
      "publication_date": "2019-05-17",
      "published": "2019-05-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing",
      "summary": "Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter β. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for β, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing βmultiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",
      "abstract": "Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter β. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for β, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing βmultiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",
      "doi": "https://doi.org/10.48550/arxiv.1903.10145",
      "openalex_id": "https://openalex.org/W2951670304",
      "arxiv_id": "",
      "publication_date": "2019-03-25",
      "published": "2019-03-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Experimental and theoretical advances in prosody: A review",
      "summary": "Research on prosody has recently become an important focus in various disciplines, including Linguistics, Psychology, and Computer Science. This article reviews recent research advances on two key issues: prosodic phrasing and prosodic prominence. Both aspects of prosody are influenced by linguistic factors such as syntactic constituent structure, semantic relations, phonological rhythm, pragmatic considerations, and also by processing factors such as the length, complexity or predictability of linguistic material. Our review summarizes recent insights into the production and perception of these two components of prosody and their grammatical underpinnings. While this review only covers a subset of a broader set of research topics on prosody in cognitive science, they are representative of a tendency in the field toward a more interdisciplinary approach.",
      "abstract": "Research on prosody has recently become an important focus in various disciplines, including Linguistics, Psychology, and Computer Science. This article reviews recent research advances on two key issues: prosodic phrasing and prosodic prominence. Both aspects of prosody are influenced by linguistic factors such as syntactic constituent structure, semantic relations, phonological rhythm, pragmatic considerations, and also by processing factors such as the length, complexity or predictability of linguistic material. Our review summarizes recent insights into the production and perception of these two components of prosody and their grammatical underpinnings. While this review only covers a subset of a broader set of research topics on prosody in cognitive science, they are representative of a tendency in the field toward a more interdisciplinary approach.",
      "doi": "https://doi.org/10.1080/01690961003589492",
      "openalex_id": "https://openalex.org/W2069859485",
      "arxiv_id": "",
      "publication_date": "2010-05-26",
      "published": "2010-05-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
      "summary": "This paper presents a novel design of neural network system for fine-grained\\nstyle modeling, transfer and prediction in expressive text-to-speech (TTS)\\nsynthesis. Fine-grained modeling is realized by extracting style embeddings\\nfrom the mel-spectrograms of phone-level speech segments. Collaborative\\nlearning and adversarial learning strategies are applied in order to achieve\\neffective disentanglement of content and style factors in speech and alleviate\\nthe \"content leakage\" problem in style modeling. The proposed system can be\\nused for varying-content speech style transfer in the single-speaker scenario.\\nThe results of objective and subjective evaluation show that our system\\nperforms better than other fine-grained speech style transfer models,\\nespecially in the aspect of content preservation. By incorporating a style\\npredictor, the proposed system can also be used for text-to-speech synthesis.\\nAudio samples are provided for system demonstration\\nhttps://daxintan-cuhk.github.io/pl-csd-speech .\\n",
      "abstract": "This paper presents a novel design of neural network system for fine-grained\\nstyle modeling, transfer and prediction in expressive text-to-speech (TTS)\\nsynthesis. Fine-grained modeling is realized by extracting style embeddings\\nfrom the mel-spectrograms of phone-level speech segments. Collaborative\\nlearning and adversarial learning strategies are applied in order to achieve\\neffective disentanglement of content and style factors in speech and alleviate\\nthe \"content leakage\" problem in style modeling. The proposed system can be\\nused for varying-content speech style transfer in the single-speaker scenario.\\nThe results of objective and subjective evaluation show that our system\\nperforms better than other fine-grained speech style transfer models,\\nespecially in the aspect of content preservation. By incorporating a style\\npredictor, the proposed system can also be used for text-to-speech synthesis.\\nAudio samples are provided for system demonstration\\nhttps://daxintan-cuhk.github.io/pl-csd-speech .\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-1129",
      "openalex_id": "https://openalex.org/W3152136404",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Architecture of the Festival Speech Synthesis System",
      "summary": "We describe a new formalism for storing linguistic data in a text to speech system. Linguistic entities such as words and phones are stored as feature structures in a general object called an linguistic item. Items are configurable at run time and via the feature structure can contain arbitrary information. Linguistic relations are used to store the relationship between items of the same linguistic type. Relations can take any graph structure but are commonly trees or lists. Utterance structures contain all the items and relations contained in a single utterance. We first describe the design goals when building a synthesis architecture, and then describe some problems with previous architectures. We then discuss our new formalism in general along with the implementation details and consequences of our approach. 1.",
      "abstract": "We describe a new formalism for storing linguistic data in a text to speech system. Linguistic entities such as words and phones are stored as feature structures in a general object called an linguistic item. Items are configurable at run time and via the feature structure can contain arbitrary information. Linguistic relations are used to store the relationship between items of the same linguistic type. Relations can take any graph structure but are commonly trees or lists. Utterance structures contain all the items and relations contained in a single utterance. We first describe the design goals when building a synthesis architecture, and then describe some problems with previous architectures. We then discuss our new formalism in general along with the implementation details and consequences of our approach. 1.",
      "doi": "",
      "openalex_id": "https://openalex.org/W1878590289",
      "arxiv_id": "",
      "publication_date": "1998-11-01",
      "published": "1998-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Topic Identification for Speech Without ASR",
      "summary": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.",
      "abstract": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.",
      "doi": "https://doi.org/10.21437/interspeech.2017-1093",
      "openalex_id": "https://openalex.org/W2963230678",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Indexing by latent semantic analysis",
      "summary": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\"semantic structure\") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley & Sons, Inc.",
      "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\"semantic structure\") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley & Sons, Inc.",
      "doi": "https://doi.org/10.1002/(sici)1097-4571(199009)41:6<391::aid-asi1>3.0.co;2-9",
      "openalex_id": "https://openalex.org/W2147152072",
      "arxiv_id": "",
      "publication_date": "1990-09-01",
      "published": "1990-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Topic identification of spoken documents using unsupervised acoustic unit discovery",
      "summary": "This paper investigates the application of unsupervised acoustic unit discovery for topic identification (topic ID) of spoken audio documents. The acoustic unit discovery method is based on a non-parametric Bayesian phone-loop model that segments a speech utterance into phone-like categories. The discovered phone-like (acoustic) units are further fed into the conventional topic ID framework. Using multilingual bottleneck features for the acoustic unit discovery, we show that the proposed method outperforms other systems that are based on cross-lingual phoneme recognizer.",
      "abstract": "This paper investigates the application of unsupervised acoustic unit discovery for topic identification (topic ID) of spoken audio documents. The acoustic unit discovery method is based on a non-parametric Bayesian phone-loop model that segments a speech utterance into phone-like categories. The discovered phone-like (acoustic) units are further fed into the conventional topic ID framework. Using multilingual bottleneck features for the acoustic unit discovery, we show that the proposed method outperforms other systems that are based on cross-lingual phoneme recognizer.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953257",
      "openalex_id": "https://openalex.org/W2719865699",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploiting latent semantic information in statistical language modeling",
      "summary": "Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance.",
      "abstract": "Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance.",
      "doi": "https://doi.org/10.1109/5.880084",
      "openalex_id": "https://openalex.org/W2118714763",
      "arxiv_id": "",
      "publication_date": "2000-08-01",
      "published": "2000-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning",
      "summary": "We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",
      "abstract": "We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",
      "doi": "https://doi.org/10.48550/arxiv.2208.04202",
      "openalex_id": "https://openalex.org/W4300425011",
      "arxiv_id": "",
      "publication_date": "2022-08-08",
      "published": "2022-08-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "summary": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
      "abstract": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
      "doi": "https://doi.org/10.48550/arxiv.2210.08933",
      "openalex_id": "https://openalex.org/W4306802991",
      "arxiv_id": "",
      "publication_date": "2022-10-17",
      "published": "2022-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical\\n Distributions",
      "summary": "Generative flows and diffusion models have been predominantly trained on\\nordinal data, for example natural images. This paper introduces two extensions\\nof flows and diffusion for categorical data such as language or image\\nsegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined\\nby a composition of a continuous distribution (such as a normalizing flow), and\\nan argmax function. To optimize this model, we learn a probabilistic inverse\\nfor the argmax that lifts the categorical data to a continuous space.\\nMultinomial Diffusion gradually adds categorical noise in a diffusion process,\\nfor which the generative denoising process is learned. We demonstrate that our\\nmethod outperforms existing dequantization approaches on text modelling and\\nmodelling on image segmentation maps in log-likelihood.\\n",
      "abstract": "Generative flows and diffusion models have been predominantly trained on\\nordinal data, for example natural images. This paper introduces two extensions\\nof flows and diffusion for categorical data such as language or image\\nsegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined\\nby a composition of a continuous distribution (such as a normalizing flow), and\\nan argmax function. To optimize this model, we learn a probabilistic inverse\\nfor the argmax that lifts the categorical data to a continuous space.\\nMultinomial Diffusion gradually adds categorical noise in a diffusion process,\\nfor which the generative denoising process is learned. We demonstrate that our\\nmethod outperforms existing dequantization approaches on text modelling and\\nmodelling on image segmentation maps in log-likelihood.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2102.05379",
      "openalex_id": "https://openalex.org/W4287329820",
      "arxiv_id": "",
      "publication_date": "2021-02-10",
      "published": "2021-02-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Accelerating Large-Scale Inference with Anisotropic Vector Quantization",
      "summary": "Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \\url{ann-benchmarks.com}.",
      "abstract": "Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \\url{ann-benchmarks.com}.",
      "doi": "https://doi.org/10.48550/arxiv.1908.10396",
      "openalex_id": "https://openalex.org/W3024786184",
      "arxiv_id": "",
      "publication_date": "2019-08-27",
      "published": "2019-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "summary": "Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
      "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
      "doi": "https://doi.org/10.48550/arxiv.2212.09412",
      "openalex_id": "https://openalex.org/W4312051726",
      "arxiv_id": "",
      "publication_date": "2022-12-19",
      "published": "2022-12-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
      "summary": "India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
      "abstract": "India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
      "doi": "https://doi.org/10.48550/arxiv.2305.16307",
      "openalex_id": "https://openalex.org/W4378505287",
      "arxiv_id": "",
      "publication_date": "2023-05-25",
      "published": "2023-05-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Machine Translation System for English to Indian Language Translation Using MTIL Parallel Corpus",
      "summary": "Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.",
      "abstract": "Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.",
      "doi": "https://doi.org/10.1515/jisys-2019-2510",
      "openalex_id": "https://openalex.org/W2924093092",
      "arxiv_id": "",
      "publication_date": "2019-03-20",
      "published": "2019-03-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
      "summary": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
      "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
      "doi": "https://doi.org/10.48550/arxiv.2305.14716",
      "openalex_id": "https://openalex.org/W4378473793",
      "arxiv_id": "",
      "publication_date": "2023-05-24",
      "published": "2023-05-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Developing a Speech Recognition System for Recognizing Tonal Speech Signals Using a Convolutional Neural Network",
      "summary": "Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.",
      "abstract": "Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.",
      "doi": "https://doi.org/10.3390/app12126223",
      "openalex_id": "https://openalex.org/W4283121045",
      "arxiv_id": "",
      "publication_date": "2022-06-19",
      "published": "2022-06-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech-to-Speech Translation into Multiple Target Languages",
      "summary": "Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",
      "abstract": "Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",
      "doi": "https://doi.org/10.48550/arxiv.2307.08655",
      "openalex_id": "https://openalex.org/W4384648564",
      "arxiv_id": "",
      "publication_date": "2023-07-17",
      "published": "2023-07-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Speech to Speech Machine Translation focusing on Indian Languages",
      "summary": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.",
      "abstract": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.",
      "doi": "https://doi.org/10.18653/v1/2023.eacl-demo.19",
      "openalex_id": "https://openalex.org/W4386566860",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ClassRoute: An English to Punjabi Educational Video Translation Pipeline for Supporting Punjabi Mother-Tongue Education",
      "summary": "Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.",
      "abstract": "Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.",
      "doi": "https://doi.org/10.1109/ghtc53159.2021.9612485",
      "openalex_id": "https://openalex.org/W3215465553",
      "arxiv_id": "",
      "publication_date": "2021-10-19",
      "published": "2021-10-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
      "summary": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
      "abstract": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
      "doi": "https://doi.org/10.48550/arxiv.2212.05409",
      "openalex_id": "https://openalex.org/W4311550865",
      "arxiv_id": "",
      "publication_date": "2022-12-11",
      "published": "2022-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Common Voice: A Massively-Multilingual Speech Corpus",
      "summary": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1912.06670",
      "openalex_id": "https://openalex.org/W2995929068",
      "arxiv_id": "",
      "publication_date": "2019-12-13",
      "published": "2019-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VOXLINGUA107: A Dataset for Spoken Language Recognition",
      "summary": "This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.",
      "abstract": "This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383459",
      "openalex_id": "https://openalex.org/W3106807794",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken\\n Utterances Extracted from the Bible",
      "summary": "The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\\npublished multilingual speech dataset based on recorded readings of the New\\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\\nthat the source content (the Bible) is the same for all the languages is not\\nexploited to date.Therefore, this article proposes to add multilingual links\\nbetween speech segments in different languages, and shares a large and clean\\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\\nalignment as well as on translation for typologically different language pairs.\\nThe quality of the final corpus is attested by human evaluation performed on a\\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\\nusefulness of the final product on a bilingual speech retrieval task.\\n",
      "abstract": "The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\\npublished multilingual speech dataset based on recorded readings of the New\\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\\nthat the source content (the Bible) is the same for all the languages is not\\nexploited to date.Therefore, this article proposes to add multilingual links\\nbetween speech segments in different languages, and shares a large and clean\\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\\nalignment as well as on translation for typologically different language pairs.\\nThe quality of the final corpus is attested by human evaluation performed on a\\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\\nusefulness of the final product on a bilingual speech retrieval task.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1907.12895",
      "openalex_id": "https://openalex.org/W2966095117",
      "arxiv_id": "",
      "publication_date": "2019-07-30",
      "published": "2019-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages",
      "summary": "A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",
      "abstract": "A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",
      "doi": "https://doi.org/10.48550/arxiv.2208.11761",
      "openalex_id": "https://openalex.org/W4293332626",
      "arxiv_id": "",
      "publication_date": "2022-08-24",
      "published": "2022-08-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust Low Rate Speech Coding Based on Cloned Networks and Wavenet",
      "summary": "Rapid advances in machine-learning based generative modeling of speech make its use in speech coding attractive. However, the current performance of such models drops rapidly with noise contamination of the input, preventing use in practical applications. We present a new speech-coding scheme that is based on features that are robust to the distortions occurring in speech-coder input signals. To this purpose, we encourage the feature encoder to provide the same independent features for each of a set of linguistically equivalent signals, obtained by adding various noises to a common clean signal. The independent features, subjected to scalar quantization, are used as a conditioning vector sequence for WaveNet. Our experiments show that a 1.8 kb/s implementation of the resulting coder provides state-of-the-art performance for clean signals, and is additionally robust to noisy input.",
      "abstract": "Rapid advances in machine-learning based generative modeling of speech make its use in speech coding attractive. However, the current performance of such models drops rapidly with noise contamination of the input, preventing use in practical applications. We present a new speech-coding scheme that is based on features that are robust to the distortions occurring in speech-coder input signals. To this purpose, we encourage the feature encoder to provide the same independent features for each of a set of linguistically equivalent signals, obtained by adding various noises to a common clean signal. The independent features, subjected to scalar quantization, are used as a conditioning vector sequence for WaveNet. Our experiments show that a 1.8 kb/s implementation of the resulting coder provides state-of-the-art performance for clean signals, and is additionally robust to noisy input.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053657",
      "openalex_id": "https://openalex.org/W3016098186",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Regression Approach to Speech Enhancement Based on Deep Neural Networks",
      "summary": "In contrast to the conventional minimum mean square error (MMSE)-based noise reduction techniques, we propose a supervised method to enhance speech by means of finding a mapping function between noisy and clean speech signals based on deep neural networks (DNNs). In order to be able to handle a wide range of additive noises in real-world situations, a large training set that encompasses many possible combinations of speech and noise types, is first designed. A DNN architecture is then employed as a nonlinear regression function to ensure a powerful modeling capability. Several techniques have also been proposed to improve the DNN-based speech enhancement system, including global variance equalization to alleviate the over-smoothing problem of the regression model, and the dropout and noise-aware training strategies to further improve the generalization capability of DNNs to unseen noise conditions. Experimental results demonstrate that the proposed framework can achieve significant improvements in both objective and subjective measures over the conventional MMSE based technique. It is also interesting to observe that the proposed DNN approach can well suppress highly nonstationary noise, which is tough to handle in general. Furthermore, the resulting DNN model, trained with artificial synthesized data, is also effective in dealing with noisy speech data recorded in real-world scenarios without the generation of the annoying musical artifact commonly observed in conventional enhancement methods.",
      "abstract": "In contrast to the conventional minimum mean square error (MMSE)-based noise reduction techniques, we propose a supervised method to enhance speech by means of finding a mapping function between noisy and clean speech signals based on deep neural networks (DNNs). In order to be able to handle a wide range of additive noises in real-world situations, a large training set that encompasses many possible combinations of speech and noise types, is first designed. A DNN architecture is then employed as a nonlinear regression function to ensure a powerful modeling capability. Several techniques have also been proposed to improve the DNN-based speech enhancement system, including global variance equalization to alleviate the over-smoothing problem of the regression model, and the dropout and noise-aware training strategies to further improve the generalization capability of DNNs to unseen noise conditions. Experimental results demonstrate that the proposed framework can achieve significant improvements in both objective and subjective measures over the conventional MMSE based technique. It is also interesting to observe that the proposed DNN approach can well suppress highly nonstationary noise, which is tough to handle in general. Furthermore, the resulting DNN model, trained with artificial synthesized data, is also effective in dealing with noisy speech data recorded in real-world scenarios without the generation of the annoying musical artifact commonly observed in conventional enhancement methods.",
      "doi": "https://doi.org/10.1109/taslp.2014.2364452",
      "openalex_id": "https://openalex.org/W2044893557",
      "arxiv_id": "",
      "publication_date": "2014-10-21",
      "published": "2014-10-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "summary": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
      "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
      "doi": "https://doi.org/10.1109/icassp.2017.7952261",
      "openalex_id": "https://openalex.org/W2593116425",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Noisy speech database for training speech enhancement algorithms and TTS models",
      "summary": "Clean and noisy parallel speech database. The database was designed to train and test speech enhancement methods that operate at 48kHz. A more detailed description can be found in the papers associated with the database. For the 28 speaker dataset, details can be found in: C. Valentini-Botinhao, X. Wang, S. Takaki &amp; J. Yamagishi, \"Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System using Deep Recurrent Neural Networks\", In Proc. Interspeech 2016. For the 56 speaker dataset: C. Valentini-Botinhao, X. Wang, S. Takaki &amp; J. Yamagishi, \"Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech”, In Proc. SSW 2016. Some of the noises used to create the noisy speech were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the CSTR VCTK Corpus, available here: https://doi.org/10.7488/ds/1994. The speech-shaped and babble noise files that were used to create this dataset are available here: http://homepages.inf.ed.ac.uk/cvbotinh/se/noises/.",
      "abstract": "Clean and noisy parallel speech database. The database was designed to train and test speech enhancement methods that operate at 48kHz. A more detailed description can be found in the papers associated with the database. For the 28 speaker dataset, details can be found in: C. Valentini-Botinhao, X. Wang, S. Takaki &amp; J. Yamagishi, \"Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System using Deep Recurrent Neural Networks\", In Proc. Interspeech 2016. For the 56 speaker dataset: C. Valentini-Botinhao, X. Wang, S. Takaki &amp; J. Yamagishi, \"Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech”, In Proc. SSW 2016. Some of the noises used to create the noisy speech were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the CSTR VCTK Corpus, available here: https://doi.org/10.7488/ds/1994. The speech-shaped and babble noise files that were used to create this dataset are available here: http://homepages.inf.ed.ac.uk/cvbotinh/se/noises/.",
      "doi": "https://doi.org/10.7488/ds/2117",
      "openalex_id": "https://openalex.org/W2757519008",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised HMM posteriograms for language independent acoustic modeling in zero resource conditions",
      "summary": "The task of language independent acoustic unit modeling in unlabeled raw speech (zero-resource setting) has gained significant interest over the recent years. The main challenge here is the extraction of acoustic representations that elicit good similarity between the same words or linguistic tokens spoken by different speakers and to derive these representations in a language independent manner. In this paper, we explore the use of Hidden Markov Model (HMM) based posteriograms for unsupervised acoustic unit modeling. The states of the HMM (which represent the language independent acoustic units) are initialized using a Gaussian mixture model (GMM) - Universal Background Model (UBM). The trained HMM is subsequently used to generate a temporally contiguous state alignment which are then modeled in a hybrid deep neural network (DNN) model. For the purpose of testing, we use the frame level HMM state posteriors obtained from the DNN as features for the ZeroSpeech challenge task. The minimal pair ABX error rate is measured for both the within and across speaker pairs. With several experiments on multiple languages in the ZeroSpeech corpus, we show that the proposed HMM based posterior features provides significant improvements over the baseline system using MFCC features (average relative improvements of 25% for within speaker pairs and 40% for across speaker pairs). Furthermore, the experiments where the target language is not seen training illustrate the proposed modeling approach is capable of learning global language independent representations.",
      "abstract": "The task of language independent acoustic unit modeling in unlabeled raw speech (zero-resource setting) has gained significant interest over the recent years. The main challenge here is the extraction of acoustic representations that elicit good similarity between the same words or linguistic tokens spoken by different speakers and to derive these representations in a language independent manner. In this paper, we explore the use of Hidden Markov Model (HMM) based posteriograms for unsupervised acoustic unit modeling. The states of the HMM (which represent the language independent acoustic units) are initialized using a Gaussian mixture model (GMM) - Universal Background Model (UBM). The trained HMM is subsequently used to generate a temporally contiguous state alignment which are then modeled in a hybrid deep neural network (DNN) model. For the purpose of testing, we use the frame level HMM state posteriors obtained from the DNN as features for the ZeroSpeech challenge task. The minimal pair ABX error rate is measured for both the within and across speaker pairs. With several experiments on multiple languages in the ZeroSpeech corpus, we show that the proposed HMM based posterior features provides significant improvements over the baseline system using MFCC features (average relative improvements of 25% for within speaker pairs and 40% for across speaker pairs). Furthermore, the experiments where the target language is not seen training illustrate the proposed modeling approach is capable of learning global language independent representations.",
      "doi": "https://doi.org/10.1109/asru.2017.8269014",
      "openalex_id": "https://openalex.org/W2785860501",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep learning methods for unsupervised acoustic modeling — Leap submission to ZeroSpeech challenge 2017",
      "summary": "In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.",
      "abstract": "In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.",
      "doi": "https://doi.org/10.1109/asru.2017.8269013",
      "openalex_id": "https://openalex.org/W2787223168",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
      "summary": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.",
      "abstract": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2605",
      "openalex_id": "https://openalex.org/W2935542736",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-supervised Learning: Generative or Contrastive",
      "summary": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",
      "abstract": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",
      "doi": "https://doi.org/10.1109/tkde.2021.3090866",
      "openalex_id": "https://openalex.org/W3035725276",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An investigation on the use of acoustic sub-word units for automatic speech recognition",
      "summary": "An approach to automatic speech recognition is described which attempts to link together ideas from pattern recognition such as dynamic time warping and hidden Markov modeling, with ideas from linguistically motivated approaches. In this approach, the basic sub-word units are defined acoustically, but not necessarily phonetically. An algorithm was developed which automatically decomposed speech into multiple sub-word segments, based solely upon strict acoustic criteria, without any reference to linguistic content. By repeating this procedure on a large corpus of speech data we obtained an extensive pool of unlabeled sub-word speech segments. Then using well defined clustering techniques, a small set of representative acoustic sub-word units (e.g. an inventory of units) was created. This process is fast, easy to use, and required no human intervention. The interpretation of these sub-word units, in a linguistic sense, in the context of word decoding is an important issue which must be addressed for them to be useful in a large vocabulary system. We have not yet addressed this issue; instead a couple of simple experiments were performed to determine if these acoustic sub-word units had any potential value for speech recognition. For these experiments we used a connected digits database from a single female talker. A 25 sub-word unit codebook of acoustic segments was created from about 1600 segments drawn from 100 connected digit strings. A simple isolated digit recognition system, designed using the statistics of the codewords in the acoustic sub-word unit codebook had a recognition accuracy of 100%. In another experiment a connected digit recognition system was created with representative digit templates created by concatenating the sub-word units in an appropriate manner. The system had a string recognition accuracy of 96%.",
      "abstract": "An approach to automatic speech recognition is described which attempts to link together ideas from pattern recognition such as dynamic time warping and hidden Markov modeling, with ideas from linguistically motivated approaches. In this approach, the basic sub-word units are defined acoustically, but not necessarily phonetically. An algorithm was developed which automatically decomposed speech into multiple sub-word segments, based solely upon strict acoustic criteria, without any reference to linguistic content. By repeating this procedure on a large corpus of speech data we obtained an extensive pool of unlabeled sub-word speech segments. Then using well defined clustering techniques, a small set of representative acoustic sub-word units (e.g. an inventory of units) was created. This process is fast, easy to use, and required no human intervention. The interpretation of these sub-word units, in a linguistic sense, in the context of word decoding is an important issue which must be addressed for them to be useful in a large vocabulary system. We have not yet addressed this issue; instead a couple of simple experiments were performed to determine if these acoustic sub-word units had any potential value for speech recognition. For these experiments we used a connected digits database from a single female talker. A 25 sub-word unit codebook of acoustic segments was created from about 1600 segments drawn from 100 connected digit strings. A simple isolated digit recognition system, designed using the statistics of the codewords in the acoustic sub-word unit codebook had a recognition accuracy of 100%. In another experiment a connected digit recognition system was created with representative digit templates created by concatenating the sub-word units in an appropriate manner. The system had a string recognition accuracy of 96%.",
      "doi": "https://doi.org/10.1109/icassp.1987.1169589",
      "openalex_id": "https://openalex.org/W1949782964",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A segment model based approach to speech recognition",
      "summary": "Proposes a global acoustic segment model for characterizing fundamental speech sound units and their interactions based upon a general framework of hidden Markov models (HMM). Each segment model represents a class of acoustically similar sounds. The intra-segment variability of each sound class is modeled by an HMM, and the sound-to-sound transition rules are characterized by a probabilistic intersegment transition matrix. An acoustically-derived lexicon is used to construct word models based upon subword segment models. The proposed segment model was tested on a speaker-trained, isolated word, speech recognition task with a vocabulary of 1109 basic English words. In the current study, only 128 segment models were used, and recognition was performed by optimally aligning the test utterance with all acoustic lexicon entries using a maximum likelihood Viterbi decoding algorithm. Based upon a database of three male speakers, the average word recognition accuracy for the top candidate was 85% and increased to 96% and 98% for the top 3 and top 5 candidates, respectively.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Proposes a global acoustic segment model for characterizing fundamental speech sound units and their interactions based upon a general framework of hidden Markov models (HMM). Each segment model represents a class of acoustically similar sounds. The intra-segment variability of each sound class is modeled by an HMM, and the sound-to-sound transition rules are characterized by a probabilistic intersegment transition matrix. An acoustically-derived lexicon is used to construct word models based upon subword segment models. The proposed segment model was tested on a speaker-trained, isolated word, speech recognition task with a vocabulary of 1109 basic English words. In the current study, only 128 segment models were used, and recognition was performed by optimally aligning the test utterance with all acoustic lexicon entries using a maximum likelihood Viterbi decoding algorithm. Based upon a database of three male speakers, the average word recognition accuracy for the top candidate was 85% and increased to 96% and 98% for the top 3 and top 5 candidates, respectively.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1988.196629",
      "openalex_id": "https://openalex.org/W1957665339",
      "arxiv_id": "",
      "publication_date": "2003-01-06",
      "published": "2003-01-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
      "summary": "This work tackles the problem of learning a set of language specific acoustic\\nunits from unlabeled speech recordings given a set of labeled recordings from\\nother languages. Our approach may be described by the following two steps\\nprocedure: first the model learns the notion of acoustic units from the\\nlabelled data and then the model uses its knowledge to find new acoustic units\\non the target language. We implement this process with the Bayesian Subspace\\nHidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model\\n(SGMM) where each low dimensional embedding represents an acoustic unit rather\\nthan just a HMM's state. The subspace is trained on 3 languages from the\\nGlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on\\nthe TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that\\nthis approach significantly outperforms previous HMM based acoustic units\\ndiscovery systems and compares favorably with the Variational Auto Encoder-HMM.\\n",
      "abstract": "This work tackles the problem of learning a set of language specific acoustic\\nunits from unlabeled speech recordings given a set of labeled recordings from\\nother languages. Our approach may be described by the following two steps\\nprocedure: first the model learns the notion of acoustic units from the\\nlabelled data and then the model uses its knowledge to find new acoustic units\\non the target language. We implement this process with the Bayesian Subspace\\nHidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model\\n(SGMM) where each low dimensional embedding represents an acoustic unit rather\\nthan just a HMM's state. The subspace is trained on 3 languages from the\\nGlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on\\nthe TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that\\nthis approach significantly outperforms previous HMM based acoustic units\\ndiscovery systems and compares favorably with the Variational Auto Encoder-HMM.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-2224",
      "openalex_id": "https://openalex.org/W2927191280",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Word Segmentation from Speech with Attention",
      "summary": "We present a first attempt to perform attentional word segmentation directly from the speech signal, with the final goal to automatically identify lexical units in a low-resource, unwritten language (UL). Our methodology assumes a pairing between recordings in the UL with translations in a well-resourced language. It uses Acoustic Unit Discovery (AUD) to convert speech into a sequence of pseudo-phones that is segmented using neural soft-alignments produced by a neural machine translation model. Evaluation uses an actual Bantu UL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the potential of attentional word segmentation for language documentation.",
      "abstract": "We present a first attempt to perform attentional word segmentation directly from the speech signal, with the final goal to automatically identify lexical units in a low-resource, unwritten language (UL). Our methodology assumes a pairing between recordings in the UL with translations in a well-resourced language. It uses Acoustic Unit Discovery (AUD) to convert speech into a sequence of pseudo-phones that is segmented using neural soft-alignments produced by a neural machine translation model. Evaluation uses an actual Bantu UL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the potential of attentional word segmentation for language documentation.",
      "doi": "https://doi.org/10.48550/arxiv.1806.06734",
      "openalex_id": "https://openalex.org/W2950057603",
      "arxiv_id": "",
      "publication_date": "2018-06-18",
      "published": "2018-06-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models",
      "summary": "Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art.",
      "abstract": "Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art.",
      "doi": "https://doi.org/10.48550/arxiv.1904.04100",
      "openalex_id": "https://openalex.org/W2934852845",
      "arxiv_id": "",
      "publication_date": "2019-04-08",
      "published": "2019-04-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR",
      "summary": "We investigate the use of cross-lingual acoustic data to initialise deep neural network (DNN) acoustic models by means of unsupervised restricted Boltzmann machine (RBM) pre-training. DNNs for German are pretrained using one or all of German, Portuguese, Spanish and Swedish. The DNNs are used in a tandem configuration, where the network outputs are used as features for a hidden Markov model (HMM) whose emission densities are modeled by Gaussian mixture models (GMMs), as well as in a hybrid configuration, where the network outputs are used as the HMM state likelihoods. The experiments show that unsupervised pretraining is more crucial for the hybrid setups, particularly with limited amounts of transcribed training data. More importantly, unsupervised pretraining is shown to be language-independent.",
      "abstract": "We investigate the use of cross-lingual acoustic data to initialise deep neural network (DNN) acoustic models by means of unsupervised restricted Boltzmann machine (RBM) pre-training. DNNs for German are pretrained using one or all of German, Portuguese, Spanish and Swedish. The DNNs are used in a tandem configuration, where the network outputs are used as features for a hidden Markov model (HMM) whose emission densities are modeled by Gaussian mixture models (GMMs), as well as in a hybrid configuration, where the network outputs are used as the HMM state likelihoods. The experiments show that unsupervised pretraining is more crucial for the hybrid setups, particularly with limited amounts of transcribed training data. More importantly, unsupervised pretraining is shown to be language-independent.",
      "doi": "https://doi.org/10.1109/slt.2012.6424230",
      "openalex_id": "https://openalex.org/W2120209245",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-language bootstrapping for unsupervised acoustic model training: rapid development of a Polish speech recognition system",
      "summary": "This paper describes the rapid development of a Polish language speech recognition system.The system development was performed without access to any transcribed acoustic training data.This was achieved through the combined use of cross-language bootstrapping and confidence based unsupervised acoustic model training.A Spanish acoustic model was ported to Polish, through the use of a manually constructed phoneme mapping.This initial model was refined through iterative recognition and retraining of the untranscribed audio data.The system was trained and evaluated on recordings from the European Parliament, and included several state-of-the-art speech recognition techniques in addition to the use of unsupervised model training.Confidence based speaker adaptive training using features space transform adaptation, as well as vocal tract length normalization and maximum likelihood linear regression, was used to refine the acoustic model.Through the combination of the different techniques, good performance was achieved on the domain of parliamentary speeches.",
      "abstract": "This paper describes the rapid development of a Polish language speech recognition system.The system development was performed without access to any transcribed acoustic training data.This was achieved through the combined use of cross-language bootstrapping and confidence based unsupervised acoustic model training.A Spanish acoustic model was ported to Polish, through the use of a manually constructed phoneme mapping.This initial model was refined through iterative recognition and retraining of the untranscribed audio data.The system was trained and evaluated on recordings from the European Parliament, and included several state-of-the-art speech recognition techniques in addition to the use of unsupervised model training.Confidence based speaker adaptive training using features space transform adaptation, as well as vocal tract length normalization and maximum likelihood linear regression, was used to refine the acoustic model.Through the combination of the different techniques, good performance was achieved on the domain of parliamentary speeches.",
      "doi": "https://doi.org/10.21437/interspeech.2009-20",
      "openalex_id": "https://openalex.org/W2164505566",
      "arxiv_id": "",
      "publication_date": "2009-09-06",
      "published": "2009-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards unsupervised pattern discovery in speech",
      "summary": "We present an unsupervised algorithm for discovering acoustic patterns in speech by finding matching subsequences between pairs of utterances. The approach we describe is, in theory, language and topic independent, and is particularly well suited for processing large amounts of speech from a single speaker. A variation of dynamic time warping (DTW), which we call segmental DTW, is used to performing the pairwise utterance comparison. Using academic lecture data, we describe two potentially useful applications for the segmental DTW output: augmenting speech recognition transcriptions for information retrieval and speech segment clustering for unsupervised word discovery. Some preliminary qualitative results for both experiments are shown and the implications for future work and applications are discussed",
      "abstract": "We present an unsupervised algorithm for discovering acoustic patterns in speech by finding matching subsequences between pairs of utterances. The approach we describe is, in theory, language and topic independent, and is particularly well suited for processing large amounts of speech from a single speaker. A variation of dynamic time warping (DTW), which we call segmental DTW, is used to performing the pairwise utterance comparison. Using academic lecture data, we describe two potentially useful applications for the segmental DTW output: augmenting speech recognition transcriptions for information retrieval and speech segment clustering for unsupervised word discovery. Some preliminary qualitative results for both experiments are shown and the implications for future work and applications are discussed",
      "doi": "https://doi.org/10.1109/asru.2005.1566529",
      "openalex_id": "https://openalex.org/W2118841860",
      "arxiv_id": "",
      "publication_date": "2005-01-01",
      "published": "2005-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual and crosslingual speech recognition",
      "summary": "This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. For our experiments we used six of these languages to train and test several recognition engines in monolingual, multilingual and crosslingual setups. Based on a global phoneme set we built a multilingual speech recognition system which can handle five different languages. The acoustic models of the five languages are combined into a monolithic system and context dependent phoneme models are created using language questions.",
      "abstract": "This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. For our experiments we used six of these languages to train and test several recognition engines in monolingual, multilingual and crosslingual setups. Based on a global phoneme set we built a multilingual speech recognition system which can handle five different languages. The acoustic models of the five languages are combined into a monolithic system and context dependent phoneme models are created using language questions.",
      "doi": "https://doi.org/10.5445/ir/44598",
      "openalex_id": "https://openalex.org/W47568227",
      "arxiv_id": "",
      "publication_date": "1998-01-01",
      "published": "1998-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings",
      "summary": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation.We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.",
      "abstract": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation.We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.",
      "doi": "https://doi.org/10.1109/taslp.2016.2517567",
      "openalex_id": "https://openalex.org/W2295297373",
      "arxiv_id": "",
      "publication_date": "2016-01-12",
      "published": "2016-01-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "High-Dimensional Probability: An Introduction with Applications in Data Science",
      "summary": "High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.",
      "abstract": "High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.",
      "doi": "https://doi.org/10.1017/9781108231596",
      "openalex_id": "https://openalex.org/W2787248994",
      "arxiv_id": "",
      "publication_date": "2018-09-27",
      "published": "2018-09-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Acoustic Unit Discovery by Leveraging a\\n Language-Independent Subword Discriminative Feature Representation",
      "summary": "This paper tackles automatically discovering phone-like acoustic units (AUD)\\nfrom unlabeled speech data. Past studies usually proposed single-step\\napproaches. We propose a two-stage approach: the first stage learns a\\nsubword-discriminative feature representation and the second stage applies\\nclustering to the learned representation and obtains phone-like clusters as the\\ndiscovered acoustic units. In the first stage, a recently proposed method in\\nthe task of unsupervised subword modeling is improved by replacing a\\nmonolingual out-of-domain (OOD) ASR system with a multilingual one to create a\\nsubword-discriminative representation that is more language-independent. In the\\nsecond stage, segment-level k-means is adopted, and two methods to represent\\nthe variable-length speech segments as fixed-dimension feature vectors are\\ncompared. Experiments on a very low-resource Mboshi language corpus show that\\nour approach outperforms state-of-the-art AUD in both normalized mutual\\ninformation (NMI) and F-score. The multilingual ASR improved upon the\\nmonolingual ASR in providing OOD phone labels and in estimating the phone\\nboundaries. A comparison of our systems with and without knowing the\\nground-truth phone boundaries showed a 16% NMI performance gap, suggesting that\\nthe current approach can significantly benefit from improved phone boundary\\nestimation.\\n",
      "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD)\\nfrom unlabeled speech data. Past studies usually proposed single-step\\napproaches. We propose a two-stage approach: the first stage learns a\\nsubword-discriminative feature representation and the second stage applies\\nclustering to the learned representation and obtains phone-like clusters as the\\ndiscovered acoustic units. In the first stage, a recently proposed method in\\nthe task of unsupervised subword modeling is improved by replacing a\\nmonolingual out-of-domain (OOD) ASR system with a multilingual one to create a\\nsubword-discriminative representation that is more language-independent. In the\\nsecond stage, segment-level k-means is adopted, and two methods to represent\\nthe variable-length speech segments as fixed-dimension feature vectors are\\ncompared. Experiments on a very low-resource Mboshi language corpus show that\\nour approach outperforms state-of-the-art AUD in both normalized mutual\\ninformation (NMI) and F-score. The multilingual ASR improved upon the\\nmonolingual ASR in providing OOD phone labels and in estimating the phone\\nboundaries. A comparison of our systems with and without knowing the\\nground-truth phone boundaries showed a 16% NMI performance gap, suggesting that\\nthe current approach can significantly benefit from improved phone boundary\\nestimation.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2104.00994",
      "openalex_id": "https://openalex.org/W4287241729",
      "arxiv_id": "",
      "publication_date": "2021-04-02",
      "published": "2021-04-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?",
      "summary": "Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.",
      "abstract": "Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054567",
      "openalex_id": "https://openalex.org/W3015315843",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
      "summary": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.",
      "abstract": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1410",
      "openalex_id": "https://openalex.org/W3097514409",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ByteSing: A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders",
      "summary": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.",
      "abstract": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.",
      "doi": "https://doi.org/10.1109/iscslp49672.2021.9362104",
      "openalex_id": "https://openalex.org/W3133525064",
      "arxiv_id": "",
      "publication_date": "2021-01-24",
      "published": "2021-01-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
      "summary": "Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).",
      "abstract": "Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).",
      "doi": "https://doi.org/10.21437/interspeech.2016-872",
      "openalex_id": "https://openalex.org/W2516406502",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An HMM-based singing voice synthesis system",
      "summary": "Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different",
      "abstract": "Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different",
      "doi": "https://doi.org/10.21437/interspeech.2006-584",
      "openalex_id": "https://openalex.org/W29794711",
      "arxiv_id": "",
      "publication_date": "2006-09-17",
      "published": "2006-09-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy",
      "summary": "This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.",
      "abstract": "This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.",
      "doi": "https://doi.org/10.23919/apsipa.2018.8659797",
      "openalex_id": "https://openalex.org/W2921576841",
      "arxiv_id": "",
      "publication_date": "2018-11-01",
      "published": "2018-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems",
      "summary": "Singing voice synthesis (SVS) is a task that aims to generate audio signals according to musical scores and lyrics. With its multifaceted nature concerning music and language, producing singing voices indistinguishable from that of human singers has always remained an unfulfilled pursuit. Nonetheless, the advancements of deep learning techniques have brought about a substantial leap in the quality and naturalness of synthesized singing voice. This paper aims to review some of the state-of-the-art deep learning-driven SVS systems. We intend to summarize their deployed model architectures and identify the strengths and limitations for each of the introduced systems. Thereby, we picture the recent advancement trajectory of this field and conclude the challenges left to be resolved both in commercial applications and academic research.",
      "abstract": "Singing voice synthesis (SVS) is a task that aims to generate audio signals according to musical scores and lyrics. With its multifaceted nature concerning music and language, producing singing voices indistinguishable from that of human singers has always remained an unfulfilled pursuit. Nonetheless, the advancements of deep learning techniques have brought about a substantial leap in the quality and naturalness of synthesized singing voice. This paper aims to review some of the state-of-the-art deep learning-driven SVS systems. We intend to summarize their deployed model architectures and identify the strengths and limitations for each of the introduced systems. Thereby, we picture the recent advancement trajectory of this field and conclude the challenges left to be resolved both in commercial applications and academic research.",
      "doi": "https://doi.org/10.1109/aivr52153.2021.00067",
      "openalex_id": "https://openalex.org/W3204116061",
      "arxiv_id": "",
      "publication_date": "2021-11-01",
      "published": "2021-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast and High-Quality Singing Voice Synthesis System Based on Convolutional Neural Networks",
      "summary": "The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.",
      "abstract": "The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053811",
      "openalex_id": "https://openalex.org/W3015437531",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Singing Voice Synthesis: History, Current Work, and Future Directions",
      "summary": "This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.",
      "abstract": "This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.",
      "doi": "https://doi.org/10.2307/3680822",
      "openalex_id": "https://openalex.org/W2030149476",
      "arxiv_id": "",
      "publication_date": "1996-01-01",
      "published": "1996-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
      "summary": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
      "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
      "doi": "https://doi.org/10.48550/arxiv.2006.16236",
      "openalex_id": "https://openalex.org/W3034573343",
      "arxiv_id": "",
      "publication_date": "2020-06-29",
      "published": "2020-06-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VOCALOID - Commercial singing synthesizer based on sample concatenation",
      "summary": "The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.",
      "abstract": "The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2124097505",
      "arxiv_id": "",
      "publication_date": "2007-01-01",
      "published": "2007-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection",
      "summary": "While Word2Vec represents words (in text) as vectors carrying semantic information, audio Word2Vec was shown to be able to represent signal segments of spoken words as vectors carrying phonetic structure information. Audio Word2Vec can be trained in an unsupervised way from an unlabeled corpus, except the word boundaries are needed. In this paper, we extend audio Word2Vec from word-level to utterance-level by proposing a new segmental audio Word2Vec, in which unsupervised spoken word boundary segmentation and audio Word2Vec are jointly learned and mutually enhanced, so an utterance can be directly represented as a sequence of vectors carrying phonetic structure information. This is achieved by a segmental sequence-to-sequence autoencoder (SSAE), in which a segmentation gate trained with reinforcement learning is inserted in the encoder. Experiments on English, Czech, French and German show very good performance in both unsupervised spoken word segmentation and spoken term detection applications (significantly better than frame-based DTW).",
      "abstract": "While Word2Vec represents words (in text) as vectors carrying semantic information, audio Word2Vec was shown to be able to represent signal segments of spoken words as vectors carrying phonetic structure information. Audio Word2Vec can be trained in an unsupervised way from an unlabeled corpus, except the word boundaries are needed. In this paper, we extend audio Word2Vec from word-level to utterance-level by proposing a new segmental audio Word2Vec, in which unsupervised spoken word boundary segmentation and audio Word2Vec are jointly learned and mutually enhanced, so an utterance can be directly represented as a sequence of vectors carrying phonetic structure information. This is achieved by a segmental sequence-to-sequence autoencoder (SSAE), in which a segmentation gate trained with reinforcement learning is inserted in the encoder. Experiments on English, Czech, French and German show very good performance in both unsupervised spoken word segmentation and spoken term detection applications (significantly better than frame-based DTW).",
      "doi": "https://doi.org/10.1109/icassp.2018.8462002",
      "openalex_id": "https://openalex.org/W2802557066",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations",
      "summary": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
      "abstract": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
      "doi": "https://doi.org/10.18653/v1/n19-1254",
      "openalex_id": "https://openalex.org/W2931212643",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Supervised Learning of Universal Sentence Representations from Natural\\n Language Inference Data",
      "summary": "Many modern NLP systems rely on word embeddings, previously trained in an\\nunsupervised manner on large corpora, as base features. Efforts to obtain\\nembeddings for larger chunks of text, such as sentences, have however not been\\nso successful. Several attempts at learning unsupervised representations of\\nsentences have not reached satisfactory enough performance to be widely\\nadopted. In this paper, we show how universal sentence representations trained\\nusing the supervised data of the Stanford Natural Language Inference datasets\\ncan consistently outperform unsupervised methods like SkipThought vectors on a\\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\\nobtain features, which can then be transferred to other tasks, our work tends\\nto indicate the suitability of natural language inference for transfer learning\\nto other NLP tasks. Our encoder is publicly available.\\n",
      "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\\nunsupervised manner on large corpora, as base features. Efforts to obtain\\nembeddings for larger chunks of text, such as sentences, have however not been\\nso successful. Several attempts at learning unsupervised representations of\\nsentences have not reached satisfactory enough performance to be widely\\nadopted. In this paper, we show how universal sentence representations trained\\nusing the supervised data of the Stanford Natural Language Inference datasets\\ncan consistently outperform unsupervised methods like SkipThought vectors on a\\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\\nobtain features, which can then be transferred to other tasks, our work tends\\nto indicate the suitability of natural language inference for transfer learning\\nto other NLP tasks. Our encoder is publicly available.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1705.02364",
      "openalex_id": "https://openalex.org/W2963918774",
      "arxiv_id": "",
      "publication_date": "2017-05-05",
      "published": "2017-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lattice Indexing for Spoken Term Detection",
      "summary": "This paper considers the problem of constructing an efficient inverted index for the spoken term detection (STD) task. More specifically, we construct a deterministic weighted finite-state transducer storing soft-hits in the form of (utterance ID, start time, end time, posterior score) quadruplets. We propose a generalized factor transducer structure which retains the time information necessary for performing STD. The required information is embedded into the path weights of the factor transducer without disrupting the inherent optimality. We also describe how to index all substrings seen in a collection of raw automatic speech recognition lattices using the proposed structure. Our STD indexing/search implementation is built upon the OpenFst Library and is designed to scale well to large problems. Experiments on Turkish and English data sets corroborate our claims.",
      "abstract": "This paper considers the problem of constructing an efficient inverted index for the spoken term detection (STD) task. More specifically, we construct a deterministic weighted finite-state transducer storing soft-hits in the form of (utterance ID, start time, end time, posterior score) quadruplets. We propose a generalized factor transducer structure which retains the time information necessary for performing STD. The required information is embedded into the path weights of the factor transducer without disrupting the inherent optimality. We also describe how to index all substrings seen in a collection of raw automatic speech recognition lattices using the proposed structure. Our STD indexing/search implementation is built upon the OpenFst Library and is designed to scale well to large problems. Experiments on Turkish and English data sets corroborate our claims.",
      "doi": "https://doi.org/10.1109/tasl.2011.2134087",
      "openalex_id": "https://openalex.org/W2167338739",
      "arxiv_id": "",
      "publication_date": "2011-04-26",
      "published": "2011-04-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DistilCSE: Effective Knowledge Distillation For Contrastive Sentence Embeddings",
      "summary": "Large-scale contrastive learning models can learn very informative sentence embeddings, but are hard to serve online due to the huge model size. Therefore, they often play the role of \"teacher\", transferring abilities to small \"student\" models through knowledge distillation. However, knowledge distillation inevitably brings some drop in embedding effect. To tackle that, we propose an effective knowledge distillation framework for contrastive sentence embeddings, termed DistilCSE. It first applies knowledge distillation on a large amount of unlabeled data, and then fine-tunes student models through contrastive learning on limited labeled data. To achieve better distillation results, we further propose Contrastive Knowledge Distillation (CKD). CKD uses InfoNCE as the loss function in knowledge distillation, enhancing the objective consistency among teacher model training, knowledge distillation, and student model fine-tuning. Extensive experiments show that student models trained with the proposed DistilCSE and CKD suffer from little or even no performance decrease and consistently outperform the corresponding counterparts of the same parameter size. Impressively, our 110M student model outperforms the latest state-of-the-art model, i.e., Sentence-T5 (11B), with only 1% parameters and 0.25% unlabeled data.",
      "abstract": "Large-scale contrastive learning models can learn very informative sentence embeddings, but are hard to serve online due to the huge model size. Therefore, they often play the role of \"teacher\", transferring abilities to small \"student\" models through knowledge distillation. However, knowledge distillation inevitably brings some drop in embedding effect. To tackle that, we propose an effective knowledge distillation framework for contrastive sentence embeddings, termed DistilCSE. It first applies knowledge distillation on a large amount of unlabeled data, and then fine-tunes student models through contrastive learning on limited labeled data. To achieve better distillation results, we further propose Contrastive Knowledge Distillation (CKD). CKD uses InfoNCE as the loss function in knowledge distillation, enhancing the objective consistency among teacher model training, knowledge distillation, and student model fine-tuning. Extensive experiments show that student models trained with the proposed DistilCSE and CKD suffer from little or even no performance decrease and consistently outperform the corresponding counterparts of the same parameter size. Impressively, our 110M student model outperforms the latest state-of-the-art model, i.e., Sentence-T5 (11B), with only 1% parameters and 0.25% unlabeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2112.05638",
      "openalex_id": "https://openalex.org/W4200635076",
      "arxiv_id": "",
      "publication_date": "2021-12-10",
      "published": "2021-12-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Query-by-example spoken term detection using phonetic posteriorgram templates",
      "summary": "This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus.",
      "abstract": "This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus.",
      "doi": "https://doi.org/10.1109/asru.2009.5372889",
      "openalex_id": "https://openalex.org/W2171019095",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity",
      "summary": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from pre-viously existing paraphrase datasets and ma-chine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine trans-lation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The sim-ilarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation&amp;gt;80%, well above a simple lexical baseline that only scored a 31 % correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric. 1",
      "abstract": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from pre-viously existing paraphrase datasets and ma-chine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine trans-lation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The sim-ilarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation&amp;gt;80%, well above a simple lexical baseline that only scored a 31 % correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2251861449",
      "arxiv_id": "",
      "publication_date": "2012-06-07",
      "published": "2012-06-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",
      "summary": "Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe. Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). 2016.",
      "abstract": "Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe. Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). 2016.",
      "doi": "https://doi.org/10.18653/v1/s16-1081",
      "openalex_id": "https://openalex.org/W2462305634",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Query-by-example keyword spotting using long short-term memory networks",
      "summary": "We present a novel approach to query-by-example keyword spotting (KWS) using a long short-term memory (LSTM) recurrent neural network-based feature extractor. In our approach, we represent each keyword using a fixed-length feature vector obtained by running the keyword audio through a word-based LSTM acoustic model. We use the activations prior to the softmax layer of the LSTM as our keyword-vector. At runtime, we detect the keyword by extracting the same feature vector from a sliding window and computing a simple similarity score between this test vector and the keyword vector. With clean speech, we achieve 86% relative false rejection rate reduction at 0.5% false alarm rate when compared to a competitive phoneme posteriorgram with dynamic time warping KWS system, while the reduction in the presence of babble noise is 67%. Our system has a small memory footprint, low computational cost, and high precision, making it suitable for on-device applications.",
      "abstract": "We present a novel approach to query-by-example keyword spotting (KWS) using a long short-term memory (LSTM) recurrent neural network-based feature extractor. In our approach, we represent each keyword using a fixed-length feature vector obtained by running the keyword audio through a word-based LSTM acoustic model. We use the activations prior to the softmax layer of the LSTM as our keyword-vector. At runtime, we detect the keyword by extracting the same feature vector from a sliding window and computing a simple similarity score between this test vector and the keyword vector. With clean speech, we achieve 86% relative false rejection rate reduction at 0.5% false alarm rate when compared to a competitive phoneme posteriorgram with dynamic time warping KWS system, while the reduction in the presence of babble noise is 67%. Our system has a small memory footprint, low computational cost, and high precision, making it suitable for on-device applications.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178970",
      "openalex_id": "https://openalex.org/W1496120315",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",
      "summary": "In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs.",
      "abstract": "In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs.",
      "doi": "https://doi.org/10.18653/v1/s15-2045",
      "openalex_id": "https://openalex.org/W2133458109",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Query-by-example Spoken Term Detection For OOV terms",
      "summary": "The goal of spoken term detection (STD) technology is to allow open vocabulary search over large collections of speech content. In this paper, we address cases where search term(s) of interest (queries) are acoustic examples. This is provided either by identifying a region of interest in a speech stream or by speaking the query term. Queries often relate to named-entities and foreign words, which typically have poor coverage in the vocabulary of large vocabulary continuous speech recognition (LVCSR) systems. Throughout this paper, we focus on query-by-example search for such out-of-vocabulary (OOV) query terms. We build upon a finite state transducer (FST) based search and indexing system to address the query by example search for OOV terms by representing both the query and the index as phonetic lattices from the output of an LVCSR system. We provide results comparing different representations and generation mechanisms for both queries and indexes built with word and combined word and subword units. We also present a two-pass method which uses query-by-example search using the best hit identified in an initial pass to augment the STD search results. The results demonstrate that query-by-example search can yield a significantly better performance, measured using actual term-weighted value (ATWV), of 0.479 when compared to a baseline ATWV of 0.325 that uses reference pronunciations for OOVs. Further improvements can be obtained with the proposed two pass approach and filtering using the expected unigram counts from the LVCSR system's lexicon.",
      "abstract": "The goal of spoken term detection (STD) technology is to allow open vocabulary search over large collections of speech content. In this paper, we address cases where search term(s) of interest (queries) are acoustic examples. This is provided either by identifying a region of interest in a speech stream or by speaking the query term. Queries often relate to named-entities and foreign words, which typically have poor coverage in the vocabulary of large vocabulary continuous speech recognition (LVCSR) systems. Throughout this paper, we focus on query-by-example search for such out-of-vocabulary (OOV) query terms. We build upon a finite state transducer (FST) based search and indexing system to address the query by example search for OOV terms by representing both the query and the index as phonetic lattices from the output of an LVCSR system. We provide results comparing different representations and generation mechanisms for both queries and indexes built with word and combined word and subword units. We also present a two-pass method which uses query-by-example search using the best hit identified in an initial pass to augment the STD search results. The results demonstrate that query-by-example search can yield a significantly better performance, measured using actual term-weighted value (ATWV), of 0.479 when compared to a baseline ATWV of 0.325 that uses reference pronunciations for OOVs. Further improvements can be obtained with the proposed two pass approach and filtering using the expected unigram counts from the LVCSR system's lexicon.",
      "doi": "https://doi.org/10.1109/asru.2009.5373341",
      "openalex_id": "https://openalex.org/W2110625382",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformers: State-of-the-Art Natural Language Processing",
      "summary": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",
      "abstract": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-demos.6",
      "openalex_id": "https://openalex.org/W2979826702",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\\n Representations",
      "summary": "Increasing model size when pretraining natural language representations often\\nresults in improved performance on downstream tasks. However, at some point\\nfurther model increases become harder due to GPU/TPU memory limitations and\\nlonger training times. To address these problems, we present two\\nparameter-reduction techniques to lower memory consumption and increase the\\ntraining speed of BERT. Comprehensive empirical evidence shows that our\\nproposed methods lead to models that scale much better compared to the original\\nBERT. We also use a self-supervised loss that focuses on modeling\\ninter-sentence coherence, and show it consistently helps downstream tasks with\\nmulti-sentence inputs. As a result, our best model establishes new\\nstate-of-the-art results on the GLUE, RACE, and \\\\squad benchmarks while having\\nfewer parameters compared to BERT-large. The code and the pretrained models are\\navailable at https://github.com/google-research/ALBERT.\\n",
      "abstract": "Increasing model size when pretraining natural language representations often\\nresults in improved performance on downstream tasks. However, at some point\\nfurther model increases become harder due to GPU/TPU memory limitations and\\nlonger training times. To address these problems, we present two\\nparameter-reduction techniques to lower memory consumption and increase the\\ntraining speed of BERT. Comprehensive empirical evidence shows that our\\nproposed methods lead to models that scale much better compared to the original\\nBERT. We also use a self-supervised loss that focuses on modeling\\ninter-sentence coherence, and show it consistently helps downstream tasks with\\nmulti-sentence inputs. As a result, our best model establishes new\\nstate-of-the-art results on the GLUE, RACE, and \\\\squad benchmarks while having\\nfewer parameters compared to BERT-large. The code and the pretrained models are\\navailable at https://github.com/google-research/ALBERT.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1909.11942",
      "openalex_id": "https://openalex.org/W2996428491",
      "arxiv_id": "",
      "publication_date": "2019-09-26",
      "published": "2019-09-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
      "summary": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting...",
      "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting...",
      "doi": "https://doi.org/10.5555/1756006.1953039",
      "openalex_id": "https://openalex.org/W2997574889",
      "arxiv_id": "",
      "publication_date": "2010-12-01",
      "published": "2010-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contrastive Distillation on Intermediate Representations for Language Model Compression",
      "summary": "Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.",
      "abstract": "Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.36",
      "openalex_id": "https://openalex.org/W3101066076",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Understanding Contrastive Representation Learning through Alignment and\\n Uniformity on the Hypersphere",
      "summary": "Contrastive representation learning has been outstandingly successful in\\npractice. In this work, we identify two key properties related to the\\ncontrastive loss: (1) alignment (closeness) of features from positive pairs,\\nand (2) uniformity of the induced distribution of the (normalized) features on\\nthe hypersphere. We prove that, asymptotically, the contrastive loss optimizes\\nthese properties, and analyze their positive effects on downstream tasks.\\nEmpirically, we introduce an optimizable metric to quantify each property.\\nExtensive experiments on standard vision and language datasets confirm the\\nstrong agreement between both metrics and downstream task performance.\\nRemarkably, directly optimizing for these two metrics leads to representations\\nwith comparable or better performance at downstream tasks than contrastive\\nlearning.\\n Project Page: https://tongzhouwang.info/hypersphere\\n Code: https://github.com/SsnL/align_uniform ,\\nhttps://github.com/SsnL/moco_align_uniform\\n",
      "abstract": "Contrastive representation learning has been outstandingly successful in\\npractice. In this work, we identify two key properties related to the\\ncontrastive loss: (1) alignment (closeness) of features from positive pairs,\\nand (2) uniformity of the induced distribution of the (normalized) features on\\nthe hypersphere. We prove that, asymptotically, the contrastive loss optimizes\\nthese properties, and analyze their positive effects on downstream tasks.\\nEmpirically, we introduce an optimizable metric to quantify each property.\\nExtensive experiments on standard vision and language datasets confirm the\\nstrong agreement between both metrics and downstream task performance.\\nRemarkably, directly optimizing for these two metrics leads to representations\\nwith comparable or better performance at downstream tasks than contrastive\\nlearning.\\n Project Page: https://tongzhouwang.info/hypersphere\\n Code: https://github.com/SsnL/align_uniform ,\\nhttps://github.com/SsnL/moco_align_uniform\\n",
      "doi": "https://doi.org/10.48550/arxiv.2005.10242",
      "openalex_id": "https://openalex.org/W3118062200",
      "arxiv_id": "",
      "publication_date": "2020-05-20",
      "published": "2020-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings",
      "summary": "Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, James Glass. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, James Glass. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.311",
      "openalex_id": "https://openalex.org/W4224313754",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations",
      "summary": "In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g. sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform cross-encoders. Cross-encoders can leverage their attention heads to exploit inter-sentence interactions for better performance but they require task fine-tuning and are computationally more expensive. In this paper, we present a completely unsupervised sentence representation model termed as Trans-Encoder that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi- and cross-encoders. Specifically, on top of a pre-trained Language Model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi- and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT and SimCSE by up to 5% on the sentence similarity benchmarks.",
      "abstract": "In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g. sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform cross-encoders. Cross-encoders can leverage their attention heads to exploit inter-sentence interactions for better performance but they require task fine-tuning and are computationally more expensive. In this paper, we present a completely unsupervised sentence representation model termed as Trans-Encoder that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi- and cross-encoders. Specifically, on top of a pre-trained Language Model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi- and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT and SimCSE by up to 5% on the sentence similarity benchmarks.",
      "doi": "https://doi.org/10.17863/cam.93939",
      "openalex_id": "https://openalex.org/W4286959591",
      "arxiv_id": "",
      "publication_date": "2021-09-27",
      "published": "2021-09-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations",
      "summary": "John Giorgi, Osvald Nitski, Bo Wang, Gary Bader. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "abstract": "John Giorgi, Osvald Nitski, Bo Wang, Gary Bader. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-long.72",
      "openalex_id": "https://openalex.org/W3173783447",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "summary": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman’s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
      "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman’s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.552",
      "openalex_id": "https://openalex.org/W3156636935",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Datasets: A Community Library for Natural Language Processing",
      "summary": "Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, Thomas Wolf. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2021.",
      "abstract": "Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, Thomas Wolf. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-demo.21",
      "openalex_id": "https://openalex.org/W3197876970",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Software for a cascade/parallel formant synthesizer",
      "summary": "A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",
      "abstract": "A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",
      "doi": "https://doi.org/10.1121/1.383940",
      "openalex_id": "https://openalex.org/W1999885698",
      "arxiv_id": "",
      "publication_date": "1980-03-01",
      "published": "1980-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Music and computer composition",
      "summary": "The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.",
      "abstract": "The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.",
      "doi": "https://doi.org/10.1145/361254.361265",
      "openalex_id": "https://openalex.org/W1997640156",
      "arxiv_id": "",
      "publication_date": "1972-02-01",
      "published": "1972-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders",
      "summary": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
      "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
      "doi": "https://doi.org/10.48550/arxiv.1704.01279",
      "openalex_id": "https://openalex.org/W2606176153",
      "arxiv_id": "",
      "publication_date": "2017-04-05",
      "published": "2017-04-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "summary": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "doi": "https://doi.org/10.1109/cvpr.2017.634",
      "openalex_id": "https://openalex.org/W2549139847",
      "arxiv_id": "",
      "publication_date": "2017-07-01",
      "published": "2017-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improved Variational Inference with Inverse Autoregressive Flow",
      "summary": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",
      "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2587284713",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music",
      "summary": "The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the \"posterior collapse\" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a \"flat\" baseline model. An implementation of our \"MusicVAE\" is available online at http://g.co/magenta/musicvae-code.",
      "abstract": "The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the \"posterior collapse\" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a \"flat\" baseline model. An implementation of our \"MusicVAE\" is available online at http://g.co/magenta/musicvae-code.",
      "doi": "https://doi.org/10.48550/arxiv.1803.05428",
      "openalex_id": "https://openalex.org/W2792210438",
      "arxiv_id": "",
      "publication_date": "2018-03-13",
      "published": "2018-03-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unit selection in a concatenative speech synthesis system using a large speech database",
      "summary": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",
      "abstract": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",
      "doi": "https://doi.org/10.1109/icassp.1996.541110",
      "openalex_id": "https://openalex.org/W2150658333",
      "arxiv_id": "",
      "publication_date": "2002-12-24",
      "published": "2002-12-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Constraint programming systems for modeling music theories and composition",
      "summary": "Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.",
      "abstract": "Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.",
      "doi": "https://doi.org/10.1145/1978802.1978809",
      "openalex_id": "https://openalex.org/W2116973068",
      "arxiv_id": "",
      "publication_date": "2011-10-01",
      "published": "2011-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LakhNES: Improving multi-instrumental music generation with cross-domain\\n pre-training",
      "summary": "We are interested in the task of generating multi-instrumental music scores.\\nThe Transformer architecture has recently shown great promise for the task of\\npiano score generation; here we adapt it to the multi-instrumental setting.\\nTransformers are complex, high-dimensional language models which are capable of\\ncapturing long-term structure in sequence data, but require large amounts of\\ndata to fit. Their success on piano score generation is partially explained by\\nthe large volumes of symbolic data readily available for that domain. We\\nleverage the recently-introduced NES-MDB dataset of four-instrument scores from\\nan early video game sound synthesis chip (the NES), which we find to be\\nwell-suited to training with the Transformer architecture. To further improve\\nthe performance of our model, we propose a pre-training technique to leverage\\nthe information in a large collection of heterogeneous music, namely the Lakh\\nMIDI dataset. Despite differences between the two corpora, we find that this\\ntransfer learning procedure improves both quantitative and qualitative\\nperformance for our primary task.\\n",
      "abstract": "We are interested in the task of generating multi-instrumental music scores.\\nThe Transformer architecture has recently shown great promise for the task of\\npiano score generation; here we adapt it to the multi-instrumental setting.\\nTransformers are complex, high-dimensional language models which are capable of\\ncapturing long-term structure in sequence data, but require large amounts of\\ndata to fit. Their success on piano score generation is partially explained by\\nthe large volumes of symbolic data readily available for that domain. We\\nleverage the recently-introduced NES-MDB dataset of four-instrument scores from\\nan early video game sound synthesis chip (the NES), which we find to be\\nwell-suited to training with the Transformer architecture. To further improve\\nthe performance of our model, we propose a pre-training technique to leverage\\nthe information in a large collection of heterogeneous music, namely the Lakh\\nMIDI dataset. Despite differences between the two corpora, we find that this\\ntransfer learning procedure improves both quantitative and qualitative\\nperformance for our primary task.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1907.04868",
      "openalex_id": "https://openalex.org/W2991108091",
      "arxiv_id": "",
      "publication_date": "2019-07-10",
      "published": "2019-07-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Axial Attention in Multidimensional Transformers",
      "summary": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
      "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
      "doi": "https://doi.org/10.48550/arxiv.1912.12180",
      "openalex_id": "https://openalex.org/W2998108143",
      "arxiv_id": "",
      "publication_date": "2019-12-20",
      "published": "2019-12-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "summary": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "doi": "https://doi.org/10.48550/arxiv.1912.06680",
      "openalex_id": "https://openalex.org/W2996037775",
      "arxiv_id": "",
      "publication_date": "2019-12-13",
      "published": "2019-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Conditional Image Generation with PixelCNN Decoders",
      "summary": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
      "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
      "doi": "https://doi.org/10.48550/arxiv.1606.05328",
      "openalex_id": "https://openalex.org/W2963636093",
      "arxiv_id": "",
      "publication_date": "2016-06-16",
      "published": "2016-06-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Hierarchical Recurrent Neural Network for Symbolic Melody Generation",
      "summary": "In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.",
      "abstract": "In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.",
      "doi": "https://doi.org/10.1109/tcyb.2019.2953194",
      "openalex_id": "https://openalex.org/W2992790584",
      "arxiv_id": "",
      "publication_date": "2019-12-02",
      "published": "2019-12-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Variational Inference with Normalizing Flows",
      "summary": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
      "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
      "doi": "https://doi.org/10.48550/arxiv.1505.05770",
      "openalex_id": "https://openalex.org/W2963090522",
      "arxiv_id": "",
      "publication_date": "2015-05-21",
      "published": "2015-05-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Lyrics Transcription in Polyphonic Music: Does Background Music Help?",
      "summary": "Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",
      "abstract": "Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2973975824",
      "arxiv_id": "",
      "publication_date": "2019-09-23",
      "published": "2019-09-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Synthesis of the Singing Voice by Performance Sampling and Spectral Models",
      "summary": "Among the many existing approaches to the synthesis of musical sounds, the ones that have had the biggest success are without any doubt the sampling based ones, which sequentially concatenate samples from a corpus database [1]. Strictly speaking, we could say that sampling is not a synthesis technique, but from a practical perspective it is convenient to treat it as such. From what we explain in this article it should become",
      "abstract": "Among the many existing approaches to the synthesis of musical sounds, the ones that have had the biggest success are without any doubt the sampling based ones, which sequentially concatenate samples from a corpus database [1]. Strictly speaking, we could say that sampling is not a synthesis technique, but from a practical perspective it is convenient to treat it as such. From what we explain in this article it should become",
      "doi": "https://doi.org/10.1109/msp.2007.323266",
      "openalex_id": "https://openalex.org/W2102870814",
      "arxiv_id": "",
      "publication_date": "2007-03-01",
      "published": "2007-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
      "summary": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
      "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
      "doi": "https://doi.org/10.48550/arxiv.1809.11096",
      "openalex_id": "https://openalex.org/W2893749619",
      "arxiv_id": "",
      "publication_date": "2018-09-28",
      "published": "2018-09-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer",
      "summary": "We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.",
      "abstract": "We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.",
      "doi": "https://doi.org/10.3929/ethz-b-000292318",
      "openalex_id": "https://openalex.org/W2892104732",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual representations for low resource speech recognition and keyword search",
      "summary": "This paper examines the impact of multilingual (ML) acoustic representations on Automatic Speech Recognition (ASR) and keyword search (KWS) for low resource languages in the context of the OpenKWS15 evaluation of the IARPA Babel program. The task is to develop Swahili ASR and KWS systems within two weeks using as little as 3 hours of transcribed data. Multilingual acoustic representations proved to be crucial for building these systems under strict time constraints. The paper discusses several key insights on how these representations are derived and used. First, we present a data sampling strategy that can speed up the training of multilingual representations without appreciable loss in ASR performance. Second, we show that fusion of diverse multilingual representations developed at different LORELEI sites yields substantial ASR and KWS gains. Speaker adaptation and data augmentation of these representations improves both ASR and KWS performance (up to 8.7% relative). Third, incorporating un-transcribed data through semi-supervised learning, improves WER and KWS performance. Finally, we show that these multilingual representations significantly improve ASR and KWS performance (relative 9% for WER and 5% for MTWV) even when forty hours of transcribed audio in the target language is available. Multilingual representations significantly contributed to the LORELEI KWS systems winning the OpenKWS 15 evaluation.",
      "abstract": "This paper examines the impact of multilingual (ML) acoustic representations on Automatic Speech Recognition (ASR) and keyword search (KWS) for low resource languages in the context of the OpenKWS15 evaluation of the IARPA Babel program. The task is to develop Swahili ASR and KWS systems within two weeks using as little as 3 hours of transcribed data. Multilingual acoustic representations proved to be crucial for building these systems under strict time constraints. The paper discusses several key insights on how these representations are derived and used. First, we present a data sampling strategy that can speed up the training of multilingual representations without appreciable loss in ASR performance. Second, we show that fusion of diverse multilingual representations developed at different LORELEI sites yields substantial ASR and KWS gains. Speaker adaptation and data augmentation of these representations improves both ASR and KWS performance (up to 8.7% relative). Third, incorporating un-transcribed data through semi-supervised learning, improves WER and KWS performance. Finally, we show that these multilingual representations significantly improve ASR and KWS performance (relative 9% for WER and 5% for MTWV) even when forty hours of transcribed audio in the target language is available. Multilingual representations significantly contributed to the LORELEI KWS systems winning the OpenKWS 15 evaluation.",
      "doi": "https://doi.org/10.1109/asru.2015.7404803",
      "openalex_id": "https://openalex.org/W2291975472",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual deep neural network based acoustic modeling for rapid language adaptation",
      "summary": "This paper presents a study on multilingual deep neural network (DNN) based acoustic modeling and its application to new languages. We investigate the effect of phone merging on multilingual DNN in context of rapid language adaptation. Moreover, the combination of multilingual DNNs with Kullback--Leibler divergence based acoustic modeling (KL-HMM) is explored. Using ten different languages from the Globalphone database, our studies reveal that crosslingual acoustic model transfer through multilingual DNNs is superior to unsupervised RBM pre-training and greedy layer-wise supervised training. We also found that KL-HMM based decoding consistently outperforms conventional hybrid decoding, especially in low-resource scenarios. Furthermore, the experiments indicate that multilingual DNN training equally benefits from simple phoneset concatenation and manually derived universal phonesets.",
      "abstract": "This paper presents a study on multilingual deep neural network (DNN) based acoustic modeling and its application to new languages. We investigate the effect of phone merging on multilingual DNN in context of rapid language adaptation. Moreover, the combination of multilingual DNNs with Kullback--Leibler divergence based acoustic modeling (KL-HMM) is explored. Using ten different languages from the Globalphone database, our studies reveal that crosslingual acoustic model transfer through multilingual DNNs is superior to unsupervised RBM pre-training and greedy layer-wise supervised training. We also found that KL-HMM based decoding consistently outperforms conventional hybrid decoding, especially in low-resource scenarios. Furthermore, the experiments indicate that multilingual DNN training equally benefits from simple phoneset concatenation and manually derived universal phonesets.",
      "doi": "https://doi.org/10.1109/icassp.2014.6855086",
      "openalex_id": "https://openalex.org/W2106440210",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning words from sights and sounds: a computational model",
      "summary": "This paper presents an implemented computational model of word acquisition which learns directly from raw multimodal sensory input. Set in an information theoretic framework, the model acquires a lexicon by finding and statistically modeling consistent cross-modal structure. The model has been implemented in a system using novel speech processing, computer vision, and machine learning algorithms. In evaluations the model successfully performed speech segmentation, word discovery and visual categorization from spontaneous infant-directed speech paired with video images of single objects. These results demonstrate the possibility of using state-of-the-art techniques from sensory pattern recognition and machine learning to implement cognitive models which can process raw sensor data without the need for human transcription or labeling.",
      "abstract": "This paper presents an implemented computational model of word acquisition which learns directly from raw multimodal sensory input. Set in an information theoretic framework, the model acquires a lexicon by finding and statistically modeling consistent cross-modal structure. The model has been implemented in a system using novel speech processing, computer vision, and machine learning algorithms. In evaluations the model successfully performed speech segmentation, word discovery and visual categorization from spontaneous infant-directed speech paired with video images of single objects. These results demonstrate the possibility of using state-of-the-art techniques from sensory pattern recognition and machine learning to implement cognitive models which can process raw sensor data without the need for human transcription or labeling.",
      "doi": "https://doi.org/10.1016/s0364-0213(01)00061-1",
      "openalex_id": "https://openalex.org/W2107917162",
      "arxiv_id": "",
      "publication_date": "2002-02-01",
      "published": "2002-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Classification with Unlabeled Data",
      "summary": "One of the advantages of supervised learning is that the final error met-ric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortu-nately, when modeling human learning or constructing classifiers for au-tonomous robots, supervisory labels are often not available or too ex-pensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sen-sory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding ap-propriate placement for the codebook vectors particularly when the con-fuseable classes are different for the two modalities. 1",
      "abstract": "One of the advantages of supervised learning is that the final error met-ric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortu-nately, when modeling human learning or constructing classifiers for au-tonomous robots, supervisory labels are often not available or too ex-pensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sen-sory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding ap-propriate placement for the codebook vectors particularly when the con-fuseable classes are different for the two modalities. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2113896236",
      "arxiv_id": "",
      "publication_date": "1993-11-29",
      "published": "1993-11-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Grounded spoken language acquisition: experiments in word learning",
      "summary": "Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently most machines which process language are not grounded. Instead, semantic representations are abstract, pre-specified, and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences leading to richer levels of machine understanding. A key element of this work is the development of effective architectures for processing multisensory data. Inspired by theories of infant cognition, we present a computational model which learns words from untranscribed acoustic and video input. Channels of input derived from different sensors are integrated in an information -theoretic framework. Acquired words are represented in terms of associations between acoustic and visual sensory experience. The model has been implemented in a real-time robotic system which performs interactive language learning and understanding. Successful learning has also been demonstrated using infant-directed speech and images.",
      "abstract": "Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently most machines which process language are not grounded. Instead, semantic representations are abstract, pre-specified, and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences leading to richer levels of machine understanding. A key element of this work is the development of effective architectures for processing multisensory data. Inspired by theories of infant cognition, we present a computational model which learns words from untranscribed acoustic and video input. Channels of input derived from different sensors are integrated in an information -theoretic framework. Acquired words are represented in terms of associations between acoustic and visual sensory experience. The model has been implemented in a real-time robotic system which performs interactive language learning and understanding. Successful learning has also been demonstrated using infant-directed speech and images.",
      "doi": "https://doi.org/10.1109/tmm.2003.811618",
      "openalex_id": "https://openalex.org/W2132921748",
      "arxiv_id": "",
      "publication_date": "2003-06-01",
      "published": "2003-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semantic Query-by-example Speech Search Using Visual Grounding",
      "summary": "A number of recent studies have started to investigate how speech systems can be trained on untranscribed speech by leveraging accompanying images at training time. Examples of tasks include keyword prediction and within- and across-mode retrieval. Here we consider how such models can be used for query-by-example (QbE) search, the task of retrieving utterances relevant to a given spoken query. We are particularly interested in semantic QbE, where the task is not only to retrieve utterances containing exact instances of the query, but also utterances whose meaning is relevant to the query. We follow a segmental QbE approach where variable-duration speech segments (queries, search utterances) are mapped to fixed-dimensional embedding vectors. We show that a QbE system using an embedding function trained on visually grounded speech data outperforms a purely acoustic QbE system in terms of both exact and semantic retrieval performance.",
      "abstract": "A number of recent studies have started to investigate how speech systems can be trained on untranscribed speech by leveraging accompanying images at training time. Examples of tasks include keyword prediction and within- and across-mode retrieval. Here we consider how such models can be used for query-by-example (QbE) search, the task of retrieving utterances relevant to a given spoken query. We are particularly interested in semantic QbE, where the task is not only to retrieve utterances containing exact instances of the query, but also utterances whose meaning is relevant to the query. We follow a segmental QbE approach where variable-duration speech segments (queries, search utterances) are mapped to fixed-dimensional embedding vectors. We show that a QbE system using an embedding function trained on visually grounded speech data outperforms a purely acoustic QbE system in terms of both exact and semantic retrieval performance.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683275",
      "openalex_id": "https://openalex.org/W2938991416",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Learning of Semantic Audio Representations",
      "summary": "Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.",
      "abstract": "Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461684",
      "openalex_id": "https://openalex.org/W2767754137",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model",
      "summary": "Current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries.In settings where unlabelled speech data alone is available, unsupervised methods are required to discover categorical linguistic structure directly from the audio.We present a novel Bayesian model which segments unlabelled input speech into word-like units, resulting in a complete unsupervised transcription of the speech in terms of discovered word types.In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional space; the model (implemented as a Gibbs sampler) then builds a whole-word acoustic model in this space while jointly doing segmentation.We report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions.Our model outperforms a previously developed HMM-based system, even when the model is not constrained to discover only the 11 word types present in the data.",
      "abstract": "Current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries.In settings where unlabelled speech data alone is available, unsupervised methods are required to discover categorical linguistic structure directly from the audio.We present a novel Bayesian model which segments unlabelled input speech into word-like units, resulting in a complete unsupervised transcription of the speech in terms of discovered word types.In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional space; the model (implemented as a Gibbs sampler) then builds a whole-word acoustic model in this space while jointly doing segmentation.We report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions.Our model outperforms a previously developed HMM-based system, even when the model is not constrained to discover only the 11 word types present in the data.",
      "doi": "https://doi.org/10.21437/interspeech.2015-239",
      "openalex_id": "https://openalex.org/W1942713348",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",
      "summary": "We investigate training end-to-end speech recognition models with the recurrent neural network transducer (RNN-T): a streaming, all-neural, sequence-to-sequence architecture which jointly learns acoustic and language model components from transcribed acoustic data. We explore various model architectures and demonstrate how the model can be improved further if additional text or pronunciation data are available. The model consists of an `encoder', which is initialized from a connectionist temporal classification-based (CTC) acoustic model, and a `decoder' which is partially initialized from a recurrent neural network language model trained on text data alone. The entire neural network is trained with the RNN-T loss and directly outputs the recognized transcript as a sequence of graphemes, thus performing end-to-end speech recognition. We find that performance can be improved further through the use of sub-word units ('wordpieces') which capture longer context and significantly reduce substitution errors. The best RNN-T system, a twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000 wordpieces as output targets achieves a word error rate of 8.5% on voice-search and 5.2% on voice-dictation tasks and is comparable to a state-of-the-art baseline at 8.3% on voice-search and 5.4% voice-dictation.",
      "abstract": "We investigate training end-to-end speech recognition models with the recurrent neural network transducer (RNN-T): a streaming, all-neural, sequence-to-sequence architecture which jointly learns acoustic and language model components from transcribed acoustic data. We explore various model architectures and demonstrate how the model can be improved further if additional text or pronunciation data are available. The model consists of an `encoder', which is initialized from a connectionist temporal classification-based (CTC) acoustic model, and a `decoder' which is partially initialized from a recurrent neural network language model trained on text data alone. The entire neural network is trained with the RNN-T loss and directly outputs the recognized transcript as a sequence of graphemes, thus performing end-to-end speech recognition. We find that performance can be improved further through the use of sub-word units ('wordpieces') which capture longer context and significantly reduce substitution errors. The best RNN-T system, a twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000 wordpieces as output targets achieves a word error rate of 8.5% on voice-search and 5.2% on voice-dictation tasks and is comparable to a state-of-the-art baseline at 8.3% on voice-search and 5.4% voice-dictation.",
      "doi": "https://doi.org/10.1109/asru.2017.8268935",
      "openalex_id": "https://openalex.org/W2963414781",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving RNN Transducer Modeling for End-to-End Speech Recognition",
      "summary": "In the last few years, an emerging trend in automatic speech recognition research is the study of end-to-end (E2E) systems. Connectionist Temporal Classification (CTC), Attention Encoder-Decoder (AED), and RNN Transducer (RNN-T) are the most popular three methods. Among these three methods, RNN-T has the advantages to do online streaming which is challenging to AED and it doesn't have CTC's frame-independence assumption. In this paper, we improve the RNN-T training in two aspects. First, we optimize the training algorithm of RNN-T to reduce the memory consumption so that we can have larger training minibatch for faster training speed. Second, we propose better model structures so that we obtain RNN-T models with the very good accuracy but small footprint. Trained with 30 thousand hours anonymized and transcribed Microsoft production data, the best RNN-T model with even smaller model size (216 Megabytes) achieves up-to 11.8% relative word error rate (WER) reduction from the baseline RNN-T model. This best RNN-T model is significantly better than the device hybrid model with similar size by achieving up-to 15.0% relative WER reduction, and obtains similar WERs as the server hybrid model of 5120 Megabytes in size.",
      "abstract": "In the last few years, an emerging trend in automatic speech recognition research is the study of end-to-end (E2E) systems. Connectionist Temporal Classification (CTC), Attention Encoder-Decoder (AED), and RNN Transducer (RNN-T) are the most popular three methods. Among these three methods, RNN-T has the advantages to do online streaming which is challenging to AED and it doesn't have CTC's frame-independence assumption. In this paper, we improve the RNN-T training in two aspects. First, we optimize the training algorithm of RNN-T to reduce the memory consumption so that we can have larger training minibatch for faster training speed. Second, we propose better model structures so that we obtain RNN-T models with the very good accuracy but small footprint. Trained with 30 thousand hours anonymized and transcribed Microsoft production data, the best RNN-T model with even smaller model size (216 Megabytes) achieves up-to 11.8% relative word error rate (WER) reduction from the baseline RNN-T model. This best RNN-T model is significantly better than the device hybrid model with similar size by achieving up-to 15.0% relative WER reduction, and obtains similar WERs as the server hybrid model of 5120 Megabytes in size.",
      "doi": "https://doi.org/10.1109/asru46091.2019.9003906",
      "openalex_id": "https://openalex.org/W3007227084",
      "arxiv_id": "",
      "publication_date": "2019-12-01",
      "published": "2019-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Greedy Layer-Wise Training of Deep Networks",
      "summary": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.",
      "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.",
      "doi": "https://doi.org/10.7551/mitpress/7503.003.0024",
      "openalex_id": "https://openalex.org/W2110798204",
      "arxiv_id": "",
      "publication_date": "2007-09-07",
      "published": "2007-09-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-Supervised Training in Deep Learning Acoustic Model",
      "summary": "We studied the semi-supervised training in a fully connected deep neural network (DNN), unfolded recurrent neural network (RNN), and long short-term memory recurrent neural network (LSTM-RNN) with respect to the transcription quality, the importance data sampling, and the training data amount. We found that DNN, unfolded RNN, and LSTM-RNN are increasingly more sensitive to labeling errors. For example, with the simulated erroneous training transcription at 5%, 10%, or 15% word error rate (WER) level, the semi-supervised DNN yields 2.37%, 4.84%, or 7.46% relative WER increase against the baseline model trained with the perfect transcription; in comparison, the corresponding WER increase is 2.53%, 4.89%, or 8.85% in an unfolded RNN and 4.47%, 9.38%, or 14.01% in an LSTM-RNN. We further found that the importance sampling has similar impact on all three models with 2~3% relative WER reduction comparing to the random sampling. Lastly, we compared the modeling capability with increased training data. Experimental results suggested that LSTM-RNN can benefit more from enlarged training data comparing to unfolded RNN and DNN. We trained a semi-supervised LSTM-RNN using 2600 hr transcribed and 10000 hr untranscribed data on a mobile speech task. The semi-supervised LSTM-RNN yields 7.9\\% relative WER reduction against the supervised baseline.",
      "abstract": "We studied the semi-supervised training in a fully connected deep neural network (DNN), unfolded recurrent neural network (RNN), and long short-term memory recurrent neural network (LSTM-RNN) with respect to the transcription quality, the importance data sampling, and the training data amount. We found that DNN, unfolded RNN, and LSTM-RNN are increasingly more sensitive to labeling errors. For example, with the simulated erroneous training transcription at 5%, 10%, or 15% word error rate (WER) level, the semi-supervised DNN yields 2.37%, 4.84%, or 7.46% relative WER increase against the baseline model trained with the perfect transcription; in comparison, the corresponding WER increase is 2.53%, 4.89%, or 8.85% in an unfolded RNN and 4.47%, 9.38%, or 14.01% in an LSTM-RNN. We further found that the importance sampling has similar impact on all three models with 2~3% relative WER reduction comparing to the random sampling. Lastly, we compared the modeling capability with increased training data. Experimental results suggested that LSTM-RNN can benefit more from enlarged training data comparing to unfolded RNN and DNN. We trained a semi-supervised LSTM-RNN using 2600 hr transcribed and 10000 hr untranscribed data on a mobile speech task. The semi-supervised LSTM-RNN yields 7.9\\% relative WER reduction against the supervised baseline.",
      "doi": "https://doi.org/10.21437/interspeech.2016-1596",
      "openalex_id": "https://openalex.org/W2512655038",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "summary": "[ Four research groups share their views] &amp;lt;AU: please check that added subtitle is Ok as given or please supply short alternative&amp;gt; Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition. Digital Object Identifier 10.1109/MSP.2012.2205597 Date of publication: IEEE © xxxxx",
      "abstract": "[ Four research groups share their views] &amp;lt;AU: please check that added subtitle is Ok as given or please supply short alternative&amp;gt; Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition. Digital Object Identifier 10.1109/MSP.2012.2205597 Date of publication: IEEE © xxxxx",
      "doi": "",
      "openalex_id": "https://openalex.org/W2184045248",
      "arxiv_id": "",
      "publication_date": "2012-11-01",
      "published": "2012-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CROWDMOS: An approach for crowdsourcing mean opinion score studies",
      "summary": "MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this pa per, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.",
      "abstract": "MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this pa per, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.",
      "doi": "https://doi.org/10.1109/icassp.2011.5946971",
      "openalex_id": "https://openalex.org/W2160473997",
      "arxiv_id": "",
      "publication_date": "2011-05-01",
      "published": "2011-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unified Language Model Pre-training for Natural Language Understanding\\n and Generation",
      "summary": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\\nbe fine-tuned for both natural language understanding and generation tasks. The\\nmodel is pre-trained using three types of language modeling tasks:\\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\\nmodeling is achieved by employing a shared Transformer network and utilizing\\nspecific self-attention masks to control what context the prediction conditions\\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UniLM achieves new\\nstate-of-the-art results on five natural language generation datasets,\\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\\nis 2.65). The code and pre-trained models are available at\\nhttps://github.com/microsoft/unilm.\\n",
      "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\\nbe fine-tuned for both natural language understanding and generation tasks. The\\nmodel is pre-trained using three types of language modeling tasks:\\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\\nmodeling is achieved by employing a shared Transformer network and utilizing\\nspecific self-attention masks to control what context the prediction conditions\\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UniLM achieves new\\nstate-of-the-art results on five natural language generation datasets,\\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\\nis 2.65). The code and pre-trained models are available at\\nhttps://github.com/microsoft/unilm.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1905.03197",
      "openalex_id": "https://openalex.org/W2971274815",
      "arxiv_id": "",
      "publication_date": "2019-05-08",
      "published": "2019-05-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
      "summary": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.",
      "abstract": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.",
      "doi": "https://doi.org/10.21437/interspeech.2021-349",
      "openalex_id": "https://openalex.org/W3095292526",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Million Song Dataset",
      "summary": "We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset.",
      "abstract": "We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset.",
      "doi": "https://doi.org/10.7916/d8nz8j07",
      "openalex_id": "https://openalex.org/W1556219185",
      "arxiv_id": "",
      "publication_date": "2011-01-01",
      "published": "2011-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Framework for Contrastive and Generative Learning of Audio Representations.",
      "summary": "In this paper, we present a framework for contrastive learning for audio representations, in a self supervised frame work without access to any ground truth labels. The core idea in self supervised contrastive learning is to map an audio signal and its various augmented versions (representative of salient aspects of audio like pitch, timbre etc.) to a space where they are close together, and are separated from other different signals. In addition we also explore generative models based on state of the art transformer based architectures for learning latent spaces for audio signals, without access to any labels. Here, we map audio signals on a smaller scale to discrete dictionary elements and train transformers to predict the next dictionary element. We only use data as a method of supervision, bypassing the need of labels needed to act as a supervision for training the deep neural networks. We then use a linear classifier head in order to evaluate the performance of our models, for both self supervised contrastive and generative transformer based representations that are learned. Our system achieves considerable performance, compared to a fully supervised method, with access to ground truth labels to train the neural network model. These representations, with avail-ability of large scale audio data show promise in various tasks for audio understanding tasks",
      "abstract": "In this paper, we present a framework for contrastive learning for audio representations, in a self supervised frame work without access to any ground truth labels. The core idea in self supervised contrastive learning is to map an audio signal and its various augmented versions (representative of salient aspects of audio like pitch, timbre etc.) to a space where they are close together, and are separated from other different signals. In addition we also explore generative models based on state of the art transformer based architectures for learning latent spaces for audio signals, without access to any labels. Here, we map audio signals on a smaller scale to discrete dictionary elements and train transformers to predict the next dictionary element. We only use data as a method of supervision, bypassing the need of labels needed to act as a supervision for training the deep neural networks. We then use a linear classifier head in order to evaluate the performance of our models, for both self supervised contrastive and generative transformer based representations that are learned. Our system achieves considerable performance, compared to a fully supervised method, with access to ground truth labels to train the neural network model. These representations, with avail-ability of large scale audio data show promise in various tasks for audio understanding tasks",
      "doi": "",
      "openalex_id": "https://openalex.org/W3093494400",
      "arxiv_id": "",
      "publication_date": "2020-10-22",
      "published": "2020-10-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparison and Analysis of Deep Audio Embeddings for Music Emotion Recognition",
      "summary": "Emotion is a complicated notion present in music that is hard to capture even with fine-tuned feature engineering. In this paper, we investigate the utility of state-of-the-art pre-trained deep audio embedding methods to be used in the Music Emotion Recognition (MER) task. Deep audio embedding methods allow us to efficiently capture the high dimensional features into a compact representation. We implement several multi-class classifiers with deep audio embeddings to predict emotion semantics in music. We investigate the effectiveness of L3-Net and VGGish deep audio embedding methods for music emotion inference over four music datasets. The experiments with several classifiers on the task show that the deep audio embedding solutions can improve the performances of the previous baseline MER models. We conclude that deep audio embeddings represent musical emotion semantics for the MER task without expert human engineering.",
      "abstract": "Emotion is a complicated notion present in music that is hard to capture even with fine-tuned feature engineering. In this paper, we investigate the utility of state-of-the-art pre-trained deep audio embedding methods to be used in the Music Emotion Recognition (MER) task. Deep audio embedding methods allow us to efficiently capture the high dimensional features into a compact representation. We implement several multi-class classifiers with deep audio embeddings to predict emotion semantics in music. We investigate the effectiveness of L3-Net and VGGish deep audio embedding methods for music emotion inference over four music datasets. The experiments with several classifiers on the task show that the deep audio embedding solutions can improve the performances of the previous baseline MER models. We conclude that deep audio embeddings represent musical emotion semantics for the MER task without expert human engineering.",
      "doi": "https://doi.org/10.48550/arxiv.2104.06517",
      "openalex_id": "https://openalex.org/W3155776249",
      "arxiv_id": "",
      "publication_date": "2021-04-13",
      "published": "2021-04-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio and Tags",
      "summary": "Self-supervised audio representation learning offers an attractive alternative for obtaining generic audio embeddings, capable to be employed into various downstream tasks. Published approaches that consider both audio and words/tags associated with audio do not employ text processing models that are capable to generalize to tags unknown during training. In this work we propose a method for learning audio representations using an audio autoencoder (AAE), a general word embeddings model (WEM), and a multi-head self-attention (MHA) mechanism. MHA attends on the output of the WEM, providing a contextualized representation of the tags associated with the audio, and we align the output of MHA with the output of the encoder of AAE using a contrastive loss. We jointly optimize AAE and MHA and we evaluate the audio representations (i.e. the output of the encoder of AAE) by utilizing them in three different downstream tasks, namely sound, music genre, and music instrument classification. Our results show that employing multi-head self-attention with multiple heads in the tag-based network can induce better learned audio representations.",
      "abstract": "Self-supervised audio representation learning offers an attractive alternative for obtaining generic audio embeddings, capable to be employed into various downstream tasks. Published approaches that consider both audio and words/tags associated with audio do not employ text processing models that are capable to generalize to tags unknown during training. In this work we propose a method for learning audio representations using an audio autoencoder (AAE), a general word embeddings model (WEM), and a multi-head self-attention (MHA) mechanism. MHA attends on the output of the WEM, providing a contextualized representation of the tags associated with the audio, and we align the output of MHA with the output of the encoder of AAE using a contrastive loss. We jointly optimize AAE and MHA and we evaluate the audio representations (i.e. the output of the encoder of AAE) by utilizing them in three different downstream tasks, namely sound, music genre, and music instrument classification. Our results show that employing multi-head self-attention with multiple heads in the tag-based network can induce better learned audio representations.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414638",
      "openalex_id": "https://openalex.org/W3095513901",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use",
      "summary": "The GTZAN dataset appears in at least 100 published works, and is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR). Our recent work, however, shows GTZAN has several faults (repetitions, mislabelings, and distortions), which challenge the interpretability of any result derived using it. In this article, we disprove the claims that all MGR systems are affected in the same ways by these faults, and that the performances of MGR systems in GTZAN are still meaningfully comparable since they all face the same faults. We identify and analyze the contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has been used in MGR research, and find few indications that its faults have been known and considered. Finally, we rigorously study the effects of its faults on evaluating five different MGR systems. The lesson is not to banish GTZAN, but to use it with consideration of its contents.",
      "abstract": "The GTZAN dataset appears in at least 100 published works, and is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR). Our recent work, however, shows GTZAN has several faults (repetitions, mislabelings, and distortions), which challenge the interpretability of any result derived using it. In this article, we disprove the claims that all MGR systems are affected in the same ways by these faults, and that the performances of MGR systems in GTZAN are still meaningfully comparable since they all face the same faults. We identify and analyze the contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has been used in MGR research, and find few indications that its faults have been known and considered. Finally, we rigorously study the effects of its faults on evaluating five different MGR systems. The lesson is not to banish GTZAN, but to use it with consideration of its contents.",
      "doi": "https://doi.org/10.48550/arxiv.1306.1461",
      "openalex_id": "https://openalex.org/W3102568015",
      "arxiv_id": "",
      "publication_date": "2013-06-06",
      "published": "2013-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contrastive Learning of Musical Representations",
      "summary": "While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.",
      "abstract": "While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.",
      "doi": "https://doi.org/10.48550/arxiv.2103.09410",
      "openalex_id": "https://openalex.org/W3139211892",
      "arxiv_id": "",
      "publication_date": "2021-03-17",
      "published": "2021-03-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enriched Music Representations With Multiple Cross-Modal Contrastive Learning",
      "summary": "&lt;p&gt;Modeling various aspects that make a music piece unique is a challenging task, requiring the combination of multiple sources of information. Deep learning is commonly used to obtain representations using various sources of information, such as the audio, interactions between users and songs, or associated genre metadata. Recently, contrastive learning has led to representations that generalize better compared to traditional supervised methods. In this paper, we present a novel approach that combines multiple types of information related to music using cross-modal contrastive learning, allowing us to learn an audio feature from heterogeneous data simultaneously. We align the latent representations obtained from playlists-track interactions, genre metadata, and the tracks&rsquo; audio, by maximizing the agreement between these modality representations using a contrastive loss. We evaluate our approach in three tasks, namely, genre classification, playlist continuation and automatic tagging. We compare the performances with a baseline audio-based CNN trained to predict these modalities. We also study the importance of including multiple sources of information when training our embedding model. The results suggest that the proposed method outperforms the baseline in all the three downstream tasks and achieves comparable performance to the state-of-the-art.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Modeling various aspects that make a music piece unique is a challenging task, requiring the combination of multiple sources of information. Deep learning is commonly used to obtain representations using various sources of information, such as the audio, interactions between users and songs, or associated genre metadata. Recently, contrastive learning has led to representations that generalize better compared to traditional supervised methods. In this paper, we present a novel approach that combines multiple types of information related to music using cross-modal contrastive learning, allowing us to learn an audio feature from heterogeneous data simultaneously. We align the latent representations obtained from playlists-track interactions, genre metadata, and the tracks&rsquo; audio, by maximizing the agreement between these modality representations using a contrastive loss. We evaluate our approach in three tasks, namely, genre classification, playlist continuation and automatic tagging. We compare the performances with a baseline audio-based CNN trained to predict these modalities. We also study the importance of including multiple sources of information when training our embedding model. The results suggest that the proposed method outperforms the baseline in all the three downstream tasks and achieves comparable performance to the state-of-the-art.&lt;/p&gt;",
      "doi": "https://doi.org/10.1109/lsp.2021.3071082",
      "openalex_id": "https://openalex.org/W3143852976",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scaling Laws for Neural Language Models",
      "summary": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
      "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
      "doi": "https://doi.org/10.48550/arxiv.2001.08361",
      "openalex_id": "https://openalex.org/W3001279689",
      "arxiv_id": "",
      "publication_date": "2020-01-23",
      "published": "2020-01-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Learning and Music Adversaries",
      "summary": "An adversary is an agent designed to make a classification system perform in some particular way, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, exploiting the parameters of the system to find the minimal perturbation of the input image such that the system misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the system inputs are magnitude spectral frames, which require special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different architectures, we find that this adversary is very effective. We find that convolutional architectures are more robust compared to systems based on a majority vote over individually classified audio frames. Furthermore, we experiment with a new system that integrates an adversary into the training loop, but do not find that this improves the resilience of the system to new adversaries.",
      "abstract": "An adversary is an agent designed to make a classification system perform in some particular way, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, exploiting the parameters of the system to find the minimal perturbation of the input image such that the system misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the system inputs are magnitude spectral frames, which require special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different architectures, we find that this adversary is very effective. We find that convolutional architectures are more robust compared to systems based on a majority vote over individually classified audio frames. Furthermore, we experiment with a new system that integrates an adversary into the training loop, but do not find that this improves the resilience of the system to new adversaries.",
      "doi": "https://doi.org/10.1109/tmm.2015.2478068",
      "openalex_id": "https://openalex.org/W2962904371",
      "arxiv_id": "",
      "publication_date": "2016-02-18",
      "published": "2016-02-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Automatic Jazz Melody Generation by Transfer Learning Techniques",
      "summary": "In this paper, we tackle the problem of transfer learning for Jazz automatic generation. Jazz is one of representative types of music, but the lack of Jazz data in the MIDI format hinders the construction of a generative model for Jazz. Transfer learning is an approach aiming to solve the problem of data insufficiency, so as to transfer the common feature from one domain to another. In view of its success in other machine learning problems, we investigate whether, and how much, it can help improve automatic music generation for under-resourced musical genres. Specifically, we use a recurrent variational autoencoder as the generative model, and use a genre-unspecified dataset as the source dataset and a Jazz-only dataset as the target dataset. Two transfer learning methods are evaluated using six levels of source-to-target data ratios. The first method is to train the model on the source dataset, and then fine-tune the resulting model parameters on the target dataset. The second method is to train the model on both the source and target datasets at the same time, but add genre labels to the latent vectors and use a genre classifier to improve Jazz generation. Our subjective evaluation shows that both methods outperform the baseline method that uses Jazz data only for training by a large margin. Among the two methods, the first method seems to perform better. Our evaluation also shows the limits of existing objective metrics in evaluating the performance of music generation models.",
      "abstract": "In this paper, we tackle the problem of transfer learning for Jazz automatic generation. Jazz is one of representative types of music, but the lack of Jazz data in the MIDI format hinders the construction of a generative model for Jazz. Transfer learning is an approach aiming to solve the problem of data insufficiency, so as to transfer the common feature from one domain to another. In view of its success in other machine learning problems, we investigate whether, and how much, it can help improve automatic music generation for under-resourced musical genres. Specifically, we use a recurrent variational autoencoder as the generative model, and use a genre-unspecified dataset as the source dataset and a Jazz-only dataset as the target dataset. Two transfer learning methods are evaluated using six levels of source-to-target data ratios. The first method is to train the model on the source dataset, and then fine-tune the resulting model parameters on the target dataset. The second method is to train the model on both the source and target datasets at the same time, but add genre labels to the latent vectors and use a genre classifier to improve Jazz generation. Our subjective evaluation shows that both methods outperform the baseline method that uses Jazz data only for training by a large margin. Among the two methods, the first method seems to perform better. Our evaluation also shows the limits of existing objective metrics in evaluating the performance of music generation models.",
      "doi": "https://doi.org/10.1109/apsipaasc47483.2019.9023224",
      "openalex_id": "https://openalex.org/W3010903955",
      "arxiv_id": "",
      "publication_date": "2019-11-01",
      "published": "2019-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representation Learning of Music Using Artist, Album, and Track Information",
      "summary": "Supervised music representation learning has been performed mainly using semantic labels such as music genres. However, annotating music with semantic labels requires time and cost. In this work, we investigate the use of factual metadata such as artist, album, and track information, which are naturally annotated to songs, for supervised music representation learning. The results show that each of the metadata has individual concept characteristics, and using them jointly improves overall performance.",
      "abstract": "Supervised music representation learning has been performed mainly using semantic labels such as music genres. However, annotating music with semantic labels requires time and cost. In this work, we investigate the use of factual metadata such as artist, album, and track information, which are naturally annotated to songs, for supervised music representation learning. The results show that each of the metadata has individual concept characteristics, and using them jointly improves overall performance.",
      "doi": "https://doi.org/10.48550/arxiv.1906.11783",
      "openalex_id": "https://openalex.org/W2955142818",
      "arxiv_id": "",
      "publication_date": "2019-06-27",
      "published": "2019-06-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Usage in MIR: History & Future Recommendations.",
      "summary": "The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility. Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders.",
      "abstract": "The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility. Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders.",
      "doi": "https://doi.org/10.5072/zenodo.415282",
      "openalex_id": "https://openalex.org/W2990244497",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluation of CNN-based Automatic Music Tagging Models",
      "summary": "Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.",
      "abstract": "Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.",
      "doi": "https://doi.org/10.48550/arxiv.2006.00751",
      "openalex_id": "https://openalex.org/W3029858316",
      "arxiv_id": "",
      "publication_date": "2020-06-01",
      "published": "2020-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "musicnn: Pre-trained Convolutional Neural Networks for Music Audio Tagging",
      "summary": "Pronounced as musician, the musicnn library contains a set of pre-trained musically motivated convolutional neural networks for music audio tagging: this https URL. This repository also includes some pre-trained vgg-like baselines. These models can be used as out-of-the-box music audio taggers, as music feature extractors, or as pre-trained models for transfer learning. \r\nWe also provide the code to train the aforementioned models: this https URL. This framework also allows implementing novel models. For example, a musically motivated convolutional neural network with an attention-based output layer (instead of the temporal pooling layer) can achieve state-of-the-art results for music audio tagging: 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset.",
      "abstract": "Pronounced as musician, the musicnn library contains a set of pre-trained musically motivated convolutional neural networks for music audio tagging: this https URL. This repository also includes some pre-trained vgg-like baselines. These models can be used as out-of-the-box music audio taggers, as music feature extractors, or as pre-trained models for transfer learning. \r\nWe also provide the code to train the aforementioned models: this https URL. This framework also allows implementing novel models. For example, a musically motivated convolutional neural network with an attention-based output layer (instead of the temporal pooling layer) can achieve state-of-the-art results for music audio tagging: 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3000400453",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automated Music Emotion Recognition: A Systematic Evaluation",
      "summary": "Abstract Automated music emotion recognition (MER) is a challenging task in Music Information Retrieval with wide-ranging applications. Some recent studies pose MER as a continuous regression problem in the Arousal-Valence (AV) plane. These consist of variations on a common architecture having a universal model of emotional response, a common repertoire of low-level audio features, a bag-of-frames approach to audio analysis, and relatively small data sets. These approaches achieve some success at MER and suggest that further improvements are possible with current technology. Our contribution to the state of the art is to examine just how far one can go within this framework, and to investigate what the limitations of this framework are. We present the results of a systematic study conducted in an attempt to maximize the prediction performance of an automated MER system using the architecture described. We begin with a carefully constructed data set, emphasizing quality over quantity. We address affect induction rather than affect attribution. We consider a variety of algorithms at each stage of the training process, from preprocessing to feature selection and model selection, and we report the results of extensive testing. We found that: (1) none of the variations we considered leads to a substantial improvement in performance, which we present as evidence of a limit on what is achievable under this architecture, and (2) the size of the small data sets that are commonly used in the MER literature limits the possibility of improving the set of features used in MER due to the phenomenon of Subset Selection Bias. We conclude with some proposals for advancing the state of the art.",
      "abstract": "Abstract Automated music emotion recognition (MER) is a challenging task in Music Information Retrieval with wide-ranging applications. Some recent studies pose MER as a continuous regression problem in the Arousal-Valence (AV) plane. These consist of variations on a common architecture having a universal model of emotional response, a common repertoire of low-level audio features, a bag-of-frames approach to audio analysis, and relatively small data sets. These approaches achieve some success at MER and suggest that further improvements are possible with current technology. Our contribution to the state of the art is to examine just how far one can go within this framework, and to investigate what the limitations of this framework are. We present the results of a systematic study conducted in an attempt to maximize the prediction performance of an automated MER system using the architecture described. We begin with a carefully constructed data set, emphasizing quality over quantity. We address affect induction rather than affect attribution. We consider a variety of algorithms at each stage of the training process, from preprocessing to feature selection and model selection, and we report the results of extensive testing. We found that: (1) none of the variations we considered leads to a substantial improvement in performance, which we present as evidence of a limit on what is achievable under this architecture, and (2) the size of the small data sets that are commonly used in the MER literature limits the possibility of improving the set of features used in MER due to the phenomenon of Subset Selection Bias. We conclude with some proposals for advancing the state of the art.",
      "doi": "https://doi.org/10.1080/09298215.2010.513733",
      "openalex_id": "https://openalex.org/W2023001347",
      "arxiv_id": "",
      "publication_date": "2010-09-01",
      "published": "2010-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On-line continuous-time music mood regression with deep recurrent neural networks",
      "summary": "This paper proposes a novel machine learning approach for the task of on-line continuous-time music mood regression, i.e., low-latency prediction of the time-varying arousal and valence in musical pieces. On the front-end, a large set of segmental acoustic features is extracted to model short-term variations. Then, multi-variate regression is performed by deep recurrent neural networks to model longer-range context and capture the time-varying emotional profile of musical pieces appropriately. Evaluation is done on the 2013 MediaEval Challenge corpus consisting of 1000 pieces annotated in continous time and continuous arousal and valence by crowd-sourcing. In the result, recurrent neural networks outperform SVR and feedforward neural networks both in continuous-time and static music mood regression, and achieve an R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> of up to .70 and .50 with arousal and valence annotations.",
      "abstract": "This paper proposes a novel machine learning approach for the task of on-line continuous-time music mood regression, i.e., low-latency prediction of the time-varying arousal and valence in musical pieces. On the front-end, a large set of segmental acoustic features is extracted to model short-term variations. Then, multi-variate regression is performed by deep recurrent neural networks to model longer-range context and capture the time-varying emotional profile of musical pieces appropriately. Evaluation is done on the 2013 MediaEval Challenge corpus consisting of 1000 pieces annotated in continous time and continuous arousal and valence by crowd-sourcing. In the result, recurrent neural networks outperform SVR and feedforward neural networks both in continuous-time and static music mood regression, and achieve an R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> of up to .70 and .50 with arousal and valence annotations.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854637",
      "openalex_id": "https://openalex.org/W2019360207",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Linguistic Knowledge and Transferability of Contextual Representations",
      "summary": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
      "abstract": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
      "doi": "https://doi.org/10.18653/v1/n19-1112",
      "openalex_id": "https://openalex.org/W2922565841",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Open-Source Practices for Music Signal Processing Research: Recommendations for Transparent, Sustainable, and Reproducible Audio Research",
      "summary": "In the early years of music information retrieval (MIR), research problems were often centered around conceptually simple tasks, and methods were evaluated on small, idealized data sets. A canonical example of this is genre recognition-i.e., Which one of n genres describes this song?-which was often evaluated on the GTZAN data set (1,000 musical excerpts balanced across ten genres) [1]. As task definitions were simple, so too were signal analysis pipelines, which often derived from methods for speech processing and recognition and typically consisted of simple methods for feature extraction, statistical modeling, and evaluation. When describing a research system, the expected level of detail was superficial: it was sufficient to state, e.g., the number of mel-frequency cepstral coefficients used, the statistical model (e.g., a Gaussian mixture model), the choice of data set, and the evaluation criteria, without stating the underlying software dependencies or implementation details. Because of an increased abundance of methods, the proliferation of software toolkits, the explosion of machine learning, and a focus shift toward more realistic problem settings, modern research systems are substantially more complex than their predecessors. Modern MIR researchers must pay careful attention to detail when processing metadata, implementing evaluation criteria, and disseminating results.",
      "abstract": "In the early years of music information retrieval (MIR), research problems were often centered around conceptually simple tasks, and methods were evaluated on small, idealized data sets. A canonical example of this is genre recognition-i.e., Which one of n genres describes this song?-which was often evaluated on the GTZAN data set (1,000 musical excerpts balanced across ten genres) [1]. As task definitions were simple, so too were signal analysis pipelines, which often derived from methods for speech processing and recognition and typically consisted of simple methods for feature extraction, statistical modeling, and evaluation. When describing a research system, the expected level of detail was superficial: it was sufficient to state, e.g., the number of mel-frequency cepstral coefficients used, the statistical model (e.g., a Gaussian mixture model), the choice of data set, and the evaluation criteria, without stating the underlying software dependencies or implementation details. Because of an increased abundance of methods, the proliferation of software toolkits, the explosion of machine learning, and a focus shift toward more realistic problem settings, modern research systems are substantially more complex than their predecessors. Modern MIR researchers must pay careful attention to detail when processing metadata, implementing evaluation criteria, and disseminating results.",
      "doi": "https://doi.org/10.1109/msp.2018.2875349",
      "openalex_id": "https://openalex.org/W2906658932",
      "arxiv_id": "",
      "publication_date": "2018-12-25",
      "published": "2018-12-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Tasking with Joint Semantic Spaces for Large-Scale Music Annotation and Retrieval",
      "summary": "Abstract Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modelling audio, artist names, and tags in a single low-dimensional semantic embedding space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our single model learnt by training on the joint objective function is shown experimentally to have improved accuracy over training on each task alone. Our method also outperforms the baseline methods tried and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.",
      "abstract": "Abstract Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modelling audio, artist names, and tags in a single low-dimensional semantic embedding space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our single model learnt by training on the joint objective function is shown experimentally to have improved accuracy over training on each task alone. Our method also outperforms the baseline methods tried and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.",
      "doi": "https://doi.org/10.1080/09298215.2011.603834",
      "openalex_id": "https://openalex.org/W1989445502",
      "arxiv_id": "",
      "publication_date": "2011-11-21",
      "published": "2011-11-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Finding temporal structure in music: blues improvisation with LSTM recurrent networks",
      "summary": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
      "abstract": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
      "doi": "https://doi.org/10.1109/nnsp.2002.1030094",
      "openalex_id": "https://openalex.org/W2137619888",
      "arxiv_id": "",
      "publication_date": "2003-06-25",
      "published": "2003-06-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Musical genre classification of audio signals",
      "summary": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.",
      "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.",
      "doi": "https://doi.org/10.1109/tsa.2002.800560",
      "openalex_id": "https://openalex.org/W2133824856",
      "arxiv_id": "",
      "publication_date": "2002-07-01",
      "published": "2002-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "1000 songs for emotional analysis of music",
      "summary": "Music is composed to be emotionally expressive, and emotional associations provide an especially natural domain for indexing and recommendation in today's vast digital music libraries. But such libraries require powerful automated tools, and the development of systems for automatic prediction of musical emotion presents a myriad challenges. The perceptual nature of musical emotion necessitates the collection of data from human subjects. The interpretation of emotion varies between listeners thus each clip needs to be annotated by a distribution of subjects. In addition, the sharing of large music content libraries for the development of such systems, even for academic research, presents complicated legal issues which vary by country. This work presents a new publicly available dataset for music emotion recognition research and a baseline system. In addressing the difficulties of emotion annotation we have turned to crowdsourcing, using Amazon Mechanical Turk, and have developed a two-stage procedure for filtering out poor quality workers. The dataset consists entirely of creative commons music from the Free Music Archive, which as the name suggests, can be shared freely without penalty. The final dataset contains 1000 songs, each annotated by a minimum of 10 subjects, which is larger than many currently available music emotion dataset.",
      "abstract": "Music is composed to be emotionally expressive, and emotional associations provide an especially natural domain for indexing and recommendation in today's vast digital music libraries. But such libraries require powerful automated tools, and the development of systems for automatic prediction of musical emotion presents a myriad challenges. The perceptual nature of musical emotion necessitates the collection of data from human subjects. The interpretation of emotion varies between listeners thus each clip needs to be annotated by a distribution of subjects. In addition, the sharing of large music content libraries for the development of such systems, even for academic research, presents complicated legal issues which vary by country. This work presents a new publicly available dataset for music emotion recognition research and a baseline system. In addressing the difficulties of emotion annotation we have turned to crowdsourcing, using Amazon Mechanical Turk, and have developed a two-stage procedure for filtering out poor quality workers. The dataset consists entirely of creative commons music from the Free Music Archive, which as the name suggests, can be shared freely without penalty. The final dataset contains 1000 songs, each annotated by a minimum of 10 subjects, which is larger than many currently available music emotion dataset.",
      "doi": "https://doi.org/10.1145/2506364.2506365",
      "openalex_id": "https://openalex.org/W2136129419",
      "arxiv_id": "",
      "publication_date": "2013-10-17",
      "published": "2013-10-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representation Learning of Music Using Artist Labels",
      "summary": "In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.",
      "abstract": "In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.",
      "doi": "https://doi.org/10.5281/zenodo.1492516",
      "openalex_id": "https://openalex.org/W2765325162",
      "arxiv_id": "",
      "publication_date": "2018-09-23",
      "published": "2018-09-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SampleCNN: End-to-End Deep Convolutional Neural Networks Using Very Small Filters for Music Classification",
      "summary": "Convolutional Neural Networks (CNN) have been applied to diverse machine learning tasks for different modalities of raw data in an end-to-end fashion. In the audio domain, a raw waveform-based approach has been explored to directly learn hierarchical characteristics of audio. However, the majority of previous studies have limited their model capacity by taking a frame-level structure similar to short-time Fourier transforms. We previously proposed a CNN architecture which learns representations using sample-level filters beyond typical frame-level input representations. The architecture showed comparable performance to the spectrogram-based CNN model in music auto-tagging. In this paper, we extend the previous work in three ways. First, considering the sample-level model requires much longer training time, we progressively downsample the input signals and examine how it affects the performance. Second, we extend the model using multi-level and multi-scale feature aggregation technique and subsequently conduct transfer learning for several music classification tasks. Finally, we visualize filters learned by the sample-level CNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency.",
      "abstract": "Convolutional Neural Networks (CNN) have been applied to diverse machine learning tasks for different modalities of raw data in an end-to-end fashion. In the audio domain, a raw waveform-based approach has been explored to directly learn hierarchical characteristics of audio. However, the majority of previous studies have limited their model capacity by taking a frame-level structure similar to short-time Fourier transforms. We previously proposed a CNN architecture which learns representations using sample-level filters beyond typical frame-level input representations. The architecture showed comparable performance to the spectrogram-based CNN model in music auto-tagging. In this paper, we extend the previous work in three ways. First, considering the sample-level model requires much longer training time, we progressively downsample the input signals and examine how it affects the performance. Second, we extend the model using multi-level and multi-scale feature aggregation technique and subsequently conduct transfer learning for several music classification tasks. Finally, we visualize filters learned by the sample-level CNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency.",
      "doi": "https://doi.org/10.3390/app8010150",
      "openalex_id": "https://openalex.org/W2794150026",
      "arxiv_id": "",
      "publication_date": "2018-01-22",
      "published": "2018-01-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis",
      "summary": "Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the \"JSUT corpus,\" that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.",
      "abstract": "Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the \"JSUT corpus,\" that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.",
      "doi": "https://doi.org/10.48550/arxiv.1711.00354",
      "openalex_id": "https://openalex.org/W2765486990",
      "arxiv_id": "",
      "publication_date": "2017-10-28",
      "published": "2017-10-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recent Development of Open-Source Speech Recognition Engine Julius",
      "summary": "Abstract—Julius is an open-source large-vocabulary speech recognition software used for both academic research and in-dustrial applications. It executes real-time speech recognition of a 60k-word dictation task on low-spec PCs with small footprint, and even on embedded devices. Julius supports standard lan-guage models such as statistical N-gram model and rule-based grammars, as well as Hidden Markov Model (HMM) as an acoustic model. One can build a speech recognition system of his own purpose, or can integrate the speech recognition capability to a variety of applications using Julius. This article describes an overview of Julius, major features and specifications, and summarizes the developments conducted in the recent years. I.",
      "abstract": "Abstract—Julius is an open-source large-vocabulary speech recognition software used for both academic research and in-dustrial applications. It executes real-time speech recognition of a 60k-word dictation task on low-spec PCs with small footprint, and even on embedded devices. Julius supports standard lan-guage models such as statistical N-gram model and rule-based grammars, as well as Hidden Markov Model (HMM) as an acoustic model. One can build a speech recognition system of his own purpose, or can integrate the speech recognition capability to a variety of applications using Julius. This article describes an overview of Julius, major features and specifications, and summarizes the developments conducted in the recent years. I.",
      "doi": "",
      "openalex_id": "https://openalex.org/W72347498",
      "arxiv_id": "",
      "publication_date": "2009-10-04",
      "published": "2009-10-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "summary": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",
      "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",
      "doi": "https://doi.org/10.18653/v1/d18-2012",
      "openalex_id": "https://openalex.org/W2885185669",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Investigating gated recurrent networks for speech synthesis",
      "summary": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.",
      "abstract": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472657",
      "openalex_id": "https://openalex.org/W2964060510",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "THCHS-30 : A Free Chinese Speech Corpus",
      "summary": "Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database THCHS-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions.",
      "abstract": "Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database THCHS-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions.",
      "doi": "https://doi.org/10.48550/arxiv.1512.01882",
      "openalex_id": "https://openalex.org/W2284628133",
      "arxiv_id": "",
      "publication_date": "2015-12-07",
      "published": "2015-12-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NONOTO: A Model-agnostic Web Interface for Interactive Music Composition by Inpainting.",
      "summary": "Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.",
      "abstract": "Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3032697904",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
      "summary": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).",
      "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).",
      "doi": "https://doi.org/10.48550/arxiv.1711.05772",
      "openalex_id": "https://openalex.org/W2769811909",
      "arxiv_id": "",
      "publication_date": "2017-11-15",
      "published": "2017-11-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Attention with Relative Position Representations",
      "summary": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
      "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
      "doi": "https://doi.org/10.18653/v1/n18-2074",
      "openalex_id": "https://openalex.org/W2789541106",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PixelSNAIL: An Improved Autoregressive Generative Model",
      "summary": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public",
      "abstract": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public",
      "doi": "https://doi.org/10.48550/arxiv.1712.09763",
      "openalex_id": "https://openalex.org/W2778792233",
      "arxiv_id": "",
      "publication_date": "2017-12-28",
      "published": "2017-12-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Music transcription modelling and composition using deep learning",
      "summary": "We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: \\url{https://github.com/IraKorshunova/folk-rnn}.",
      "abstract": "We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: \\url{https://github.com/IraKorshunova/folk-rnn}.",
      "doi": "https://doi.org/10.48550/arxiv.1604.08723",
      "openalex_id": "https://openalex.org/W2343635552",
      "arxiv_id": "",
      "publication_date": "2016-04-29",
      "published": "2016-04-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sampling Matters in Deep Embedding Learning",
      "summary": "Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.",
      "abstract": "Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.",
      "doi": "https://doi.org/10.1109/iccv.2017.309",
      "openalex_id": "https://openalex.org/W2963350250",
      "arxiv_id": "",
      "publication_date": "2017-10-01",
      "published": "2017-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Large Memory Layers with Product Keys",
      "summary": "This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.",
      "abstract": "This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.",
      "doi": "https://doi.org/10.48550/arxiv.1907.05242",
      "openalex_id": "https://openalex.org/W2970401203",
      "arxiv_id": "",
      "publication_date": "2019-07-10",
      "published": "2019-07-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning to Traverse Latent Spaces for Musical Score Inpainting",
      "summary": "Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",
      "abstract": "Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",
      "doi": "https://doi.org/10.5281/zenodo.3527813",
      "openalex_id": "https://openalex.org/W2991421901",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Bach Doodle: Approachable music composition with machine learning at scale",
      "summary": "To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.",
      "abstract": "To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.",
      "doi": "https://doi.org/10.48550/arxiv.1907.06637",
      "openalex_id": "https://openalex.org/W2962212541",
      "arxiv_id": "",
      "publication_date": "2019-07-14",
      "published": "2019-07-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Encoding Musical Style with Transformer Autoencoders",
      "summary": "We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.",
      "abstract": "We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.",
      "doi": "https://doi.org/10.48550/arxiv.1912.05537",
      "openalex_id": "https://openalex.org/W2995416527",
      "arxiv_id": "",
      "publication_date": "2019-12-10",
      "published": "2019-12-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture\\n Likelihood and Other Modifications",
      "summary": "PixelCNNs are a recently proposed class of powerful generative models with\\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\\nmake available at https://github.com/openai/pixel-cnn. Our implementation\\ncontains a number of modifications to the original model that both simplify its\\nstructure and improve its performance. 1) We use a discretized logistic mixture\\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\\nup training. 2) We condition on whole pixels, rather than R/G/B sub-pixels,\\nsimplifying the model structure. 3) We use downsampling to efficiently capture\\nstructure at multiple resolutions. 4) We introduce additional short-cut\\nconnections to further speed up optimization. 5) We regularize the model using\\ndropout. Finally, we present state-of-the-art log likelihood results on\\nCIFAR-10 to demonstrate the usefulness of these modifications.\\n",
      "abstract": "PixelCNNs are a recently proposed class of powerful generative models with\\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\\nmake available at https://github.com/openai/pixel-cnn. Our implementation\\ncontains a number of modifications to the original model that both simplify its\\nstructure and improve its performance. 1) We use a discretized logistic mixture\\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\\nup training. 2) We condition on whole pixels, rather than R/G/B sub-pixels,\\nsimplifying the model structure. 3) We use downsampling to efficiently capture\\nstructure at multiple resolutions. 4) We introduce additional short-cut\\nconnections to further speed up optimization. 5) We regularize the model using\\ndropout. Finally, we present state-of-the-art log likelihood results on\\nCIFAR-10 to demonstrate the usefulness of these modifications.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1701.05517",
      "openalex_id": "https://openalex.org/W2964122153",
      "arxiv_id": "",
      "publication_date": "2017-01-19",
      "published": "2017-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sampling Variations of Lead Sheets",
      "summary": "Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.",
      "abstract": "Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.",
      "doi": "https://doi.org/10.48550/arxiv.1703.00760",
      "openalex_id": "https://openalex.org/W2591710685",
      "arxiv_id": "",
      "publication_date": "2017-03-02",
      "published": "2017-03-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bootstrapping a Data-Set and Model for Question-Answering in Portuguese (Short Paper)",
      "summary": "Question answering systems are mainly concerned with fulfilling an information query written in natural language, given a collection of documents with relevant information. They are key elements in many popular application systems as personal assistants, chat-bots, or even FAQ-based online support systems. This paper describes an exploratory work carried out to come up with a state-of-the-art model for question-answering tasks, for the Portuguese language, based on deep neural networks. We also describe the automatic construction of a data-set for training and testing the model. The final model is not trained in any specific topic or context, and is able to handle generic documents, achieving 50% accuracy in the testing data-set. While the results are not exceptional, this work can support further development in the area, as both the data-set and model are publicly available.",
      "abstract": "Question answering systems are mainly concerned with fulfilling an information query written in natural language, given a collection of documents with relevant information. They are key elements in many popular application systems as personal assistants, chat-bots, or even FAQ-based online support systems. This paper describes an exploratory work carried out to come up with a state-of-the-art model for question-answering tasks, for the Portuguese language, based on deep neural networks. We also describe the automatic construction of a data-set for training and testing the model. The final model is not trained in any specific topic or context, and is able to handle generic documents, achieving 50% accuracy in the testing data-set. While the results are not exceptional, this work can support further development in the area, as both the data-set and model are publicly available.",
      "doi": "https://doi.org/10.4230/oasics.slate.2021.18",
      "openalex_id": "https://openalex.org/W2911109671",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Anticipation-RNN: enforcing unary constraints in sequence generation, with application to interactive music generation",
      "summary": "Recurrent neural networks (RNNs) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This article introduces a novel architecture called anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined unary constraints. We demonstrate its efficiency on the task of generating melodies satisfying unary constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.",
      "abstract": "Recurrent neural networks (RNNs) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This article introduces a novel architecture called anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined unary constraints. We demonstrate its efficiency on the task of generating melodies satisfying unary constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.",
      "doi": "https://doi.org/10.1007/s00521-018-3868-4",
      "openalex_id": "https://openalex.org/W2901638613",
      "arxiv_id": "",
      "publication_date": "2018-11-20",
      "published": "2018-11-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adaptive Attention Span in Transformers",
      "summary": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
      "abstract": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
      "doi": "https://doi.org/10.18653/v1/p19-1032",
      "openalex_id": "https://openalex.org/W2946567085",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An argument for basic emotions",
      "summary": "Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.",
      "abstract": "Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.",
      "doi": "https://doi.org/10.1080/02699939208411068",
      "openalex_id": "https://openalex.org/W1966797434",
      "arxiv_id": "",
      "publication_date": "1992-05-01",
      "published": "1992-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion",
      "summary": "Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.",
      "abstract": "Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683865",
      "openalex_id": "https://openalex.org/W2938833595",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems",
      "summary": "In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.",
      "abstract": "In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.",
      "doi": "https://doi.org/10.48550/arxiv.1806.09514",
      "openalex_id": "https://openalex.org/W2810914326",
      "arxiv_id": "",
      "publication_date": "2018-06-25",
      "published": "2018-06-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
      "summary": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.",
      "abstract": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1323",
      "openalex_id": "https://openalex.org/W3045354608",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Expressive Text-to-Speech using Style Tag",
      "summary": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.",
      "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.",
      "doi": "https://doi.org/10.48550/arxiv.2104.00436",
      "openalex_id": "https://openalex.org/W3144988954",
      "arxiv_id": "",
      "publication_date": "2021-04-01",
      "published": "2021-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GMM-Based Emotional Voice Conversion Using Spectrum and Prosody Features",
      "summary": "We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.",
      "abstract": "We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.",
      "doi": "https://doi.org/10.5923/j.ajsp.20120205.06",
      "openalex_id": "https://openalex.org/W2077801020",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Do People Agree on How Positive Emotions Are Expressed? A Survey of Four Emotions and Five Modalities Across 11 Cultures",
      "summary": "Abstract While much is known about how negative emotions are expressed in different modalities, our understanding of the nonverbal expressions of positive emotions remains limited. In the present research, we draw upon disparate lines of theoretical and empirical work on positive emotions, and systematically examine which channels are thought to be used for expressing four positive emotions: feeling moved, gratitude, interest, and triumph. Employing the intersubjective approach, an established method in cross-cultural psychology, we first explored how the four positive emotions were reported to be expressed in two North American community samples (Studies 1a and 1b: n = 1466). We next confirmed the cross-cultural generalizability of our findings by surveying respondents from ten countries that diverged on cultural values (Study 2: n = 1826). Feeling moved was thought to be signaled with facial expressions, gratitude with the use of words, interest with words, face and voice, and triumph with body posture, vocal cues, facial expressions, and words. These findings provide cross-culturally consistent findings of differential expressions across positive emotions. Notably, positive emotions were thought to be expressed via modalities that go beyond the face.",
      "abstract": "Abstract While much is known about how negative emotions are expressed in different modalities, our understanding of the nonverbal expressions of positive emotions remains limited. In the present research, we draw upon disparate lines of theoretical and empirical work on positive emotions, and systematically examine which channels are thought to be used for expressing four positive emotions: feeling moved, gratitude, interest, and triumph. Employing the intersubjective approach, an established method in cross-cultural psychology, we first explored how the four positive emotions were reported to be expressed in two North American community samples (Studies 1a and 1b: n = 1466). We next confirmed the cross-cultural generalizability of our findings by surveying respondents from ten countries that diverged on cultural values (Study 2: n = 1826). Feeling moved was thought to be signaled with facial expressions, gratitude with the use of words, interest with words, face and voice, and triumph with body posture, vocal cues, facial expressions, and words. These findings provide cross-culturally consistent findings of differential expressions across positive emotions. Notably, positive emotions were thought to be expressed via modalities that go beyond the face.",
      "doi": "https://doi.org/10.1007/s10919-021-00376-0",
      "openalex_id": "https://openalex.org/W3185206490",
      "arxiv_id": "",
      "publication_date": "2021-07-22",
      "published": "2021-07-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Target Emotional Voice Conversion With Neural Vocoders",
      "summary": "Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.",
      "abstract": "Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.",
      "doi": "https://doi.org/10.48550/arxiv.2004.03782",
      "openalex_id": "https://openalex.org/W3015669407",
      "arxiv_id": "",
      "publication_date": "2020-04-08",
      "published": "2020-04-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Theory behind Controllable Expressive Speech Synthesis: A Cross-Disciplinary Approach",
      "summary": "As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.",
      "abstract": "As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.",
      "doi": "https://doi.org/10.5772/intechopen.89849",
      "openalex_id": "https://openalex.org/W2979790850",
      "arxiv_id": "",
      "publication_date": "2019-12-02",
      "published": "2019-12-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotional Voice Conversion Using Multitask Learning with Text-To-Speech",
      "summary": "Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.",
      "abstract": "Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053255",
      "openalex_id": "https://openalex.org/W3015719316",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Does the Lombard Effect Improve Emotional Communication in Noise? — Analysis of Emotional Speech Acted in Noise",
      "summary": "Speakers usually adjust their way of talking in noisy environments involuntarily for effective communication. This adaptation is known as the Lombard effect. Although speech accompanying the Lombard effect can improve the intelligibility of a speaker's voice, the changes in acoustic features (e.g. fundamental frequency, speech intensity, and spectral tilt) caused by the Lombard effect may also affect the listener's judgment of emotional content. To the best of our knowledge, there is no published study on the influence of the Lombard effect in emotional speech. Therefore, we recorded parallel emotional speech waveforms uttered by 12 speakers under both quiet and noisy conditions in a professional recording studio in order to explore how the Lombard effect interacts with emotional speech. By analyzing confusion matrices and acoustic features, we aim to answer the following questions: 1) Can speakers express their emotions correctly even under adverse conditions? 2) Can listeners recognize the emotion contained in speech signals even under noise? 3) How does emotional speech uttered in noise differ from emotional speech uttered in quiet conditions in terms of acoustic characteristic?",
      "abstract": "Speakers usually adjust their way of talking in noisy environments involuntarily for effective communication. This adaptation is known as the Lombard effect. Although speech accompanying the Lombard effect can improve the intelligibility of a speaker's voice, the changes in acoustic features (e.g. fundamental frequency, speech intensity, and spectral tilt) caused by the Lombard effect may also affect the listener's judgment of emotional content. To the best of our knowledge, there is no published study on the influence of the Lombard effect in emotional speech. Therefore, we recorded parallel emotional speech waveforms uttered by 12 speakers under both quiet and noisy conditions in a professional recording studio in order to explore how the Lombard effect interacts with emotional speech. By analyzing confusion matrices and acoustic features, we aim to answer the following questions: 1) Can speakers express their emotions correctly even under adverse conditions? 2) Can listeners recognize the emotion contained in speech signals even under noise? 3) How does emotional speech uttered in noise differ from emotional speech uttered in quiet conditions in terms of acoustic characteristic?",
      "doi": "https://doi.org/10.21437/interspeech.2019-1605",
      "openalex_id": "https://openalex.org/W2973224100",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning deep representations by mutual information estimation and maximization",
      "summary": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",
      "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2951873722",
      "arxiv_id": "",
      "publication_date": "2018-08-20",
      "published": "2018-08-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Acoustic Model Adaptation for Emotional Speech Recognition Using Twitter-Based Emotional Speech Corpus",
      "summary": "In recent years, Japanese Twitter-based emotional speech (JTES) was constructed as an emotional speech corpus. This corpus is based on tweets, and has features wherein an emotional label is assigned to each sentence, and sentences are selected considering the balance of both phoneme and prosody. Compared to speech recognition without emotion, emotional speech recognition is a difficult task. In this study, we aim to improve the performance of emotional speech recognition on the JTES corpus using acoustic model adaptation. For recognition, a deep neural network-based hidden Markov model (DNN-HMM) is used as the acoustic model. As a baseline, a word error rate (WER) of 38.0% was obtained when the DNN-HMM was trained by the corpus of spontaneous Japanese. This model was used as an initial model for adaptation. In this study, various types of adaptation were examined, and substantial performance improvement was achieved. Finally, a WER of 23.05% was obtained using speaker adaptation.",
      "abstract": "In recent years, Japanese Twitter-based emotional speech (JTES) was constructed as an emotional speech corpus. This corpus is based on tweets, and has features wherein an emotional label is assigned to each sentence, and sentences are selected considering the balance of both phoneme and prosody. Compared to speech recognition without emotion, emotional speech recognition is a difficult task. In this study, we aim to improve the performance of emotional speech recognition on the JTES corpus using acoustic model adaptation. For recognition, a deep neural network-based hidden Markov model (DNN-HMM) is used as the acoustic model. As a baseline, a word error rate (WER) of 38.0% was obtained when the DNN-HMM was trained by the corpus of spontaneous Japanese. This model was used as an initial model for adaptation. In this study, various types of adaptation were examined, and substantial performance improvement was achieved. Finally, a WER of 23.05% was obtained using speaker adaptation.",
      "doi": "https://doi.org/10.23919/apsipa.2018.8659756",
      "openalex_id": "https://openalex.org/W2921306346",
      "arxiv_id": "",
      "publication_date": "2018-11-01",
      "published": "2018-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Adaptation of a Multilingual Acoustic Model for Cross-Language Synthesis",
      "summary": "Several studies have shown promising results in adapting DNN-based acoustic models as a mechanism to transfer characteristics from pre-trained models. One such example is speaker adaptation using a small amount of data, where fine-tuning has helped train models that extrapolate well to diverse linguistic contexts that are not present in the adaptation data. In the current work, our objective is to synthesize speech in different languages using the target speaker's voice, regardless of the language of their data. To achieve this goal, we create a multilingual model using a corpus that consists of recordings from a large number of monolingual and a few bilingual speakers in multiple languages. The model is then adapted using the target speaker's recordings in a language other than the target language. We also explore if additional adaptation data from a native speaker of the target language improves the performance. The subjective evaluation shows that the proposed approach of cross-language speaker adaptation is able to synthesize speech in the target language, in the target speaker's voice, without data spoken by the target speaker in that language. Also, extra data from a native speaker of the target language can improve model performance.",
      "abstract": "Several studies have shown promising results in adapting DNN-based acoustic models as a mechanism to transfer characteristics from pre-trained models. One such example is speaker adaptation using a small amount of data, where fine-tuning has helped train models that extrapolate well to diverse linguistic contexts that are not present in the adaptation data. In the current work, our objective is to synthesize speech in different languages using the target speaker's voice, regardless of the language of their data. To achieve this goal, we create a multilingual model using a corpus that consists of recordings from a large number of monolingual and a few bilingual speakers in multiple languages. The model is then adapted using the target speaker's recordings in a language other than the target language. We also explore if additional adaptation data from a native speaker of the target language improves the performance. The subjective evaluation shows that the proposed approach of cross-language speaker adaptation is able to synthesize speech in the target language, in the target speaker's voice, without data spoken by the target speaker in that language. Also, extra data from a native speaker of the target language can improve model performance.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053642",
      "openalex_id": "https://openalex.org/W3015484365",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Preech: A System for Privacy-Preserving Speech Transcription",
      "summary": "New Advances in machine learning have made Automated Speech Recognition (ASR) systems practical and more scalable. These systems, however, pose serious privacy threats as speech is a rich source of sensitive acoustic and textual information. Although offline and open-source ASR eliminates the privacy risks, its transcription performance is inferior to that of cloud-based ASR systems, especially for real-world use cases. In this paper, we propose Pr$\\epsilon\\epsilon$ch, an end-to-end speech transcription system which lies at an intermediate point in the privacy-utility spectrum. It protects the acoustic features of the speakers' voices and protects the privacy of the textual content at an improved performance relative to offline ASR. Additionally, Pr$\\epsilon\\epsilon$ch provides several control knobs to allow customizable utility-usability-privacy trade-off. It relies on cloud-based services to transcribe a speech file after applying a series of privacy-preserving operations on the user's side. We perform a comprehensive evaluation of Pr$\\epsilon\\epsilon$ch, using diverse real-world datasets, that demonstrates its effectiveness. Pr$\\epsilon\\epsilon$ch provides transcriptions at a 2% to 32.25% (mean 17.34%) relative improvement in word error rate over Deep Speech, while fully obfuscating the speakers' voice biometrics and allowing only a differentially private view of the textual content.",
      "abstract": "New Advances in machine learning have made Automated Speech Recognition (ASR) systems practical and more scalable. These systems, however, pose serious privacy threats as speech is a rich source of sensitive acoustic and textual information. Although offline and open-source ASR eliminates the privacy risks, its transcription performance is inferior to that of cloud-based ASR systems, especially for real-world use cases. In this paper, we propose Pr$\\epsilon\\epsilon$ch, an end-to-end speech transcription system which lies at an intermediate point in the privacy-utility spectrum. It protects the acoustic features of the speakers' voices and protects the privacy of the textual content at an improved performance relative to offline ASR. Additionally, Pr$\\epsilon\\epsilon$ch provides several control knobs to allow customizable utility-usability-privacy trade-off. It relies on cloud-based services to transcribe a speech file after applying a series of privacy-preserving operations on the user's side. We perform a comprehensive evaluation of Pr$\\epsilon\\epsilon$ch, using diverse real-world datasets, that demonstrates its effectiveness. Pr$\\epsilon\\epsilon$ch provides transcriptions at a 2% to 32.25% (mean 17.34%) relative improvement in word error rate over Deep Speech, while fully obfuscating the speakers' voice biometrics and allowing only a differentially private view of the textual content.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3082522567",
      "arxiv_id": "",
      "publication_date": "2019-09-09",
      "published": "2019-09-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "a novel cross-lingual voice cloning approach with a few text-free samples",
      "summary": "In this paper, we present a cross-lingual voice cloning approach. BN features obtained by SI-ASR model are used as a bridge across speakers and language boundaries. The relationships between text and BN features are modeled by the latent prosody model. The acoustic model learns the translation from BN features to acoustic features. The acoustic model is fine-tuned with a few samples of the target speaker to realize voice cloning. This system can generate speech of arbitrary utterance of target language in cross-lingual speakers' voice. We verify that with small amount of audio data, our proposed approach can well handle cross-lingual tasks. And in intra-lingual tasks, our proposed approach also performs better than baseline approach in naturalness and similarity.",
      "abstract": "In this paper, we present a cross-lingual voice cloning approach. BN features obtained by SI-ASR model are used as a bridge across speakers and language boundaries. The relationships between text and BN features are modeled by the latent prosody model. The acoustic model learns the translation from BN features to acoustic features. The acoustic model is fine-tuned with a few samples of the target speaker to realize voice cloning. This system can generate speech of arbitrary utterance of target language in cross-lingual speakers' voice. We verify that with small amount of audio data, our proposed approach can well handle cross-lingual tasks. And in intra-lingual tasks, our proposed approach also performs better than baseline approach in naturalness and similarity.",
      "doi": "https://doi.org/10.48550/arxiv.1910.13276",
      "openalex_id": "https://openalex.org/W2982290890",
      "arxiv_id": "",
      "publication_date": "2019-10-29",
      "published": "2019-10-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ICRA Noises: Artificial Noise Signals with Speech-like Spectral and Temporal Properties for Hearing Instrument Assessment: Ruidos ICRA: Señates de ruido artificial con espectro similar al habla y propiedades temporales para pruebas de instrumentos auditivos",
      "summary": "Current standards involving technical specification of hearing aids provide limited possibilities for assessing the influence of the spectral and temporal characteristics of the input signal, and these characteristics have a significant effect on the output signal of many recent types of hearing aids. This is particularly true of digital hearing instruments, which typically include non-linear amplification in multiple channels. Furthermore, these instruments often incorporate additional non-linear functions such as “noise reduction” and “feedback cancellation”. The output signal produced by a non-linear hearing instrument relates to the characteristics of the input signal in a complex manner. Therefore, the choice of input signal significantly influences the outcome of any acoustic or psychophysical assessment of a non-linear hearing instrument. For this reason, the International Collegium for Rehabilitative Audiology (ICRA) has introduced a collection of noise signals that can be used for hearing aid testing (including real-ear measurements) and psychophysical evaluation. This paper describes the design criteria, the realisation process, and the final selection of nine test signals on a CD. Also, the spectral and temporal characteristics of these signals are documented. The ICRA noises provide a well-specified set of speech-like noises with spectra shaped according to gender and vocal effort, and with different amounts of speech modulation simulating one or more speakers. These noises can be applied as well-specified background noise in psychophysical experiments. They can also serve as test signals for the evaluation of digital hearing aids with noise reduction. It is demonstrated that the ICRA noises show the effectiveness of the noise reduction schemes. Based on these initial measurements, some initial steps are proposed to develop a standard method of technical specification of noise reduction based on the modulation characteristics. For this purpose, the sensitivity of different noise reduction schemes is compared by measurements with ICRA noises with a varying ratio between unmodulated and modulated test signals: a modulated-unmodulated ratio. It can be anticipated that this information is important to understand the differences between the different implementations of noise reduction schemes in different hearing aid models and makes.Los estándares actuales de las especificaciones técnicas de los auxiliares auditivos proporcionan posibilidades limitadas para conocer la influencia de las characterísticas temporales y espectrales de la señal de entrada y estas caracteristicas tienen un efecto significativo en la señal de salida de muchos tipos actuales de auxiliares auditivos. Esto es particularmente cierto en el caso de los instrumentos digitales, que típicamente presentan una amplification no lineal en canales multiples. Incluso, estos instrumentos a menudo incorporan funciones no lineales adicionales como la “reduction de ruido” y la “cancelatión de la retroalimentación”. La señal de salida producida por un instrumento no lineal se relaciona en una forma muy compleja con la señal de entrada, por lo que la selectión de la señal de entrada influye significativamente en la salida de cualquier prueba acustica o psicoaciistica de un instrumento auditivo. Por esta razón el Colegio International de Audiología Rehabilitatoria (ICRA) ha introducido una serie de señales de ruido que pueden ser utilizadas en las pruebas de auxiliares auditivos (incluyendo mediciones in-situ) y evaluaciones psicoacusticas. Este trabajo describe los criterios de diseño, el proceso de realizatión y la selectión final de 9 señales de prueba en un CD. También se describen las characterísticas espectrales y temporales de estas señales. Los ruidos ICRA son un conjunto muy específico de sonidos vocales con forma espectral acorde al género y el esfuerzo vocal y con diversas dosis de modulation vocal que simulan uno o más hablantes. Estos ruidos pueden ser utilizados como ruidos de fondo muy especificos en experimentos psicoacsticos. Tambien pueden servir como señales de prueba para evaluar auxiliares auditivos digitales con reductión de ruido. Se demuestra que los ruidos ICRA tienen la efectividad de los esquemas de reductión de ruido. Con base en estas mediciones iniciales, se proponen algunos pasos iniciales para desarrollar métodos estándar de las especificaciones técnicas de reductión de ruido tomando en cuenta las characterísticas de modulation. Para ello se compara la sensibilidad de diferentes esquemas de reductión de ruido mediante las mediciones de ruidos ICRA con un rango de variation entre senales de prueba modulares y no modulares. Se puede anticipar que esta informatión es importante para comprender las diferencias al implementar diferentes esquemas de reductión de ruido en diferentes modelos y tipos de auxiliares auditivos.",
      "abstract": "Current standards involving technical specification of hearing aids provide limited possibilities for assessing the influence of the spectral and temporal characteristics of the input signal, and these characteristics have a significant effect on the output signal of many recent types of hearing aids. This is particularly true of digital hearing instruments, which typically include non-linear amplification in multiple channels. Furthermore, these instruments often incorporate additional non-linear functions such as “noise reduction” and “feedback cancellation”. The output signal produced by a non-linear hearing instrument relates to the characteristics of the input signal in a complex manner. Therefore, the choice of input signal significantly influences the outcome of any acoustic or psychophysical assessment of a non-linear hearing instrument. For this reason, the International Collegium for Rehabilitative Audiology (ICRA) has introduced a collection of noise signals that can be used for hearing aid testing (including real-ear measurements) and psychophysical evaluation. This paper describes the design criteria, the realisation process, and the final selection of nine test signals on a CD. Also, the spectral and temporal characteristics of these signals are documented. The ICRA noises provide a well-specified set of speech-like noises with spectra shaped according to gender and vocal effort, and with different amounts of speech modulation simulating one or more speakers. These noises can be applied as well-specified background noise in psychophysical experiments. They can also serve as test signals for the evaluation of digital hearing aids with noise reduction. It is demonstrated that the ICRA noises show the effectiveness of the noise reduction schemes. Based on these initial measurements, some initial steps are proposed to develop a standard method of technical specification of noise reduction based on the modulation characteristics. For this purpose, the sensitivity of different noise reduction schemes is compared by measurements with ICRA noises with a varying ratio between unmodulated and modulated test signals: a modulated-unmodulated ratio. It can be anticipated that this information is important to understand the differences between the different implementations of noise reduction schemes in different hearing aid models and makes.Los estándares actuales de las especificaciones técnicas de los auxiliares auditivos proporcionan posibilidades limitadas para conocer la influencia de las characterísticas temporales y espectrales de la señal de entrada y estas caracteristicas tienen un efecto significativo en la señal de salida de muchos tipos actuales de auxiliares auditivos. Esto es particularmente cierto en el caso de los instrumentos digitales, que típicamente presentan una amplification no lineal en canales multiples. Incluso, estos instrumentos a menudo incorporan funciones no lineales adicionales como la “reduction de ruido” y la “cancelatión de la retroalimentación”. La señal de salida producida por un instrumento no lineal se relaciona en una forma muy compleja con la señal de entrada, por lo que la selectión de la señal de entrada influye significativamente en la salida de cualquier prueba acustica o psicoaciistica de un instrumento auditivo. Por esta razón el Colegio International de Audiología Rehabilitatoria (ICRA) ha introducido una serie de señales de ruido que pueden ser utilizadas en las pruebas de auxiliares auditivos (incluyendo mediciones in-situ) y evaluaciones psicoacusticas. Este trabajo describe los criterios de diseño, el proceso de realizatión y la selectión final de 9 señales de prueba en un CD. También se describen las characterísticas espectrales y temporales de estas señales. Los ruidos ICRA son un conjunto muy específico de sonidos vocales con forma espectral acorde al género y el esfuerzo vocal y con diversas dosis de modulation vocal que simulan uno o más hablantes. Estos ruidos pueden ser utilizados como ruidos de fondo muy especificos en experimentos psicoacsticos. Tambien pueden servir como señales de prueba para evaluar auxiliares auditivos digitales con reductión de ruido. Se demuestra que los ruidos ICRA tienen la efectividad de los esquemas de reductión de ruido. Con base en estas mediciones iniciales, se proponen algunos pasos iniciales para desarrollar métodos estándar de las especificaciones técnicas de reductión de ruido tomando en cuenta las characterísticas de modulation. Para ello se compara la sensibilidad de diferentes esquemas de reductión de ruido mediante las mediciones de ruidos ICRA con un rango de variation entre senales de prueba modulares y no modulares. Se puede anticipar que esta informatión es importante para comprender las diferencias al implementar diferentes esquemas de reductión de ruido en diferentes modelos y tipos de auxiliares auditivos.",
      "doi": "https://doi.org/10.3109/00206090109073110",
      "openalex_id": "https://openalex.org/W2199505332",
      "arxiv_id": "",
      "publication_date": "2001-01-01",
      "published": "2001-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The SIWIS Database: A Multilingual Speech Database with Acted Emphasis",
      "summary": "We describe here a collection of speech data of bilingual and trilingual speakers of English, French, German and Italian. In the context of speech to speech translation (S2ST), this database is designed for several purposes and studies: training CLSA systems (cross-language speaker adaptation), conveying emphasis through S2ST systems, and evaluating TTS systems. More precisely, 36 speakers judged as accentless (22 bilingual and 14 trilingual speakers) were recorded for a set of 171 prompts in two or three languages, amounting to a total of 24 hours of speech. These sets of prompts include 100 sentences from news, 25 sentences from Europarl, the same 25 sentences with one acted emphasised word, 20 semantically unpredictable sentences, and finally a 240-word long text. All in all, it yielded 64 bilingual session pairs of the six possible combinations of the four languages. The database is freely available for non-commercial use and scientific research purposes.",
      "abstract": "We describe here a collection of speech data of bilingual and trilingual speakers of English, French, German and Italian. In the context of speech to speech translation (S2ST), this database is designed for several purposes and studies: training CLSA systems (cross-language speaker adaptation), conveying emphasis through S2ST systems, and evaluating TTS systems. More precisely, 36 speakers judged as accentless (22 bilingual and 14 trilingual speakers) were recorded for a set of 171 prompts in two or three languages, amounting to a total of 24 hours of speech. These sets of prompts include 100 sentences from news, 25 sentences from Europarl, the same 25 sentences with one acted emphasised word, 20 semantically unpredictable sentences, and finally a 240-word long text. All in all, it yielded 64 bilingual session pairs of the six possible combinations of the four languages. The database is freely available for non-commercial use and scientific research purposes.",
      "doi": "https://doi.org/10.21437/interspeech.2016-1003",
      "openalex_id": "https://openalex.org/W2470254032",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Privacy-preserving sound to degrade automatic speaker verification performance",
      "summary": "In this paper, a privacy protection method to prevent speaker identification from recorded speech is proposed and evaluated. Although many techniques for preserving various private information included in speech have been proposed, their impacts on human speech communication in physical space are not taken into account. To overcome this problem, this paper proposes privacy-preserving sound as a privacy protection method. The privacy-preserving sound can degrade speaker verification performance without interfering with human speech communication in physical space. To make a first step toward solving this problem, suitable sound characteristics for preserving privacy are evaluated in terms of the speaker verification performance and speech intelligibility. The experimental results show that appropriate sound can efficiently degrade the speaker verification performance without degrading speech intelligibility.",
      "abstract": "In this paper, a privacy protection method to prevent speaker identification from recorded speech is proposed and evaluated. Although many techniques for preserving various private information included in speech have been proposed, their impacts on human speech communication in physical space are not taken into account. To overcome this problem, this paper proposes privacy-preserving sound as a privacy protection method. The privacy-preserving sound can degrade speaker verification performance without interfering with human speech communication in physical space. To make a first step toward solving this problem, suitable sound characteristics for preserving privacy are evaluated in terms of the speaker verification performance and speech intelligibility. The experimental results show that appropriate sound can efficiently degrade the speaker verification performance without degrading speech intelligibility.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472729",
      "openalex_id": "https://openalex.org/W2407075323",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
      "summary": "When the available data of a target speaker is insufficient to train a high quality speaker-dependent neural text-to-speech (TTS) system, we can combine data from multiple speakers and train a multi-speaker TTS model instead. Many studies have shown that neural multi-speaker TTS model trained with a small amount data from multiple speakers combined can generate synthetic speech with better quality and stability than a speaker-dependent one. However when the amount of data from each speaker is highly unbalanced, the best approach to make use of the excessive data remains unknown. Our experiments showed that simply combining all available data from every speaker to train a multi-speaker model produces better than or at least similar performance to its speaker-dependent counterpart. Moreover by using an ensemble multi-speaker model, in which each subsystem is trained on a subset of available data, we can further improve the quality of the synthetic speech especially for underrepresented speakers whose training data is limited.<br/><br/>",
      "abstract": "When the available data of a target speaker is insufficient to train a high quality speaker-dependent neural text-to-speech (TTS) system, we can combine data from multiple speakers and train a multi-speaker TTS model instead. Many studies have shown that neural multi-speaker TTS model trained with a small amount data from multiple speakers combined can generate synthetic speech with better quality and stability than a speaker-dependent one. However when the amount of data from each speaker is highly unbalanced, the best approach to make use of the excessive data remains unknown. Our experiments showed that simply combining all available data from every speaker to train a multi-speaker model produces better than or at least similar performance to its speaker-dependent counterpart. Moreover by using an ensemble multi-speaker model, in which each subsystem is trained on a subset of available data, we can further improve the quality of the synthetic speech especially for underrepresented speakers whose training data is limited.<br/><br/>",
      "doi": "https://doi.org/10.21437/interspeech.2019-1311",
      "openalex_id": "https://openalex.org/W2972595148",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Anonymization Using X-vector and Neural Waveform Models",
      "summary": "The social media revolution has produced a plethora of web services to which users can easily upload and share multimedia documents.Despite the popularity and convenience of such services, the sharing of such inherently personal data, including speech data, raises obvious security and privacy concerns.In particular, a user's speech data may be acquired and used with speech synthesis systems to produce high-quality speech utterances which reflect the same user's speaker identity.These utterances may then be used to attack speaker verification systems.One solution to mitigate these concerns involves the concealing of speaker identities before the sharing of speech data.For this purpose, we present a new approach to speaker anonymization.The idea is to extract linguistic and speaker identity features from an utterance and then to use these with neural acoustic and waveform models to synthesize anonymized speech.The original speaker identity, in the form of timbre, is suppressed and replaced with that of an anonymous pseudo identity.The approach exploits state-of-the-art x-vector speaker representations.These are used to derive anonymized pseudo speaker identities through the combination of multiple, random speaker x-vectors.Experimental results show that the proposed approach is effective in concealing speaker identities.It increases the equal error rate of a speaker verification system while maintaining high quality, anonymized speech.",
      "abstract": "The social media revolution has produced a plethora of web services to which users can easily upload and share multimedia documents.Despite the popularity and convenience of such services, the sharing of such inherently personal data, including speech data, raises obvious security and privacy concerns.In particular, a user's speech data may be acquired and used with speech synthesis systems to produce high-quality speech utterances which reflect the same user's speaker identity.These utterances may then be used to attack speaker verification systems.One solution to mitigate these concerns involves the concealing of speaker identities before the sharing of speech data.For this purpose, we present a new approach to speaker anonymization.The idea is to extract linguistic and speaker identity features from an utterance and then to use these with neural acoustic and waveform models to synthesize anonymized speech.The original speaker identity, in the form of timbre, is suppressed and replaced with that of an anonymous pseudo identity.The approach exploits state-of-the-art x-vector speaker representations.These are used to derive anonymized pseudo speaker identities through the combination of multiple, random speaker x-vectors.Experimental results show that the proposed approach is effective in concealing speaker identities.It increases the equal error rate of a speaker verification system while maintaining high quality, anonymized speech.",
      "doi": "https://doi.org/10.21437/ssw.2019-28",
      "openalex_id": "https://openalex.org/W2972848589",
      "arxiv_id": "",
      "publication_date": "2019-09-14",
      "published": "2019-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal Speech Synthesis Architecture for Unsupervised Speaker Adaptation",
      "summary": "This paper proposes a new architecture for speaker adaptation of multi-speaker neural-network speech synthesis systems, in which an unseen speaker’s voice can be built using a relatively small amount of speech data without transcriptions. This is sometimes called “unsupervised speaker adaptation”. More specifically, we concatenate the layers to the audio inputs when performing unsupervised speaker adaptation while we concatenate them to the text inputs when synthesizing speech from text. Two new training schemes for the new architecture are also proposed in this paper. These training schemes are not limited to speech synthesis; other applications are suggested. Experimental results show that the proposed model not only enables adaptation to unseen speakers using untranscribed speech but it also improves the performance of multi-speaker modeling and speaker adaptation using transcribed audio files.",
      "abstract": "This paper proposes a new architecture for speaker adaptation of multi-speaker neural-network speech synthesis systems, in which an unseen speaker’s voice can be built using a relatively small amount of speech data without transcriptions. This is sometimes called “unsupervised speaker adaptation”. More specifically, we concatenate the layers to the audio inputs when performing unsupervised speaker adaptation while we concatenate them to the text inputs when synthesizing speech from text. Two new training schemes for the new architecture are also proposed in this paper. These training schemes are not limited to speech synthesis; other applications are suggested. Experimental results show that the proposed model not only enables adaptation to unseen speakers using untranscribed speech but it also improves the performance of multi-speaker modeling and speaker adaptation using transcribed audio files.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1791",
      "openalex_id": "https://openalex.org/W2963912679",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-Supervised Learning with Deep Generative Models",
      "summary": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",
      "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",
      "doi": "https://doi.org/10.48550/arxiv.1406.5298",
      "openalex_id": "https://openalex.org/W2108501770",
      "arxiv_id": "",
      "publication_date": "2014-06-20",
      "published": "2014-06-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semi-supervised End-to-end Speech Recognition Using Text-to-speech and Autoencoders",
      "summary": "We introduce speech and text autoencoders that share encoders and decoders with an automatic speech recognition (ASR) model to improve ASR performance with large speech only and text only training datasets. To build the speech and text autoencoders, we leverage state-of-the-art ASR and text-to-speech (TTS) encoder decoder architectures. These autoencoders learn features from speech only and text only datasets by switching the encoders and decoders used in the ASR and TTS models. Simultaneously, they aim to encode features to be compatible with ASR and TTS models by a multi-task loss. Additionally, we anticipate that TTS joint training can also improve the ASR performance because both ASR and TTS models learn transformations between speech and text. The experimental result we obtained with our semi-supervised end-to-end ASR/TTS training revealed reductions from a model initially trained with a small paired subset of the LibriSpeech corpus in the character error rate from 10.4% to 8.4% and word error rate from 20.6% to 18.0% by retraining the model with a large unpaired subset of the corpus.",
      "abstract": "We introduce speech and text autoencoders that share encoders and decoders with an automatic speech recognition (ASR) model to improve ASR performance with large speech only and text only training datasets. To build the speech and text autoencoders, we leverage state-of-the-art ASR and text-to-speech (TTS) encoder decoder architectures. These autoencoders learn features from speech only and text only datasets by switching the encoders and decoders used in the ASR and TTS models. Simultaneously, they aim to encode features to be compatible with ASR and TTS models by a multi-task loss. Additionally, we anticipate that TTS joint training can also improve the ASR performance because both ASR and TTS models learn transformations between speech and text. The experimental result we obtained with our semi-supervised end-to-end ASR/TTS training revealed reductions from a model initially trained with a small paired subset of the LibriSpeech corpus in the character error rate from 10.4% to 8.4% and word error rate from 20.6% to 18.0% by retraining the model with a large unpaired subset of the corpus.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682890",
      "openalex_id": "https://openalex.org/W2938947737",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ACVAE-VC: Non-Parallel Voice Conversion With Auxiliary Classifier Variational Autoencoder",
      "summary": "This paper proposes a non-parallel voice conversion (VC) method using a variant of the conditional variational autoencoder (VAE) called an auxiliary classifier VAE. The proposed method has two key features. First, it adopts fully convolutional architectures to construct the encoder and decoder networks so that the networks can learn conversion rules that capture the time dependencies in the acoustic feature sequences of source and target speech. Second, it uses information-theoretic regularization for the model training to ensure that the information in the attribute class label will not be lost in the conversion process. With regular conditional VAEs, the encoder and decoder are free to ignore the attribute class label input. This can be problematic since in such a situation, the attribute class label will have little effect on controlling the voice characteristics of input speech at test time. Such situations can be avoided by introducing an auxiliary classifier and training the encoder and decoder so that the attribute classes of the decoder outputs are correctly predicted by the classifier. We also present several ways to convert the feature sequence of input speech using the trained encoder and decoder and compare them in terms of audio quality through objective and subjective evaluations. We confirmed experimentally that the proposed method outperformed baseline non-parallel VC systems and performed comparably to an open-source parallel VC system trained using a parallel corpus in a speaker identity conversion task.",
      "abstract": "This paper proposes a non-parallel voice conversion (VC) method using a variant of the conditional variational autoencoder (VAE) called an auxiliary classifier VAE. The proposed method has two key features. First, it adopts fully convolutional architectures to construct the encoder and decoder networks so that the networks can learn conversion rules that capture the time dependencies in the acoustic feature sequences of source and target speech. Second, it uses information-theoretic regularization for the model training to ensure that the information in the attribute class label will not be lost in the conversion process. With regular conditional VAEs, the encoder and decoder are free to ignore the attribute class label input. This can be problematic since in such a situation, the attribute class label will have little effect on controlling the voice characteristics of input speech at test time. Such situations can be avoided by introducing an auxiliary classifier and training the encoder and decoder so that the attribute classes of the decoder outputs are correctly predicted by the classifier. We also present several ways to convert the feature sequence of input speech using the trained encoder and decoder and compare them in terms of audio quality through objective and subjective evaluations. We confirmed experimentally that the proposed method outperformed baseline non-parallel VC systems and performed comparably to an open-source parallel VC system trained using a parallel corpus in a speaker identity conversion task.",
      "doi": "https://doi.org/10.1109/taslp.2019.2917232",
      "openalex_id": "https://openalex.org/W2946555236",
      "arxiv_id": "",
      "publication_date": "2019-05-25",
      "published": "2019-05-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adapting and controlling DNN-based speech synthesis using input codes",
      "summary": "Methods for adapting and controlling the characteristics of output speech are important topics in speech synthesis. In this work, we investigated the performance of DNN-based text-to-speech systems that in parallel to conventional text input also take speaker, gender, and age codes as inputs, in order to 1) perform multi-speaker synthesis, 2) perform speaker adaptation using small amounts of targetspeaker adaptation data, and 3) modify synthetic speech characteristics based on the input codes. Using a large-scale, studio-quality speech corpus with 135 speakers of both genders and ages between<br/>tens and eighties, we performed three experiments: 1) First, we used a subset of speakers to construct a DNN-based, multi-speaker acoustic model with speaker codes. 2) Next, we performed speaker adaptation by estimating code vectors for new speakers via backpropagation from a small amount of adaptation material. 3) Finally, we experimented with manually manipulating input code vectors to alter the gender and/or age characteristics of the synthesised speech. Experimental results show that high-performance multi-speaker models<br/>can be constructed using the proposed code vectors with a variety of encoding schemes, and that adaptation and manipulation can be performed effectively using the codes.",
      "abstract": "Methods for adapting and controlling the characteristics of output speech are important topics in speech synthesis. In this work, we investigated the performance of DNN-based text-to-speech systems that in parallel to conventional text input also take speaker, gender, and age codes as inputs, in order to 1) perform multi-speaker synthesis, 2) perform speaker adaptation using small amounts of targetspeaker adaptation data, and 3) modify synthetic speech characteristics based on the input codes. Using a large-scale, studio-quality speech corpus with 135 speakers of both genders and ages between<br/>tens and eighties, we performed three experiments: 1) First, we used a subset of speakers to construct a DNN-based, multi-speaker acoustic model with speaker codes. 2) Next, we performed speaker adaptation by estimating code vectors for new speakers via backpropagation from a small amount of adaptation material. 3) Finally, we experimented with manually manipulating input code vectors to alter the gender and/or age characteristics of the synthesised speech. Experimental results show that high-performance multi-speaker models<br/>can be constructed using the proposed code vectors with a variety of encoding schemes, and that adaptation and manipulation can be performed effectively using the codes.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953089",
      "openalex_id": "https://openalex.org/W2759925408",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spontaneous Speech Corpus of Japanese",
      "summary": "Plant secondary metabolites that are released into the rhizosphere alter biotic and abiotic soil properties, which in turn affect the performance of other plants. How this type of plant-soil feedback affects agricultural productivity and food quality in the field in the context of crop rotations is unknown. Here, we assessed the performance, yield and food quality of three winter wheat varieties growing in field plots whose soils had been conditioned by either wild type or benzoxazinoid-deficient <i>bx1</i> maize mutant plants. Following maize cultivation, we detected benzoxazinoid-dependent chemical and microbial fingerprints in the soil. The benzoxazinoid fingerprint was still visible during wheat growth, but the microbial fingerprint was no longer detected. Wheat emergence, tillering, growth, and biomass increased in wild type conditioned soils compared to <i>bx1</i> mutant conditioned soils. Weed cover was similar between soil conditioning treatments, but insect herbivore abundance decreased in benzoxazinoid-conditioned soils. Wheat yield was increased by over 4% without a reduction in grain quality in benzoxazinoid-conditioned soils. This improvement was directly associated with increased germination and tillering. Taken together, our experiments provide evidence that soil conditioning by plant secondary metabolite producing plants can increase yield via plant-soil feedbacks under agronomically realistic conditions. If this phenomenon holds true across different soils and environments, optimizing root exudation chemistry could be a powerful, genetically tractable strategy to enhance crop yields without additional inputs.",
      "abstract": "Plant secondary metabolites that are released into the rhizosphere alter biotic and abiotic soil properties, which in turn affect the performance of other plants. How this type of plant-soil feedback affects agricultural productivity and food quality in the field in the context of crop rotations is unknown. Here, we assessed the performance, yield and food quality of three winter wheat varieties growing in field plots whose soils had been conditioned by either wild type or benzoxazinoid-deficient <i>bx1</i> maize mutant plants. Following maize cultivation, we detected benzoxazinoid-dependent chemical and microbial fingerprints in the soil. The benzoxazinoid fingerprint was still visible during wheat growth, but the microbial fingerprint was no longer detected. Wheat emergence, tillering, growth, and biomass increased in wild type conditioned soils compared to <i>bx1</i> mutant conditioned soils. Weed cover was similar between soil conditioning treatments, but insect herbivore abundance decreased in benzoxazinoid-conditioned soils. Wheat yield was increased by over 4% without a reduction in grain quality in benzoxazinoid-conditioned soils. This improvement was directly associated with increased germination and tillering. Taken together, our experiments provide evidence that soil conditioning by plant secondary metabolite producing plants can increase yield via plant-soil feedbacks under agronomically realistic conditions. If this phenomenon holds true across different soils and environments, optimizing root exudation chemistry could be a powerful, genetically tractable strategy to enhance crop yields without additional inputs.",
      "doi": "https://doi.org/10.7554/elife.84988",
      "openalex_id": "https://openalex.org/W37526647",
      "arxiv_id": "",
      "publication_date": "2000-05-01",
      "published": "2000-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Controllable Emphatic Speech Synthesis based on Forward Attention for Expressive Speech Synthesis",
      "summary": "In speech interaction scenarios, speech emphasis is essential for expressing the underlying intention and attitude. Recently, end-to-end emphatic speech synthesis greatly improves the naturalness of synthetic speech, but also brings new problems: 1) lack of interpretability for how emphatic codes affect the model; 2) no separate control of emphasis on duration and on intonation and energy. We propose a novel way to build an interpretable and controllable emphatic speech synthesis framework based on forward attention. Firstly, we explicitly model the local variation of speaking rate for emphasized words and neutral words with modified forward attention to manifest emphasized words in terms of duration. The 2-layers LSTM in decoder is further divided into attention-RNN and decoder-RNN to disentangle the influence of emphasis on duration and on intonation and energy. The emphasis information is injected into decoder-RNN for highlighting emphasized words in the aspects of intonation and energy. Experimental results have shown that our model can not only provide separate control of emphasis on duration and on intonation and energy, but also generate more robust and prominent emphatic speech with high quality and naturalness.",
      "abstract": "In speech interaction scenarios, speech emphasis is essential for expressing the underlying intention and attitude. Recently, end-to-end emphatic speech synthesis greatly improves the naturalness of synthetic speech, but also brings new problems: 1) lack of interpretability for how emphatic codes affect the model; 2) no separate control of emphasis on duration and on intonation and energy. We propose a novel way to build an interpretable and controllable emphatic speech synthesis framework based on forward attention. Firstly, we explicitly model the local variation of speaking rate for emphasized words and neutral words with modified forward attention to manifest emphasized words in terms of duration. The 2-layers LSTM in decoder is further divided into attention-RNN and decoder-RNN to disentangle the influence of emphasis on duration and on intonation and energy. The emphasis information is injected into decoder-RNN for highlighting emphasized words in the aspects of intonation and energy. Experimental results have shown that our model can not only provide separate control of emphasis on duration and on intonation and energy, but also generate more robust and prominent emphatic speech with high quality and naturalness.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383537",
      "openalex_id": "https://openalex.org/W3144044466",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NAUTILUS: A Versatile Voice Cloning System",
      "summary": "We introduce a novel speech synthesis system, called NAUTILUS, that can generate speech with a target voice either from a text input or a reference utterance of an arbitrary source speaker. By using a multi-speaker speech corpus to train all requisite encoders and decoders in the initial training stage, our system can clone unseen voices using untranscribed speech of target speakers on the basis of the backpropagation algorithm. Moreover, depending on the data circumstance of the target speaker, the cloning strategy can be adjusted to take advantage of additional data and modify the behaviors of text-to-speech (TTS) and/or voice conversion (VC) systems to accommodate the situation. We test the performance of the proposed framework by using deep convolution layers to model the encoders, decoders and WaveNet vocoder. Evaluations show that it achieves comparable quality with state-of-the-art TTS and VC systems when cloning with just five minutes of untranscribed speech. Moreover, it is demonstrated that the proposed framework has the ability to switch between TTS and VC with high speaker consistency, which will be useful for many applications.",
      "abstract": "We introduce a novel speech synthesis system, called NAUTILUS, that can generate speech with a target voice either from a text input or a reference utterance of an arbitrary source speaker. By using a multi-speaker speech corpus to train all requisite encoders and decoders in the initial training stage, our system can clone unseen voices using untranscribed speech of target speakers on the basis of the backpropagation algorithm. Moreover, depending on the data circumstance of the target speaker, the cloning strategy can be adjusted to take advantage of additional data and modify the behaviors of text-to-speech (TTS) and/or voice conversion (VC) systems to accommodate the situation. We test the performance of the proposed framework by using deep convolution layers to model the encoders, decoders and WaveNet vocoder. Evaluations show that it achieves comparable quality with state-of-the-art TTS and VC systems when cloning with just five minutes of untranscribed speech. Moreover, it is demonstrated that the proposed framework has the ability to switch between TTS and VC with high speaker consistency, which will be useful for many applications.",
      "doi": "https://doi.org/10.1109/taslp.2020.3034994",
      "openalex_id": "https://openalex.org/W3095990227",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Latent linguistic embedding for cross-lingual text-to-speech and voice conversion",
      "summary": "As the recently proposed voice cloning system, NAUTILUS, is capable of cloning unseen voices using untranscribed speech, we investigate the feasibility of using it to develop a unified cross-lingual TTS/VC system.Cross-lingual speech generation is the scenario in which speech utterances are generated with the voices of target speakers in a language not spoken by them originally.This type of system is not simply cloning the voice of the target speaker, but essentially creating a new voice that can be considered better than the original under a specific framing.By using a well-trained English latent linguistic embedding to create a cross-lingual TTS and VC system for several German, Finnish, and Mandarin speakers included in the Voice Conversion Challenge 2020, we show that our method not only creates cross-lingual VC with high speaker similarity but also can be seamlessly used for cross-lingual TTS without having to perform any extra steps.However, the subjective evaluations of perceived naturalness seemed to vary between target speakers, which is one aspect for future improvement.",
      "abstract": "As the recently proposed voice cloning system, NAUTILUS, is capable of cloning unseen voices using untranscribed speech, we investigate the feasibility of using it to develop a unified cross-lingual TTS/VC system.Cross-lingual speech generation is the scenario in which speech utterances are generated with the voices of target speakers in a language not spoken by them originally.This type of system is not simply cloning the voice of the target speaker, but essentially creating a new voice that can be considered better than the original under a specific framing.By using a well-trained English latent linguistic embedding to create a cross-lingual TTS and VC system for several German, Finnish, and Mandarin speakers included in the Voice Conversion Challenge 2020, we show that our method not only creates cross-lingual VC with high speaker similarity but also can be seamlessly used for cross-lingual TTS without having to perform any extra steps.However, the subjective evaluations of perceived naturalness seemed to vary between target speakers, which is one aspect for future improvement.",
      "doi": "https://doi.org/10.21437/vccbc.2020-22",
      "openalex_id": "https://openalex.org/W3092496982",
      "arxiv_id": "",
      "publication_date": "2020-10-30",
      "published": "2020-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An End-to-End Conversational Style Matching Agent",
      "summary": "We present an end-to-end voice-based conversational agent that is able to\\nengage in naturalistic multi-turn dialogue and align with the interlocutor's\\nconversational style. The system uses a series of deep neural network\\ncomponents for speech recognition, dialogue generation, prosodic analysis and\\nspeech synthesis to generate language and prosodic expression with qualities\\nthat match those of the user. We conducted a user study (N=30) in which\\nparticipants talked with the agent for 15 to 20 minutes, resulting in over 8\\nhours of natural interaction data. Users with high consideration conversational\\nstyles reported the agent to be more trustworthy when it matched their\\nconversational style. Whereas, users with high involvement conversational\\nstyles were indifferent. Finally, we provide design guidelines for multi-turn\\ndialogue interactions using conversational style adaptation.\\n",
      "abstract": "We present an end-to-end voice-based conversational agent that is able to\\nengage in naturalistic multi-turn dialogue and align with the interlocutor's\\nconversational style. The system uses a series of deep neural network\\ncomponents for speech recognition, dialogue generation, prosodic analysis and\\nspeech synthesis to generate language and prosodic expression with qualities\\nthat match those of the user. We conducted a user study (N=30) in which\\nparticipants talked with the agent for 15 to 20 minutes, resulting in over 8\\nhours of natural interaction data. Users with high consideration conversational\\nstyles reported the agent to be more trustworthy when it matched their\\nconversational style. Whereas, users with high involvement conversational\\nstyles were indifferent. Finally, we provide design guidelines for multi-turn\\ndialogue interactions using conversational style adaptation.\\n",
      "doi": "https://doi.org/10.1145/3308532.3329473",
      "openalex_id": "https://openalex.org/W2933971837",
      "arxiv_id": "",
      "publication_date": "2019-07-01",
      "published": "2019-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable",
      "summary": "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",
      "abstract": "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.9",
      "openalex_id": "https://openalex.org/W3035451444",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spoken Conversational AI in Video Games",
      "summary": "In a traditional role-playing game (RPG) conversing with a Non-Playable Character (NPC) typically appears somewhat unrealistic and can break immersion and user engagement. In commercial games, the player usually selects one of several possible predefined conversation options which are displayed as text or labels on the screen, to progress the conversation. In contrast, we first present a spoken conversational interface, built using a state-of-the-art open-domain social conversational AI developed for the Amazon Alexa Challenge, which was modified for use in a video game. This system is designed to keep users engaged in the conversation -- which we measure by time taken speaking with the character. In particular, we use emotion detection and emotional dialogue management to enhance the conversational experience. We then evaluate the contribution of emotion detection and conversational responses in a spoken dialogue system for a role-playing video game. In order to do this, two prototypes of the same game were created: one system using sentiment analysis and emotional modelling and the other system that does not detect or react to emotions. Both systems use a spoken conversational AI system where the user can freely talk to a Non-Playable-Character using unconstrained speech input.",
      "abstract": "In a traditional role-playing game (RPG) conversing with a Non-Playable Character (NPC) typically appears somewhat unrealistic and can break immersion and user engagement. In commercial games, the player usually selects one of several possible predefined conversation options which are displayed as text or labels on the screen, to progress the conversation. In contrast, we first present a spoken conversational interface, built using a state-of-the-art open-domain social conversational AI developed for the Amazon Alexa Challenge, which was modified for use in a video game. This system is designed to keep users engaged in the conversation -- which we measure by time taken speaking with the character. In particular, we use emotion detection and emotional dialogue management to enhance the conversational experience. We then evaluate the contribution of emotion detection and conversational responses in a spoken dialogue system for a role-playing video game. In order to do this, two prototypes of the same game were created: one system using sentiment analysis and emotional modelling and the other system that does not detect or react to emotions. Both systems use a spoken conversational AI system where the user can freely talk to a Non-Playable-Character using unconstrained speech input.",
      "doi": "https://doi.org/10.1145/3267851.3267896",
      "openalex_id": "https://openalex.org/W2901492641",
      "arxiv_id": "",
      "publication_date": "2018-11-05",
      "published": "2018-11-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LaMDA: Language Models for Dialog Applications",
      "summary": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
      "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
      "doi": "https://doi.org/10.48550/arxiv.2201.08239",
      "openalex_id": "https://openalex.org/W4226399820",
      "arxiv_id": "",
      "publication_date": "2022-01-20",
      "published": "2022-01-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
      "summary": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available.Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response.We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",
      "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available.Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response.We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",
      "doi": "https://doi.org/10.18653/v1/d16-1230",
      "openalex_id": "https://openalex.org/W2963903950",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems",
      "summary": "In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model. Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, they did not analyze how the differences of fine-tuning datasets affect on user's detailed impression. In addition, the Transformer-based approach has only been verified for English, not for such languages with large inter-language distances as Japanese. In this study, we develop large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets to examine the effectiveness of the Transformer-based approach for building chit-chat dialogue systems. We evaluated and analyzed the impressions of human dialogues in different fine-tuning datasets, model parameters, and the use of additional information.",
      "abstract": "In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model. Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, they did not analyze how the differences of fine-tuning datasets affect on user's detailed impression. In addition, the Transformer-based approach has only been verified for English, not for such languages with large inter-language distances as Japanese. In this study, we develop large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets to examine the effectiveness of the Transformer-based approach for building chit-chat dialogue systems. We evaluated and analyzed the impressions of human dialogues in different fine-tuning datasets, model parameters, and the use of additional information.",
      "doi": "https://doi.org/10.48550/arxiv.2109.05217",
      "openalex_id": "https://openalex.org/W3201566090",
      "arxiv_id": "",
      "publication_date": "2021-09-11",
      "published": "2021-09-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emerging Properties in Self-Supervised Vision Transformers",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
      "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
      "doi": "https://doi.org/10.1109/iccv48922.2021.00951",
      "openalex_id": "https://openalex.org/W3159481202",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "summary": "The ultimate goal of transfer learning is to reduce labeled data requirements\\nby exploiting a pre-existing embedding model trained for different datasets or\\ntasks. The visual and language communities have established benchmarks to\\ncompare embeddings, but the speech community has yet to do so. This paper\\nproposes a benchmark for comparing speech representations on non-semantic\\ntasks, and proposes a representation based on an unsupervised triplet-loss\\nobjective. The proposed representation outperforms other representations on the\\nbenchmark, and even exceeds state-of-the-art performance on a number of\\ntransfer learning tasks. The embedding is trained on a publicly available\\ndataset, and it is tested on a variety of low-resource downstream tasks,\\nincluding personalization tasks and medical domain. The benchmark, models, and\\nevaluation code are publicly released.\\n",
      "abstract": "The ultimate goal of transfer learning is to reduce labeled data requirements\\nby exploiting a pre-existing embedding model trained for different datasets or\\ntasks. The visual and language communities have established benchmarks to\\ncompare embeddings, but the speech community has yet to do so. This paper\\nproposes a benchmark for comparing speech representations on non-semantic\\ntasks, and proposes a representation based on an unsupervised triplet-loss\\nobjective. The proposed representation outperforms other representations on the\\nbenchmark, and even exceeds state-of-the-art performance on a number of\\ntransfer learning tasks. The embedding is trained on a publicly available\\ndataset, and it is tested on a variety of low-resource downstream tasks,\\nincluding personalization tasks and medical domain. The benchmark, models, and\\nevaluation code are publicly released.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-1242",
      "openalex_id": "https://openalex.org/W3006926732",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contrastive Learning for Label Efficient Semantic Segmentation",
      "summary": "Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.",
      "abstract": "Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.",
      "doi": "https://doi.org/10.1109/iccv48922.2021.01045",
      "openalex_id": "https://openalex.org/W3113328489",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Learning Contrastive Representations for Learning with Noisy Labels",
      "summary": "Deep neural networks are able to memorize noisy labels easily with a softmax cross entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.",
      "abstract": "Deep neural networks are able to memorize noisy labels easily with a softmax cross entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.01618",
      "openalex_id": "https://openalex.org/W4312766345",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification",
      "summary": "Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated.",
      "abstract": "Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.359",
      "openalex_id": "https://openalex.org/W3200253633",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A dendrite method for cluster analysis",
      "summary": "Abstract A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the \"best number\" of clusters is suggested. It is a\"variance ratio criterion\" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965). Keywords: numerical taxonomy cluster analysis minimum variance (WGSS) criterion for optimal grouping approximate grouping procedure shortest dendrite = minimum spanning tree variance ratio criterion for best number of groups",
      "abstract": "Abstract A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the \"best number\" of clusters is suggested. It is a\"variance ratio criterion\" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965). Keywords: numerical taxonomy cluster analysis minimum variance (WGSS) criterion for optimal grouping approximate grouping procedure shortest dendrite = minimum spanning tree variance ratio criterion for best number of groups",
      "doi": "https://doi.org/10.1080/03610927408827101",
      "openalex_id": "https://openalex.org/W2085487226",
      "arxiv_id": "",
      "publication_date": "1974-01-01",
      "published": "1974-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "summary": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.",
      "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.",
      "doi": "https://doi.org/10.21437/interspeech.2017-950",
      "openalex_id": "https://openalex.org/W2726515241",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emergent Abilities of Large Language Models",
      "summary": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
      "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
      "doi": "https://doi.org/10.48550/arxiv.2206.07682",
      "openalex_id": "https://openalex.org/W4283026156",
      "arxiv_id": "",
      "publication_date": "2022-06-15",
      "published": "2022-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning",
      "summary": "Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.",
      "abstract": "Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.",
      "doi": "https://doi.org/10.48550/arxiv.1606.04586",
      "openalex_id": "https://openalex.org/W2431080869",
      "arxiv_id": "",
      "publication_date": "2016-06-14",
      "published": "2016-06-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning with Pseudo-Ensembles",
      "summary": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.",
      "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.",
      "doi": "https://doi.org/10.48550/arxiv.1412.4864",
      "openalex_id": "https://openalex.org/W2129068307",
      "arxiv_id": "",
      "publication_date": "2014-12-16",
      "published": "2014-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification",
      "summary": "This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker verification. Usually, speaker embeddings are extracted from a speaker-classification DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker's frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker's input speech. Finally, a PLDA classifier is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We find that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances. © 2018 International Speech Communication Association. All rights reserved.",
      "abstract": "This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker verification. Usually, speaker embeddings are extracted from a speaker-classification DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker's frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker's input speech. Finally, a PLDA classifier is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We find that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances. © 2018 International Speech Communication Association. All rights reserved.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1158",
      "openalex_id": "https://openalex.org/W2889519245",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Art of Mixing",
      "summary": "David Gibson uses 3D visual representations of sounds in a mix as a tool to explain the dynamics that can be created in a mix. This book provides an in-depth exploration into the aesthetics of what makes a great mix. Gibson's unique approach explains how to map sounds to visuals in order to create a visual framework that can be used to analyze what is going on in any mix. Once you have the framework down, Gibson then uses it to explain the traditions that have be developed over time by great recording engineers for different styles of music and songs. You will come to understand everything that can be done in a mix to create dynamics that affect people in really deep ways. Once you understand what engineers are doing to create the great mixes they do, you can then use this framework to develop your own values as to what you feel is a good mix. Once you have a perspective on what all can be done, you have the power to be truly creative on your own – to create whole new mixing possibilities. It is all about creating art out of technology. This book goes beyond explaining what the equipment does – it explains what to do with the equipment to make the best possible mixes.",
      "abstract": "David Gibson uses 3D visual representations of sounds in a mix as a tool to explain the dynamics that can be created in a mix. This book provides an in-depth exploration into the aesthetics of what makes a great mix. Gibson's unique approach explains how to map sounds to visuals in order to create a visual framework that can be used to analyze what is going on in any mix. Once you have the framework down, Gibson then uses it to explain the traditions that have be developed over time by great recording engineers for different styles of music and songs. You will come to understand everything that can be done in a mix to create dynamics that affect people in really deep ways. Once you understand what engineers are doing to create the great mixes they do, you can then use this framework to develop your own values as to what you feel is a good mix. Once you have a perspective on what all can be done, you have the power to be truly creative on your own – to create whole new mixing possibilities. It is all about creating art out of technology. This book goes beyond explaining what the equipment does – it explains what to do with the equipment to make the best possible mixes.",
      "doi": "https://doi.org/10.4324/9781351252225",
      "openalex_id": "https://openalex.org/W4240826525",
      "arxiv_id": "",
      "publication_date": "2019-01-10",
      "published": "2019-01-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
      "summary": "We propose a novel deep learning training criterion, named permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from the multi-class regression technique and the deep clustering (DPCL) technique, our novel approach minimizes the separation error directly. This strategy effectively solves the long-lasting label permutation problem, that has prevented progress on deep learning based techniques for speech separation. We evaluated PIT on the WSJ0 and Danish mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), and DPCL and generalizes well over unseen speakers and languages. Since PIT is simple to implement and can be easily integrated and combined with other advanced techniques, we believe improvements built upon PIT can eventually solve the cocktail-party problem.",
      "abstract": "We propose a novel deep learning training criterion, named permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from the multi-class regression technique and the deep clustering (DPCL) technique, our novel approach minimizes the separation error directly. This strategy effectively solves the long-lasting label permutation problem, that has prevented progress on deep learning based techniques for speech separation. We evaluated PIT on the WSJ0 and Danish mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), and DPCL and generalizes well over unseen speakers and languages. Since PIT is simple to implement and can be easily integrated and combined with other advanced techniques, we believe improvements built upon PIT can eventually solve the cocktail-party problem.",
      "doi": "https://doi.org/10.1109/icassp.2017.7952154",
      "openalex_id": "https://openalex.org/W2460742184",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "summary": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
      "abstract": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-demo.49",
      "openalex_id": "https://openalex.org/W4389519587",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Scalable Diffusion Models with Transformers",
      "summary": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "doi": "https://doi.org/10.1109/iccv51070.2023.00387",
      "openalex_id": "https://openalex.org/W4390872297",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Matcha-TTS: A Fast TTS Architecture with Conditional Flow Matching",
      "summary": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.",
      "abstract": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448291",
      "openalex_id": "https://openalex.org/W4392931276",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vggsound: A Large-Scale Audio-Visual Dataset",
      "summary": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.",
      "abstract": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053174",
      "openalex_id": "https://openalex.org/W3015371781",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Can Audio Captions Be Evaluated With Image Caption Metrics?",
      "summary": "Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746427",
      "openalex_id": "https://openalex.org/W3205860970",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  }
]